{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-02T12:24:11.783131Z",
     "start_time": "2025-12-02T12:24:05.078132Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# big_smiles 폴더 안의 polyBERT_len85_로 시작하는 모든 csv 경로 가져오기\n",
    "paths = sorted(glob.glob(\"big_smiles/polyBERT_len85_*.csv\"))\n",
    "\n",
    "# 각 파일을 읽어서 리스트로 만든 다음\n",
    "dfs = [pd.read_csv(p) for p in paths]\n",
    "\n",
    "# 하나의 DataFrame으로 합치기\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(len(paths), \"files loaded\")\n",
    "print(df.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 files loaded\n",
      "(4926212, 2)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T12:27:00.229763Z",
     "start_time": "2025-12-02T12:27:00.160679Z"
    }
   },
   "cell_type": "code",
   "source": "big_smiles=df['0'].tolist()",
   "id": "e37956448a4252b6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-02T12:30:17.093745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# bigsmiles_chemberta_mlm.py\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# 1) BigSMILES 리스트 (실제에선 외부에서 읽어오면 됨)\n",
    "# big_smiles = [...]\n",
    "\n",
    "\n",
    "# 2) HF Dataset으로 감싸기\n",
    "dataset = Dataset.from_dict({\"text\": big_smiles})\n",
    "\n",
    "# 3) ChemBERTa 토크나이저 & 모델 불러오기\n",
    "base_model_name = \"DeepChem/ChemBERTa-77M-MTR\"  # 예시, HF에 있는 ChemBERTa 중 하나\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# (선택) BigSMILES 특수 기호 추가\n",
    "#  - ChemBERTa vocab에 없을 가능성이 높은 것들: \"{\", \"}\", \"$\", \"[*]\" 등\n",
    "new_tokens = []\n",
    "for tok in [\"{\", \"}\", \"$\", \"[*]\"]:\n",
    "    if tok not in tokenizer.vocab:\n",
    "        new_tokens.append(tok)\n",
    "\n",
    "if new_tokens:\n",
    "    num_added = tokenizer.add_tokens(new_tokens)\n",
    "    print(f\"Added {num_added} new tokens: {new_tokens}\")\n",
    "else:\n",
    "    num_added = 0\n",
    "    print(\"No new tokens added.\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "\n",
    "if num_added > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 4) 토크나이징 함수\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "# 5) MLM용 collator (BERT-style 15% 마스킹 자동 처리)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,\n",
    ")\n",
    "\n",
    "# 6) TrainingArguments & Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bigsmiles-chemberta-mlm\",\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.06,\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "\n",
    "    # ↓↓↓ 여기 2줄 추가\n",
    "    logging_dir=\"runs/bigsmiles_mlm\",   # TensorBoard 로그 저장 폴더\n",
    "    report_to=[\"tensorboard\"],          # wandb 말고 텐서보드로 보고\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 7) 학습\n",
    "trainer.train()\n",
    "\n",
    "# 8) 저장\n",
    "trainer.save_model(\"bigsmiles-mlm\")\n",
    "tokenizer.save_pretrained(\"bigsmiles-mlm\")\n",
    "print(\"Saved to bigsmiles-mlm/\")\n"
   ],
   "id": "59e28ddb69ee40c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DSHS_AI_2\\Documents\\GitHub\\KSEF\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
