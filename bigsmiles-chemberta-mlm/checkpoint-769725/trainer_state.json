{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 769725,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.000649582643151775,
      "grad_norm": 172.71131896972656,
      "learning_rate": 1.071799757491772e-07,
      "loss": 15.263,
      "step": 100
    },
    {
      "epoch": 0.00129916528630355,
      "grad_norm": 173.31895446777344,
      "learning_rate": 2.1544257751602288e-07,
      "loss": 14.8954,
      "step": 200
    },
    {
      "epoch": 0.001948747929455325,
      "grad_norm": 170.36915588378906,
      "learning_rate": 3.2370517928286857e-07,
      "loss": 13.9713,
      "step": 300
    },
    {
      "epoch": 0.0025983305726071,
      "grad_norm": 153.19277954101562,
      "learning_rate": 4.319677810497142e-07,
      "loss": 12.7889,
      "step": 400
    },
    {
      "epoch": 0.003247913215758875,
      "grad_norm": 147.2138214111328,
      "learning_rate": 5.402303828165599e-07,
      "loss": 11.367,
      "step": 500
    },
    {
      "epoch": 0.00389749585891065,
      "grad_norm": 115.25909423828125,
      "learning_rate": 6.484929845834056e-07,
      "loss": 9.7776,
      "step": 600
    },
    {
      "epoch": 0.004547078502062425,
      "grad_norm": 93.92813110351562,
      "learning_rate": 7.567555863502512e-07,
      "loss": 8.256,
      "step": 700
    },
    {
      "epoch": 0.0051966611452142,
      "grad_norm": 69.08202362060547,
      "learning_rate": 8.650181881170968e-07,
      "loss": 7.166,
      "step": 800
    },
    {
      "epoch": 0.005846243788365975,
      "grad_norm": 42.0914306640625,
      "learning_rate": 9.732807898839426e-07,
      "loss": 6.1719,
      "step": 900
    },
    {
      "epoch": 0.00649582643151775,
      "grad_norm": 43.866355895996094,
      "learning_rate": 1.0815433916507882e-06,
      "loss": 5.5959,
      "step": 1000
    },
    {
      "epoch": 0.007145409074669525,
      "grad_norm": 34.50088882446289,
      "learning_rate": 1.1898059934176339e-06,
      "loss": 5.0461,
      "step": 1100
    },
    {
      "epoch": 0.0077949917178213,
      "grad_norm": 27.507734298706055,
      "learning_rate": 1.2980685951844796e-06,
      "loss": 4.6208,
      "step": 1200
    },
    {
      "epoch": 0.008444574360973074,
      "grad_norm": 24.40224838256836,
      "learning_rate": 1.4063311969513252e-06,
      "loss": 4.3783,
      "step": 1300
    },
    {
      "epoch": 0.00909415700412485,
      "grad_norm": 24.38991355895996,
      "learning_rate": 1.5145937987181709e-06,
      "loss": 4.1139,
      "step": 1400
    },
    {
      "epoch": 0.009743739647276624,
      "grad_norm": 17.373062133789062,
      "learning_rate": 1.6228564004850168e-06,
      "loss": 3.9372,
      "step": 1500
    },
    {
      "epoch": 0.0103933222904284,
      "grad_norm": 20.69586753845215,
      "learning_rate": 1.731119002251862e-06,
      "loss": 3.7518,
      "step": 1600
    },
    {
      "epoch": 0.011042904933580174,
      "grad_norm": 14.81323528289795,
      "learning_rate": 1.8393816040187077e-06,
      "loss": 3.5749,
      "step": 1700
    },
    {
      "epoch": 0.01169248757673195,
      "grad_norm": 16.960979461669922,
      "learning_rate": 1.9476442057855535e-06,
      "loss": 3.4442,
      "step": 1800
    },
    {
      "epoch": 0.012342070219883724,
      "grad_norm": 14.477481842041016,
      "learning_rate": 2.055906807552399e-06,
      "loss": 3.3284,
      "step": 1900
    },
    {
      "epoch": 0.0129916528630355,
      "grad_norm": 12.650497436523438,
      "learning_rate": 2.164169409319245e-06,
      "loss": 3.2085,
      "step": 2000
    },
    {
      "epoch": 0.013641235506187274,
      "grad_norm": 11.783854484558105,
      "learning_rate": 2.2724320110860905e-06,
      "loss": 3.1377,
      "step": 2100
    },
    {
      "epoch": 0.01429081814933905,
      "grad_norm": 10.864336013793945,
      "learning_rate": 2.380694612852936e-06,
      "loss": 3.016,
      "step": 2200
    },
    {
      "epoch": 0.014940400792490824,
      "grad_norm": 12.657109260559082,
      "learning_rate": 2.488957214619782e-06,
      "loss": 2.9552,
      "step": 2300
    },
    {
      "epoch": 0.0155899834356426,
      "grad_norm": 10.008211135864258,
      "learning_rate": 2.5972198163866275e-06,
      "loss": 2.8667,
      "step": 2400
    },
    {
      "epoch": 0.016239566078794374,
      "grad_norm": 9.766833305358887,
      "learning_rate": 2.705482418153473e-06,
      "loss": 2.8314,
      "step": 2500
    },
    {
      "epoch": 0.01688914872194615,
      "grad_norm": 11.0057954788208,
      "learning_rate": 2.813745019920319e-06,
      "loss": 2.7618,
      "step": 2600
    },
    {
      "epoch": 0.017538731365097926,
      "grad_norm": 7.132268905639648,
      "learning_rate": 2.9220076216871645e-06,
      "loss": 2.7184,
      "step": 2700
    },
    {
      "epoch": 0.0181883140082497,
      "grad_norm": 8.97634506225586,
      "learning_rate": 3.03027022345401e-06,
      "loss": 2.6508,
      "step": 2800
    },
    {
      "epoch": 0.018837896651401474,
      "grad_norm": 7.306450843811035,
      "learning_rate": 3.138532825220856e-06,
      "loss": 2.5837,
      "step": 2900
    },
    {
      "epoch": 0.01948747929455325,
      "grad_norm": 5.6641387939453125,
      "learning_rate": 3.246795426987702e-06,
      "loss": 2.5638,
      "step": 3000
    },
    {
      "epoch": 0.020137061937705026,
      "grad_norm": 5.830150604248047,
      "learning_rate": 3.355058028754547e-06,
      "loss": 2.5322,
      "step": 3100
    },
    {
      "epoch": 0.0207866445808568,
      "grad_norm": 8.776111602783203,
      "learning_rate": 3.4633206305213933e-06,
      "loss": 2.5006,
      "step": 3200
    },
    {
      "epoch": 0.021436227224008574,
      "grad_norm": 7.055364608764648,
      "learning_rate": 3.5715832322882385e-06,
      "loss": 2.4608,
      "step": 3300
    },
    {
      "epoch": 0.02208580986716035,
      "grad_norm": 6.444230556488037,
      "learning_rate": 3.6798458340550838e-06,
      "loss": 2.4285,
      "step": 3400
    },
    {
      "epoch": 0.022735392510312126,
      "grad_norm": 7.201916694641113,
      "learning_rate": 3.78810843582193e-06,
      "loss": 2.3915,
      "step": 3500
    },
    {
      "epoch": 0.0233849751534639,
      "grad_norm": 6.407629489898682,
      "learning_rate": 3.8963710375887755e-06,
      "loss": 2.3745,
      "step": 3600
    },
    {
      "epoch": 0.024034557796615674,
      "grad_norm": 5.730886936187744,
      "learning_rate": 4.004633639355621e-06,
      "loss": 2.3255,
      "step": 3700
    },
    {
      "epoch": 0.02468414043976745,
      "grad_norm": 5.918024063110352,
      "learning_rate": 4.112896241122467e-06,
      "loss": 2.304,
      "step": 3800
    },
    {
      "epoch": 0.025333723082919226,
      "grad_norm": 6.260011196136475,
      "learning_rate": 4.2211588428893125e-06,
      "loss": 2.2976,
      "step": 3900
    },
    {
      "epoch": 0.025983305726071,
      "grad_norm": 5.461220741271973,
      "learning_rate": 4.329421444656158e-06,
      "loss": 2.2492,
      "step": 4000
    },
    {
      "epoch": 0.026632888369222774,
      "grad_norm": 5.796297073364258,
      "learning_rate": 4.437684046423004e-06,
      "loss": 2.2243,
      "step": 4100
    },
    {
      "epoch": 0.02728247101237455,
      "grad_norm": 5.3563971519470215,
      "learning_rate": 4.5459466481898495e-06,
      "loss": 2.1893,
      "step": 4200
    },
    {
      "epoch": 0.027932053655526326,
      "grad_norm": 5.473658561706543,
      "learning_rate": 4.654209249956695e-06,
      "loss": 2.1714,
      "step": 4300
    },
    {
      "epoch": 0.0285816362986781,
      "grad_norm": 5.68095588684082,
      "learning_rate": 4.762471851723541e-06,
      "loss": 2.1292,
      "step": 4400
    },
    {
      "epoch": 0.029231218941829874,
      "grad_norm": 7.618390083312988,
      "learning_rate": 4.8707344534903865e-06,
      "loss": 2.1232,
      "step": 4500
    },
    {
      "epoch": 0.02988080158498165,
      "grad_norm": 5.425239562988281,
      "learning_rate": 4.978997055257232e-06,
      "loss": 2.1013,
      "step": 4600
    },
    {
      "epoch": 0.030530384228133423,
      "grad_norm": 6.200144290924072,
      "learning_rate": 5.087259657024078e-06,
      "loss": 2.0834,
      "step": 4700
    },
    {
      "epoch": 0.0311799668712852,
      "grad_norm": 5.118000030517578,
      "learning_rate": 5.1955222587909235e-06,
      "loss": 2.0594,
      "step": 4800
    },
    {
      "epoch": 0.03182954951443697,
      "grad_norm": 6.5697221755981445,
      "learning_rate": 5.303784860557769e-06,
      "loss": 2.0364,
      "step": 4900
    },
    {
      "epoch": 0.03247913215758875,
      "grad_norm": 4.749838352203369,
      "learning_rate": 5.412047462324615e-06,
      "loss": 2.011,
      "step": 5000
    },
    {
      "epoch": 0.033128714800740526,
      "grad_norm": 6.962647438049316,
      "learning_rate": 5.5203100640914605e-06,
      "loss": 2.0193,
      "step": 5100
    },
    {
      "epoch": 0.0337782974438923,
      "grad_norm": 6.076015472412109,
      "learning_rate": 5.628572665858306e-06,
      "loss": 1.9904,
      "step": 5200
    },
    {
      "epoch": 0.034427880087044074,
      "grad_norm": 8.429304122924805,
      "learning_rate": 5.736835267625152e-06,
      "loss": 1.9688,
      "step": 5300
    },
    {
      "epoch": 0.03507746273019585,
      "grad_norm": 4.1984782218933105,
      "learning_rate": 5.8450978693919975e-06,
      "loss": 1.9373,
      "step": 5400
    },
    {
      "epoch": 0.03572704537334762,
      "grad_norm": 4.9552178382873535,
      "learning_rate": 5.953360471158843e-06,
      "loss": 1.9155,
      "step": 5500
    },
    {
      "epoch": 0.0363766280164994,
      "grad_norm": 5.681390762329102,
      "learning_rate": 6.061623072925689e-06,
      "loss": 1.905,
      "step": 5600
    },
    {
      "epoch": 0.03702621065965117,
      "grad_norm": 5.420708179473877,
      "learning_rate": 6.1698856746925345e-06,
      "loss": 1.8867,
      "step": 5700
    },
    {
      "epoch": 0.03767579330280295,
      "grad_norm": 5.136974811553955,
      "learning_rate": 6.278148276459381e-06,
      "loss": 1.8689,
      "step": 5800
    },
    {
      "epoch": 0.038325375945954726,
      "grad_norm": 6.42716646194458,
      "learning_rate": 6.386410878226226e-06,
      "loss": 1.8525,
      "step": 5900
    },
    {
      "epoch": 0.0389749585891065,
      "grad_norm": 5.711775779724121,
      "learning_rate": 6.4946734799930715e-06,
      "loss": 1.8369,
      "step": 6000
    },
    {
      "epoch": 0.039624541232258274,
      "grad_norm": 4.921201229095459,
      "learning_rate": 6.602936081759917e-06,
      "loss": 1.7985,
      "step": 6100
    },
    {
      "epoch": 0.04027412387541005,
      "grad_norm": 4.40421199798584,
      "learning_rate": 6.711198683526764e-06,
      "loss": 1.7825,
      "step": 6200
    },
    {
      "epoch": 0.04092370651856182,
      "grad_norm": 4.944761276245117,
      "learning_rate": 6.8194612852936085e-06,
      "loss": 1.7799,
      "step": 6300
    },
    {
      "epoch": 0.0415732891617136,
      "grad_norm": 4.371004104614258,
      "learning_rate": 6.927723887060454e-06,
      "loss": 1.7527,
      "step": 6400
    },
    {
      "epoch": 0.04222287180486537,
      "grad_norm": 5.985897064208984,
      "learning_rate": 7.0359864888273e-06,
      "loss": 1.7406,
      "step": 6500
    },
    {
      "epoch": 0.04287245444801715,
      "grad_norm": 4.649040222167969,
      "learning_rate": 7.144249090594146e-06,
      "loss": 1.7307,
      "step": 6600
    },
    {
      "epoch": 0.043522037091168926,
      "grad_norm": 4.313827991485596,
      "learning_rate": 7.252511692360991e-06,
      "loss": 1.6973,
      "step": 6700
    },
    {
      "epoch": 0.0441716197343207,
      "grad_norm": 8.309679985046387,
      "learning_rate": 7.360774294127837e-06,
      "loss": 1.6891,
      "step": 6800
    },
    {
      "epoch": 0.044821202377472474,
      "grad_norm": 7.527894020080566,
      "learning_rate": 7.4690368958946825e-06,
      "loss": 1.6549,
      "step": 6900
    },
    {
      "epoch": 0.04547078502062425,
      "grad_norm": 5.465060234069824,
      "learning_rate": 7.577299497661529e-06,
      "loss": 1.6433,
      "step": 7000
    },
    {
      "epoch": 0.04612036766377602,
      "grad_norm": 5.312313079833984,
      "learning_rate": 7.685562099428374e-06,
      "loss": 1.6321,
      "step": 7100
    },
    {
      "epoch": 0.0467699503069278,
      "grad_norm": 5.610072135925293,
      "learning_rate": 7.79382470119522e-06,
      "loss": 1.6027,
      "step": 7200
    },
    {
      "epoch": 0.04741953295007957,
      "grad_norm": 4.672554969787598,
      "learning_rate": 7.902087302962065e-06,
      "loss": 1.5931,
      "step": 7300
    },
    {
      "epoch": 0.04806911559323135,
      "grad_norm": 5.295709133148193,
      "learning_rate": 8.01034990472891e-06,
      "loss": 1.5808,
      "step": 7400
    },
    {
      "epoch": 0.048718698236383126,
      "grad_norm": 4.021748065948486,
      "learning_rate": 8.118612506495756e-06,
      "loss": 1.5534,
      "step": 7500
    },
    {
      "epoch": 0.0493682808795349,
      "grad_norm": 7.223932266235352,
      "learning_rate": 8.226875108262602e-06,
      "loss": 1.5444,
      "step": 7600
    },
    {
      "epoch": 0.050017863522686674,
      "grad_norm": 5.022888660430908,
      "learning_rate": 8.335137710029448e-06,
      "loss": 1.5261,
      "step": 7700
    },
    {
      "epoch": 0.05066744616583845,
      "grad_norm": 4.54396390914917,
      "learning_rate": 8.443400311796293e-06,
      "loss": 1.5184,
      "step": 7800
    },
    {
      "epoch": 0.05131702880899022,
      "grad_norm": 4.549941062927246,
      "learning_rate": 8.551662913563139e-06,
      "loss": 1.5015,
      "step": 7900
    },
    {
      "epoch": 0.051966611452142,
      "grad_norm": 6.7722883224487305,
      "learning_rate": 8.659925515329985e-06,
      "loss": 1.4816,
      "step": 8000
    },
    {
      "epoch": 0.05261619409529377,
      "grad_norm": 5.562516689300537,
      "learning_rate": 8.76818811709683e-06,
      "loss": 1.4515,
      "step": 8100
    },
    {
      "epoch": 0.05326577673844555,
      "grad_norm": 4.906988143920898,
      "learning_rate": 8.876450718863676e-06,
      "loss": 1.4453,
      "step": 8200
    },
    {
      "epoch": 0.053915359381597326,
      "grad_norm": 5.11532735824585,
      "learning_rate": 8.984713320630522e-06,
      "loss": 1.4218,
      "step": 8300
    },
    {
      "epoch": 0.0545649420247491,
      "grad_norm": 4.589938163757324,
      "learning_rate": 9.092975922397367e-06,
      "loss": 1.434,
      "step": 8400
    },
    {
      "epoch": 0.055214524667900874,
      "grad_norm": 5.19254207611084,
      "learning_rate": 9.201238524164213e-06,
      "loss": 1.4095,
      "step": 8500
    },
    {
      "epoch": 0.05586410731105265,
      "grad_norm": 5.855080604553223,
      "learning_rate": 9.309501125931059e-06,
      "loss": 1.3821,
      "step": 8600
    },
    {
      "epoch": 0.05651368995420442,
      "grad_norm": 5.484579563140869,
      "learning_rate": 9.417763727697904e-06,
      "loss": 1.372,
      "step": 8700
    },
    {
      "epoch": 0.0571632725973562,
      "grad_norm": 7.39951229095459,
      "learning_rate": 9.52602632946475e-06,
      "loss": 1.3587,
      "step": 8800
    },
    {
      "epoch": 0.05781285524050797,
      "grad_norm": 6.047770977020264,
      "learning_rate": 9.634288931231596e-06,
      "loss": 1.333,
      "step": 8900
    },
    {
      "epoch": 0.05846243788365975,
      "grad_norm": 5.605748653411865,
      "learning_rate": 9.742551532998441e-06,
      "loss": 1.319,
      "step": 9000
    },
    {
      "epoch": 0.059112020526811526,
      "grad_norm": 6.330765247344971,
      "learning_rate": 9.850814134765287e-06,
      "loss": 1.3122,
      "step": 9100
    },
    {
      "epoch": 0.0597616031699633,
      "grad_norm": 5.341206073760986,
      "learning_rate": 9.959076736532133e-06,
      "loss": 1.3004,
      "step": 9200
    },
    {
      "epoch": 0.060411185813115074,
      "grad_norm": 4.410128116607666,
      "learning_rate": 1.0067339338298978e-05,
      "loss": 1.2932,
      "step": 9300
    },
    {
      "epoch": 0.061060768456266845,
      "grad_norm": 5.646936893463135,
      "learning_rate": 1.0175601940065824e-05,
      "loss": 1.2754,
      "step": 9400
    },
    {
      "epoch": 0.06171035109941862,
      "grad_norm": 5.336750507354736,
      "learning_rate": 1.028386454183267e-05,
      "loss": 1.2649,
      "step": 9500
    },
    {
      "epoch": 0.0623599337425704,
      "grad_norm": 4.6288652420043945,
      "learning_rate": 1.0392127143599515e-05,
      "loss": 1.2402,
      "step": 9600
    },
    {
      "epoch": 0.06300951638572218,
      "grad_norm": 6.317248821258545,
      "learning_rate": 1.0500389745366361e-05,
      "loss": 1.2466,
      "step": 9700
    },
    {
      "epoch": 0.06365909902887394,
      "grad_norm": 6.323509216308594,
      "learning_rate": 1.0608652347133207e-05,
      "loss": 1.2275,
      "step": 9800
    },
    {
      "epoch": 0.06430868167202572,
      "grad_norm": 5.420101642608643,
      "learning_rate": 1.0716914948900052e-05,
      "loss": 1.2195,
      "step": 9900
    },
    {
      "epoch": 0.0649582643151775,
      "grad_norm": 4.5441789627075195,
      "learning_rate": 1.0825177550666898e-05,
      "loss": 1.1882,
      "step": 10000
    },
    {
      "epoch": 0.06560784695832927,
      "grad_norm": 5.2444562911987305,
      "learning_rate": 1.0933440152433744e-05,
      "loss": 1.1807,
      "step": 10100
    },
    {
      "epoch": 0.06625742960148105,
      "grad_norm": 4.92851448059082,
      "learning_rate": 1.104170275420059e-05,
      "loss": 1.1638,
      "step": 10200
    },
    {
      "epoch": 0.06690701224463283,
      "grad_norm": 4.292231559753418,
      "learning_rate": 1.1149965355967435e-05,
      "loss": 1.1412,
      "step": 10300
    },
    {
      "epoch": 0.0675565948877846,
      "grad_norm": 5.305934906005859,
      "learning_rate": 1.125822795773428e-05,
      "loss": 1.1384,
      "step": 10400
    },
    {
      "epoch": 0.06820617753093637,
      "grad_norm": 5.572756290435791,
      "learning_rate": 1.1366490559501126e-05,
      "loss": 1.1263,
      "step": 10500
    },
    {
      "epoch": 0.06885576017408815,
      "grad_norm": 3.9331204891204834,
      "learning_rate": 1.1474753161267972e-05,
      "loss": 1.1105,
      "step": 10600
    },
    {
      "epoch": 0.06950534281723993,
      "grad_norm": 3.955376625061035,
      "learning_rate": 1.1583015763034818e-05,
      "loss": 1.1066,
      "step": 10700
    },
    {
      "epoch": 0.0701549254603917,
      "grad_norm": 4.4300312995910645,
      "learning_rate": 1.1691278364801663e-05,
      "loss": 1.1008,
      "step": 10800
    },
    {
      "epoch": 0.07080450810354347,
      "grad_norm": 5.863423824310303,
      "learning_rate": 1.1799540966568509e-05,
      "loss": 1.0792,
      "step": 10900
    },
    {
      "epoch": 0.07145409074669525,
      "grad_norm": 4.338170051574707,
      "learning_rate": 1.1907803568335355e-05,
      "loss": 1.0721,
      "step": 11000
    },
    {
      "epoch": 0.07210367338984702,
      "grad_norm": 7.1229939460754395,
      "learning_rate": 1.20160661701022e-05,
      "loss": 1.0701,
      "step": 11100
    },
    {
      "epoch": 0.0727532560329988,
      "grad_norm": 6.104506969451904,
      "learning_rate": 1.2124328771869046e-05,
      "loss": 1.0458,
      "step": 11200
    },
    {
      "epoch": 0.07340283867615058,
      "grad_norm": 4.4986491203308105,
      "learning_rate": 1.2232591373635892e-05,
      "loss": 1.0588,
      "step": 11300
    },
    {
      "epoch": 0.07405242131930234,
      "grad_norm": 4.934604167938232,
      "learning_rate": 1.2340853975402737e-05,
      "loss": 1.0322,
      "step": 11400
    },
    {
      "epoch": 0.07470200396245412,
      "grad_norm": 5.4794464111328125,
      "learning_rate": 1.2449116577169583e-05,
      "loss": 1.0315,
      "step": 11500
    },
    {
      "epoch": 0.0753515866056059,
      "grad_norm": 4.2947163581848145,
      "learning_rate": 1.2557379178936429e-05,
      "loss": 1.0144,
      "step": 11600
    },
    {
      "epoch": 0.07600116924875767,
      "grad_norm": 4.855937957763672,
      "learning_rate": 1.2665641780703274e-05,
      "loss": 1.0096,
      "step": 11700
    },
    {
      "epoch": 0.07665075189190945,
      "grad_norm": 5.199321269989014,
      "learning_rate": 1.277390438247012e-05,
      "loss": 0.9982,
      "step": 11800
    },
    {
      "epoch": 0.07730033453506123,
      "grad_norm": 4.421681880950928,
      "learning_rate": 1.2882166984236966e-05,
      "loss": 0.9707,
      "step": 11900
    },
    {
      "epoch": 0.077949917178213,
      "grad_norm": 4.96693754196167,
      "learning_rate": 1.2990429586003811e-05,
      "loss": 0.9827,
      "step": 12000
    },
    {
      "epoch": 0.07859949982136477,
      "grad_norm": 6.382270812988281,
      "learning_rate": 1.3098692187770655e-05,
      "loss": 0.9894,
      "step": 12100
    },
    {
      "epoch": 0.07924908246451655,
      "grad_norm": 5.600364685058594,
      "learning_rate": 1.3206954789537504e-05,
      "loss": 0.9713,
      "step": 12200
    },
    {
      "epoch": 0.07989866510766833,
      "grad_norm": 4.325362682342529,
      "learning_rate": 1.331521739130435e-05,
      "loss": 0.9654,
      "step": 12300
    },
    {
      "epoch": 0.0805482477508201,
      "grad_norm": 5.132561683654785,
      "learning_rate": 1.3423479993071194e-05,
      "loss": 0.9529,
      "step": 12400
    },
    {
      "epoch": 0.08119783039397187,
      "grad_norm": 4.735001087188721,
      "learning_rate": 1.353174259483804e-05,
      "loss": 0.9536,
      "step": 12500
    },
    {
      "epoch": 0.08184741303712365,
      "grad_norm": 5.146622180938721,
      "learning_rate": 1.3640005196604885e-05,
      "loss": 0.9327,
      "step": 12600
    },
    {
      "epoch": 0.08249699568027542,
      "grad_norm": 4.654733657836914,
      "learning_rate": 1.3748267798371731e-05,
      "loss": 0.9232,
      "step": 12700
    },
    {
      "epoch": 0.0831465783234272,
      "grad_norm": 5.4267964363098145,
      "learning_rate": 1.3856530400138577e-05,
      "loss": 0.9144,
      "step": 12800
    },
    {
      "epoch": 0.08379616096657898,
      "grad_norm": 3.851811170578003,
      "learning_rate": 1.396479300190542e-05,
      "loss": 0.9133,
      "step": 12900
    },
    {
      "epoch": 0.08444574360973074,
      "grad_norm": 4.997294902801514,
      "learning_rate": 1.407305560367227e-05,
      "loss": 0.9199,
      "step": 13000
    },
    {
      "epoch": 0.08509532625288252,
      "grad_norm": 5.5552659034729,
      "learning_rate": 1.4181318205439115e-05,
      "loss": 0.8925,
      "step": 13100
    },
    {
      "epoch": 0.0857449088960343,
      "grad_norm": 6.695860385894775,
      "learning_rate": 1.428958080720596e-05,
      "loss": 0.8858,
      "step": 13200
    },
    {
      "epoch": 0.08639449153918607,
      "grad_norm": 4.536307334899902,
      "learning_rate": 1.4397843408972805e-05,
      "loss": 0.8761,
      "step": 13300
    },
    {
      "epoch": 0.08704407418233785,
      "grad_norm": 4.734796047210693,
      "learning_rate": 1.450610601073965e-05,
      "loss": 0.8942,
      "step": 13400
    },
    {
      "epoch": 0.08769365682548963,
      "grad_norm": 3.7827553749084473,
      "learning_rate": 1.4614368612506496e-05,
      "loss": 0.8914,
      "step": 13500
    },
    {
      "epoch": 0.0883432394686414,
      "grad_norm": 4.6235551834106445,
      "learning_rate": 1.4722631214273342e-05,
      "loss": 0.8701,
      "step": 13600
    },
    {
      "epoch": 0.08899282211179317,
      "grad_norm": 4.591507911682129,
      "learning_rate": 1.4830893816040186e-05,
      "loss": 0.8693,
      "step": 13700
    },
    {
      "epoch": 0.08964240475494495,
      "grad_norm": 4.487903118133545,
      "learning_rate": 1.4939156417807035e-05,
      "loss": 0.8609,
      "step": 13800
    },
    {
      "epoch": 0.09029198739809673,
      "grad_norm": 4.581269264221191,
      "learning_rate": 1.504741901957388e-05,
      "loss": 0.8488,
      "step": 13900
    },
    {
      "epoch": 0.0909415700412485,
      "grad_norm": 3.950481414794922,
      "learning_rate": 1.5155681621340725e-05,
      "loss": 0.8334,
      "step": 14000
    },
    {
      "epoch": 0.09159115268440027,
      "grad_norm": 4.529350280761719,
      "learning_rate": 1.5263944223107572e-05,
      "loss": 0.8448,
      "step": 14100
    },
    {
      "epoch": 0.09224073532755205,
      "grad_norm": 4.506885528564453,
      "learning_rate": 1.5372206824874418e-05,
      "loss": 0.8469,
      "step": 14200
    },
    {
      "epoch": 0.09289031797070382,
      "grad_norm": 4.685573577880859,
      "learning_rate": 1.548046942664126e-05,
      "loss": 0.8313,
      "step": 14300
    },
    {
      "epoch": 0.0935399006138556,
      "grad_norm": 4.249308109283447,
      "learning_rate": 1.5588732028408106e-05,
      "loss": 0.8161,
      "step": 14400
    },
    {
      "epoch": 0.09418948325700738,
      "grad_norm": 4.597025394439697,
      "learning_rate": 1.569699463017495e-05,
      "loss": 0.826,
      "step": 14500
    },
    {
      "epoch": 0.09483906590015914,
      "grad_norm": 4.7338995933532715,
      "learning_rate": 1.5805257231941797e-05,
      "loss": 0.8264,
      "step": 14600
    },
    {
      "epoch": 0.09548864854331092,
      "grad_norm": 4.963253021240234,
      "learning_rate": 1.5913519833708646e-05,
      "loss": 0.807,
      "step": 14700
    },
    {
      "epoch": 0.0961382311864627,
      "grad_norm": 3.98785400390625,
      "learning_rate": 1.602178243547549e-05,
      "loss": 0.8067,
      "step": 14800
    },
    {
      "epoch": 0.09678781382961447,
      "grad_norm": 4.595481872558594,
      "learning_rate": 1.6130045037242337e-05,
      "loss": 0.807,
      "step": 14900
    },
    {
      "epoch": 0.09743739647276625,
      "grad_norm": 4.542028427124023,
      "learning_rate": 1.6238307639009183e-05,
      "loss": 0.8,
      "step": 15000
    },
    {
      "epoch": 0.09808697911591802,
      "grad_norm": 3.4456942081451416,
      "learning_rate": 1.6346570240776025e-05,
      "loss": 0.7876,
      "step": 15100
    },
    {
      "epoch": 0.0987365617590698,
      "grad_norm": 3.7862138748168945,
      "learning_rate": 1.645483284254287e-05,
      "loss": 0.794,
      "step": 15200
    },
    {
      "epoch": 0.09938614440222157,
      "grad_norm": 4.823272705078125,
      "learning_rate": 1.6563095444309717e-05,
      "loss": 0.7863,
      "step": 15300
    },
    {
      "epoch": 0.10003572704537335,
      "grad_norm": 5.0574517250061035,
      "learning_rate": 1.6671358046076562e-05,
      "loss": 0.7793,
      "step": 15400
    },
    {
      "epoch": 0.10068530968852513,
      "grad_norm": 4.517956733703613,
      "learning_rate": 1.677962064784341e-05,
      "loss": 0.7714,
      "step": 15500
    },
    {
      "epoch": 0.1013348923316769,
      "grad_norm": 3.951514482498169,
      "learning_rate": 1.6887883249610257e-05,
      "loss": 0.7594,
      "step": 15600
    },
    {
      "epoch": 0.10198447497482867,
      "grad_norm": 4.964385986328125,
      "learning_rate": 1.6996145851377103e-05,
      "loss": 0.7797,
      "step": 15700
    },
    {
      "epoch": 0.10263405761798045,
      "grad_norm": 4.8115973472595215,
      "learning_rate": 1.710440845314395e-05,
      "loss": 0.7719,
      "step": 15800
    },
    {
      "epoch": 0.10328364026113222,
      "grad_norm": 5.358951091766357,
      "learning_rate": 1.721267105491079e-05,
      "loss": 0.7565,
      "step": 15900
    },
    {
      "epoch": 0.103933222904284,
      "grad_norm": 4.953998565673828,
      "learning_rate": 1.7320933656677636e-05,
      "loss": 0.7519,
      "step": 16000
    },
    {
      "epoch": 0.10458280554743578,
      "grad_norm": 4.8489155769348145,
      "learning_rate": 1.7429196258444482e-05,
      "loss": 0.7514,
      "step": 16100
    },
    {
      "epoch": 0.10523238819058754,
      "grad_norm": 5.372807502746582,
      "learning_rate": 1.7537458860211328e-05,
      "loss": 0.7557,
      "step": 16200
    },
    {
      "epoch": 0.10588197083373932,
      "grad_norm": 4.3486809730529785,
      "learning_rate": 1.7645721461978177e-05,
      "loss": 0.745,
      "step": 16300
    },
    {
      "epoch": 0.1065315534768911,
      "grad_norm": 4.352665424346924,
      "learning_rate": 1.7753984063745022e-05,
      "loss": 0.7508,
      "step": 16400
    },
    {
      "epoch": 0.10718113612004287,
      "grad_norm": 3.422872543334961,
      "learning_rate": 1.7862246665511868e-05,
      "loss": 0.7386,
      "step": 16500
    },
    {
      "epoch": 0.10783071876319465,
      "grad_norm": 3.9475083351135254,
      "learning_rate": 1.7970509267278714e-05,
      "loss": 0.7307,
      "step": 16600
    },
    {
      "epoch": 0.10848030140634642,
      "grad_norm": 4.486069679260254,
      "learning_rate": 1.807877186904556e-05,
      "loss": 0.7243,
      "step": 16700
    },
    {
      "epoch": 0.1091298840494982,
      "grad_norm": 5.100476264953613,
      "learning_rate": 1.81870344708124e-05,
      "loss": 0.7171,
      "step": 16800
    },
    {
      "epoch": 0.10977946669264997,
      "grad_norm": 4.2766337394714355,
      "learning_rate": 1.8295297072579247e-05,
      "loss": 0.7121,
      "step": 16900
    },
    {
      "epoch": 0.11042904933580175,
      "grad_norm": 3.8281288146972656,
      "learning_rate": 1.8403559674346093e-05,
      "loss": 0.7199,
      "step": 17000
    },
    {
      "epoch": 0.11107863197895353,
      "grad_norm": 4.07171106338501,
      "learning_rate": 1.8511822276112942e-05,
      "loss": 0.7015,
      "step": 17100
    },
    {
      "epoch": 0.1117282146221053,
      "grad_norm": 4.33717155456543,
      "learning_rate": 1.8620084877879788e-05,
      "loss": 0.7239,
      "step": 17200
    },
    {
      "epoch": 0.11237779726525707,
      "grad_norm": 4.508883476257324,
      "learning_rate": 1.8728347479646633e-05,
      "loss": 0.7239,
      "step": 17300
    },
    {
      "epoch": 0.11302737990840885,
      "grad_norm": 4.624152660369873,
      "learning_rate": 1.883661008141348e-05,
      "loss": 0.7095,
      "step": 17400
    },
    {
      "epoch": 0.11367696255156062,
      "grad_norm": 3.6539671421051025,
      "learning_rate": 1.8944872683180325e-05,
      "loss": 0.6996,
      "step": 17500
    },
    {
      "epoch": 0.1143265451947124,
      "grad_norm": 4.297671794891357,
      "learning_rate": 1.9053135284947167e-05,
      "loss": 0.7032,
      "step": 17600
    },
    {
      "epoch": 0.11497612783786418,
      "grad_norm": 4.443413257598877,
      "learning_rate": 1.9161397886714013e-05,
      "loss": 0.7168,
      "step": 17700
    },
    {
      "epoch": 0.11562571048101594,
      "grad_norm": 3.4791908264160156,
      "learning_rate": 1.9269660488480858e-05,
      "loss": 0.6899,
      "step": 17800
    },
    {
      "epoch": 0.11627529312416772,
      "grad_norm": 4.334533214569092,
      "learning_rate": 1.9377923090247707e-05,
      "loss": 0.6859,
      "step": 17900
    },
    {
      "epoch": 0.1169248757673195,
      "grad_norm": 3.9669198989868164,
      "learning_rate": 1.9486185692014553e-05,
      "loss": 0.6749,
      "step": 18000
    },
    {
      "epoch": 0.11757445841047127,
      "grad_norm": 4.106194019317627,
      "learning_rate": 1.95944482937814e-05,
      "loss": 0.6998,
      "step": 18100
    },
    {
      "epoch": 0.11822404105362305,
      "grad_norm": 4.642569541931152,
      "learning_rate": 1.9702710895548244e-05,
      "loss": 0.6832,
      "step": 18200
    },
    {
      "epoch": 0.11887362369677482,
      "grad_norm": 4.455293655395508,
      "learning_rate": 1.981097349731509e-05,
      "loss": 0.6679,
      "step": 18300
    },
    {
      "epoch": 0.1195232063399266,
      "grad_norm": 3.444640874862671,
      "learning_rate": 1.9919236099081932e-05,
      "loss": 0.6589,
      "step": 18400
    },
    {
      "epoch": 0.12017278898307837,
      "grad_norm": 3.6423356533050537,
      "learning_rate": 2.0027498700848778e-05,
      "loss": 0.6761,
      "step": 18500
    },
    {
      "epoch": 0.12082237162623015,
      "grad_norm": 3.502171516418457,
      "learning_rate": 2.0135761302615624e-05,
      "loss": 0.6754,
      "step": 18600
    },
    {
      "epoch": 0.12147195426938193,
      "grad_norm": 3.2944915294647217,
      "learning_rate": 2.024402390438247e-05,
      "loss": 0.6763,
      "step": 18700
    },
    {
      "epoch": 0.12212153691253369,
      "grad_norm": 3.2826244831085205,
      "learning_rate": 2.0352286506149318e-05,
      "loss": 0.6728,
      "step": 18800
    },
    {
      "epoch": 0.12277111955568547,
      "grad_norm": 4.32794189453125,
      "learning_rate": 2.0460549107916164e-05,
      "loss": 0.665,
      "step": 18900
    },
    {
      "epoch": 0.12342070219883725,
      "grad_norm": 3.781487464904785,
      "learning_rate": 2.056881170968301e-05,
      "loss": 0.6602,
      "step": 19000
    },
    {
      "epoch": 0.12407028484198902,
      "grad_norm": 4.132390975952148,
      "learning_rate": 2.0677074311449855e-05,
      "loss": 0.6472,
      "step": 19100
    },
    {
      "epoch": 0.1247198674851408,
      "grad_norm": 4.020686626434326,
      "learning_rate": 2.0785336913216698e-05,
      "loss": 0.6489,
      "step": 19200
    },
    {
      "epoch": 0.12536945012829256,
      "grad_norm": 3.7717835903167725,
      "learning_rate": 2.0893599514983543e-05,
      "loss": 0.6535,
      "step": 19300
    },
    {
      "epoch": 0.12601903277144436,
      "grad_norm": 4.378877639770508,
      "learning_rate": 2.100186211675039e-05,
      "loss": 0.6419,
      "step": 19400
    },
    {
      "epoch": 0.12666861541459612,
      "grad_norm": 4.3816447257995605,
      "learning_rate": 2.1110124718517235e-05,
      "loss": 0.6455,
      "step": 19500
    },
    {
      "epoch": 0.12731819805774788,
      "grad_norm": 3.3033783435821533,
      "learning_rate": 2.1218387320284084e-05,
      "loss": 0.6271,
      "step": 19600
    },
    {
      "epoch": 0.12796778070089967,
      "grad_norm": 3.620870351791382,
      "learning_rate": 2.132664992205093e-05,
      "loss": 0.632,
      "step": 19700
    },
    {
      "epoch": 0.12861736334405144,
      "grad_norm": 4.23325252532959,
      "learning_rate": 2.1434912523817775e-05,
      "loss": 0.6312,
      "step": 19800
    },
    {
      "epoch": 0.12926694598720323,
      "grad_norm": 3.8640379905700684,
      "learning_rate": 2.154317512558462e-05,
      "loss": 0.6368,
      "step": 19900
    },
    {
      "epoch": 0.129916528630355,
      "grad_norm": 3.9886250495910645,
      "learning_rate": 2.1651437727351466e-05,
      "loss": 0.6338,
      "step": 20000
    },
    {
      "epoch": 0.13056611127350679,
      "grad_norm": 4.7044758796691895,
      "learning_rate": 2.175970032911831e-05,
      "loss": 0.6357,
      "step": 20100
    },
    {
      "epoch": 0.13121569391665855,
      "grad_norm": 3.702359914779663,
      "learning_rate": 2.1867962930885154e-05,
      "loss": 0.6361,
      "step": 20200
    },
    {
      "epoch": 0.1318652765598103,
      "grad_norm": 3.461238384246826,
      "learning_rate": 2.1976225532652e-05,
      "loss": 0.604,
      "step": 20300
    },
    {
      "epoch": 0.1325148592029621,
      "grad_norm": 3.5618653297424316,
      "learning_rate": 2.208448813441885e-05,
      "loss": 0.6274,
      "step": 20400
    },
    {
      "epoch": 0.13316444184611387,
      "grad_norm": 3.6844489574432373,
      "learning_rate": 2.2192750736185695e-05,
      "loss": 0.6272,
      "step": 20500
    },
    {
      "epoch": 0.13381402448926566,
      "grad_norm": 3.0322017669677734,
      "learning_rate": 2.230101333795254e-05,
      "loss": 0.6315,
      "step": 20600
    },
    {
      "epoch": 0.13446360713241742,
      "grad_norm": 4.088907718658447,
      "learning_rate": 2.2409275939719386e-05,
      "loss": 0.618,
      "step": 20700
    },
    {
      "epoch": 0.1351131897755692,
      "grad_norm": 3.887104034423828,
      "learning_rate": 2.251753854148623e-05,
      "loss": 0.62,
      "step": 20800
    },
    {
      "epoch": 0.13576277241872098,
      "grad_norm": 3.8575706481933594,
      "learning_rate": 2.2625801143253074e-05,
      "loss": 0.6079,
      "step": 20900
    },
    {
      "epoch": 0.13641235506187274,
      "grad_norm": 3.0116026401519775,
      "learning_rate": 2.273406374501992e-05,
      "loss": 0.6217,
      "step": 21000
    },
    {
      "epoch": 0.13706193770502453,
      "grad_norm": 5.835114002227783,
      "learning_rate": 2.2842326346786765e-05,
      "loss": 0.6086,
      "step": 21100
    },
    {
      "epoch": 0.1377115203481763,
      "grad_norm": 3.7108922004699707,
      "learning_rate": 2.2950588948553614e-05,
      "loss": 0.6027,
      "step": 21200
    },
    {
      "epoch": 0.13836110299132806,
      "grad_norm": 2.960242986679077,
      "learning_rate": 2.305885155032046e-05,
      "loss": 0.6036,
      "step": 21300
    },
    {
      "epoch": 0.13901068563447985,
      "grad_norm": 4.286569595336914,
      "learning_rate": 2.3167114152087306e-05,
      "loss": 0.5952,
      "step": 21400
    },
    {
      "epoch": 0.13966026827763162,
      "grad_norm": 3.3981611728668213,
      "learning_rate": 2.327537675385415e-05,
      "loss": 0.588,
      "step": 21500
    },
    {
      "epoch": 0.1403098509207834,
      "grad_norm": 2.854743480682373,
      "learning_rate": 2.3383639355620997e-05,
      "loss": 0.6087,
      "step": 21600
    },
    {
      "epoch": 0.14095943356393517,
      "grad_norm": 3.146981954574585,
      "learning_rate": 2.349190195738784e-05,
      "loss": 0.6046,
      "step": 21700
    },
    {
      "epoch": 0.14160901620708694,
      "grad_norm": 3.51212739944458,
      "learning_rate": 2.3600164559154685e-05,
      "loss": 0.6026,
      "step": 21800
    },
    {
      "epoch": 0.14225859885023873,
      "grad_norm": 3.4499588012695312,
      "learning_rate": 2.370842716092153e-05,
      "loss": 0.586,
      "step": 21900
    },
    {
      "epoch": 0.1429081814933905,
      "grad_norm": 3.457366943359375,
      "learning_rate": 2.3816689762688376e-05,
      "loss": 0.5964,
      "step": 22000
    },
    {
      "epoch": 0.14355776413654228,
      "grad_norm": 3.9720370769500732,
      "learning_rate": 2.3924952364455225e-05,
      "loss": 0.5901,
      "step": 22100
    },
    {
      "epoch": 0.14420734677969405,
      "grad_norm": 4.437506198883057,
      "learning_rate": 2.403321496622207e-05,
      "loss": 0.5901,
      "step": 22200
    },
    {
      "epoch": 0.1448569294228458,
      "grad_norm": 2.7368967533111572,
      "learning_rate": 2.4141477567988917e-05,
      "loss": 0.5914,
      "step": 22300
    },
    {
      "epoch": 0.1455065120659976,
      "grad_norm": 4.703571319580078,
      "learning_rate": 2.4249740169755762e-05,
      "loss": 0.5774,
      "step": 22400
    },
    {
      "epoch": 0.14615609470914936,
      "grad_norm": 3.5789408683776855,
      "learning_rate": 2.4358002771522604e-05,
      "loss": 0.5787,
      "step": 22500
    },
    {
      "epoch": 0.14680567735230116,
      "grad_norm": 3.660405397415161,
      "learning_rate": 2.446626537328945e-05,
      "loss": 0.5714,
      "step": 22600
    },
    {
      "epoch": 0.14745525999545292,
      "grad_norm": 3.3852522373199463,
      "learning_rate": 2.4574527975056296e-05,
      "loss": 0.5789,
      "step": 22700
    },
    {
      "epoch": 0.14810484263860468,
      "grad_norm": 4.098393440246582,
      "learning_rate": 2.468279057682314e-05,
      "loss": 0.5677,
      "step": 22800
    },
    {
      "epoch": 0.14875442528175647,
      "grad_norm": 5.063717365264893,
      "learning_rate": 2.479105317858999e-05,
      "loss": 0.561,
      "step": 22900
    },
    {
      "epoch": 0.14940400792490824,
      "grad_norm": 4.3534321784973145,
      "learning_rate": 2.4899315780356836e-05,
      "loss": 0.5677,
      "step": 23000
    },
    {
      "epoch": 0.15005359056806003,
      "grad_norm": 3.389719247817993,
      "learning_rate": 2.5007578382123682e-05,
      "loss": 0.5784,
      "step": 23100
    },
    {
      "epoch": 0.1507031732112118,
      "grad_norm": 2.779405355453491,
      "learning_rate": 2.5115840983890528e-05,
      "loss": 0.555,
      "step": 23200
    },
    {
      "epoch": 0.15135275585436356,
      "grad_norm": 4.583988189697266,
      "learning_rate": 2.5224103585657373e-05,
      "loss": 0.563,
      "step": 23300
    },
    {
      "epoch": 0.15200233849751535,
      "grad_norm": 3.1721503734588623,
      "learning_rate": 2.5332366187424215e-05,
      "loss": 0.5585,
      "step": 23400
    },
    {
      "epoch": 0.1526519211406671,
      "grad_norm": 3.2774581909179688,
      "learning_rate": 2.544062878919106e-05,
      "loss": 0.5535,
      "step": 23500
    },
    {
      "epoch": 0.1533015037838189,
      "grad_norm": 3.5603227615356445,
      "learning_rate": 2.5548891390957907e-05,
      "loss": 0.5552,
      "step": 23600
    },
    {
      "epoch": 0.15395108642697067,
      "grad_norm": 3.4625813961029053,
      "learning_rate": 2.5657153992724752e-05,
      "loss": 0.5547,
      "step": 23700
    },
    {
      "epoch": 0.15460066907012246,
      "grad_norm": 3.4536783695220947,
      "learning_rate": 2.5765416594491598e-05,
      "loss": 0.5572,
      "step": 23800
    },
    {
      "epoch": 0.15525025171327422,
      "grad_norm": 4.410678386688232,
      "learning_rate": 2.5873679196258444e-05,
      "loss": 0.5464,
      "step": 23900
    },
    {
      "epoch": 0.155899834356426,
      "grad_norm": 4.016534805297852,
      "learning_rate": 2.598194179802529e-05,
      "loss": 0.5462,
      "step": 24000
    },
    {
      "epoch": 0.15654941699957778,
      "grad_norm": 3.8175954818725586,
      "learning_rate": 2.609020439979214e-05,
      "loss": 0.5484,
      "step": 24100
    },
    {
      "epoch": 0.15719899964272954,
      "grad_norm": 3.643770933151245,
      "learning_rate": 2.6198467001558984e-05,
      "loss": 0.55,
      "step": 24200
    },
    {
      "epoch": 0.15784858228588133,
      "grad_norm": 3.0853850841522217,
      "learning_rate": 2.630672960332583e-05,
      "loss": 0.5483,
      "step": 24300
    },
    {
      "epoch": 0.1584981649290331,
      "grad_norm": 3.6952037811279297,
      "learning_rate": 2.6414992205092675e-05,
      "loss": 0.5455,
      "step": 24400
    },
    {
      "epoch": 0.15914774757218486,
      "grad_norm": 3.370398759841919,
      "learning_rate": 2.652325480685952e-05,
      "loss": 0.5413,
      "step": 24500
    },
    {
      "epoch": 0.15979733021533665,
      "grad_norm": 3.6768412590026855,
      "learning_rate": 2.6631517408626367e-05,
      "loss": 0.5421,
      "step": 24600
    },
    {
      "epoch": 0.16044691285848842,
      "grad_norm": 3.3168411254882812,
      "learning_rate": 2.6739780010393212e-05,
      "loss": 0.5386,
      "step": 24700
    },
    {
      "epoch": 0.1610964955016402,
      "grad_norm": 4.437736511230469,
      "learning_rate": 2.6848042612160058e-05,
      "loss": 0.5397,
      "step": 24800
    },
    {
      "epoch": 0.16174607814479197,
      "grad_norm": 3.2372329235076904,
      "learning_rate": 2.6956305213926904e-05,
      "loss": 0.542,
      "step": 24900
    },
    {
      "epoch": 0.16239566078794374,
      "grad_norm": 2.7634963989257812,
      "learning_rate": 2.7064567815693746e-05,
      "loss": 0.5354,
      "step": 25000
    },
    {
      "epoch": 0.16304524343109553,
      "grad_norm": 3.160076379776001,
      "learning_rate": 2.7172830417460592e-05,
      "loss": 0.5358,
      "step": 25100
    },
    {
      "epoch": 0.1636948260742473,
      "grad_norm": 3.284109592437744,
      "learning_rate": 2.7281093019227437e-05,
      "loss": 0.528,
      "step": 25200
    },
    {
      "epoch": 0.16434440871739908,
      "grad_norm": 3.7328412532806396,
      "learning_rate": 2.7389355620994283e-05,
      "loss": 0.5211,
      "step": 25300
    },
    {
      "epoch": 0.16499399136055085,
      "grad_norm": 3.6397299766540527,
      "learning_rate": 2.749761822276113e-05,
      "loss": 0.5293,
      "step": 25400
    },
    {
      "epoch": 0.1656435740037026,
      "grad_norm": 3.126466989517212,
      "learning_rate": 2.7605880824527974e-05,
      "loss": 0.5343,
      "step": 25500
    },
    {
      "epoch": 0.1662931566468544,
      "grad_norm": 3.162813663482666,
      "learning_rate": 2.771414342629482e-05,
      "loss": 0.5278,
      "step": 25600
    },
    {
      "epoch": 0.16694273929000616,
      "grad_norm": 3.2512588500976562,
      "learning_rate": 2.7822406028061666e-05,
      "loss": 0.5196,
      "step": 25700
    },
    {
      "epoch": 0.16759232193315796,
      "grad_norm": 3.085801601409912,
      "learning_rate": 2.7930668629828515e-05,
      "loss": 0.5205,
      "step": 25800
    },
    {
      "epoch": 0.16824190457630972,
      "grad_norm": 3.3248610496520996,
      "learning_rate": 2.803893123159536e-05,
      "loss": 0.5083,
      "step": 25900
    },
    {
      "epoch": 0.16889148721946148,
      "grad_norm": 3.935189962387085,
      "learning_rate": 2.8147193833362206e-05,
      "loss": 0.5165,
      "step": 26000
    },
    {
      "epoch": 0.16954106986261328,
      "grad_norm": 3.0264790058135986,
      "learning_rate": 2.8255456435129052e-05,
      "loss": 0.5148,
      "step": 26100
    },
    {
      "epoch": 0.17019065250576504,
      "grad_norm": 3.585324287414551,
      "learning_rate": 2.8363719036895897e-05,
      "loss": 0.5201,
      "step": 26200
    },
    {
      "epoch": 0.17084023514891683,
      "grad_norm": 2.964690923690796,
      "learning_rate": 2.8471981638662743e-05,
      "loss": 0.5152,
      "step": 26300
    },
    {
      "epoch": 0.1714898177920686,
      "grad_norm": 3.324002504348755,
      "learning_rate": 2.858024424042959e-05,
      "loss": 0.5085,
      "step": 26400
    },
    {
      "epoch": 0.17213940043522036,
      "grad_norm": 3.1238009929656982,
      "learning_rate": 2.8688506842196434e-05,
      "loss": 0.5129,
      "step": 26500
    },
    {
      "epoch": 0.17278898307837215,
      "grad_norm": 3.6403188705444336,
      "learning_rate": 2.879676944396328e-05,
      "loss": 0.5028,
      "step": 26600
    },
    {
      "epoch": 0.1734385657215239,
      "grad_norm": 3.230618953704834,
      "learning_rate": 2.8905032045730122e-05,
      "loss": 0.4948,
      "step": 26700
    },
    {
      "epoch": 0.1740881483646757,
      "grad_norm": 3.2725112438201904,
      "learning_rate": 2.9013294647496968e-05,
      "loss": 0.5081,
      "step": 26800
    },
    {
      "epoch": 0.17473773100782747,
      "grad_norm": 3.2218728065490723,
      "learning_rate": 2.9121557249263814e-05,
      "loss": 0.502,
      "step": 26900
    },
    {
      "epoch": 0.17538731365097926,
      "grad_norm": 2.9652161598205566,
      "learning_rate": 2.922981985103066e-05,
      "loss": 0.4936,
      "step": 27000
    },
    {
      "epoch": 0.17603689629413102,
      "grad_norm": 3.531059503555298,
      "learning_rate": 2.9338082452797505e-05,
      "loss": 0.4951,
      "step": 27100
    },
    {
      "epoch": 0.1766864789372828,
      "grad_norm": 3.3397507667541504,
      "learning_rate": 2.944634505456435e-05,
      "loss": 0.5005,
      "step": 27200
    },
    {
      "epoch": 0.17733606158043458,
      "grad_norm": 3.6294941902160645,
      "learning_rate": 2.9554607656331196e-05,
      "loss": 0.5057,
      "step": 27300
    },
    {
      "epoch": 0.17798564422358634,
      "grad_norm": 4.487335681915283,
      "learning_rate": 2.9662870258098045e-05,
      "loss": 0.5028,
      "step": 27400
    },
    {
      "epoch": 0.17863522686673813,
      "grad_norm": 3.524876117706299,
      "learning_rate": 2.977113285986489e-05,
      "loss": 0.484,
      "step": 27500
    },
    {
      "epoch": 0.1792848095098899,
      "grad_norm": 3.4981861114501953,
      "learning_rate": 2.9879395461631737e-05,
      "loss": 0.4861,
      "step": 27600
    },
    {
      "epoch": 0.17993439215304166,
      "grad_norm": 3.3790600299835205,
      "learning_rate": 2.9987658063398582e-05,
      "loss": 0.4965,
      "step": 27700
    },
    {
      "epoch": 0.18058397479619345,
      "grad_norm": 3.274496555328369,
      "learning_rate": 3.0095920665165428e-05,
      "loss": 0.4828,
      "step": 27800
    },
    {
      "epoch": 0.18123355743934522,
      "grad_norm": 2.5395171642303467,
      "learning_rate": 3.0204183266932274e-05,
      "loss": 0.5018,
      "step": 27900
    },
    {
      "epoch": 0.181883140082497,
      "grad_norm": 3.398024797439575,
      "learning_rate": 3.031244586869912e-05,
      "loss": 0.4819,
      "step": 28000
    },
    {
      "epoch": 0.18253272272564877,
      "grad_norm": 3.7546701431274414,
      "learning_rate": 3.0420708470465965e-05,
      "loss": 0.4793,
      "step": 28100
    },
    {
      "epoch": 0.18318230536880054,
      "grad_norm": 3.4931135177612305,
      "learning_rate": 3.052897107223281e-05,
      "loss": 0.4746,
      "step": 28200
    },
    {
      "epoch": 0.18383188801195233,
      "grad_norm": 2.9732651710510254,
      "learning_rate": 3.0637233673999656e-05,
      "loss": 0.4975,
      "step": 28300
    },
    {
      "epoch": 0.1844814706551041,
      "grad_norm": 2.9664814472198486,
      "learning_rate": 3.07454962757665e-05,
      "loss": 0.4929,
      "step": 28400
    },
    {
      "epoch": 0.18513105329825588,
      "grad_norm": 3.3226935863494873,
      "learning_rate": 3.085375887753335e-05,
      "loss": 0.4762,
      "step": 28500
    },
    {
      "epoch": 0.18578063594140765,
      "grad_norm": 3.6432089805603027,
      "learning_rate": 3.096202147930019e-05,
      "loss": 0.484,
      "step": 28600
    },
    {
      "epoch": 0.1864302185845594,
      "grad_norm": 2.7851274013519287,
      "learning_rate": 3.107028408106704e-05,
      "loss": 0.4835,
      "step": 28700
    },
    {
      "epoch": 0.1870798012277112,
      "grad_norm": 3.3265116214752197,
      "learning_rate": 3.117854668283388e-05,
      "loss": 0.4708,
      "step": 28800
    },
    {
      "epoch": 0.18772938387086296,
      "grad_norm": 3.02359676361084,
      "learning_rate": 3.1286809284600724e-05,
      "loss": 0.4729,
      "step": 28900
    },
    {
      "epoch": 0.18837896651401476,
      "grad_norm": 3.543100118637085,
      "learning_rate": 3.139507188636758e-05,
      "loss": 0.4875,
      "step": 29000
    },
    {
      "epoch": 0.18902854915716652,
      "grad_norm": 3.2671356201171875,
      "learning_rate": 3.150333448813442e-05,
      "loss": 0.475,
      "step": 29100
    },
    {
      "epoch": 0.18967813180031828,
      "grad_norm": 2.9748141765594482,
      "learning_rate": 3.161159708990127e-05,
      "loss": 0.4752,
      "step": 29200
    },
    {
      "epoch": 0.19032771444347008,
      "grad_norm": 3.5865683555603027,
      "learning_rate": 3.171985969166811e-05,
      "loss": 0.4797,
      "step": 29300
    },
    {
      "epoch": 0.19097729708662184,
      "grad_norm": 3.013449192047119,
      "learning_rate": 3.182812229343496e-05,
      "loss": 0.4785,
      "step": 29400
    },
    {
      "epoch": 0.19162687972977363,
      "grad_norm": 3.8835508823394775,
      "learning_rate": 3.1936384895201804e-05,
      "loss": 0.4747,
      "step": 29500
    },
    {
      "epoch": 0.1922764623729254,
      "grad_norm": 3.2688190937042236,
      "learning_rate": 3.204464749696865e-05,
      "loss": 0.4813,
      "step": 29600
    },
    {
      "epoch": 0.19292604501607716,
      "grad_norm": 2.93947172164917,
      "learning_rate": 3.2152910098735496e-05,
      "loss": 0.4735,
      "step": 29700
    },
    {
      "epoch": 0.19357562765922895,
      "grad_norm": 3.2782411575317383,
      "learning_rate": 3.226117270050234e-05,
      "loss": 0.4635,
      "step": 29800
    },
    {
      "epoch": 0.1942252103023807,
      "grad_norm": 3.7063682079315186,
      "learning_rate": 3.236943530226919e-05,
      "loss": 0.4663,
      "step": 29900
    },
    {
      "epoch": 0.1948747929455325,
      "grad_norm": 2.833982467651367,
      "learning_rate": 3.247769790403603e-05,
      "loss": 0.4667,
      "step": 30000
    },
    {
      "epoch": 0.19552437558868427,
      "grad_norm": 3.210214614868164,
      "learning_rate": 3.258596050580288e-05,
      "loss": 0.4685,
      "step": 30100
    },
    {
      "epoch": 0.19617395823183603,
      "grad_norm": 3.2406039237976074,
      "learning_rate": 3.269422310756972e-05,
      "loss": 0.4682,
      "step": 30200
    },
    {
      "epoch": 0.19682354087498782,
      "grad_norm": 3.105177879333496,
      "learning_rate": 3.280248570933657e-05,
      "loss": 0.4593,
      "step": 30300
    },
    {
      "epoch": 0.1974731235181396,
      "grad_norm": 3.1350255012512207,
      "learning_rate": 3.291074831110341e-05,
      "loss": 0.4622,
      "step": 30400
    },
    {
      "epoch": 0.19812270616129138,
      "grad_norm": 3.4245362281799316,
      "learning_rate": 3.3019010912870254e-05,
      "loss": 0.4676,
      "step": 30500
    },
    {
      "epoch": 0.19877228880444314,
      "grad_norm": 3.2366490364074707,
      "learning_rate": 3.31272735146371e-05,
      "loss": 0.4602,
      "step": 30600
    },
    {
      "epoch": 0.19942187144759493,
      "grad_norm": 3.874466896057129,
      "learning_rate": 3.323553611640395e-05,
      "loss": 0.4563,
      "step": 30700
    },
    {
      "epoch": 0.2000714540907467,
      "grad_norm": 3.2767069339752197,
      "learning_rate": 3.33437987181708e-05,
      "loss": 0.4478,
      "step": 30800
    },
    {
      "epoch": 0.20072103673389846,
      "grad_norm": 3.402214288711548,
      "learning_rate": 3.3452061319937644e-05,
      "loss": 0.4618,
      "step": 30900
    },
    {
      "epoch": 0.20137061937705025,
      "grad_norm": 3.146848440170288,
      "learning_rate": 3.356032392170449e-05,
      "loss": 0.4714,
      "step": 31000
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 3.0474495887756348,
      "learning_rate": 3.3668586523471335e-05,
      "loss": 0.4536,
      "step": 31100
    },
    {
      "epoch": 0.2026697846633538,
      "grad_norm": 2.9603452682495117,
      "learning_rate": 3.377684912523818e-05,
      "loss": 0.4516,
      "step": 31200
    },
    {
      "epoch": 0.20331936730650557,
      "grad_norm": 2.7701923847198486,
      "learning_rate": 3.3885111727005026e-05,
      "loss": 0.4516,
      "step": 31300
    },
    {
      "epoch": 0.20396894994965734,
      "grad_norm": 3.4369378089904785,
      "learning_rate": 3.399337432877187e-05,
      "loss": 0.464,
      "step": 31400
    },
    {
      "epoch": 0.20461853259280913,
      "grad_norm": 3.179860830307007,
      "learning_rate": 3.410163693053872e-05,
      "loss": 0.4515,
      "step": 31500
    },
    {
      "epoch": 0.2052681152359609,
      "grad_norm": 2.8849475383758545,
      "learning_rate": 3.420989953230556e-05,
      "loss": 0.4599,
      "step": 31600
    },
    {
      "epoch": 0.20591769787911268,
      "grad_norm": 2.9526548385620117,
      "learning_rate": 3.431816213407241e-05,
      "loss": 0.4424,
      "step": 31700
    },
    {
      "epoch": 0.20656728052226445,
      "grad_norm": 3.2358453273773193,
      "learning_rate": 3.442642473583925e-05,
      "loss": 0.4503,
      "step": 31800
    },
    {
      "epoch": 0.2072168631654162,
      "grad_norm": 3.7183549404144287,
      "learning_rate": 3.45346873376061e-05,
      "loss": 0.4475,
      "step": 31900
    },
    {
      "epoch": 0.207866445808568,
      "grad_norm": 2.990206003189087,
      "learning_rate": 3.464294993937294e-05,
      "loss": 0.4526,
      "step": 32000
    },
    {
      "epoch": 0.20851602845171976,
      "grad_norm": 2.794564723968506,
      "learning_rate": 3.4751212541139785e-05,
      "loss": 0.4365,
      "step": 32100
    },
    {
      "epoch": 0.20916561109487156,
      "grad_norm": 2.8559160232543945,
      "learning_rate": 3.4859475142906634e-05,
      "loss": 0.442,
      "step": 32200
    },
    {
      "epoch": 0.20981519373802332,
      "grad_norm": 3.26896595954895,
      "learning_rate": 3.496773774467348e-05,
      "loss": 0.441,
      "step": 32300
    },
    {
      "epoch": 0.21046477638117508,
      "grad_norm": 3.2098283767700195,
      "learning_rate": 3.507600034644033e-05,
      "loss": 0.4389,
      "step": 32400
    },
    {
      "epoch": 0.21111435902432688,
      "grad_norm": 2.971047878265381,
      "learning_rate": 3.5184262948207174e-05,
      "loss": 0.4324,
      "step": 32500
    },
    {
      "epoch": 0.21176394166747864,
      "grad_norm": 2.7629406452178955,
      "learning_rate": 3.5292525549974023e-05,
      "loss": 0.4273,
      "step": 32600
    },
    {
      "epoch": 0.21241352431063043,
      "grad_norm": 2.8081088066101074,
      "learning_rate": 3.5400788151740866e-05,
      "loss": 0.448,
      "step": 32700
    },
    {
      "epoch": 0.2130631069537822,
      "grad_norm": 4.121133804321289,
      "learning_rate": 3.550905075350771e-05,
      "loss": 0.4359,
      "step": 32800
    },
    {
      "epoch": 0.21371268959693396,
      "grad_norm": 2.48640513420105,
      "learning_rate": 3.561731335527456e-05,
      "loss": 0.4422,
      "step": 32900
    },
    {
      "epoch": 0.21436227224008575,
      "grad_norm": 2.506401538848877,
      "learning_rate": 3.57255759570414e-05,
      "loss": 0.4313,
      "step": 33000
    },
    {
      "epoch": 0.2150118548832375,
      "grad_norm": 2.8086471557617188,
      "learning_rate": 3.583383855880825e-05,
      "loss": 0.4401,
      "step": 33100
    },
    {
      "epoch": 0.2156614375263893,
      "grad_norm": 2.60426664352417,
      "learning_rate": 3.594210116057509e-05,
      "loss": 0.4334,
      "step": 33200
    },
    {
      "epoch": 0.21631102016954107,
      "grad_norm": 3.0974276065826416,
      "learning_rate": 3.605036376234194e-05,
      "loss": 0.4387,
      "step": 33300
    },
    {
      "epoch": 0.21696060281269283,
      "grad_norm": 2.319793224334717,
      "learning_rate": 3.615862636410878e-05,
      "loss": 0.4211,
      "step": 33400
    },
    {
      "epoch": 0.21761018545584462,
      "grad_norm": 3.4599385261535645,
      "learning_rate": 3.626688896587563e-05,
      "loss": 0.4309,
      "step": 33500
    },
    {
      "epoch": 0.2182597680989964,
      "grad_norm": 3.6908915042877197,
      "learning_rate": 3.637515156764247e-05,
      "loss": 0.4415,
      "step": 33600
    },
    {
      "epoch": 0.21890935074214818,
      "grad_norm": 2.583176374435425,
      "learning_rate": 3.648341416940932e-05,
      "loss": 0.4412,
      "step": 33700
    },
    {
      "epoch": 0.21955893338529994,
      "grad_norm": 3.4332358837127686,
      "learning_rate": 3.6591676771176165e-05,
      "loss": 0.4258,
      "step": 33800
    },
    {
      "epoch": 0.2202085160284517,
      "grad_norm": 2.6423935890197754,
      "learning_rate": 3.669993937294301e-05,
      "loss": 0.4306,
      "step": 33900
    },
    {
      "epoch": 0.2208580986716035,
      "grad_norm": 2.3584015369415283,
      "learning_rate": 3.680820197470986e-05,
      "loss": 0.4429,
      "step": 34000
    },
    {
      "epoch": 0.22150768131475526,
      "grad_norm": 2.32464861869812,
      "learning_rate": 3.6916464576476705e-05,
      "loss": 0.4204,
      "step": 34100
    },
    {
      "epoch": 0.22215726395790705,
      "grad_norm": 2.700147867202759,
      "learning_rate": 3.7024727178243554e-05,
      "loss": 0.4308,
      "step": 34200
    },
    {
      "epoch": 0.22280684660105882,
      "grad_norm": 2.5506339073181152,
      "learning_rate": 3.7132989780010396e-05,
      "loss": 0.4154,
      "step": 34300
    },
    {
      "epoch": 0.2234564292442106,
      "grad_norm": 2.646740198135376,
      "learning_rate": 3.724125238177724e-05,
      "loss": 0.423,
      "step": 34400
    },
    {
      "epoch": 0.22410601188736237,
      "grad_norm": 2.5053341388702393,
      "learning_rate": 3.734951498354409e-05,
      "loss": 0.4147,
      "step": 34500
    },
    {
      "epoch": 0.22475559453051414,
      "grad_norm": 2.283155918121338,
      "learning_rate": 3.745777758531093e-05,
      "loss": 0.4174,
      "step": 34600
    },
    {
      "epoch": 0.22540517717366593,
      "grad_norm": 2.653262138366699,
      "learning_rate": 3.756604018707778e-05,
      "loss": 0.4174,
      "step": 34700
    },
    {
      "epoch": 0.2260547598168177,
      "grad_norm": 2.984036922454834,
      "learning_rate": 3.767430278884462e-05,
      "loss": 0.4183,
      "step": 34800
    },
    {
      "epoch": 0.22670434245996948,
      "grad_norm": 3.5972447395324707,
      "learning_rate": 3.778256539061147e-05,
      "loss": 0.42,
      "step": 34900
    },
    {
      "epoch": 0.22735392510312125,
      "grad_norm": 2.5041282176971436,
      "learning_rate": 3.789082799237831e-05,
      "loss": 0.4164,
      "step": 35000
    },
    {
      "epoch": 0.228003507746273,
      "grad_norm": 2.4261298179626465,
      "learning_rate": 3.799909059414516e-05,
      "loss": 0.4235,
      "step": 35100
    },
    {
      "epoch": 0.2286530903894248,
      "grad_norm": 2.7236483097076416,
      "learning_rate": 3.8107353195912004e-05,
      "loss": 0.4148,
      "step": 35200
    },
    {
      "epoch": 0.22930267303257656,
      "grad_norm": 2.5856730937957764,
      "learning_rate": 3.821561579767885e-05,
      "loss": 0.4107,
      "step": 35300
    },
    {
      "epoch": 0.22995225567572836,
      "grad_norm": 2.7574660778045654,
      "learning_rate": 3.8323878399445695e-05,
      "loss": 0.403,
      "step": 35400
    },
    {
      "epoch": 0.23060183831888012,
      "grad_norm": 2.271536350250244,
      "learning_rate": 3.843214100121254e-05,
      "loss": 0.3931,
      "step": 35500
    },
    {
      "epoch": 0.23125142096203188,
      "grad_norm": 2.2032201290130615,
      "learning_rate": 3.854040360297939e-05,
      "loss": 0.4116,
      "step": 35600
    },
    {
      "epoch": 0.23190100360518368,
      "grad_norm": 3.1876583099365234,
      "learning_rate": 3.8648666204746236e-05,
      "loss": 0.4122,
      "step": 35700
    },
    {
      "epoch": 0.23255058624833544,
      "grad_norm": 3.162471055984497,
      "learning_rate": 3.8756928806513085e-05,
      "loss": 0.41,
      "step": 35800
    },
    {
      "epoch": 0.23320016889148723,
      "grad_norm": 2.7811944484710693,
      "learning_rate": 3.886519140827993e-05,
      "loss": 0.4153,
      "step": 35900
    },
    {
      "epoch": 0.233849751534639,
      "grad_norm": 2.68800950050354,
      "learning_rate": 3.8973454010046776e-05,
      "loss": 0.41,
      "step": 36000
    },
    {
      "epoch": 0.23449933417779076,
      "grad_norm": 2.4808075428009033,
      "learning_rate": 3.908171661181362e-05,
      "loss": 0.4084,
      "step": 36100
    },
    {
      "epoch": 0.23514891682094255,
      "grad_norm": 2.314910650253296,
      "learning_rate": 3.918997921358046e-05,
      "loss": 0.4142,
      "step": 36200
    },
    {
      "epoch": 0.2357984994640943,
      "grad_norm": 2.632319211959839,
      "learning_rate": 3.929824181534731e-05,
      "loss": 0.4083,
      "step": 36300
    },
    {
      "epoch": 0.2364480821072461,
      "grad_norm": 3.114312171936035,
      "learning_rate": 3.940650441711415e-05,
      "loss": 0.4091,
      "step": 36400
    },
    {
      "epoch": 0.23709766475039787,
      "grad_norm": 2.8172616958618164,
      "learning_rate": 3.9514767018881e-05,
      "loss": 0.4084,
      "step": 36500
    },
    {
      "epoch": 0.23774724739354963,
      "grad_norm": 2.6603689193725586,
      "learning_rate": 3.962302962064784e-05,
      "loss": 0.3981,
      "step": 36600
    },
    {
      "epoch": 0.23839683003670142,
      "grad_norm": 3.166304588317871,
      "learning_rate": 3.973129222241469e-05,
      "loss": 0.3932,
      "step": 36700
    },
    {
      "epoch": 0.2390464126798532,
      "grad_norm": 2.447661876678467,
      "learning_rate": 3.9839554824181535e-05,
      "loss": 0.4005,
      "step": 36800
    },
    {
      "epoch": 0.23969599532300498,
      "grad_norm": 2.8271186351776123,
      "learning_rate": 3.9947817425948384e-05,
      "loss": 0.4038,
      "step": 36900
    },
    {
      "epoch": 0.24034557796615674,
      "grad_norm": 2.860792875289917,
      "learning_rate": 4.0056080027715226e-05,
      "loss": 0.4103,
      "step": 37000
    },
    {
      "epoch": 0.2409951606093085,
      "grad_norm": 2.8916027545928955,
      "learning_rate": 4.016434262948207e-05,
      "loss": 0.399,
      "step": 37100
    },
    {
      "epoch": 0.2416447432524603,
      "grad_norm": 3.586259603500366,
      "learning_rate": 4.027260523124892e-05,
      "loss": 0.3891,
      "step": 37200
    },
    {
      "epoch": 0.24229432589561206,
      "grad_norm": 2.3333985805511475,
      "learning_rate": 4.0380867833015766e-05,
      "loss": 0.3965,
      "step": 37300
    },
    {
      "epoch": 0.24294390853876385,
      "grad_norm": 2.5953240394592285,
      "learning_rate": 4.0489130434782615e-05,
      "loss": 0.3963,
      "step": 37400
    },
    {
      "epoch": 0.24359349118191562,
      "grad_norm": 2.7698347568511963,
      "learning_rate": 4.059739303654946e-05,
      "loss": 0.3923,
      "step": 37500
    },
    {
      "epoch": 0.24424307382506738,
      "grad_norm": 2.03143048286438,
      "learning_rate": 4.070565563831631e-05,
      "loss": 0.3941,
      "step": 37600
    },
    {
      "epoch": 0.24489265646821917,
      "grad_norm": 2.7826485633850098,
      "learning_rate": 4.081391824008315e-05,
      "loss": 0.3856,
      "step": 37700
    },
    {
      "epoch": 0.24554223911137094,
      "grad_norm": 3.1825554370880127,
      "learning_rate": 4.092218084184999e-05,
      "loss": 0.3991,
      "step": 37800
    },
    {
      "epoch": 0.24619182175452273,
      "grad_norm": 2.2518904209136963,
      "learning_rate": 4.103044344361684e-05,
      "loss": 0.3851,
      "step": 37900
    },
    {
      "epoch": 0.2468414043976745,
      "grad_norm": 2.637112617492676,
      "learning_rate": 4.113870604538368e-05,
      "loss": 0.3976,
      "step": 38000
    },
    {
      "epoch": 0.24749098704082628,
      "grad_norm": 3.167764902114868,
      "learning_rate": 4.124696864715053e-05,
      "loss": 0.3947,
      "step": 38100
    },
    {
      "epoch": 0.24814056968397805,
      "grad_norm": 2.9929332733154297,
      "learning_rate": 4.1355231248917374e-05,
      "loss": 0.3897,
      "step": 38200
    },
    {
      "epoch": 0.2487901523271298,
      "grad_norm": 3.2113707065582275,
      "learning_rate": 4.146349385068422e-05,
      "loss": 0.3968,
      "step": 38300
    },
    {
      "epoch": 0.2494397349702816,
      "grad_norm": 2.6664140224456787,
      "learning_rate": 4.1571756452451065e-05,
      "loss": 0.3927,
      "step": 38400
    },
    {
      "epoch": 0.2500893176134334,
      "grad_norm": 2.226836919784546,
      "learning_rate": 4.1680019054217914e-05,
      "loss": 0.391,
      "step": 38500
    },
    {
      "epoch": 0.25073890025658513,
      "grad_norm": 2.334990978240967,
      "learning_rate": 4.1788281655984757e-05,
      "loss": 0.3845,
      "step": 38600
    },
    {
      "epoch": 0.2513884828997369,
      "grad_norm": 2.454359292984009,
      "learning_rate": 4.18965442577516e-05,
      "loss": 0.3865,
      "step": 38700
    },
    {
      "epoch": 0.2520380655428887,
      "grad_norm": 3.592161178588867,
      "learning_rate": 4.200480685951845e-05,
      "loss": 0.3867,
      "step": 38800
    },
    {
      "epoch": 0.25268764818604045,
      "grad_norm": 2.5219573974609375,
      "learning_rate": 4.21130694612853e-05,
      "loss": 0.3854,
      "step": 38900
    },
    {
      "epoch": 0.25333723082919224,
      "grad_norm": 2.505077838897705,
      "learning_rate": 4.2221332063052146e-05,
      "loss": 0.3911,
      "step": 39000
    },
    {
      "epoch": 0.25398681347234403,
      "grad_norm": 3.024765729904175,
      "learning_rate": 4.232959466481899e-05,
      "loss": 0.383,
      "step": 39100
    },
    {
      "epoch": 0.25463639611549577,
      "grad_norm": 2.8009254932403564,
      "learning_rate": 4.243785726658584e-05,
      "loss": 0.3868,
      "step": 39200
    },
    {
      "epoch": 0.25528597875864756,
      "grad_norm": 1.9894695281982422,
      "learning_rate": 4.254611986835268e-05,
      "loss": 0.3866,
      "step": 39300
    },
    {
      "epoch": 0.25593556140179935,
      "grad_norm": 2.8033502101898193,
      "learning_rate": 4.265438247011952e-05,
      "loss": 0.3758,
      "step": 39400
    },
    {
      "epoch": 0.25658514404495114,
      "grad_norm": 2.8694794178009033,
      "learning_rate": 4.276264507188637e-05,
      "loss": 0.3695,
      "step": 39500
    },
    {
      "epoch": 0.2572347266881029,
      "grad_norm": 2.522812843322754,
      "learning_rate": 4.287090767365321e-05,
      "loss": 0.3839,
      "step": 39600
    },
    {
      "epoch": 0.25788430933125467,
      "grad_norm": 2.846363067626953,
      "learning_rate": 4.297917027542006e-05,
      "loss": 0.3856,
      "step": 39700
    },
    {
      "epoch": 0.25853389197440646,
      "grad_norm": 2.8580784797668457,
      "learning_rate": 4.3087432877186904e-05,
      "loss": 0.3736,
      "step": 39800
    },
    {
      "epoch": 0.2591834746175582,
      "grad_norm": 2.1897530555725098,
      "learning_rate": 4.3195695478953754e-05,
      "loss": 0.368,
      "step": 39900
    },
    {
      "epoch": 0.25983305726071,
      "grad_norm": 2.7640023231506348,
      "learning_rate": 4.3303958080720596e-05,
      "loss": 0.368,
      "step": 40000
    },
    {
      "epoch": 0.2604826399038618,
      "grad_norm": 2.891368865966797,
      "learning_rate": 4.3412220682487445e-05,
      "loss": 0.383,
      "step": 40100
    },
    {
      "epoch": 0.26113222254701357,
      "grad_norm": 2.2955095767974854,
      "learning_rate": 4.352048328425429e-05,
      "loss": 0.3768,
      "step": 40200
    },
    {
      "epoch": 0.2617818051901653,
      "grad_norm": 2.960461378097534,
      "learning_rate": 4.3628745886021136e-05,
      "loss": 0.3685,
      "step": 40300
    },
    {
      "epoch": 0.2624313878333171,
      "grad_norm": 2.8778738975524902,
      "learning_rate": 4.373700848778798e-05,
      "loss": 0.3739,
      "step": 40400
    },
    {
      "epoch": 0.2630809704764689,
      "grad_norm": 3.1748974323272705,
      "learning_rate": 4.384527108955482e-05,
      "loss": 0.3658,
      "step": 40500
    },
    {
      "epoch": 0.2637305531196206,
      "grad_norm": 2.6516644954681396,
      "learning_rate": 4.3953533691321677e-05,
      "loss": 0.3599,
      "step": 40600
    },
    {
      "epoch": 0.2643801357627724,
      "grad_norm": 2.421844720840454,
      "learning_rate": 4.406179629308852e-05,
      "loss": 0.3675,
      "step": 40700
    },
    {
      "epoch": 0.2650297184059242,
      "grad_norm": 2.5863046646118164,
      "learning_rate": 4.417005889485537e-05,
      "loss": 0.3735,
      "step": 40800
    },
    {
      "epoch": 0.26567930104907594,
      "grad_norm": 2.6836342811584473,
      "learning_rate": 4.427832149662221e-05,
      "loss": 0.3554,
      "step": 40900
    },
    {
      "epoch": 0.26632888369222774,
      "grad_norm": 2.3638131618499756,
      "learning_rate": 4.438658409838905e-05,
      "loss": 0.3786,
      "step": 41000
    },
    {
      "epoch": 0.2669784663353795,
      "grad_norm": 2.723102331161499,
      "learning_rate": 4.44948467001559e-05,
      "loss": 0.3636,
      "step": 41100
    },
    {
      "epoch": 0.2676280489785313,
      "grad_norm": 2.8917925357818604,
      "learning_rate": 4.4603109301922744e-05,
      "loss": 0.369,
      "step": 41200
    },
    {
      "epoch": 0.26827763162168305,
      "grad_norm": 3.472559690475464,
      "learning_rate": 4.471137190368959e-05,
      "loss": 0.3723,
      "step": 41300
    },
    {
      "epoch": 0.26892721426483485,
      "grad_norm": 2.7064132690429688,
      "learning_rate": 4.4819634505456435e-05,
      "loss": 0.364,
      "step": 41400
    },
    {
      "epoch": 0.26957679690798664,
      "grad_norm": 2.513477325439453,
      "learning_rate": 4.4927897107223284e-05,
      "loss": 0.3599,
      "step": 41500
    },
    {
      "epoch": 0.2702263795511384,
      "grad_norm": 2.705134630203247,
      "learning_rate": 4.5036159708990126e-05,
      "loss": 0.3773,
      "step": 41600
    },
    {
      "epoch": 0.27087596219429017,
      "grad_norm": 2.8261220455169678,
      "learning_rate": 4.5144422310756976e-05,
      "loss": 0.3682,
      "step": 41700
    },
    {
      "epoch": 0.27152554483744196,
      "grad_norm": 3.0514347553253174,
      "learning_rate": 4.525268491252382e-05,
      "loss": 0.3483,
      "step": 41800
    },
    {
      "epoch": 0.2721751274805937,
      "grad_norm": 3.6889843940734863,
      "learning_rate": 4.536094751429067e-05,
      "loss": 0.3587,
      "step": 41900
    },
    {
      "epoch": 0.2728247101237455,
      "grad_norm": 2.1126720905303955,
      "learning_rate": 4.546921011605751e-05,
      "loss": 0.3678,
      "step": 42000
    },
    {
      "epoch": 0.2734742927668973,
      "grad_norm": 2.6654391288757324,
      "learning_rate": 4.557747271782435e-05,
      "loss": 0.3532,
      "step": 42100
    },
    {
      "epoch": 0.27412387541004907,
      "grad_norm": 3.3128647804260254,
      "learning_rate": 4.568573531959121e-05,
      "loss": 0.364,
      "step": 42200
    },
    {
      "epoch": 0.2747734580532008,
      "grad_norm": 2.3305180072784424,
      "learning_rate": 4.579399792135805e-05,
      "loss": 0.3426,
      "step": 42300
    },
    {
      "epoch": 0.2754230406963526,
      "grad_norm": 2.164527416229248,
      "learning_rate": 4.59022605231249e-05,
      "loss": 0.3528,
      "step": 42400
    },
    {
      "epoch": 0.2760726233395044,
      "grad_norm": 2.2311415672302246,
      "learning_rate": 4.601052312489174e-05,
      "loss": 0.3527,
      "step": 42500
    },
    {
      "epoch": 0.2767222059826561,
      "grad_norm": 2.762068033218384,
      "learning_rate": 4.611878572665859e-05,
      "loss": 0.362,
      "step": 42600
    },
    {
      "epoch": 0.2773717886258079,
      "grad_norm": 2.5072579383850098,
      "learning_rate": 4.622704832842543e-05,
      "loss": 0.3659,
      "step": 42700
    },
    {
      "epoch": 0.2780213712689597,
      "grad_norm": 2.2554781436920166,
      "learning_rate": 4.6335310930192274e-05,
      "loss": 0.36,
      "step": 42800
    },
    {
      "epoch": 0.27867095391211144,
      "grad_norm": 2.312837600708008,
      "learning_rate": 4.6443573531959123e-05,
      "loss": 0.3602,
      "step": 42900
    },
    {
      "epoch": 0.27932053655526323,
      "grad_norm": 2.526580333709717,
      "learning_rate": 4.6551836133725966e-05,
      "loss": 0.3484,
      "step": 43000
    },
    {
      "epoch": 0.279970119198415,
      "grad_norm": 2.0570428371429443,
      "learning_rate": 4.6660098735492815e-05,
      "loss": 0.3564,
      "step": 43100
    },
    {
      "epoch": 0.2806197018415668,
      "grad_norm": 2.4699532985687256,
      "learning_rate": 4.676836133725966e-05,
      "loss": 0.3526,
      "step": 43200
    },
    {
      "epoch": 0.28126928448471855,
      "grad_norm": 2.4180550575256348,
      "learning_rate": 4.6876623939026506e-05,
      "loss": 0.3514,
      "step": 43300
    },
    {
      "epoch": 0.28191886712787034,
      "grad_norm": 2.1389191150665283,
      "learning_rate": 4.698488654079335e-05,
      "loss": 0.3464,
      "step": 43400
    },
    {
      "epoch": 0.28256844977102213,
      "grad_norm": 2.3888580799102783,
      "learning_rate": 4.70931491425602e-05,
      "loss": 0.3557,
      "step": 43500
    },
    {
      "epoch": 0.28321803241417387,
      "grad_norm": 2.0366806983947754,
      "learning_rate": 4.720141174432704e-05,
      "loss": 0.3526,
      "step": 43600
    },
    {
      "epoch": 0.28386761505732566,
      "grad_norm": 2.3610525131225586,
      "learning_rate": 4.730967434609388e-05,
      "loss": 0.3494,
      "step": 43700
    },
    {
      "epoch": 0.28451719770047745,
      "grad_norm": 2.6317594051361084,
      "learning_rate": 4.741793694786073e-05,
      "loss": 0.3525,
      "step": 43800
    },
    {
      "epoch": 0.28516678034362924,
      "grad_norm": 1.9241808652877808,
      "learning_rate": 4.752619954962758e-05,
      "loss": 0.3461,
      "step": 43900
    },
    {
      "epoch": 0.285816362986781,
      "grad_norm": 2.143282175064087,
      "learning_rate": 4.763446215139443e-05,
      "loss": 0.3426,
      "step": 44000
    },
    {
      "epoch": 0.28646594562993277,
      "grad_norm": 2.1282918453216553,
      "learning_rate": 4.774272475316127e-05,
      "loss": 0.3448,
      "step": 44100
    },
    {
      "epoch": 0.28711552827308456,
      "grad_norm": 2.5144577026367188,
      "learning_rate": 4.785098735492812e-05,
      "loss": 0.3443,
      "step": 44200
    },
    {
      "epoch": 0.2877651109162363,
      "grad_norm": 2.2157013416290283,
      "learning_rate": 4.795924995669496e-05,
      "loss": 0.3358,
      "step": 44300
    },
    {
      "epoch": 0.2884146935593881,
      "grad_norm": 1.9314383268356323,
      "learning_rate": 4.8067512558461805e-05,
      "loss": 0.3486,
      "step": 44400
    },
    {
      "epoch": 0.2890642762025399,
      "grad_norm": 2.381544589996338,
      "learning_rate": 4.8175775160228654e-05,
      "loss": 0.3349,
      "step": 44500
    },
    {
      "epoch": 0.2897138588456916,
      "grad_norm": 2.1024856567382812,
      "learning_rate": 4.8284037761995496e-05,
      "loss": 0.3479,
      "step": 44600
    },
    {
      "epoch": 0.2903634414888434,
      "grad_norm": 2.3331243991851807,
      "learning_rate": 4.8392300363762345e-05,
      "loss": 0.3464,
      "step": 44700
    },
    {
      "epoch": 0.2910130241319952,
      "grad_norm": 2.255082368850708,
      "learning_rate": 4.850056296552919e-05,
      "loss": 0.3446,
      "step": 44800
    },
    {
      "epoch": 0.291662606775147,
      "grad_norm": 2.6054909229278564,
      "learning_rate": 4.860882556729604e-05,
      "loss": 0.3498,
      "step": 44900
    },
    {
      "epoch": 0.29231218941829873,
      "grad_norm": 3.121412992477417,
      "learning_rate": 4.871708816906288e-05,
      "loss": 0.3436,
      "step": 45000
    },
    {
      "epoch": 0.2929617720614505,
      "grad_norm": 2.557814598083496,
      "learning_rate": 4.882535077082973e-05,
      "loss": 0.3362,
      "step": 45100
    },
    {
      "epoch": 0.2936113547046023,
      "grad_norm": 2.4309496879577637,
      "learning_rate": 4.893361337259657e-05,
      "loss": 0.3449,
      "step": 45200
    },
    {
      "epoch": 0.29426093734775405,
      "grad_norm": 1.8981510400772095,
      "learning_rate": 4.904187597436341e-05,
      "loss": 0.3365,
      "step": 45300
    },
    {
      "epoch": 0.29491051999090584,
      "grad_norm": 2.4516239166259766,
      "learning_rate": 4.915013857613026e-05,
      "loss": 0.3403,
      "step": 45400
    },
    {
      "epoch": 0.29556010263405763,
      "grad_norm": 1.709852933883667,
      "learning_rate": 4.925840117789711e-05,
      "loss": 0.3402,
      "step": 45500
    },
    {
      "epoch": 0.29620968527720937,
      "grad_norm": 2.81250262260437,
      "learning_rate": 4.936666377966396e-05,
      "loss": 0.3411,
      "step": 45600
    },
    {
      "epoch": 0.29685926792036116,
      "grad_norm": 1.8161734342575073,
      "learning_rate": 4.94749263814308e-05,
      "loss": 0.3392,
      "step": 45700
    },
    {
      "epoch": 0.29750885056351295,
      "grad_norm": 2.298096179962158,
      "learning_rate": 4.958318898319765e-05,
      "loss": 0.3285,
      "step": 45800
    },
    {
      "epoch": 0.29815843320666474,
      "grad_norm": 1.930300235748291,
      "learning_rate": 4.9691451584964493e-05,
      "loss": 0.3418,
      "step": 45900
    },
    {
      "epoch": 0.2988080158498165,
      "grad_norm": 2.6280102729797363,
      "learning_rate": 4.9799714186731336e-05,
      "loss": 0.3269,
      "step": 46000
    },
    {
      "epoch": 0.29945759849296827,
      "grad_norm": 2.5191290378570557,
      "learning_rate": 4.9907976788498185e-05,
      "loss": 0.3315,
      "step": 46100
    },
    {
      "epoch": 0.30010718113612006,
      "grad_norm": 2.201505661010742,
      "learning_rate": 4.99989634312361e-05,
      "loss": 0.3423,
      "step": 46200
    },
    {
      "epoch": 0.3007567637792718,
      "grad_norm": 1.897446870803833,
      "learning_rate": 4.999205297281011e-05,
      "loss": 0.3478,
      "step": 46300
    },
    {
      "epoch": 0.3014063464224236,
      "grad_norm": 1.8218539953231812,
      "learning_rate": 4.998514251438412e-05,
      "loss": 0.3329,
      "step": 46400
    },
    {
      "epoch": 0.3020559290655754,
      "grad_norm": 2.1814684867858887,
      "learning_rate": 4.997823205595813e-05,
      "loss": 0.3281,
      "step": 46500
    },
    {
      "epoch": 0.3027055117087271,
      "grad_norm": 2.411534309387207,
      "learning_rate": 4.997132159753214e-05,
      "loss": 0.3281,
      "step": 46600
    },
    {
      "epoch": 0.3033550943518789,
      "grad_norm": 2.090480327606201,
      "learning_rate": 4.996441113910615e-05,
      "loss": 0.3385,
      "step": 46700
    },
    {
      "epoch": 0.3040046769950307,
      "grad_norm": 2.6212024688720703,
      "learning_rate": 4.995750068068016e-05,
      "loss": 0.3442,
      "step": 46800
    },
    {
      "epoch": 0.3046542596381825,
      "grad_norm": 1.8411500453948975,
      "learning_rate": 4.995059022225417e-05,
      "loss": 0.3302,
      "step": 46900
    },
    {
      "epoch": 0.3053038422813342,
      "grad_norm": 2.673694372177124,
      "learning_rate": 4.994367976382818e-05,
      "loss": 0.3307,
      "step": 47000
    },
    {
      "epoch": 0.305953424924486,
      "grad_norm": 2.7117278575897217,
      "learning_rate": 4.993676930540219e-05,
      "loss": 0.3279,
      "step": 47100
    },
    {
      "epoch": 0.3066030075676378,
      "grad_norm": 2.661158323287964,
      "learning_rate": 4.992985884697619e-05,
      "loss": 0.3212,
      "step": 47200
    },
    {
      "epoch": 0.30725259021078954,
      "grad_norm": 2.1696720123291016,
      "learning_rate": 4.99229483885502e-05,
      "loss": 0.3398,
      "step": 47300
    },
    {
      "epoch": 0.30790217285394134,
      "grad_norm": 2.0155961513519287,
      "learning_rate": 4.991603793012421e-05,
      "loss": 0.3312,
      "step": 47400
    },
    {
      "epoch": 0.3085517554970931,
      "grad_norm": 1.715134859085083,
      "learning_rate": 4.990912747169822e-05,
      "loss": 0.3256,
      "step": 47500
    },
    {
      "epoch": 0.3092013381402449,
      "grad_norm": 2.3571531772613525,
      "learning_rate": 4.990221701327223e-05,
      "loss": 0.3262,
      "step": 47600
    },
    {
      "epoch": 0.30985092078339666,
      "grad_norm": 2.1140189170837402,
      "learning_rate": 4.989530655484624e-05,
      "loss": 0.3317,
      "step": 47700
    },
    {
      "epoch": 0.31050050342654845,
      "grad_norm": 3.090815305709839,
      "learning_rate": 4.9888396096420245e-05,
      "loss": 0.327,
      "step": 47800
    },
    {
      "epoch": 0.31115008606970024,
      "grad_norm": 2.1724002361297607,
      "learning_rate": 4.9881485637994254e-05,
      "loss": 0.3201,
      "step": 47900
    },
    {
      "epoch": 0.311799668712852,
      "grad_norm": 1.9166053533554077,
      "learning_rate": 4.9874575179568264e-05,
      "loss": 0.3085,
      "step": 48000
    },
    {
      "epoch": 0.31244925135600377,
      "grad_norm": 2.1416983604431152,
      "learning_rate": 4.9867664721142274e-05,
      "loss": 0.3154,
      "step": 48100
    },
    {
      "epoch": 0.31309883399915556,
      "grad_norm": 2.4670469760894775,
      "learning_rate": 4.986075426271628e-05,
      "loss": 0.3273,
      "step": 48200
    },
    {
      "epoch": 0.3137484166423073,
      "grad_norm": 2.4588403701782227,
      "learning_rate": 4.985384380429029e-05,
      "loss": 0.329,
      "step": 48300
    },
    {
      "epoch": 0.3143979992854591,
      "grad_norm": 2.5630943775177,
      "learning_rate": 4.98469333458643e-05,
      "loss": 0.3217,
      "step": 48400
    },
    {
      "epoch": 0.3150475819286109,
      "grad_norm": 2.0618484020233154,
      "learning_rate": 4.984002288743831e-05,
      "loss": 0.3205,
      "step": 48500
    },
    {
      "epoch": 0.31569716457176267,
      "grad_norm": 2.0822339057922363,
      "learning_rate": 4.9833112429012317e-05,
      "loss": 0.3162,
      "step": 48600
    },
    {
      "epoch": 0.3163467472149144,
      "grad_norm": 2.2138020992279053,
      "learning_rate": 4.9826201970586326e-05,
      "loss": 0.3125,
      "step": 48700
    },
    {
      "epoch": 0.3169963298580662,
      "grad_norm": 2.4024109840393066,
      "learning_rate": 4.9819291512160336e-05,
      "loss": 0.3233,
      "step": 48800
    },
    {
      "epoch": 0.317645912501218,
      "grad_norm": 1.8561573028564453,
      "learning_rate": 4.9812381053734346e-05,
      "loss": 0.3199,
      "step": 48900
    },
    {
      "epoch": 0.3182954951443697,
      "grad_norm": 2.135206699371338,
      "learning_rate": 4.9805470595308356e-05,
      "loss": 0.3229,
      "step": 49000
    },
    {
      "epoch": 0.3189450777875215,
      "grad_norm": 2.24405574798584,
      "learning_rate": 4.979856013688236e-05,
      "loss": 0.3243,
      "step": 49100
    },
    {
      "epoch": 0.3195946604306733,
      "grad_norm": 2.3325366973876953,
      "learning_rate": 4.979164967845637e-05,
      "loss": 0.3205,
      "step": 49200
    },
    {
      "epoch": 0.32024424307382504,
      "grad_norm": 1.9844006299972534,
      "learning_rate": 4.978473922003038e-05,
      "loss": 0.322,
      "step": 49300
    },
    {
      "epoch": 0.32089382571697683,
      "grad_norm": 1.9792566299438477,
      "learning_rate": 4.977782876160439e-05,
      "loss": 0.3209,
      "step": 49400
    },
    {
      "epoch": 0.3215434083601286,
      "grad_norm": 2.045626401901245,
      "learning_rate": 4.97709183031784e-05,
      "loss": 0.3191,
      "step": 49500
    },
    {
      "epoch": 0.3221929910032804,
      "grad_norm": 2.004307270050049,
      "learning_rate": 4.976400784475241e-05,
      "loss": 0.3195,
      "step": 49600
    },
    {
      "epoch": 0.32284257364643215,
      "grad_norm": 2.070070505142212,
      "learning_rate": 4.975709738632642e-05,
      "loss": 0.3123,
      "step": 49700
    },
    {
      "epoch": 0.32349215628958394,
      "grad_norm": 1.9624093770980835,
      "learning_rate": 4.975018692790043e-05,
      "loss": 0.3186,
      "step": 49800
    },
    {
      "epoch": 0.32414173893273573,
      "grad_norm": 2.3682210445404053,
      "learning_rate": 4.974327646947444e-05,
      "loss": 0.3197,
      "step": 49900
    },
    {
      "epoch": 0.32479132157588747,
      "grad_norm": 2.7724807262420654,
      "learning_rate": 4.973636601104844e-05,
      "loss": 0.319,
      "step": 50000
    },
    {
      "epoch": 0.32544090421903926,
      "grad_norm": 2.3269221782684326,
      "learning_rate": 4.972945555262245e-05,
      "loss": 0.3104,
      "step": 50100
    },
    {
      "epoch": 0.32609048686219105,
      "grad_norm": 1.953114628791809,
      "learning_rate": 4.972254509419646e-05,
      "loss": 0.3174,
      "step": 50200
    },
    {
      "epoch": 0.32674006950534284,
      "grad_norm": 2.2782037258148193,
      "learning_rate": 4.971563463577047e-05,
      "loss": 0.3105,
      "step": 50300
    },
    {
      "epoch": 0.3273896521484946,
      "grad_norm": 1.8516446352005005,
      "learning_rate": 4.970872417734448e-05,
      "loss": 0.307,
      "step": 50400
    },
    {
      "epoch": 0.3280392347916464,
      "grad_norm": 2.1990246772766113,
      "learning_rate": 4.970181371891849e-05,
      "loss": 0.3048,
      "step": 50500
    },
    {
      "epoch": 0.32868881743479816,
      "grad_norm": 2.084530830383301,
      "learning_rate": 4.96949032604925e-05,
      "loss": 0.3252,
      "step": 50600
    },
    {
      "epoch": 0.3293384000779499,
      "grad_norm": 1.949838638305664,
      "learning_rate": 4.968799280206651e-05,
      "loss": 0.3092,
      "step": 50700
    },
    {
      "epoch": 0.3299879827211017,
      "grad_norm": 2.3660929203033447,
      "learning_rate": 4.968108234364052e-05,
      "loss": 0.305,
      "step": 50800
    },
    {
      "epoch": 0.3306375653642535,
      "grad_norm": 2.3350090980529785,
      "learning_rate": 4.967417188521452e-05,
      "loss": 0.3151,
      "step": 50900
    },
    {
      "epoch": 0.3312871480074052,
      "grad_norm": 1.624618411064148,
      "learning_rate": 4.966726142678853e-05,
      "loss": 0.3143,
      "step": 51000
    },
    {
      "epoch": 0.331936730650557,
      "grad_norm": 2.1718623638153076,
      "learning_rate": 4.966035096836254e-05,
      "loss": 0.3217,
      "step": 51100
    },
    {
      "epoch": 0.3325863132937088,
      "grad_norm": 1.7506072521209717,
      "learning_rate": 4.965344050993655e-05,
      "loss": 0.3009,
      "step": 51200
    },
    {
      "epoch": 0.3332358959368606,
      "grad_norm": 2.604714870452881,
      "learning_rate": 4.964653005151056e-05,
      "loss": 0.3094,
      "step": 51300
    },
    {
      "epoch": 0.33388547858001233,
      "grad_norm": 1.718411922454834,
      "learning_rate": 4.9639619593084565e-05,
      "loss": 0.3077,
      "step": 51400
    },
    {
      "epoch": 0.3345350612231641,
      "grad_norm": 1.7813570499420166,
      "learning_rate": 4.9632709134658575e-05,
      "loss": 0.2923,
      "step": 51500
    },
    {
      "epoch": 0.3351846438663159,
      "grad_norm": 1.9615920782089233,
      "learning_rate": 4.9625798676232585e-05,
      "loss": 0.3037,
      "step": 51600
    },
    {
      "epoch": 0.33583422650946765,
      "grad_norm": 2.145988702774048,
      "learning_rate": 4.9618888217806595e-05,
      "loss": 0.3194,
      "step": 51700
    },
    {
      "epoch": 0.33648380915261944,
      "grad_norm": 2.3049662113189697,
      "learning_rate": 4.9611977759380605e-05,
      "loss": 0.3158,
      "step": 51800
    },
    {
      "epoch": 0.33713339179577123,
      "grad_norm": 2.554110050201416,
      "learning_rate": 4.960506730095461e-05,
      "loss": 0.2953,
      "step": 51900
    },
    {
      "epoch": 0.33778297443892297,
      "grad_norm": 2.0730955600738525,
      "learning_rate": 4.959815684252862e-05,
      "loss": 0.2954,
      "step": 52000
    },
    {
      "epoch": 0.33843255708207476,
      "grad_norm": 2.1641077995300293,
      "learning_rate": 4.959124638410263e-05,
      "loss": 0.3013,
      "step": 52100
    },
    {
      "epoch": 0.33908213972522655,
      "grad_norm": 2.50247859954834,
      "learning_rate": 4.958433592567664e-05,
      "loss": 0.3151,
      "step": 52200
    },
    {
      "epoch": 0.33973172236837834,
      "grad_norm": 1.4195960760116577,
      "learning_rate": 4.957742546725065e-05,
      "loss": 0.3016,
      "step": 52300
    },
    {
      "epoch": 0.3403813050115301,
      "grad_norm": 2.497591733932495,
      "learning_rate": 4.957051500882466e-05,
      "loss": 0.3036,
      "step": 52400
    },
    {
      "epoch": 0.34103088765468187,
      "grad_norm": 2.4049320220947266,
      "learning_rate": 4.956360455039867e-05,
      "loss": 0.2998,
      "step": 52500
    },
    {
      "epoch": 0.34168047029783366,
      "grad_norm": 1.8045071363449097,
      "learning_rate": 4.955669409197268e-05,
      "loss": 0.2933,
      "step": 52600
    },
    {
      "epoch": 0.3423300529409854,
      "grad_norm": 2.46000075340271,
      "learning_rate": 4.954978363354669e-05,
      "loss": 0.3052,
      "step": 52700
    },
    {
      "epoch": 0.3429796355841372,
      "grad_norm": 2.1573939323425293,
      "learning_rate": 4.954287317512069e-05,
      "loss": 0.2981,
      "step": 52800
    },
    {
      "epoch": 0.343629218227289,
      "grad_norm": 1.7347701787948608,
      "learning_rate": 4.95359627166947e-05,
      "loss": 0.3052,
      "step": 52900
    },
    {
      "epoch": 0.3442788008704407,
      "grad_norm": 2.052048444747925,
      "learning_rate": 4.952905225826871e-05,
      "loss": 0.298,
      "step": 53000
    },
    {
      "epoch": 0.3449283835135925,
      "grad_norm": 2.533202886581421,
      "learning_rate": 4.952214179984272e-05,
      "loss": 0.3121,
      "step": 53100
    },
    {
      "epoch": 0.3455779661567443,
      "grad_norm": 2.0223777294158936,
      "learning_rate": 4.951523134141673e-05,
      "loss": 0.3049,
      "step": 53200
    },
    {
      "epoch": 0.3462275487998961,
      "grad_norm": 2.027445077896118,
      "learning_rate": 4.950832088299074e-05,
      "loss": 0.3025,
      "step": 53300
    },
    {
      "epoch": 0.3468771314430478,
      "grad_norm": 2.4063920974731445,
      "learning_rate": 4.950141042456475e-05,
      "loss": 0.296,
      "step": 53400
    },
    {
      "epoch": 0.3475267140861996,
      "grad_norm": 2.151881694793701,
      "learning_rate": 4.949449996613876e-05,
      "loss": 0.306,
      "step": 53500
    },
    {
      "epoch": 0.3481762967293514,
      "grad_norm": 1.8523914813995361,
      "learning_rate": 4.948758950771277e-05,
      "loss": 0.3147,
      "step": 53600
    },
    {
      "epoch": 0.34882587937250314,
      "grad_norm": 2.262833595275879,
      "learning_rate": 4.948067904928678e-05,
      "loss": 0.3008,
      "step": 53700
    },
    {
      "epoch": 0.34947546201565494,
      "grad_norm": 1.877787709236145,
      "learning_rate": 4.947376859086078e-05,
      "loss": 0.3076,
      "step": 53800
    },
    {
      "epoch": 0.35012504465880673,
      "grad_norm": 2.4539318084716797,
      "learning_rate": 4.946685813243479e-05,
      "loss": 0.3026,
      "step": 53900
    },
    {
      "epoch": 0.3507746273019585,
      "grad_norm": 2.7829182147979736,
      "learning_rate": 4.94599476740088e-05,
      "loss": 0.2977,
      "step": 54000
    },
    {
      "epoch": 0.35142420994511026,
      "grad_norm": 1.8934452533721924,
      "learning_rate": 4.945303721558281e-05,
      "loss": 0.2891,
      "step": 54100
    },
    {
      "epoch": 0.35207379258826205,
      "grad_norm": 1.6123756170272827,
      "learning_rate": 4.944612675715682e-05,
      "loss": 0.2996,
      "step": 54200
    },
    {
      "epoch": 0.35272337523141384,
      "grad_norm": 2.0540621280670166,
      "learning_rate": 4.943921629873083e-05,
      "loss": 0.2898,
      "step": 54300
    },
    {
      "epoch": 0.3533729578745656,
      "grad_norm": 2.072458267211914,
      "learning_rate": 4.943230584030484e-05,
      "loss": 0.2913,
      "step": 54400
    },
    {
      "epoch": 0.35402254051771737,
      "grad_norm": 2.484304904937744,
      "learning_rate": 4.942539538187885e-05,
      "loss": 0.3052,
      "step": 54500
    },
    {
      "epoch": 0.35467212316086916,
      "grad_norm": 1.6945854425430298,
      "learning_rate": 4.9418484923452854e-05,
      "loss": 0.296,
      "step": 54600
    },
    {
      "epoch": 0.3553217058040209,
      "grad_norm": 1.70429265499115,
      "learning_rate": 4.941157446502686e-05,
      "loss": 0.2975,
      "step": 54700
    },
    {
      "epoch": 0.3559712884471727,
      "grad_norm": 1.7714985609054565,
      "learning_rate": 4.940466400660087e-05,
      "loss": 0.2959,
      "step": 54800
    },
    {
      "epoch": 0.3566208710903245,
      "grad_norm": 1.939931035041809,
      "learning_rate": 4.939775354817488e-05,
      "loss": 0.2993,
      "step": 54900
    },
    {
      "epoch": 0.35727045373347627,
      "grad_norm": 2.0441513061523438,
      "learning_rate": 4.9390843089748886e-05,
      "loss": 0.3008,
      "step": 55000
    },
    {
      "epoch": 0.357920036376628,
      "grad_norm": 2.146442174911499,
      "learning_rate": 4.9383932631322896e-05,
      "loss": 0.297,
      "step": 55100
    },
    {
      "epoch": 0.3585696190197798,
      "grad_norm": 2.2000346183776855,
      "learning_rate": 4.9377022172896906e-05,
      "loss": 0.297,
      "step": 55200
    },
    {
      "epoch": 0.3592192016629316,
      "grad_norm": 2.271589756011963,
      "learning_rate": 4.9370111714470916e-05,
      "loss": 0.2913,
      "step": 55300
    },
    {
      "epoch": 0.3598687843060833,
      "grad_norm": 2.301147699356079,
      "learning_rate": 4.9363201256044926e-05,
      "loss": 0.2883,
      "step": 55400
    },
    {
      "epoch": 0.3605183669492351,
      "grad_norm": 2.0511796474456787,
      "learning_rate": 4.9356290797618935e-05,
      "loss": 0.2986,
      "step": 55500
    },
    {
      "epoch": 0.3611679495923869,
      "grad_norm": 2.2355754375457764,
      "learning_rate": 4.934938033919294e-05,
      "loss": 0.2929,
      "step": 55600
    },
    {
      "epoch": 0.36181753223553864,
      "grad_norm": 2.2738311290740967,
      "learning_rate": 4.934246988076695e-05,
      "loss": 0.2873,
      "step": 55700
    },
    {
      "epoch": 0.36246711487869043,
      "grad_norm": 1.9036633968353271,
      "learning_rate": 4.933555942234096e-05,
      "loss": 0.2912,
      "step": 55800
    },
    {
      "epoch": 0.3631166975218422,
      "grad_norm": 1.7222553491592407,
      "learning_rate": 4.932864896391497e-05,
      "loss": 0.2843,
      "step": 55900
    },
    {
      "epoch": 0.363766280164994,
      "grad_norm": 2.386439561843872,
      "learning_rate": 4.932173850548898e-05,
      "loss": 0.298,
      "step": 56000
    },
    {
      "epoch": 0.36441586280814575,
      "grad_norm": 1.6203378438949585,
      "learning_rate": 4.931482804706299e-05,
      "loss": 0.296,
      "step": 56100
    },
    {
      "epoch": 0.36506544545129754,
      "grad_norm": 2.331573247909546,
      "learning_rate": 4.9307917588637e-05,
      "loss": 0.2942,
      "step": 56200
    },
    {
      "epoch": 0.36571502809444933,
      "grad_norm": 2.1149988174438477,
      "learning_rate": 4.930100713021101e-05,
      "loss": 0.2912,
      "step": 56300
    },
    {
      "epoch": 0.36636461073760107,
      "grad_norm": 1.9539626836776733,
      "learning_rate": 4.929409667178502e-05,
      "loss": 0.2904,
      "step": 56400
    },
    {
      "epoch": 0.36701419338075286,
      "grad_norm": 1.8488093614578247,
      "learning_rate": 4.928718621335903e-05,
      "loss": 0.2912,
      "step": 56500
    },
    {
      "epoch": 0.36766377602390465,
      "grad_norm": 2.0403683185577393,
      "learning_rate": 4.928027575493303e-05,
      "loss": 0.2848,
      "step": 56600
    },
    {
      "epoch": 0.3683133586670564,
      "grad_norm": 1.9286662340164185,
      "learning_rate": 4.927336529650704e-05,
      "loss": 0.2826,
      "step": 56700
    },
    {
      "epoch": 0.3689629413102082,
      "grad_norm": 2.1151819229125977,
      "learning_rate": 4.926645483808105e-05,
      "loss": 0.2795,
      "step": 56800
    },
    {
      "epoch": 0.36961252395336,
      "grad_norm": 2.7631278038024902,
      "learning_rate": 4.925954437965506e-05,
      "loss": 0.2992,
      "step": 56900
    },
    {
      "epoch": 0.37026210659651176,
      "grad_norm": 1.909187912940979,
      "learning_rate": 4.925263392122907e-05,
      "loss": 0.2777,
      "step": 57000
    },
    {
      "epoch": 0.3709116892396635,
      "grad_norm": 1.6877318620681763,
      "learning_rate": 4.924572346280308e-05,
      "loss": 0.2928,
      "step": 57100
    },
    {
      "epoch": 0.3715612718828153,
      "grad_norm": 1.8015146255493164,
      "learning_rate": 4.923881300437709e-05,
      "loss": 0.2902,
      "step": 57200
    },
    {
      "epoch": 0.3722108545259671,
      "grad_norm": 1.7393742799758911,
      "learning_rate": 4.92319025459511e-05,
      "loss": 0.293,
      "step": 57300
    },
    {
      "epoch": 0.3728604371691188,
      "grad_norm": 2.230304718017578,
      "learning_rate": 4.922499208752511e-05,
      "loss": 0.2852,
      "step": 57400
    },
    {
      "epoch": 0.3735100198122706,
      "grad_norm": 1.8234847784042358,
      "learning_rate": 4.921808162909911e-05,
      "loss": 0.2856,
      "step": 57500
    },
    {
      "epoch": 0.3741596024554224,
      "grad_norm": 1.6744787693023682,
      "learning_rate": 4.921117117067312e-05,
      "loss": 0.2909,
      "step": 57600
    },
    {
      "epoch": 0.3748091850985742,
      "grad_norm": 1.6169509887695312,
      "learning_rate": 4.920426071224713e-05,
      "loss": 0.2905,
      "step": 57700
    },
    {
      "epoch": 0.37545876774172593,
      "grad_norm": 1.5522260665893555,
      "learning_rate": 4.919735025382114e-05,
      "loss": 0.2755,
      "step": 57800
    },
    {
      "epoch": 0.3761083503848777,
      "grad_norm": 1.500778317451477,
      "learning_rate": 4.919043979539515e-05,
      "loss": 0.2863,
      "step": 57900
    },
    {
      "epoch": 0.3767579330280295,
      "grad_norm": 1.643986463546753,
      "learning_rate": 4.918352933696916e-05,
      "loss": 0.2844,
      "step": 58000
    },
    {
      "epoch": 0.37740751567118125,
      "grad_norm": 1.8565016984939575,
      "learning_rate": 4.917661887854317e-05,
      "loss": 0.2852,
      "step": 58100
    },
    {
      "epoch": 0.37805709831433304,
      "grad_norm": 1.9834418296813965,
      "learning_rate": 4.9169708420117174e-05,
      "loss": 0.2854,
      "step": 58200
    },
    {
      "epoch": 0.37870668095748483,
      "grad_norm": 1.7482701539993286,
      "learning_rate": 4.9162797961691184e-05,
      "loss": 0.2771,
      "step": 58300
    },
    {
      "epoch": 0.37935626360063657,
      "grad_norm": 3.4357666969299316,
      "learning_rate": 4.9155887503265194e-05,
      "loss": 0.2774,
      "step": 58400
    },
    {
      "epoch": 0.38000584624378836,
      "grad_norm": 1.8719302415847778,
      "learning_rate": 4.9148977044839204e-05,
      "loss": 0.2839,
      "step": 58500
    },
    {
      "epoch": 0.38065542888694015,
      "grad_norm": 1.9220561981201172,
      "learning_rate": 4.914206658641321e-05,
      "loss": 0.2781,
      "step": 58600
    },
    {
      "epoch": 0.38130501153009194,
      "grad_norm": 2.43194842338562,
      "learning_rate": 4.913515612798722e-05,
      "loss": 0.276,
      "step": 58700
    },
    {
      "epoch": 0.3819545941732437,
      "grad_norm": 1.7517765760421753,
      "learning_rate": 4.912824566956123e-05,
      "loss": 0.2792,
      "step": 58800
    },
    {
      "epoch": 0.38260417681639547,
      "grad_norm": 2.063347578048706,
      "learning_rate": 4.9121335211135236e-05,
      "loss": 0.2819,
      "step": 58900
    },
    {
      "epoch": 0.38325375945954726,
      "grad_norm": 2.044548749923706,
      "learning_rate": 4.9114424752709246e-05,
      "loss": 0.2742,
      "step": 59000
    },
    {
      "epoch": 0.383903342102699,
      "grad_norm": 1.675466775894165,
      "learning_rate": 4.9107514294283256e-05,
      "loss": 0.2783,
      "step": 59100
    },
    {
      "epoch": 0.3845529247458508,
      "grad_norm": 2.04957914352417,
      "learning_rate": 4.9100603835857266e-05,
      "loss": 0.2794,
      "step": 59200
    },
    {
      "epoch": 0.3852025073890026,
      "grad_norm": 2.211398124694824,
      "learning_rate": 4.9093693377431276e-05,
      "loss": 0.2816,
      "step": 59300
    },
    {
      "epoch": 0.3858520900321543,
      "grad_norm": 1.855190396308899,
      "learning_rate": 4.908678291900528e-05,
      "loss": 0.2735,
      "step": 59400
    },
    {
      "epoch": 0.3865016726753061,
      "grad_norm": 2.7154428958892822,
      "learning_rate": 4.907987246057929e-05,
      "loss": 0.2865,
      "step": 59500
    },
    {
      "epoch": 0.3871512553184579,
      "grad_norm": 1.8595287799835205,
      "learning_rate": 4.90729620021533e-05,
      "loss": 0.2888,
      "step": 59600
    },
    {
      "epoch": 0.3878008379616097,
      "grad_norm": 1.8086013793945312,
      "learning_rate": 4.906605154372731e-05,
      "loss": 0.2758,
      "step": 59700
    },
    {
      "epoch": 0.3884504206047614,
      "grad_norm": 2.151996612548828,
      "learning_rate": 4.905914108530132e-05,
      "loss": 0.2789,
      "step": 59800
    },
    {
      "epoch": 0.3891000032479132,
      "grad_norm": 2.192809820175171,
      "learning_rate": 4.905223062687533e-05,
      "loss": 0.2807,
      "step": 59900
    },
    {
      "epoch": 0.389749585891065,
      "grad_norm": 2.185779333114624,
      "learning_rate": 4.904532016844934e-05,
      "loss": 0.2773,
      "step": 60000
    },
    {
      "epoch": 0.39039916853421675,
      "grad_norm": 2.555479049682617,
      "learning_rate": 4.903840971002335e-05,
      "loss": 0.2715,
      "step": 60100
    },
    {
      "epoch": 0.39104875117736854,
      "grad_norm": 1.6405644416809082,
      "learning_rate": 4.903149925159736e-05,
      "loss": 0.2768,
      "step": 60200
    },
    {
      "epoch": 0.39169833382052033,
      "grad_norm": 2.0450422763824463,
      "learning_rate": 4.902458879317136e-05,
      "loss": 0.2855,
      "step": 60300
    },
    {
      "epoch": 0.39234791646367206,
      "grad_norm": 2.1348230838775635,
      "learning_rate": 4.901767833474537e-05,
      "loss": 0.277,
      "step": 60400
    },
    {
      "epoch": 0.39299749910682386,
      "grad_norm": 1.23024320602417,
      "learning_rate": 4.901076787631938e-05,
      "loss": 0.2715,
      "step": 60500
    },
    {
      "epoch": 0.39364708174997565,
      "grad_norm": 1.9007471799850464,
      "learning_rate": 4.900385741789339e-05,
      "loss": 0.2766,
      "step": 60600
    },
    {
      "epoch": 0.39429666439312744,
      "grad_norm": 1.9534084796905518,
      "learning_rate": 4.89969469594674e-05,
      "loss": 0.2772,
      "step": 60700
    },
    {
      "epoch": 0.3949462470362792,
      "grad_norm": 1.8342312574386597,
      "learning_rate": 4.899003650104141e-05,
      "loss": 0.2758,
      "step": 60800
    },
    {
      "epoch": 0.39559582967943097,
      "grad_norm": 1.9471796751022339,
      "learning_rate": 4.898312604261542e-05,
      "loss": 0.2711,
      "step": 60900
    },
    {
      "epoch": 0.39624541232258276,
      "grad_norm": 2.0783987045288086,
      "learning_rate": 4.897621558418943e-05,
      "loss": 0.2806,
      "step": 61000
    },
    {
      "epoch": 0.3968949949657345,
      "grad_norm": 1.7353596687316895,
      "learning_rate": 4.896930512576344e-05,
      "loss": 0.2682,
      "step": 61100
    },
    {
      "epoch": 0.3975445776088863,
      "grad_norm": 1.6419316530227661,
      "learning_rate": 4.896239466733745e-05,
      "loss": 0.2615,
      "step": 61200
    },
    {
      "epoch": 0.3981941602520381,
      "grad_norm": 2.1782584190368652,
      "learning_rate": 4.895548420891145e-05,
      "loss": 0.2787,
      "step": 61300
    },
    {
      "epoch": 0.39884374289518987,
      "grad_norm": 2.1425087451934814,
      "learning_rate": 4.894857375048546e-05,
      "loss": 0.2784,
      "step": 61400
    },
    {
      "epoch": 0.3994933255383416,
      "grad_norm": 1.8842476606369019,
      "learning_rate": 4.894166329205947e-05,
      "loss": 0.2712,
      "step": 61500
    },
    {
      "epoch": 0.4001429081814934,
      "grad_norm": 1.880994439125061,
      "learning_rate": 4.893475283363348e-05,
      "loss": 0.2732,
      "step": 61600
    },
    {
      "epoch": 0.4007924908246452,
      "grad_norm": 1.7067809104919434,
      "learning_rate": 4.892784237520749e-05,
      "loss": 0.263,
      "step": 61700
    },
    {
      "epoch": 0.4014420734677969,
      "grad_norm": 1.6813780069351196,
      "learning_rate": 4.8920931916781495e-05,
      "loss": 0.2672,
      "step": 61800
    },
    {
      "epoch": 0.4020916561109487,
      "grad_norm": 2.061676502227783,
      "learning_rate": 4.8914021458355505e-05,
      "loss": 0.271,
      "step": 61900
    },
    {
      "epoch": 0.4027412387541005,
      "grad_norm": 1.830583930015564,
      "learning_rate": 4.8907110999929515e-05,
      "loss": 0.2648,
      "step": 62000
    },
    {
      "epoch": 0.40339082139725224,
      "grad_norm": 1.5781787633895874,
      "learning_rate": 4.8900200541503525e-05,
      "loss": 0.2706,
      "step": 62100
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 1.923588752746582,
      "learning_rate": 4.889329008307753e-05,
      "loss": 0.2708,
      "step": 62200
    },
    {
      "epoch": 0.4046899866835558,
      "grad_norm": 1.4172972440719604,
      "learning_rate": 4.888637962465154e-05,
      "loss": 0.2719,
      "step": 62300
    },
    {
      "epoch": 0.4053395693267076,
      "grad_norm": 1.429644227027893,
      "learning_rate": 4.887946916622555e-05,
      "loss": 0.2686,
      "step": 62400
    },
    {
      "epoch": 0.40598915196985935,
      "grad_norm": 1.824374794960022,
      "learning_rate": 4.887255870779956e-05,
      "loss": 0.2796,
      "step": 62500
    },
    {
      "epoch": 0.40663873461301114,
      "grad_norm": 1.6065399646759033,
      "learning_rate": 4.886564824937357e-05,
      "loss": 0.2737,
      "step": 62600
    },
    {
      "epoch": 0.40728831725616294,
      "grad_norm": 1.8329154253005981,
      "learning_rate": 4.885873779094758e-05,
      "loss": 0.2811,
      "step": 62700
    },
    {
      "epoch": 0.40793789989931467,
      "grad_norm": 2.305079221725464,
      "learning_rate": 4.885182733252159e-05,
      "loss": 0.262,
      "step": 62800
    },
    {
      "epoch": 0.40858748254246646,
      "grad_norm": 1.5633844137191772,
      "learning_rate": 4.88449168740956e-05,
      "loss": 0.2637,
      "step": 62900
    },
    {
      "epoch": 0.40923706518561825,
      "grad_norm": 1.5615135431289673,
      "learning_rate": 4.8838006415669607e-05,
      "loss": 0.271,
      "step": 63000
    },
    {
      "epoch": 0.40988664782877,
      "grad_norm": 2.1448874473571777,
      "learning_rate": 4.8831095957243616e-05,
      "loss": 0.2676,
      "step": 63100
    },
    {
      "epoch": 0.4105362304719218,
      "grad_norm": 1.7589747905731201,
      "learning_rate": 4.882418549881762e-05,
      "loss": 0.2733,
      "step": 63200
    },
    {
      "epoch": 0.4111858131150736,
      "grad_norm": 1.9405912160873413,
      "learning_rate": 4.881727504039163e-05,
      "loss": 0.2616,
      "step": 63300
    },
    {
      "epoch": 0.41183539575822536,
      "grad_norm": 1.6944855451583862,
      "learning_rate": 4.881036458196564e-05,
      "loss": 0.2585,
      "step": 63400
    },
    {
      "epoch": 0.4124849784013771,
      "grad_norm": 1.875450849533081,
      "learning_rate": 4.880345412353965e-05,
      "loss": 0.2682,
      "step": 63500
    },
    {
      "epoch": 0.4131345610445289,
      "grad_norm": 1.4902321100234985,
      "learning_rate": 4.879654366511366e-05,
      "loss": 0.2722,
      "step": 63600
    },
    {
      "epoch": 0.4137841436876807,
      "grad_norm": 1.768924355506897,
      "learning_rate": 4.878963320668767e-05,
      "loss": 0.263,
      "step": 63700
    },
    {
      "epoch": 0.4144337263308324,
      "grad_norm": 1.6286430358886719,
      "learning_rate": 4.878272274826168e-05,
      "loss": 0.2684,
      "step": 63800
    },
    {
      "epoch": 0.4150833089739842,
      "grad_norm": 1.8602079153060913,
      "learning_rate": 4.877581228983569e-05,
      "loss": 0.2709,
      "step": 63900
    },
    {
      "epoch": 0.415732891617136,
      "grad_norm": 1.9493178129196167,
      "learning_rate": 4.87689018314097e-05,
      "loss": 0.2658,
      "step": 64000
    },
    {
      "epoch": 0.41638247426028774,
      "grad_norm": 2.330254316329956,
      "learning_rate": 4.87619913729837e-05,
      "loss": 0.269,
      "step": 64100
    },
    {
      "epoch": 0.41703205690343953,
      "grad_norm": 1.8429316282272339,
      "learning_rate": 4.875508091455771e-05,
      "loss": 0.2655,
      "step": 64200
    },
    {
      "epoch": 0.4176816395465913,
      "grad_norm": 1.5989829301834106,
      "learning_rate": 4.874817045613172e-05,
      "loss": 0.2505,
      "step": 64300
    },
    {
      "epoch": 0.4183312221897431,
      "grad_norm": 1.9251210689544678,
      "learning_rate": 4.874125999770573e-05,
      "loss": 0.2656,
      "step": 64400
    },
    {
      "epoch": 0.41898080483289485,
      "grad_norm": 1.4903656244277954,
      "learning_rate": 4.873434953927974e-05,
      "loss": 0.262,
      "step": 64500
    },
    {
      "epoch": 0.41963038747604664,
      "grad_norm": 1.6315969228744507,
      "learning_rate": 4.872743908085375e-05,
      "loss": 0.2652,
      "step": 64600
    },
    {
      "epoch": 0.42027997011919843,
      "grad_norm": 2.402968406677246,
      "learning_rate": 4.872052862242776e-05,
      "loss": 0.257,
      "step": 64700
    },
    {
      "epoch": 0.42092955276235017,
      "grad_norm": 1.925356388092041,
      "learning_rate": 4.871361816400177e-05,
      "loss": 0.2599,
      "step": 64800
    },
    {
      "epoch": 0.42157913540550196,
      "grad_norm": 1.7073081731796265,
      "learning_rate": 4.870670770557578e-05,
      "loss": 0.2729,
      "step": 64900
    },
    {
      "epoch": 0.42222871804865375,
      "grad_norm": 1.894002914428711,
      "learning_rate": 4.869979724714978e-05,
      "loss": 0.2652,
      "step": 65000
    },
    {
      "epoch": 0.42287830069180554,
      "grad_norm": 1.763878345489502,
      "learning_rate": 4.869288678872379e-05,
      "loss": 0.2654,
      "step": 65100
    },
    {
      "epoch": 0.4235278833349573,
      "grad_norm": 1.8109986782073975,
      "learning_rate": 4.86859763302978e-05,
      "loss": 0.2681,
      "step": 65200
    },
    {
      "epoch": 0.42417746597810907,
      "grad_norm": 1.6984961032867432,
      "learning_rate": 4.867906587187181e-05,
      "loss": 0.2526,
      "step": 65300
    },
    {
      "epoch": 0.42482704862126086,
      "grad_norm": 2.106708526611328,
      "learning_rate": 4.8672155413445816e-05,
      "loss": 0.2582,
      "step": 65400
    },
    {
      "epoch": 0.4254766312644126,
      "grad_norm": 2.5152463912963867,
      "learning_rate": 4.8665244955019826e-05,
      "loss": 0.2551,
      "step": 65500
    },
    {
      "epoch": 0.4261262139075644,
      "grad_norm": 1.9243464469909668,
      "learning_rate": 4.8658334496593836e-05,
      "loss": 0.2599,
      "step": 65600
    },
    {
      "epoch": 0.4267757965507162,
      "grad_norm": 2.3396942615509033,
      "learning_rate": 4.8651424038167845e-05,
      "loss": 0.2617,
      "step": 65700
    },
    {
      "epoch": 0.4274253791938679,
      "grad_norm": 1.8173681497573853,
      "learning_rate": 4.8644513579741855e-05,
      "loss": 0.2606,
      "step": 65800
    },
    {
      "epoch": 0.4280749618370197,
      "grad_norm": 1.7893296480178833,
      "learning_rate": 4.8637603121315865e-05,
      "loss": 0.2632,
      "step": 65900
    },
    {
      "epoch": 0.4287245444801715,
      "grad_norm": 1.842169165611267,
      "learning_rate": 4.863069266288987e-05,
      "loss": 0.2628,
      "step": 66000
    },
    {
      "epoch": 0.4293741271233233,
      "grad_norm": 1.7992637157440186,
      "learning_rate": 4.862378220446388e-05,
      "loss": 0.2564,
      "step": 66100
    },
    {
      "epoch": 0.430023709766475,
      "grad_norm": 1.7956593036651611,
      "learning_rate": 4.861687174603789e-05,
      "loss": 0.2641,
      "step": 66200
    },
    {
      "epoch": 0.4306732924096268,
      "grad_norm": 1.620179533958435,
      "learning_rate": 4.86099612876119e-05,
      "loss": 0.2562,
      "step": 66300
    },
    {
      "epoch": 0.4313228750527786,
      "grad_norm": 1.778933048248291,
      "learning_rate": 4.860305082918591e-05,
      "loss": 0.246,
      "step": 66400
    },
    {
      "epoch": 0.43197245769593035,
      "grad_norm": 1.615139365196228,
      "learning_rate": 4.859614037075992e-05,
      "loss": 0.2574,
      "step": 66500
    },
    {
      "epoch": 0.43262204033908214,
      "grad_norm": 1.7919877767562866,
      "learning_rate": 4.858922991233393e-05,
      "loss": 0.2604,
      "step": 66600
    },
    {
      "epoch": 0.43327162298223393,
      "grad_norm": 1.9466838836669922,
      "learning_rate": 4.858231945390794e-05,
      "loss": 0.2611,
      "step": 66700
    },
    {
      "epoch": 0.43392120562538566,
      "grad_norm": 1.6830095052719116,
      "learning_rate": 4.857540899548195e-05,
      "loss": 0.2588,
      "step": 66800
    },
    {
      "epoch": 0.43457078826853746,
      "grad_norm": 1.6677906513214111,
      "learning_rate": 4.856849853705595e-05,
      "loss": 0.2617,
      "step": 66900
    },
    {
      "epoch": 0.43522037091168925,
      "grad_norm": 2.0566673278808594,
      "learning_rate": 4.856158807862996e-05,
      "loss": 0.2617,
      "step": 67000
    },
    {
      "epoch": 0.43586995355484104,
      "grad_norm": 1.7113103866577148,
      "learning_rate": 4.855467762020397e-05,
      "loss": 0.2524,
      "step": 67100
    },
    {
      "epoch": 0.4365195361979928,
      "grad_norm": 1.6472784280776978,
      "learning_rate": 4.854776716177798e-05,
      "loss": 0.2585,
      "step": 67200
    },
    {
      "epoch": 0.43716911884114457,
      "grad_norm": 1.737865924835205,
      "learning_rate": 4.854085670335199e-05,
      "loss": 0.2684,
      "step": 67300
    },
    {
      "epoch": 0.43781870148429636,
      "grad_norm": 1.2559257745742798,
      "learning_rate": 4.8533946244926e-05,
      "loss": 0.2629,
      "step": 67400
    },
    {
      "epoch": 0.4384682841274481,
      "grad_norm": 1.5701349973678589,
      "learning_rate": 4.852703578650001e-05,
      "loss": 0.2547,
      "step": 67500
    },
    {
      "epoch": 0.4391178667705999,
      "grad_norm": 1.6897457838058472,
      "learning_rate": 4.852012532807402e-05,
      "loss": 0.2569,
      "step": 67600
    },
    {
      "epoch": 0.4397674494137517,
      "grad_norm": 2.362215995788574,
      "learning_rate": 4.851321486964803e-05,
      "loss": 0.2553,
      "step": 67700
    },
    {
      "epoch": 0.4404170320569034,
      "grad_norm": 1.9655439853668213,
      "learning_rate": 4.850630441122204e-05,
      "loss": 0.2559,
      "step": 67800
    },
    {
      "epoch": 0.4410666147000552,
      "grad_norm": 1.3838965892791748,
      "learning_rate": 4.849939395279604e-05,
      "loss": 0.2553,
      "step": 67900
    },
    {
      "epoch": 0.441716197343207,
      "grad_norm": 1.884303092956543,
      "learning_rate": 4.849248349437005e-05,
      "loss": 0.2704,
      "step": 68000
    },
    {
      "epoch": 0.4423657799863588,
      "grad_norm": 1.6908293962478638,
      "learning_rate": 4.848557303594406e-05,
      "loss": 0.2407,
      "step": 68100
    },
    {
      "epoch": 0.4430153626295105,
      "grad_norm": 1.6532363891601562,
      "learning_rate": 4.847866257751807e-05,
      "loss": 0.2513,
      "step": 68200
    },
    {
      "epoch": 0.4436649452726623,
      "grad_norm": 1.8776613473892212,
      "learning_rate": 4.847175211909208e-05,
      "loss": 0.2571,
      "step": 68300
    },
    {
      "epoch": 0.4443145279158141,
      "grad_norm": 1.495202660560608,
      "learning_rate": 4.846484166066609e-05,
      "loss": 0.2549,
      "step": 68400
    },
    {
      "epoch": 0.44496411055896584,
      "grad_norm": 1.3679856061935425,
      "learning_rate": 4.84579312022401e-05,
      "loss": 0.2599,
      "step": 68500
    },
    {
      "epoch": 0.44561369320211763,
      "grad_norm": 2.18310284614563,
      "learning_rate": 4.8451020743814104e-05,
      "loss": 0.2533,
      "step": 68600
    },
    {
      "epoch": 0.4462632758452694,
      "grad_norm": 1.668081283569336,
      "learning_rate": 4.8444110285388114e-05,
      "loss": 0.2502,
      "step": 68700
    },
    {
      "epoch": 0.4469128584884212,
      "grad_norm": 1.943137526512146,
      "learning_rate": 4.8437199826962124e-05,
      "loss": 0.2545,
      "step": 68800
    },
    {
      "epoch": 0.44756244113157295,
      "grad_norm": 1.493629813194275,
      "learning_rate": 4.8430289368536134e-05,
      "loss": 0.2565,
      "step": 68900
    },
    {
      "epoch": 0.44821202377472474,
      "grad_norm": 2.3626677989959717,
      "learning_rate": 4.842337891011014e-05,
      "loss": 0.2603,
      "step": 69000
    },
    {
      "epoch": 0.44886160641787654,
      "grad_norm": 2.0036301612854004,
      "learning_rate": 4.8416468451684147e-05,
      "loss": 0.2594,
      "step": 69100
    },
    {
      "epoch": 0.44951118906102827,
      "grad_norm": 1.7743196487426758,
      "learning_rate": 4.8409557993258156e-05,
      "loss": 0.2505,
      "step": 69200
    },
    {
      "epoch": 0.45016077170418006,
      "grad_norm": 1.648819923400879,
      "learning_rate": 4.8402647534832166e-05,
      "loss": 0.2558,
      "step": 69300
    },
    {
      "epoch": 0.45081035434733185,
      "grad_norm": 1.8129847049713135,
      "learning_rate": 4.8395737076406176e-05,
      "loss": 0.252,
      "step": 69400
    },
    {
      "epoch": 0.4514599369904836,
      "grad_norm": 1.2964669466018677,
      "learning_rate": 4.8388826617980186e-05,
      "loss": 0.2472,
      "step": 69500
    },
    {
      "epoch": 0.4521095196336354,
      "grad_norm": 2.1872355937957764,
      "learning_rate": 4.8381916159554196e-05,
      "loss": 0.244,
      "step": 69600
    },
    {
      "epoch": 0.4527591022767872,
      "grad_norm": 2.026442289352417,
      "learning_rate": 4.8375005701128206e-05,
      "loss": 0.2539,
      "step": 69700
    },
    {
      "epoch": 0.45340868491993896,
      "grad_norm": 1.7474032640457153,
      "learning_rate": 4.836809524270221e-05,
      "loss": 0.2561,
      "step": 69800
    },
    {
      "epoch": 0.4540582675630907,
      "grad_norm": 1.4479464292526245,
      "learning_rate": 4.836118478427622e-05,
      "loss": 0.2451,
      "step": 69900
    },
    {
      "epoch": 0.4547078502062425,
      "grad_norm": 1.6212105751037598,
      "learning_rate": 4.835427432585023e-05,
      "loss": 0.254,
      "step": 70000
    },
    {
      "epoch": 0.4553574328493943,
      "grad_norm": 1.6117321252822876,
      "learning_rate": 4.834736386742424e-05,
      "loss": 0.2478,
      "step": 70100
    },
    {
      "epoch": 0.456007015492546,
      "grad_norm": 2.1765308380126953,
      "learning_rate": 4.834045340899825e-05,
      "loss": 0.2538,
      "step": 70200
    },
    {
      "epoch": 0.4566565981356978,
      "grad_norm": 2.1674156188964844,
      "learning_rate": 4.833354295057226e-05,
      "loss": 0.2448,
      "step": 70300
    },
    {
      "epoch": 0.4573061807788496,
      "grad_norm": 1.5478153228759766,
      "learning_rate": 4.832663249214627e-05,
      "loss": 0.2452,
      "step": 70400
    },
    {
      "epoch": 0.45795576342200134,
      "grad_norm": 1.8921254873275757,
      "learning_rate": 4.831972203372028e-05,
      "loss": 0.245,
      "step": 70500
    },
    {
      "epoch": 0.45860534606515313,
      "grad_norm": 1.8974332809448242,
      "learning_rate": 4.831281157529429e-05,
      "loss": 0.2489,
      "step": 70600
    },
    {
      "epoch": 0.4592549287083049,
      "grad_norm": 2.547855854034424,
      "learning_rate": 4.830590111686829e-05,
      "loss": 0.2495,
      "step": 70700
    },
    {
      "epoch": 0.4599045113514567,
      "grad_norm": 1.983646273612976,
      "learning_rate": 4.82989906584423e-05,
      "loss": 0.2524,
      "step": 70800
    },
    {
      "epoch": 0.46055409399460845,
      "grad_norm": 1.8128340244293213,
      "learning_rate": 4.829208020001631e-05,
      "loss": 0.2464,
      "step": 70900
    },
    {
      "epoch": 0.46120367663776024,
      "grad_norm": 1.445637583732605,
      "learning_rate": 4.828516974159032e-05,
      "loss": 0.2421,
      "step": 71000
    },
    {
      "epoch": 0.46185325928091203,
      "grad_norm": 1.7546720504760742,
      "learning_rate": 4.827825928316433e-05,
      "loss": 0.2484,
      "step": 71100
    },
    {
      "epoch": 0.46250284192406377,
      "grad_norm": 2.14963436126709,
      "learning_rate": 4.827134882473834e-05,
      "loss": 0.2539,
      "step": 71200
    },
    {
      "epoch": 0.46315242456721556,
      "grad_norm": 1.5823098421096802,
      "learning_rate": 4.826443836631235e-05,
      "loss": 0.2432,
      "step": 71300
    },
    {
      "epoch": 0.46380200721036735,
      "grad_norm": 1.729860544204712,
      "learning_rate": 4.825752790788636e-05,
      "loss": 0.2534,
      "step": 71400
    },
    {
      "epoch": 0.4644515898535191,
      "grad_norm": 1.922191858291626,
      "learning_rate": 4.825061744946037e-05,
      "loss": 0.2415,
      "step": 71500
    },
    {
      "epoch": 0.4651011724966709,
      "grad_norm": 2.162235975265503,
      "learning_rate": 4.824370699103437e-05,
      "loss": 0.2455,
      "step": 71600
    },
    {
      "epoch": 0.46575075513982267,
      "grad_norm": 1.3924493789672852,
      "learning_rate": 4.823679653260838e-05,
      "loss": 0.2435,
      "step": 71700
    },
    {
      "epoch": 0.46640033778297446,
      "grad_norm": 1.771058440208435,
      "learning_rate": 4.822988607418239e-05,
      "loss": 0.2445,
      "step": 71800
    },
    {
      "epoch": 0.4670499204261262,
      "grad_norm": 1.7872133255004883,
      "learning_rate": 4.82229756157564e-05,
      "loss": 0.2573,
      "step": 71900
    },
    {
      "epoch": 0.467699503069278,
      "grad_norm": 1.9928823709487915,
      "learning_rate": 4.821606515733041e-05,
      "loss": 0.2531,
      "step": 72000
    },
    {
      "epoch": 0.4683490857124298,
      "grad_norm": 2.127629280090332,
      "learning_rate": 4.820915469890442e-05,
      "loss": 0.2474,
      "step": 72100
    },
    {
      "epoch": 0.4689986683555815,
      "grad_norm": 1.5676501989364624,
      "learning_rate": 4.8202244240478425e-05,
      "loss": 0.243,
      "step": 72200
    },
    {
      "epoch": 0.4696482509987333,
      "grad_norm": 2.2473907470703125,
      "learning_rate": 4.8195333782052435e-05,
      "loss": 0.2523,
      "step": 72300
    },
    {
      "epoch": 0.4702978336418851,
      "grad_norm": 2.4221997261047363,
      "learning_rate": 4.8188423323626445e-05,
      "loss": 0.2534,
      "step": 72400
    },
    {
      "epoch": 0.4709474162850369,
      "grad_norm": 1.5867009162902832,
      "learning_rate": 4.8181512865200454e-05,
      "loss": 0.2464,
      "step": 72500
    },
    {
      "epoch": 0.4715969989281886,
      "grad_norm": 1.541975736618042,
      "learning_rate": 4.817460240677446e-05,
      "loss": 0.2521,
      "step": 72600
    },
    {
      "epoch": 0.4722465815713404,
      "grad_norm": 1.6624387502670288,
      "learning_rate": 4.816769194834847e-05,
      "loss": 0.2483,
      "step": 72700
    },
    {
      "epoch": 0.4728961642144922,
      "grad_norm": 1.9660152196884155,
      "learning_rate": 4.816078148992248e-05,
      "loss": 0.2352,
      "step": 72800
    },
    {
      "epoch": 0.47354574685764395,
      "grad_norm": 1.5699410438537598,
      "learning_rate": 4.815387103149649e-05,
      "loss": 0.2456,
      "step": 72900
    },
    {
      "epoch": 0.47419532950079574,
      "grad_norm": 1.9543989896774292,
      "learning_rate": 4.81469605730705e-05,
      "loss": 0.2432,
      "step": 73000
    },
    {
      "epoch": 0.47484491214394753,
      "grad_norm": 1.916986346244812,
      "learning_rate": 4.814005011464451e-05,
      "loss": 0.244,
      "step": 73100
    },
    {
      "epoch": 0.47549449478709926,
      "grad_norm": 1.5896846055984497,
      "learning_rate": 4.8133139656218517e-05,
      "loss": 0.2439,
      "step": 73200
    },
    {
      "epoch": 0.47614407743025106,
      "grad_norm": 1.8728649616241455,
      "learning_rate": 4.8126229197792526e-05,
      "loss": 0.2279,
      "step": 73300
    },
    {
      "epoch": 0.47679366007340285,
      "grad_norm": 1.8497103452682495,
      "learning_rate": 4.8119318739366536e-05,
      "loss": 0.2453,
      "step": 73400
    },
    {
      "epoch": 0.47744324271655464,
      "grad_norm": 1.826844334602356,
      "learning_rate": 4.811240828094054e-05,
      "loss": 0.2376,
      "step": 73500
    },
    {
      "epoch": 0.4780928253597064,
      "grad_norm": 1.6759387254714966,
      "learning_rate": 4.810549782251455e-05,
      "loss": 0.2332,
      "step": 73600
    },
    {
      "epoch": 0.47874240800285817,
      "grad_norm": 1.6723864078521729,
      "learning_rate": 4.809858736408856e-05,
      "loss": 0.2453,
      "step": 73700
    },
    {
      "epoch": 0.47939199064600996,
      "grad_norm": 1.7477130889892578,
      "learning_rate": 4.809167690566257e-05,
      "loss": 0.2458,
      "step": 73800
    },
    {
      "epoch": 0.4800415732891617,
      "grad_norm": 1.9392635822296143,
      "learning_rate": 4.808476644723658e-05,
      "loss": 0.2411,
      "step": 73900
    },
    {
      "epoch": 0.4806911559323135,
      "grad_norm": 2.276395559310913,
      "learning_rate": 4.807785598881059e-05,
      "loss": 0.2432,
      "step": 74000
    },
    {
      "epoch": 0.4813407385754653,
      "grad_norm": 1.7033623456954956,
      "learning_rate": 4.80709455303846e-05,
      "loss": 0.2376,
      "step": 74100
    },
    {
      "epoch": 0.481990321218617,
      "grad_norm": 1.6777113676071167,
      "learning_rate": 4.806403507195861e-05,
      "loss": 0.2391,
      "step": 74200
    },
    {
      "epoch": 0.4826399038617688,
      "grad_norm": 1.8196725845336914,
      "learning_rate": 4.805712461353262e-05,
      "loss": 0.2463,
      "step": 74300
    },
    {
      "epoch": 0.4832894865049206,
      "grad_norm": 1.2972793579101562,
      "learning_rate": 4.805021415510663e-05,
      "loss": 0.2409,
      "step": 74400
    },
    {
      "epoch": 0.4839390691480724,
      "grad_norm": 1.8089752197265625,
      "learning_rate": 4.804330369668063e-05,
      "loss": 0.2376,
      "step": 74500
    },
    {
      "epoch": 0.4845886517912241,
      "grad_norm": 1.51624596118927,
      "learning_rate": 4.803639323825464e-05,
      "loss": 0.2492,
      "step": 74600
    },
    {
      "epoch": 0.4852382344343759,
      "grad_norm": 1.8631864786148071,
      "learning_rate": 4.802948277982865e-05,
      "loss": 0.2464,
      "step": 74700
    },
    {
      "epoch": 0.4858878170775277,
      "grad_norm": 1.7996530532836914,
      "learning_rate": 4.802257232140266e-05,
      "loss": 0.2377,
      "step": 74800
    },
    {
      "epoch": 0.48653739972067944,
      "grad_norm": 1.9183237552642822,
      "learning_rate": 4.801566186297667e-05,
      "loss": 0.2458,
      "step": 74900
    },
    {
      "epoch": 0.48718698236383123,
      "grad_norm": 1.5573803186416626,
      "learning_rate": 4.800875140455068e-05,
      "loss": 0.2282,
      "step": 75000
    },
    {
      "epoch": 0.487836565006983,
      "grad_norm": 2.4715943336486816,
      "learning_rate": 4.800184094612469e-05,
      "loss": 0.2445,
      "step": 75100
    },
    {
      "epoch": 0.48848614765013476,
      "grad_norm": 1.6215569972991943,
      "learning_rate": 4.79949304876987e-05,
      "loss": 0.2378,
      "step": 75200
    },
    {
      "epoch": 0.48913573029328655,
      "grad_norm": 1.5518131256103516,
      "learning_rate": 4.798802002927271e-05,
      "loss": 0.241,
      "step": 75300
    },
    {
      "epoch": 0.48978531293643834,
      "grad_norm": 1.6296091079711914,
      "learning_rate": 4.798110957084671e-05,
      "loss": 0.2373,
      "step": 75400
    },
    {
      "epoch": 0.49043489557959014,
      "grad_norm": 1.546258807182312,
      "learning_rate": 4.797419911242072e-05,
      "loss": 0.2324,
      "step": 75500
    },
    {
      "epoch": 0.49108447822274187,
      "grad_norm": 1.6004717350006104,
      "learning_rate": 4.796728865399473e-05,
      "loss": 0.2319,
      "step": 75600
    },
    {
      "epoch": 0.49173406086589366,
      "grad_norm": 2.0789449214935303,
      "learning_rate": 4.796037819556874e-05,
      "loss": 0.2382,
      "step": 75700
    },
    {
      "epoch": 0.49238364350904545,
      "grad_norm": 1.7472771406173706,
      "learning_rate": 4.7953467737142746e-05,
      "loss": 0.2304,
      "step": 75800
    },
    {
      "epoch": 0.4930332261521972,
      "grad_norm": 1.6509735584259033,
      "learning_rate": 4.7946557278716755e-05,
      "loss": 0.2473,
      "step": 75900
    },
    {
      "epoch": 0.493682808795349,
      "grad_norm": 1.8233909606933594,
      "learning_rate": 4.7939646820290765e-05,
      "loss": 0.2408,
      "step": 76000
    },
    {
      "epoch": 0.4943323914385008,
      "grad_norm": 1.759879231452942,
      "learning_rate": 4.7932736361864775e-05,
      "loss": 0.2444,
      "step": 76100
    },
    {
      "epoch": 0.49498197408165256,
      "grad_norm": 1.5561507940292358,
      "learning_rate": 4.7925825903438785e-05,
      "loss": 0.2441,
      "step": 76200
    },
    {
      "epoch": 0.4956315567248043,
      "grad_norm": 2.014981746673584,
      "learning_rate": 4.791891544501279e-05,
      "loss": 0.246,
      "step": 76300
    },
    {
      "epoch": 0.4962811393679561,
      "grad_norm": 1.9172275066375732,
      "learning_rate": 4.79120049865868e-05,
      "loss": 0.2337,
      "step": 76400
    },
    {
      "epoch": 0.4969307220111079,
      "grad_norm": 1.5633387565612793,
      "learning_rate": 4.790509452816081e-05,
      "loss": 0.2427,
      "step": 76500
    },
    {
      "epoch": 0.4975803046542596,
      "grad_norm": 1.310832142829895,
      "learning_rate": 4.789818406973482e-05,
      "loss": 0.237,
      "step": 76600
    },
    {
      "epoch": 0.4982298872974114,
      "grad_norm": 1.9565906524658203,
      "learning_rate": 4.789127361130883e-05,
      "loss": 0.2328,
      "step": 76700
    },
    {
      "epoch": 0.4988794699405632,
      "grad_norm": 1.7719624042510986,
      "learning_rate": 4.788436315288284e-05,
      "loss": 0.2421,
      "step": 76800
    },
    {
      "epoch": 0.49952905258371494,
      "grad_norm": 2.225106954574585,
      "learning_rate": 4.787745269445685e-05,
      "loss": 0.2367,
      "step": 76900
    },
    {
      "epoch": 0.5001786352268668,
      "grad_norm": 1.355344295501709,
      "learning_rate": 4.787054223603086e-05,
      "loss": 0.2352,
      "step": 77000
    },
    {
      "epoch": 0.5008282178700185,
      "grad_norm": 1.5322872400283813,
      "learning_rate": 4.786363177760487e-05,
      "loss": 0.2383,
      "step": 77100
    },
    {
      "epoch": 0.5014778005131703,
      "grad_norm": 1.5148637294769287,
      "learning_rate": 4.785672131917888e-05,
      "loss": 0.2412,
      "step": 77200
    },
    {
      "epoch": 0.5021273831563221,
      "grad_norm": 1.8923488855361938,
      "learning_rate": 4.784981086075288e-05,
      "loss": 0.2373,
      "step": 77300
    },
    {
      "epoch": 0.5027769657994738,
      "grad_norm": 1.9526631832122803,
      "learning_rate": 4.784290040232689e-05,
      "loss": 0.2298,
      "step": 77400
    },
    {
      "epoch": 0.5034265484426256,
      "grad_norm": 1.7598625421524048,
      "learning_rate": 4.78359899439009e-05,
      "loss": 0.238,
      "step": 77500
    },
    {
      "epoch": 0.5040761310857774,
      "grad_norm": 1.5321555137634277,
      "learning_rate": 4.782907948547491e-05,
      "loss": 0.237,
      "step": 77600
    },
    {
      "epoch": 0.5047257137289292,
      "grad_norm": 1.8896604776382446,
      "learning_rate": 4.782216902704892e-05,
      "loss": 0.2284,
      "step": 77700
    },
    {
      "epoch": 0.5053752963720809,
      "grad_norm": 1.718421220779419,
      "learning_rate": 4.781525856862293e-05,
      "loss": 0.2492,
      "step": 77800
    },
    {
      "epoch": 0.5060248790152327,
      "grad_norm": 1.6909253597259521,
      "learning_rate": 4.780834811019694e-05,
      "loss": 0.2349,
      "step": 77900
    },
    {
      "epoch": 0.5066744616583845,
      "grad_norm": 1.859501838684082,
      "learning_rate": 4.780143765177095e-05,
      "loss": 0.2388,
      "step": 78000
    },
    {
      "epoch": 0.5073240443015362,
      "grad_norm": 1.665373682975769,
      "learning_rate": 4.779452719334496e-05,
      "loss": 0.2394,
      "step": 78100
    },
    {
      "epoch": 0.5079736269446881,
      "grad_norm": 1.7985588312149048,
      "learning_rate": 4.778761673491896e-05,
      "loss": 0.2343,
      "step": 78200
    },
    {
      "epoch": 0.5086232095878398,
      "grad_norm": 1.9827611446380615,
      "learning_rate": 4.778070627649297e-05,
      "loss": 0.2313,
      "step": 78300
    },
    {
      "epoch": 0.5092727922309915,
      "grad_norm": 1.3407262563705444,
      "learning_rate": 4.777379581806698e-05,
      "loss": 0.2333,
      "step": 78400
    },
    {
      "epoch": 0.5099223748741434,
      "grad_norm": 1.8327947854995728,
      "learning_rate": 4.776688535964099e-05,
      "loss": 0.2339,
      "step": 78500
    },
    {
      "epoch": 0.5105719575172951,
      "grad_norm": 2.047719955444336,
      "learning_rate": 4.7759974901215e-05,
      "loss": 0.2332,
      "step": 78600
    },
    {
      "epoch": 0.511221540160447,
      "grad_norm": 1.4893746376037598,
      "learning_rate": 4.775306444278901e-05,
      "loss": 0.225,
      "step": 78700
    },
    {
      "epoch": 0.5118711228035987,
      "grad_norm": 1.2140696048736572,
      "learning_rate": 4.774615398436302e-05,
      "loss": 0.23,
      "step": 78800
    },
    {
      "epoch": 0.5125207054467504,
      "grad_norm": 1.6190299987792969,
      "learning_rate": 4.773924352593703e-05,
      "loss": 0.231,
      "step": 78900
    },
    {
      "epoch": 0.5131702880899023,
      "grad_norm": 1.7477766275405884,
      "learning_rate": 4.7732333067511034e-05,
      "loss": 0.2291,
      "step": 79000
    },
    {
      "epoch": 0.513819870733054,
      "grad_norm": 1.6986749172210693,
      "learning_rate": 4.7725422609085044e-05,
      "loss": 0.2326,
      "step": 79100
    },
    {
      "epoch": 0.5144694533762058,
      "grad_norm": 1.4086105823516846,
      "learning_rate": 4.7718512150659053e-05,
      "loss": 0.2326,
      "step": 79200
    },
    {
      "epoch": 0.5151190360193576,
      "grad_norm": 1.758885383605957,
      "learning_rate": 4.771160169223306e-05,
      "loss": 0.2309,
      "step": 79300
    },
    {
      "epoch": 0.5157686186625093,
      "grad_norm": 1.8776580095291138,
      "learning_rate": 4.7704691233807066e-05,
      "loss": 0.2341,
      "step": 79400
    },
    {
      "epoch": 0.5164182013056611,
      "grad_norm": 1.559729814529419,
      "learning_rate": 4.7697780775381076e-05,
      "loss": 0.2355,
      "step": 79500
    },
    {
      "epoch": 0.5170677839488129,
      "grad_norm": 2.233949899673462,
      "learning_rate": 4.7690870316955086e-05,
      "loss": 0.2296,
      "step": 79600
    },
    {
      "epoch": 0.5177173665919647,
      "grad_norm": 1.6672412157058716,
      "learning_rate": 4.7683959858529096e-05,
      "loss": 0.2239,
      "step": 79700
    },
    {
      "epoch": 0.5183669492351164,
      "grad_norm": 1.897761344909668,
      "learning_rate": 4.7677049400103106e-05,
      "loss": 0.2221,
      "step": 79800
    },
    {
      "epoch": 0.5190165318782682,
      "grad_norm": 1.710326075553894,
      "learning_rate": 4.7670138941677116e-05,
      "loss": 0.2229,
      "step": 79900
    },
    {
      "epoch": 0.51966611452142,
      "grad_norm": 1.6551083326339722,
      "learning_rate": 4.7663228483251126e-05,
      "loss": 0.2343,
      "step": 80000
    },
    {
      "epoch": 0.5203156971645717,
      "grad_norm": 1.9999308586120605,
      "learning_rate": 4.765631802482513e-05,
      "loss": 0.2406,
      "step": 80100
    },
    {
      "epoch": 0.5209652798077236,
      "grad_norm": 1.9340877532958984,
      "learning_rate": 4.764940756639914e-05,
      "loss": 0.2322,
      "step": 80200
    },
    {
      "epoch": 0.5216148624508753,
      "grad_norm": 1.490700364112854,
      "learning_rate": 4.764249710797315e-05,
      "loss": 0.2273,
      "step": 80300
    },
    {
      "epoch": 0.5222644450940271,
      "grad_norm": 1.5311936140060425,
      "learning_rate": 4.763558664954716e-05,
      "loss": 0.2241,
      "step": 80400
    },
    {
      "epoch": 0.5229140277371789,
      "grad_norm": 1.3735966682434082,
      "learning_rate": 4.762867619112117e-05,
      "loss": 0.2258,
      "step": 80500
    },
    {
      "epoch": 0.5235636103803306,
      "grad_norm": 1.5256980657577515,
      "learning_rate": 4.762176573269518e-05,
      "loss": 0.2353,
      "step": 80600
    },
    {
      "epoch": 0.5242131930234825,
      "grad_norm": 2.1783535480499268,
      "learning_rate": 4.761485527426919e-05,
      "loss": 0.2234,
      "step": 80700
    },
    {
      "epoch": 0.5248627756666342,
      "grad_norm": 2.2376322746276855,
      "learning_rate": 4.76079448158432e-05,
      "loss": 0.2318,
      "step": 80800
    },
    {
      "epoch": 0.5255123583097859,
      "grad_norm": 1.4373682737350464,
      "learning_rate": 4.760103435741721e-05,
      "loss": 0.2232,
      "step": 80900
    },
    {
      "epoch": 0.5261619409529378,
      "grad_norm": 1.3277003765106201,
      "learning_rate": 4.759412389899121e-05,
      "loss": 0.2328,
      "step": 81000
    },
    {
      "epoch": 0.5268115235960895,
      "grad_norm": 1.4481399059295654,
      "learning_rate": 4.758721344056522e-05,
      "loss": 0.2341,
      "step": 81100
    },
    {
      "epoch": 0.5274611062392413,
      "grad_norm": 2.067283868789673,
      "learning_rate": 4.758030298213923e-05,
      "loss": 0.2455,
      "step": 81200
    },
    {
      "epoch": 0.5281106888823931,
      "grad_norm": 2.0312488079071045,
      "learning_rate": 4.757339252371324e-05,
      "loss": 0.2264,
      "step": 81300
    },
    {
      "epoch": 0.5287602715255448,
      "grad_norm": 1.5040594339370728,
      "learning_rate": 4.756648206528725e-05,
      "loss": 0.2345,
      "step": 81400
    },
    {
      "epoch": 0.5294098541686966,
      "grad_norm": 1.8741683959960938,
      "learning_rate": 4.755957160686126e-05,
      "loss": 0.2288,
      "step": 81500
    },
    {
      "epoch": 0.5300594368118484,
      "grad_norm": 1.8442437648773193,
      "learning_rate": 4.755266114843527e-05,
      "loss": 0.2219,
      "step": 81600
    },
    {
      "epoch": 0.5307090194550002,
      "grad_norm": 1.4268993139266968,
      "learning_rate": 4.754575069000928e-05,
      "loss": 0.2294,
      "step": 81700
    },
    {
      "epoch": 0.5313586020981519,
      "grad_norm": 1.4904803037643433,
      "learning_rate": 4.753884023158329e-05,
      "loss": 0.2309,
      "step": 81800
    },
    {
      "epoch": 0.5320081847413037,
      "grad_norm": 1.4377573728561401,
      "learning_rate": 4.75319297731573e-05,
      "loss": 0.2242,
      "step": 81900
    },
    {
      "epoch": 0.5326577673844555,
      "grad_norm": 1.771943211555481,
      "learning_rate": 4.75250193147313e-05,
      "loss": 0.2204,
      "step": 82000
    },
    {
      "epoch": 0.5333073500276072,
      "grad_norm": 1.9501956701278687,
      "learning_rate": 4.751810885630531e-05,
      "loss": 0.2221,
      "step": 82100
    },
    {
      "epoch": 0.533956932670759,
      "grad_norm": 1.8004884719848633,
      "learning_rate": 4.751119839787932e-05,
      "loss": 0.2306,
      "step": 82200
    },
    {
      "epoch": 0.5346065153139108,
      "grad_norm": 1.1375937461853027,
      "learning_rate": 4.750428793945333e-05,
      "loss": 0.2302,
      "step": 82300
    },
    {
      "epoch": 0.5352560979570626,
      "grad_norm": 1.7245628833770752,
      "learning_rate": 4.749737748102734e-05,
      "loss": 0.2348,
      "step": 82400
    },
    {
      "epoch": 0.5359056806002144,
      "grad_norm": 1.6837247610092163,
      "learning_rate": 4.749046702260135e-05,
      "loss": 0.2322,
      "step": 82500
    },
    {
      "epoch": 0.5365552632433661,
      "grad_norm": 1.700613021850586,
      "learning_rate": 4.7483556564175355e-05,
      "loss": 0.2268,
      "step": 82600
    },
    {
      "epoch": 0.537204845886518,
      "grad_norm": 1.5740423202514648,
      "learning_rate": 4.7476646105749364e-05,
      "loss": 0.2182,
      "step": 82700
    },
    {
      "epoch": 0.5378544285296697,
      "grad_norm": 2.0274219512939453,
      "learning_rate": 4.7469735647323374e-05,
      "loss": 0.2319,
      "step": 82800
    },
    {
      "epoch": 0.5385040111728214,
      "grad_norm": 1.8642243146896362,
      "learning_rate": 4.7462825188897384e-05,
      "loss": 0.2422,
      "step": 82900
    },
    {
      "epoch": 0.5391535938159733,
      "grad_norm": 1.5056895017623901,
      "learning_rate": 4.745591473047139e-05,
      "loss": 0.2317,
      "step": 83000
    },
    {
      "epoch": 0.539803176459125,
      "grad_norm": 2.231792688369751,
      "learning_rate": 4.74490042720454e-05,
      "loss": 0.225,
      "step": 83100
    },
    {
      "epoch": 0.5404527591022767,
      "grad_norm": 1.6046935319900513,
      "learning_rate": 4.744209381361941e-05,
      "loss": 0.2277,
      "step": 83200
    },
    {
      "epoch": 0.5411023417454286,
      "grad_norm": 1.8888559341430664,
      "learning_rate": 4.743518335519342e-05,
      "loss": 0.2134,
      "step": 83300
    },
    {
      "epoch": 0.5417519243885803,
      "grad_norm": 1.504720687866211,
      "learning_rate": 4.7428272896767427e-05,
      "loss": 0.2143,
      "step": 83400
    },
    {
      "epoch": 0.5424015070317321,
      "grad_norm": 1.6423102617263794,
      "learning_rate": 4.7421362438341436e-05,
      "loss": 0.2218,
      "step": 83500
    },
    {
      "epoch": 0.5430510896748839,
      "grad_norm": 1.483358383178711,
      "learning_rate": 4.7414451979915446e-05,
      "loss": 0.22,
      "step": 83600
    },
    {
      "epoch": 0.5437006723180356,
      "grad_norm": 1.513951301574707,
      "learning_rate": 4.7407541521489456e-05,
      "loss": 0.2227,
      "step": 83700
    },
    {
      "epoch": 0.5443502549611874,
      "grad_norm": 1.8005791902542114,
      "learning_rate": 4.7400631063063466e-05,
      "loss": 0.2267,
      "step": 83800
    },
    {
      "epoch": 0.5449998376043392,
      "grad_norm": 1.564375877380371,
      "learning_rate": 4.739372060463747e-05,
      "loss": 0.2255,
      "step": 83900
    },
    {
      "epoch": 0.545649420247491,
      "grad_norm": 2.037313222885132,
      "learning_rate": 4.738681014621148e-05,
      "loss": 0.2238,
      "step": 84000
    },
    {
      "epoch": 0.5462990028906428,
      "grad_norm": 1.583945870399475,
      "learning_rate": 4.737989968778549e-05,
      "loss": 0.2262,
      "step": 84100
    },
    {
      "epoch": 0.5469485855337946,
      "grad_norm": 1.5779156684875488,
      "learning_rate": 4.73729892293595e-05,
      "loss": 0.2283,
      "step": 84200
    },
    {
      "epoch": 0.5475981681769463,
      "grad_norm": 1.950927734375,
      "learning_rate": 4.736607877093351e-05,
      "loss": 0.2226,
      "step": 84300
    },
    {
      "epoch": 0.5482477508200981,
      "grad_norm": 1.4168487787246704,
      "learning_rate": 4.735916831250752e-05,
      "loss": 0.2388,
      "step": 84400
    },
    {
      "epoch": 0.5488973334632499,
      "grad_norm": 1.414910078048706,
      "learning_rate": 4.735225785408153e-05,
      "loss": 0.2213,
      "step": 84500
    },
    {
      "epoch": 0.5495469161064016,
      "grad_norm": 1.627864122390747,
      "learning_rate": 4.734534739565554e-05,
      "loss": 0.2265,
      "step": 84600
    },
    {
      "epoch": 0.5501964987495535,
      "grad_norm": 1.542304515838623,
      "learning_rate": 4.733843693722955e-05,
      "loss": 0.2199,
      "step": 84700
    },
    {
      "epoch": 0.5508460813927052,
      "grad_norm": 1.5633180141448975,
      "learning_rate": 4.733152647880355e-05,
      "loss": 0.2159,
      "step": 84800
    },
    {
      "epoch": 0.5514956640358569,
      "grad_norm": 1.6334850788116455,
      "learning_rate": 4.732461602037756e-05,
      "loss": 0.2228,
      "step": 84900
    },
    {
      "epoch": 0.5521452466790088,
      "grad_norm": 1.2437268495559692,
      "learning_rate": 4.731770556195157e-05,
      "loss": 0.2204,
      "step": 85000
    },
    {
      "epoch": 0.5527948293221605,
      "grad_norm": 1.912618637084961,
      "learning_rate": 4.731079510352558e-05,
      "loss": 0.2222,
      "step": 85100
    },
    {
      "epoch": 0.5534444119653122,
      "grad_norm": 1.702429175376892,
      "learning_rate": 4.730388464509959e-05,
      "loss": 0.2261,
      "step": 85200
    },
    {
      "epoch": 0.5540939946084641,
      "grad_norm": 1.5347251892089844,
      "learning_rate": 4.72969741866736e-05,
      "loss": 0.2175,
      "step": 85300
    },
    {
      "epoch": 0.5547435772516158,
      "grad_norm": 1.686437964439392,
      "learning_rate": 4.729006372824761e-05,
      "loss": 0.2217,
      "step": 85400
    },
    {
      "epoch": 0.5553931598947676,
      "grad_norm": 1.5291438102722168,
      "learning_rate": 4.728315326982162e-05,
      "loss": 0.2256,
      "step": 85500
    },
    {
      "epoch": 0.5560427425379194,
      "grad_norm": 1.8434051275253296,
      "learning_rate": 4.727624281139563e-05,
      "loss": 0.2191,
      "step": 85600
    },
    {
      "epoch": 0.5566923251810711,
      "grad_norm": 1.1313083171844482,
      "learning_rate": 4.726933235296963e-05,
      "loss": 0.2124,
      "step": 85700
    },
    {
      "epoch": 0.5573419078242229,
      "grad_norm": 1.834615707397461,
      "learning_rate": 4.726242189454364e-05,
      "loss": 0.2182,
      "step": 85800
    },
    {
      "epoch": 0.5579914904673747,
      "grad_norm": 1.5329086780548096,
      "learning_rate": 4.725551143611765e-05,
      "loss": 0.2293,
      "step": 85900
    },
    {
      "epoch": 0.5586410731105265,
      "grad_norm": 1.7010257244110107,
      "learning_rate": 4.724860097769166e-05,
      "loss": 0.2269,
      "step": 86000
    },
    {
      "epoch": 0.5592906557536783,
      "grad_norm": 1.4881367683410645,
      "learning_rate": 4.724169051926567e-05,
      "loss": 0.2305,
      "step": 86100
    },
    {
      "epoch": 0.55994023839683,
      "grad_norm": 1.8973270654678345,
      "learning_rate": 4.7234780060839675e-05,
      "loss": 0.2193,
      "step": 86200
    },
    {
      "epoch": 0.5605898210399818,
      "grad_norm": 1.8356298208236694,
      "learning_rate": 4.7227869602413685e-05,
      "loss": 0.209,
      "step": 86300
    },
    {
      "epoch": 0.5612394036831336,
      "grad_norm": 1.8853799104690552,
      "learning_rate": 4.7220959143987695e-05,
      "loss": 0.2222,
      "step": 86400
    },
    {
      "epoch": 0.5618889863262854,
      "grad_norm": 1.457207202911377,
      "learning_rate": 4.7214048685561705e-05,
      "loss": 0.2294,
      "step": 86500
    },
    {
      "epoch": 0.5625385689694371,
      "grad_norm": 1.8167152404785156,
      "learning_rate": 4.7207138227135715e-05,
      "loss": 0.2202,
      "step": 86600
    },
    {
      "epoch": 0.563188151612589,
      "grad_norm": 1.6653258800506592,
      "learning_rate": 4.720022776870972e-05,
      "loss": 0.2311,
      "step": 86700
    },
    {
      "epoch": 0.5638377342557407,
      "grad_norm": 1.9521903991699219,
      "learning_rate": 4.719331731028373e-05,
      "loss": 0.2191,
      "step": 86800
    },
    {
      "epoch": 0.5644873168988924,
      "grad_norm": 1.7167932987213135,
      "learning_rate": 4.718640685185774e-05,
      "loss": 0.2136,
      "step": 86900
    },
    {
      "epoch": 0.5651368995420443,
      "grad_norm": 1.7761942148208618,
      "learning_rate": 4.717949639343175e-05,
      "loss": 0.2177,
      "step": 87000
    },
    {
      "epoch": 0.565786482185196,
      "grad_norm": 2.2807083129882812,
      "learning_rate": 4.717258593500576e-05,
      "loss": 0.2219,
      "step": 87100
    },
    {
      "epoch": 0.5664360648283477,
      "grad_norm": 1.5253422260284424,
      "learning_rate": 4.716567547657977e-05,
      "loss": 0.2155,
      "step": 87200
    },
    {
      "epoch": 0.5670856474714996,
      "grad_norm": 1.7075300216674805,
      "learning_rate": 4.715876501815378e-05,
      "loss": 0.214,
      "step": 87300
    },
    {
      "epoch": 0.5677352301146513,
      "grad_norm": 1.5355815887451172,
      "learning_rate": 4.715185455972779e-05,
      "loss": 0.2157,
      "step": 87400
    },
    {
      "epoch": 0.5683848127578031,
      "grad_norm": 1.2490322589874268,
      "learning_rate": 4.71449441013018e-05,
      "loss": 0.2157,
      "step": 87500
    },
    {
      "epoch": 0.5690343954009549,
      "grad_norm": 1.41506028175354,
      "learning_rate": 4.71380336428758e-05,
      "loss": 0.2175,
      "step": 87600
    },
    {
      "epoch": 0.5696839780441066,
      "grad_norm": 1.6283016204833984,
      "learning_rate": 4.713112318444981e-05,
      "loss": 0.2208,
      "step": 87700
    },
    {
      "epoch": 0.5703335606872585,
      "grad_norm": 1.9654691219329834,
      "learning_rate": 4.712421272602382e-05,
      "loss": 0.2233,
      "step": 87800
    },
    {
      "epoch": 0.5709831433304102,
      "grad_norm": 1.8969477415084839,
      "learning_rate": 4.711730226759783e-05,
      "loss": 0.2178,
      "step": 87900
    },
    {
      "epoch": 0.571632725973562,
      "grad_norm": 2.057020902633667,
      "learning_rate": 4.711039180917184e-05,
      "loss": 0.2101,
      "step": 88000
    },
    {
      "epoch": 0.5722823086167138,
      "grad_norm": 1.4062060117721558,
      "learning_rate": 4.710348135074585e-05,
      "loss": 0.2194,
      "step": 88100
    },
    {
      "epoch": 0.5729318912598655,
      "grad_norm": 1.8208718299865723,
      "learning_rate": 4.709657089231986e-05,
      "loss": 0.2255,
      "step": 88200
    },
    {
      "epoch": 0.5735814739030173,
      "grad_norm": 1.6467753648757935,
      "learning_rate": 4.708966043389387e-05,
      "loss": 0.2221,
      "step": 88300
    },
    {
      "epoch": 0.5742310565461691,
      "grad_norm": 1.7184151411056519,
      "learning_rate": 4.708274997546788e-05,
      "loss": 0.2242,
      "step": 88400
    },
    {
      "epoch": 0.5748806391893209,
      "grad_norm": 1.3527592420578003,
      "learning_rate": 4.707583951704189e-05,
      "loss": 0.2242,
      "step": 88500
    },
    {
      "epoch": 0.5755302218324726,
      "grad_norm": 1.586026906967163,
      "learning_rate": 4.706892905861589e-05,
      "loss": 0.2259,
      "step": 88600
    },
    {
      "epoch": 0.5761798044756244,
      "grad_norm": 1.1168148517608643,
      "learning_rate": 4.70620186001899e-05,
      "loss": 0.218,
      "step": 88700
    },
    {
      "epoch": 0.5768293871187762,
      "grad_norm": 2.234581232070923,
      "learning_rate": 4.705510814176391e-05,
      "loss": 0.2214,
      "step": 88800
    },
    {
      "epoch": 0.5774789697619279,
      "grad_norm": 1.4417730569839478,
      "learning_rate": 4.704819768333792e-05,
      "loss": 0.2092,
      "step": 88900
    },
    {
      "epoch": 0.5781285524050798,
      "grad_norm": 1.680384874343872,
      "learning_rate": 4.704128722491193e-05,
      "loss": 0.2161,
      "step": 89000
    },
    {
      "epoch": 0.5787781350482315,
      "grad_norm": 1.5591800212860107,
      "learning_rate": 4.703437676648594e-05,
      "loss": 0.2174,
      "step": 89100
    },
    {
      "epoch": 0.5794277176913832,
      "grad_norm": 1.4465826749801636,
      "learning_rate": 4.702746630805995e-05,
      "loss": 0.2093,
      "step": 89200
    },
    {
      "epoch": 0.5800773003345351,
      "grad_norm": 1.3487051725387573,
      "learning_rate": 4.702055584963396e-05,
      "loss": 0.2165,
      "step": 89300
    },
    {
      "epoch": 0.5807268829776868,
      "grad_norm": 1.7583723068237305,
      "learning_rate": 4.7013645391207963e-05,
      "loss": 0.2174,
      "step": 89400
    },
    {
      "epoch": 0.5813764656208386,
      "grad_norm": 2.023448944091797,
      "learning_rate": 4.700673493278197e-05,
      "loss": 0.212,
      "step": 89500
    },
    {
      "epoch": 0.5820260482639904,
      "grad_norm": 2.096703052520752,
      "learning_rate": 4.699982447435598e-05,
      "loss": 0.2099,
      "step": 89600
    },
    {
      "epoch": 0.5826756309071421,
      "grad_norm": 1.4793132543563843,
      "learning_rate": 4.6992914015929986e-05,
      "loss": 0.2228,
      "step": 89700
    },
    {
      "epoch": 0.583325213550294,
      "grad_norm": 1.118242621421814,
      "learning_rate": 4.6986003557503996e-05,
      "loss": 0.2149,
      "step": 89800
    },
    {
      "epoch": 0.5839747961934457,
      "grad_norm": 2.056469678878784,
      "learning_rate": 4.6979093099078006e-05,
      "loss": 0.2123,
      "step": 89900
    },
    {
      "epoch": 0.5846243788365975,
      "grad_norm": 1.4552366733551025,
      "learning_rate": 4.6972182640652016e-05,
      "loss": 0.2147,
      "step": 90000
    },
    {
      "epoch": 0.5852739614797493,
      "grad_norm": 1.6712132692337036,
      "learning_rate": 4.6965272182226026e-05,
      "loss": 0.2065,
      "step": 90100
    },
    {
      "epoch": 0.585923544122901,
      "grad_norm": 1.858074426651001,
      "learning_rate": 4.6958361723800036e-05,
      "loss": 0.2201,
      "step": 90200
    },
    {
      "epoch": 0.5865731267660528,
      "grad_norm": 2.2440011501312256,
      "learning_rate": 4.6951451265374045e-05,
      "loss": 0.2133,
      "step": 90300
    },
    {
      "epoch": 0.5872227094092046,
      "grad_norm": 1.8831292390823364,
      "learning_rate": 4.6944540806948055e-05,
      "loss": 0.2164,
      "step": 90400
    },
    {
      "epoch": 0.5878722920523564,
      "grad_norm": 1.4486743211746216,
      "learning_rate": 4.693763034852206e-05,
      "loss": 0.2141,
      "step": 90500
    },
    {
      "epoch": 0.5885218746955081,
      "grad_norm": 1.3013542890548706,
      "learning_rate": 4.693071989009607e-05,
      "loss": 0.2142,
      "step": 90600
    },
    {
      "epoch": 0.5891714573386599,
      "grad_norm": 1.4909512996673584,
      "learning_rate": 4.692380943167008e-05,
      "loss": 0.2181,
      "step": 90700
    },
    {
      "epoch": 0.5898210399818117,
      "grad_norm": 1.6618915796279907,
      "learning_rate": 4.691689897324409e-05,
      "loss": 0.223,
      "step": 90800
    },
    {
      "epoch": 0.5904706226249634,
      "grad_norm": 1.5637733936309814,
      "learning_rate": 4.69099885148181e-05,
      "loss": 0.2167,
      "step": 90900
    },
    {
      "epoch": 0.5911202052681153,
      "grad_norm": 1.2466330528259277,
      "learning_rate": 4.690307805639211e-05,
      "loss": 0.2192,
      "step": 91000
    },
    {
      "epoch": 0.591769787911267,
      "grad_norm": 1.3236597776412964,
      "learning_rate": 4.689616759796612e-05,
      "loss": 0.1955,
      "step": 91100
    },
    {
      "epoch": 0.5924193705544187,
      "grad_norm": 1.8040188550949097,
      "learning_rate": 4.688925713954013e-05,
      "loss": 0.2195,
      "step": 91200
    },
    {
      "epoch": 0.5930689531975706,
      "grad_norm": 1.4913917779922485,
      "learning_rate": 4.688234668111414e-05,
      "loss": 0.2117,
      "step": 91300
    },
    {
      "epoch": 0.5937185358407223,
      "grad_norm": 2.032496213912964,
      "learning_rate": 4.687543622268814e-05,
      "loss": 0.2187,
      "step": 91400
    },
    {
      "epoch": 0.5943681184838742,
      "grad_norm": 1.175683617591858,
      "learning_rate": 4.686852576426215e-05,
      "loss": 0.2108,
      "step": 91500
    },
    {
      "epoch": 0.5950177011270259,
      "grad_norm": 2.1653554439544678,
      "learning_rate": 4.686161530583616e-05,
      "loss": 0.2194,
      "step": 91600
    },
    {
      "epoch": 0.5956672837701776,
      "grad_norm": 1.7269601821899414,
      "learning_rate": 4.685470484741017e-05,
      "loss": 0.2188,
      "step": 91700
    },
    {
      "epoch": 0.5963168664133295,
      "grad_norm": 1.7859504222869873,
      "learning_rate": 4.684779438898418e-05,
      "loss": 0.2095,
      "step": 91800
    },
    {
      "epoch": 0.5969664490564812,
      "grad_norm": 1.503265619277954,
      "learning_rate": 4.684088393055819e-05,
      "loss": 0.2156,
      "step": 91900
    },
    {
      "epoch": 0.597616031699633,
      "grad_norm": 1.557062029838562,
      "learning_rate": 4.68339734721322e-05,
      "loss": 0.2255,
      "step": 92000
    },
    {
      "epoch": 0.5982656143427848,
      "grad_norm": 1.8777798414230347,
      "learning_rate": 4.682706301370621e-05,
      "loss": 0.2063,
      "step": 92100
    },
    {
      "epoch": 0.5989151969859365,
      "grad_norm": 1.5119593143463135,
      "learning_rate": 4.682015255528022e-05,
      "loss": 0.2106,
      "step": 92200
    },
    {
      "epoch": 0.5995647796290883,
      "grad_norm": 1.2696548700332642,
      "learning_rate": 4.681324209685422e-05,
      "loss": 0.2176,
      "step": 92300
    },
    {
      "epoch": 0.6002143622722401,
      "grad_norm": 1.2392884492874146,
      "learning_rate": 4.680633163842823e-05,
      "loss": 0.2093,
      "step": 92400
    },
    {
      "epoch": 0.6008639449153919,
      "grad_norm": 1.8537031412124634,
      "learning_rate": 4.679942118000224e-05,
      "loss": 0.2074,
      "step": 92500
    },
    {
      "epoch": 0.6015135275585436,
      "grad_norm": 1.5066086053848267,
      "learning_rate": 4.679251072157625e-05,
      "loss": 0.2104,
      "step": 92600
    },
    {
      "epoch": 0.6021631102016954,
      "grad_norm": 1.4600898027420044,
      "learning_rate": 4.678560026315026e-05,
      "loss": 0.2116,
      "step": 92700
    },
    {
      "epoch": 0.6028126928448472,
      "grad_norm": 1.7586231231689453,
      "learning_rate": 4.677868980472427e-05,
      "loss": 0.209,
      "step": 92800
    },
    {
      "epoch": 0.6034622754879989,
      "grad_norm": 1.3919411897659302,
      "learning_rate": 4.677177934629828e-05,
      "loss": 0.2121,
      "step": 92900
    },
    {
      "epoch": 0.6041118581311508,
      "grad_norm": 1.2799303531646729,
      "learning_rate": 4.6764868887872284e-05,
      "loss": 0.2081,
      "step": 93000
    },
    {
      "epoch": 0.6047614407743025,
      "grad_norm": 1.9336848258972168,
      "learning_rate": 4.6757958429446294e-05,
      "loss": 0.2073,
      "step": 93100
    },
    {
      "epoch": 0.6054110234174542,
      "grad_norm": 1.6657326221466064,
      "learning_rate": 4.6751047971020304e-05,
      "loss": 0.2103,
      "step": 93200
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 1.5813279151916504,
      "learning_rate": 4.674413751259431e-05,
      "loss": 0.2157,
      "step": 93300
    },
    {
      "epoch": 0.6067101887037578,
      "grad_norm": 1.8853272199630737,
      "learning_rate": 4.673722705416832e-05,
      "loss": 0.2118,
      "step": 93400
    },
    {
      "epoch": 0.6073597713469097,
      "grad_norm": 1.5987422466278076,
      "learning_rate": 4.673031659574233e-05,
      "loss": 0.2038,
      "step": 93500
    },
    {
      "epoch": 0.6080093539900614,
      "grad_norm": 1.4549744129180908,
      "learning_rate": 4.672340613731634e-05,
      "loss": 0.2146,
      "step": 93600
    },
    {
      "epoch": 0.6086589366332131,
      "grad_norm": 1.6839274168014526,
      "learning_rate": 4.6716495678890346e-05,
      "loss": 0.2035,
      "step": 93700
    },
    {
      "epoch": 0.609308519276365,
      "grad_norm": 1.6116492748260498,
      "learning_rate": 4.6709585220464356e-05,
      "loss": 0.221,
      "step": 93800
    },
    {
      "epoch": 0.6099581019195167,
      "grad_norm": 1.388478398323059,
      "learning_rate": 4.6702674762038366e-05,
      "loss": 0.2157,
      "step": 93900
    },
    {
      "epoch": 0.6106076845626685,
      "grad_norm": 1.7800463438034058,
      "learning_rate": 4.6695764303612376e-05,
      "loss": 0.2123,
      "step": 94000
    },
    {
      "epoch": 0.6112572672058203,
      "grad_norm": 1.4755131006240845,
      "learning_rate": 4.6688853845186386e-05,
      "loss": 0.2042,
      "step": 94100
    },
    {
      "epoch": 0.611906849848972,
      "grad_norm": 1.4976950883865356,
      "learning_rate": 4.668194338676039e-05,
      "loss": 0.2042,
      "step": 94200
    },
    {
      "epoch": 0.6125564324921238,
      "grad_norm": 1.5298731327056885,
      "learning_rate": 4.66750329283344e-05,
      "loss": 0.2038,
      "step": 94300
    },
    {
      "epoch": 0.6132060151352756,
      "grad_norm": 1.6255234479904175,
      "learning_rate": 4.666812246990841e-05,
      "loss": 0.2048,
      "step": 94400
    },
    {
      "epoch": 0.6138555977784274,
      "grad_norm": 1.5452152490615845,
      "learning_rate": 4.666121201148242e-05,
      "loss": 0.2101,
      "step": 94500
    },
    {
      "epoch": 0.6145051804215791,
      "grad_norm": 2.0252346992492676,
      "learning_rate": 4.665430155305643e-05,
      "loss": 0.2158,
      "step": 94600
    },
    {
      "epoch": 0.6151547630647309,
      "grad_norm": 1.955635905265808,
      "learning_rate": 4.664739109463044e-05,
      "loss": 0.2035,
      "step": 94700
    },
    {
      "epoch": 0.6158043457078827,
      "grad_norm": 1.529523253440857,
      "learning_rate": 4.664048063620445e-05,
      "loss": 0.2007,
      "step": 94800
    },
    {
      "epoch": 0.6164539283510344,
      "grad_norm": 1.8106240034103394,
      "learning_rate": 4.663357017777846e-05,
      "loss": 0.2142,
      "step": 94900
    },
    {
      "epoch": 0.6171035109941863,
      "grad_norm": 1.6845667362213135,
      "learning_rate": 4.662665971935247e-05,
      "loss": 0.2107,
      "step": 95000
    },
    {
      "epoch": 0.617753093637338,
      "grad_norm": 1.322607159614563,
      "learning_rate": 4.661974926092648e-05,
      "loss": 0.208,
      "step": 95100
    },
    {
      "epoch": 0.6184026762804898,
      "grad_norm": 1.6676989793777466,
      "learning_rate": 4.661283880250048e-05,
      "loss": 0.2035,
      "step": 95200
    },
    {
      "epoch": 0.6190522589236416,
      "grad_norm": 1.9114911556243896,
      "learning_rate": 4.660592834407449e-05,
      "loss": 0.2111,
      "step": 95300
    },
    {
      "epoch": 0.6197018415667933,
      "grad_norm": 1.1988368034362793,
      "learning_rate": 4.65990178856485e-05,
      "loss": 0.209,
      "step": 95400
    },
    {
      "epoch": 0.6203514242099452,
      "grad_norm": 1.303821086883545,
      "learning_rate": 4.659210742722251e-05,
      "loss": 0.213,
      "step": 95500
    },
    {
      "epoch": 0.6210010068530969,
      "grad_norm": 1.808588981628418,
      "learning_rate": 4.658519696879652e-05,
      "loss": 0.2164,
      "step": 95600
    },
    {
      "epoch": 0.6216505894962486,
      "grad_norm": 1.5281363725662231,
      "learning_rate": 4.657828651037053e-05,
      "loss": 0.2158,
      "step": 95700
    },
    {
      "epoch": 0.6223001721394005,
      "grad_norm": 1.7158740758895874,
      "learning_rate": 4.657137605194454e-05,
      "loss": 0.2068,
      "step": 95800
    },
    {
      "epoch": 0.6229497547825522,
      "grad_norm": 1.6921944618225098,
      "learning_rate": 4.656446559351855e-05,
      "loss": 0.2033,
      "step": 95900
    },
    {
      "epoch": 0.623599337425704,
      "grad_norm": 1.9284875392913818,
      "learning_rate": 4.655755513509256e-05,
      "loss": 0.218,
      "step": 96000
    },
    {
      "epoch": 0.6242489200688558,
      "grad_norm": 1.7528951168060303,
      "learning_rate": 4.655064467666656e-05,
      "loss": 0.2091,
      "step": 96100
    },
    {
      "epoch": 0.6248985027120075,
      "grad_norm": 1.711923360824585,
      "learning_rate": 4.654373421824057e-05,
      "loss": 0.2117,
      "step": 96200
    },
    {
      "epoch": 0.6255480853551593,
      "grad_norm": 1.2987542152404785,
      "learning_rate": 4.653682375981458e-05,
      "loss": 0.2111,
      "step": 96300
    },
    {
      "epoch": 0.6261976679983111,
      "grad_norm": 1.57215416431427,
      "learning_rate": 4.652991330138859e-05,
      "loss": 0.2027,
      "step": 96400
    },
    {
      "epoch": 0.6268472506414628,
      "grad_norm": 1.8583186864852905,
      "learning_rate": 4.6523002842962595e-05,
      "loss": 0.2098,
      "step": 96500
    },
    {
      "epoch": 0.6274968332846146,
      "grad_norm": 1.3113043308258057,
      "learning_rate": 4.6516092384536605e-05,
      "loss": 0.2052,
      "step": 96600
    },
    {
      "epoch": 0.6281464159277664,
      "grad_norm": 1.7916278839111328,
      "learning_rate": 4.6509181926110615e-05,
      "loss": 0.2104,
      "step": 96700
    },
    {
      "epoch": 0.6287959985709182,
      "grad_norm": 1.6142255067825317,
      "learning_rate": 4.6502271467684625e-05,
      "loss": 0.2111,
      "step": 96800
    },
    {
      "epoch": 0.6294455812140699,
      "grad_norm": 1.6266392469406128,
      "learning_rate": 4.6495361009258635e-05,
      "loss": 0.2076,
      "step": 96900
    },
    {
      "epoch": 0.6300951638572218,
      "grad_norm": 1.737156867980957,
      "learning_rate": 4.648845055083264e-05,
      "loss": 0.2093,
      "step": 97000
    },
    {
      "epoch": 0.6307447465003735,
      "grad_norm": 1.9735653400421143,
      "learning_rate": 4.648154009240665e-05,
      "loss": 0.2121,
      "step": 97100
    },
    {
      "epoch": 0.6313943291435253,
      "grad_norm": 1.4302407503128052,
      "learning_rate": 4.647462963398066e-05,
      "loss": 0.2076,
      "step": 97200
    },
    {
      "epoch": 0.6320439117866771,
      "grad_norm": 1.5941832065582275,
      "learning_rate": 4.646771917555467e-05,
      "loss": 0.2079,
      "step": 97300
    },
    {
      "epoch": 0.6326934944298288,
      "grad_norm": 1.5469990968704224,
      "learning_rate": 4.646080871712868e-05,
      "loss": 0.2046,
      "step": 97400
    },
    {
      "epoch": 0.6333430770729807,
      "grad_norm": 1.5460549592971802,
      "learning_rate": 4.645389825870269e-05,
      "loss": 0.2051,
      "step": 97500
    },
    {
      "epoch": 0.6339926597161324,
      "grad_norm": 1.2972548007965088,
      "learning_rate": 4.64469878002767e-05,
      "loss": 0.2149,
      "step": 97600
    },
    {
      "epoch": 0.6346422423592841,
      "grad_norm": 1.1878255605697632,
      "learning_rate": 4.644007734185071e-05,
      "loss": 0.2135,
      "step": 97700
    },
    {
      "epoch": 0.635291825002436,
      "grad_norm": 1.3358508348464966,
      "learning_rate": 4.6433166883424717e-05,
      "loss": 0.2045,
      "step": 97800
    },
    {
      "epoch": 0.6359414076455877,
      "grad_norm": 1.8742183446884155,
      "learning_rate": 4.6426256424998726e-05,
      "loss": 0.2055,
      "step": 97900
    },
    {
      "epoch": 0.6365909902887394,
      "grad_norm": 1.8325976133346558,
      "learning_rate": 4.641934596657273e-05,
      "loss": 0.1983,
      "step": 98000
    },
    {
      "epoch": 0.6372405729318913,
      "grad_norm": 1.7006791830062866,
      "learning_rate": 4.641243550814674e-05,
      "loss": 0.2096,
      "step": 98100
    },
    {
      "epoch": 0.637890155575043,
      "grad_norm": 1.7795475721359253,
      "learning_rate": 4.640552504972075e-05,
      "loss": 0.2048,
      "step": 98200
    },
    {
      "epoch": 0.6385397382181948,
      "grad_norm": 1.8217835426330566,
      "learning_rate": 4.639861459129476e-05,
      "loss": 0.2076,
      "step": 98300
    },
    {
      "epoch": 0.6391893208613466,
      "grad_norm": 1.284616470336914,
      "learning_rate": 4.639170413286877e-05,
      "loss": 0.2123,
      "step": 98400
    },
    {
      "epoch": 0.6398389035044983,
      "grad_norm": 1.3660863637924194,
      "learning_rate": 4.638479367444278e-05,
      "loss": 0.2105,
      "step": 98500
    },
    {
      "epoch": 0.6404884861476501,
      "grad_norm": 1.5782040357589722,
      "learning_rate": 4.637788321601679e-05,
      "loss": 0.2128,
      "step": 98600
    },
    {
      "epoch": 0.6411380687908019,
      "grad_norm": 1.9789890050888062,
      "learning_rate": 4.63709727575908e-05,
      "loss": 0.1932,
      "step": 98700
    },
    {
      "epoch": 0.6417876514339537,
      "grad_norm": 1.5267891883850098,
      "learning_rate": 4.636406229916481e-05,
      "loss": 0.2075,
      "step": 98800
    },
    {
      "epoch": 0.6424372340771055,
      "grad_norm": 1.8716477155685425,
      "learning_rate": 4.635715184073881e-05,
      "loss": 0.1991,
      "step": 98900
    },
    {
      "epoch": 0.6430868167202572,
      "grad_norm": 1.3444714546203613,
      "learning_rate": 4.635024138231282e-05,
      "loss": 0.2003,
      "step": 99000
    },
    {
      "epoch": 0.643736399363409,
      "grad_norm": 1.5763978958129883,
      "learning_rate": 4.634333092388683e-05,
      "loss": 0.2098,
      "step": 99100
    },
    {
      "epoch": 0.6443859820065608,
      "grad_norm": 1.398982048034668,
      "learning_rate": 4.633642046546084e-05,
      "loss": 0.2039,
      "step": 99200
    },
    {
      "epoch": 0.6450355646497126,
      "grad_norm": 1.4097962379455566,
      "learning_rate": 4.632951000703485e-05,
      "loss": 0.2071,
      "step": 99300
    },
    {
      "epoch": 0.6456851472928643,
      "grad_norm": 1.438581109046936,
      "learning_rate": 4.632259954860886e-05,
      "loss": 0.2131,
      "step": 99400
    },
    {
      "epoch": 0.6463347299360162,
      "grad_norm": 1.6739342212677002,
      "learning_rate": 4.631568909018287e-05,
      "loss": 0.1985,
      "step": 99500
    },
    {
      "epoch": 0.6469843125791679,
      "grad_norm": 1.633414387702942,
      "learning_rate": 4.630877863175688e-05,
      "loss": 0.2053,
      "step": 99600
    },
    {
      "epoch": 0.6476338952223196,
      "grad_norm": 1.7343060970306396,
      "learning_rate": 4.630186817333089e-05,
      "loss": 0.2007,
      "step": 99700
    },
    {
      "epoch": 0.6482834778654715,
      "grad_norm": 1.7279785871505737,
      "learning_rate": 4.629495771490489e-05,
      "loss": 0.2053,
      "step": 99800
    },
    {
      "epoch": 0.6489330605086232,
      "grad_norm": 1.5435876846313477,
      "learning_rate": 4.62880472564789e-05,
      "loss": 0.2073,
      "step": 99900
    },
    {
      "epoch": 0.6495826431517749,
      "grad_norm": 1.3582305908203125,
      "learning_rate": 4.628113679805291e-05,
      "loss": 0.2063,
      "step": 100000
    },
    {
      "epoch": 0.6502322257949268,
      "grad_norm": 1.2461352348327637,
      "learning_rate": 4.6274226339626916e-05,
      "loss": 0.2066,
      "step": 100100
    },
    {
      "epoch": 0.6508818084380785,
      "grad_norm": 1.5571370124816895,
      "learning_rate": 4.6267315881200926e-05,
      "loss": 0.1941,
      "step": 100200
    },
    {
      "epoch": 0.6515313910812303,
      "grad_norm": 1.8470232486724854,
      "learning_rate": 4.6260405422774936e-05,
      "loss": 0.2097,
      "step": 100300
    },
    {
      "epoch": 0.6521809737243821,
      "grad_norm": 1.1705836057662964,
      "learning_rate": 4.6253494964348946e-05,
      "loss": 0.1988,
      "step": 100400
    },
    {
      "epoch": 0.6528305563675338,
      "grad_norm": 1.5330150127410889,
      "learning_rate": 4.6246584505922955e-05,
      "loss": 0.2079,
      "step": 100500
    },
    {
      "epoch": 0.6534801390106857,
      "grad_norm": 2.128588914871216,
      "learning_rate": 4.6239674047496965e-05,
      "loss": 0.1991,
      "step": 100600
    },
    {
      "epoch": 0.6541297216538374,
      "grad_norm": 1.2022284269332886,
      "learning_rate": 4.6232763589070975e-05,
      "loss": 0.2079,
      "step": 100700
    },
    {
      "epoch": 0.6547793042969892,
      "grad_norm": 2.179147958755493,
      "learning_rate": 4.622585313064498e-05,
      "loss": 0.2031,
      "step": 100800
    },
    {
      "epoch": 0.655428886940141,
      "grad_norm": 1.264843463897705,
      "learning_rate": 4.621894267221899e-05,
      "loss": 0.1991,
      "step": 100900
    },
    {
      "epoch": 0.6560784695832927,
      "grad_norm": 1.660736322402954,
      "learning_rate": 4.6212032213793e-05,
      "loss": 0.197,
      "step": 101000
    },
    {
      "epoch": 0.6567280522264445,
      "grad_norm": 1.476681113243103,
      "learning_rate": 4.620512175536701e-05,
      "loss": 0.2006,
      "step": 101100
    },
    {
      "epoch": 0.6573776348695963,
      "grad_norm": 2.1654210090637207,
      "learning_rate": 4.619821129694102e-05,
      "loss": 0.2071,
      "step": 101200
    },
    {
      "epoch": 0.6580272175127481,
      "grad_norm": 1.602070689201355,
      "learning_rate": 4.619130083851503e-05,
      "loss": 0.2015,
      "step": 101300
    },
    {
      "epoch": 0.6586768001558998,
      "grad_norm": 1.3359355926513672,
      "learning_rate": 4.618439038008904e-05,
      "loss": 0.1968,
      "step": 101400
    },
    {
      "epoch": 0.6593263827990516,
      "grad_norm": 1.496891975402832,
      "learning_rate": 4.617747992166305e-05,
      "loss": 0.1995,
      "step": 101500
    },
    {
      "epoch": 0.6599759654422034,
      "grad_norm": 1.4541538953781128,
      "learning_rate": 4.617056946323706e-05,
      "loss": 0.1968,
      "step": 101600
    },
    {
      "epoch": 0.6606255480853551,
      "grad_norm": 1.44951331615448,
      "learning_rate": 4.616365900481106e-05,
      "loss": 0.2095,
      "step": 101700
    },
    {
      "epoch": 0.661275130728507,
      "grad_norm": 1.2665555477142334,
      "learning_rate": 4.615674854638507e-05,
      "loss": 0.1958,
      "step": 101800
    },
    {
      "epoch": 0.6619247133716587,
      "grad_norm": 1.4138603210449219,
      "learning_rate": 4.614983808795908e-05,
      "loss": 0.202,
      "step": 101900
    },
    {
      "epoch": 0.6625742960148104,
      "grad_norm": 1.850466012954712,
      "learning_rate": 4.614292762953309e-05,
      "loss": 0.1954,
      "step": 102000
    },
    {
      "epoch": 0.6632238786579623,
      "grad_norm": 1.1631406545639038,
      "learning_rate": 4.61360171711071e-05,
      "loss": 0.2013,
      "step": 102100
    },
    {
      "epoch": 0.663873461301114,
      "grad_norm": 2.0083394050598145,
      "learning_rate": 4.612910671268111e-05,
      "loss": 0.2101,
      "step": 102200
    },
    {
      "epoch": 0.6645230439442658,
      "grad_norm": 1.790936827659607,
      "learning_rate": 4.612219625425512e-05,
      "loss": 0.2049,
      "step": 102300
    },
    {
      "epoch": 0.6651726265874176,
      "grad_norm": 1.852909803390503,
      "learning_rate": 4.611528579582913e-05,
      "loss": 0.1956,
      "step": 102400
    },
    {
      "epoch": 0.6658222092305693,
      "grad_norm": 1.4190630912780762,
      "learning_rate": 4.610837533740314e-05,
      "loss": 0.2002,
      "step": 102500
    },
    {
      "epoch": 0.6664717918737212,
      "grad_norm": 1.5903719663619995,
      "learning_rate": 4.610146487897715e-05,
      "loss": 0.1991,
      "step": 102600
    },
    {
      "epoch": 0.6671213745168729,
      "grad_norm": 1.7504171133041382,
      "learning_rate": 4.609455442055115e-05,
      "loss": 0.2066,
      "step": 102700
    },
    {
      "epoch": 0.6677709571600247,
      "grad_norm": 1.175646185874939,
      "learning_rate": 4.608764396212516e-05,
      "loss": 0.2069,
      "step": 102800
    },
    {
      "epoch": 0.6684205398031765,
      "grad_norm": 1.9048784971237183,
      "learning_rate": 4.608073350369917e-05,
      "loss": 0.2,
      "step": 102900
    },
    {
      "epoch": 0.6690701224463282,
      "grad_norm": 1.1226074695587158,
      "learning_rate": 4.607382304527318e-05,
      "loss": 0.1988,
      "step": 103000
    },
    {
      "epoch": 0.66971970508948,
      "grad_norm": 2.0401077270507812,
      "learning_rate": 4.606691258684719e-05,
      "loss": 0.2017,
      "step": 103100
    },
    {
      "epoch": 0.6703692877326318,
      "grad_norm": 1.520187258720398,
      "learning_rate": 4.60600021284212e-05,
      "loss": 0.201,
      "step": 103200
    },
    {
      "epoch": 0.6710188703757836,
      "grad_norm": 1.2538098096847534,
      "learning_rate": 4.6053091669995204e-05,
      "loss": 0.2072,
      "step": 103300
    },
    {
      "epoch": 0.6716684530189353,
      "grad_norm": 1.5820213556289673,
      "learning_rate": 4.6046181211569214e-05,
      "loss": 0.194,
      "step": 103400
    },
    {
      "epoch": 0.6723180356620871,
      "grad_norm": 1.721042513847351,
      "learning_rate": 4.6039270753143224e-05,
      "loss": 0.2011,
      "step": 103500
    },
    {
      "epoch": 0.6729676183052389,
      "grad_norm": 1.6916321516036987,
      "learning_rate": 4.6032360294717234e-05,
      "loss": 0.2064,
      "step": 103600
    },
    {
      "epoch": 0.6736172009483906,
      "grad_norm": 1.3009949922561646,
      "learning_rate": 4.602544983629124e-05,
      "loss": 0.2022,
      "step": 103700
    },
    {
      "epoch": 0.6742667835915425,
      "grad_norm": 1.1407008171081543,
      "learning_rate": 4.601853937786525e-05,
      "loss": 0.1966,
      "step": 103800
    },
    {
      "epoch": 0.6749163662346942,
      "grad_norm": 1.5799399614334106,
      "learning_rate": 4.6011628919439257e-05,
      "loss": 0.1973,
      "step": 103900
    },
    {
      "epoch": 0.6755659488778459,
      "grad_norm": 1.340712070465088,
      "learning_rate": 4.6004718461013266e-05,
      "loss": 0.2034,
      "step": 104000
    },
    {
      "epoch": 0.6762155315209978,
      "grad_norm": 1.5684401988983154,
      "learning_rate": 4.5997808002587276e-05,
      "loss": 0.1922,
      "step": 104100
    },
    {
      "epoch": 0.6768651141641495,
      "grad_norm": 1.7977848052978516,
      "learning_rate": 4.5990897544161286e-05,
      "loss": 0.1998,
      "step": 104200
    },
    {
      "epoch": 0.6775146968073014,
      "grad_norm": 2.3355607986450195,
      "learning_rate": 4.5983987085735296e-05,
      "loss": 0.1988,
      "step": 104300
    },
    {
      "epoch": 0.6781642794504531,
      "grad_norm": 1.753507137298584,
      "learning_rate": 4.5977076627309306e-05,
      "loss": 0.1963,
      "step": 104400
    },
    {
      "epoch": 0.6788138620936048,
      "grad_norm": 1.4654529094696045,
      "learning_rate": 4.5970166168883316e-05,
      "loss": 0.1979,
      "step": 104500
    },
    {
      "epoch": 0.6794634447367567,
      "grad_norm": 1.5449411869049072,
      "learning_rate": 4.596325571045732e-05,
      "loss": 0.1945,
      "step": 104600
    },
    {
      "epoch": 0.6801130273799084,
      "grad_norm": 1.6236404180526733,
      "learning_rate": 4.595634525203133e-05,
      "loss": 0.2021,
      "step": 104700
    },
    {
      "epoch": 0.6807626100230602,
      "grad_norm": 1.15399169921875,
      "learning_rate": 4.594943479360534e-05,
      "loss": 0.1981,
      "step": 104800
    },
    {
      "epoch": 0.681412192666212,
      "grad_norm": 1.7131719589233398,
      "learning_rate": 4.594252433517935e-05,
      "loss": 0.1912,
      "step": 104900
    },
    {
      "epoch": 0.6820617753093637,
      "grad_norm": 2.110905885696411,
      "learning_rate": 4.593561387675336e-05,
      "loss": 0.1981,
      "step": 105000
    },
    {
      "epoch": 0.6827113579525155,
      "grad_norm": 1.7957251071929932,
      "learning_rate": 4.592870341832737e-05,
      "loss": 0.1875,
      "step": 105100
    },
    {
      "epoch": 0.6833609405956673,
      "grad_norm": 1.552824854850769,
      "learning_rate": 4.592179295990138e-05,
      "loss": 0.2061,
      "step": 105200
    },
    {
      "epoch": 0.6840105232388191,
      "grad_norm": 2.3326947689056396,
      "learning_rate": 4.591488250147539e-05,
      "loss": 0.1972,
      "step": 105300
    },
    {
      "epoch": 0.6846601058819708,
      "grad_norm": 1.6356887817382812,
      "learning_rate": 4.59079720430494e-05,
      "loss": 0.2025,
      "step": 105400
    },
    {
      "epoch": 0.6853096885251226,
      "grad_norm": 1.402724027633667,
      "learning_rate": 4.59010615846234e-05,
      "loss": 0.2026,
      "step": 105500
    },
    {
      "epoch": 0.6859592711682744,
      "grad_norm": 1.3529255390167236,
      "learning_rate": 4.589415112619741e-05,
      "loss": 0.1934,
      "step": 105600
    },
    {
      "epoch": 0.6866088538114261,
      "grad_norm": 1.6873096227645874,
      "learning_rate": 4.588724066777142e-05,
      "loss": 0.2003,
      "step": 105700
    },
    {
      "epoch": 0.687258436454578,
      "grad_norm": 1.5838838815689087,
      "learning_rate": 4.588033020934543e-05,
      "loss": 0.1961,
      "step": 105800
    },
    {
      "epoch": 0.6879080190977297,
      "grad_norm": 1.9088340997695923,
      "learning_rate": 4.587341975091944e-05,
      "loss": 0.1921,
      "step": 105900
    },
    {
      "epoch": 0.6885576017408814,
      "grad_norm": 1.8285638093948364,
      "learning_rate": 4.586650929249345e-05,
      "loss": 0.1985,
      "step": 106000
    },
    {
      "epoch": 0.6892071843840333,
      "grad_norm": 1.3790324926376343,
      "learning_rate": 4.585959883406746e-05,
      "loss": 0.1926,
      "step": 106100
    },
    {
      "epoch": 0.689856767027185,
      "grad_norm": 1.2279051542282104,
      "learning_rate": 4.585268837564147e-05,
      "loss": 0.1945,
      "step": 106200
    },
    {
      "epoch": 0.6905063496703369,
      "grad_norm": 1.4415867328643799,
      "learning_rate": 4.584577791721548e-05,
      "loss": 0.1972,
      "step": 106300
    },
    {
      "epoch": 0.6911559323134886,
      "grad_norm": 1.3529387712478638,
      "learning_rate": 4.583886745878948e-05,
      "loss": 0.1934,
      "step": 106400
    },
    {
      "epoch": 0.6918055149566403,
      "grad_norm": 1.4388891458511353,
      "learning_rate": 4.583195700036349e-05,
      "loss": 0.1969,
      "step": 106500
    },
    {
      "epoch": 0.6924550975997922,
      "grad_norm": 1.4209353923797607,
      "learning_rate": 4.58250465419375e-05,
      "loss": 0.2046,
      "step": 106600
    },
    {
      "epoch": 0.6931046802429439,
      "grad_norm": 1.417097806930542,
      "learning_rate": 4.581813608351151e-05,
      "loss": 0.1886,
      "step": 106700
    },
    {
      "epoch": 0.6937542628860957,
      "grad_norm": 1.6856995820999146,
      "learning_rate": 4.581122562508552e-05,
      "loss": 0.1871,
      "step": 106800
    },
    {
      "epoch": 0.6944038455292475,
      "grad_norm": 1.2890006303787231,
      "learning_rate": 4.5804315166659525e-05,
      "loss": 0.2007,
      "step": 106900
    },
    {
      "epoch": 0.6950534281723992,
      "grad_norm": 1.8751360177993774,
      "learning_rate": 4.5797404708233535e-05,
      "loss": 0.2057,
      "step": 107000
    },
    {
      "epoch": 0.695703010815551,
      "grad_norm": 1.55454421043396,
      "learning_rate": 4.5790494249807545e-05,
      "loss": 0.2005,
      "step": 107100
    },
    {
      "epoch": 0.6963525934587028,
      "grad_norm": 1.4486865997314453,
      "learning_rate": 4.5783583791381555e-05,
      "loss": 0.2048,
      "step": 107200
    },
    {
      "epoch": 0.6970021761018546,
      "grad_norm": 1.6286238431930542,
      "learning_rate": 4.5776673332955564e-05,
      "loss": 0.1927,
      "step": 107300
    },
    {
      "epoch": 0.6976517587450063,
      "grad_norm": 1.9752416610717773,
      "learning_rate": 4.576976287452957e-05,
      "loss": 0.2035,
      "step": 107400
    },
    {
      "epoch": 0.6983013413881581,
      "grad_norm": 1.5892009735107422,
      "learning_rate": 4.576285241610358e-05,
      "loss": 0.1961,
      "step": 107500
    },
    {
      "epoch": 0.6989509240313099,
      "grad_norm": 1.4144803285598755,
      "learning_rate": 4.575594195767759e-05,
      "loss": 0.1961,
      "step": 107600
    },
    {
      "epoch": 0.6996005066744616,
      "grad_norm": 1.2351211309432983,
      "learning_rate": 4.57490314992516e-05,
      "loss": 0.2064,
      "step": 107700
    },
    {
      "epoch": 0.7002500893176135,
      "grad_norm": 1.4192768335342407,
      "learning_rate": 4.574212104082561e-05,
      "loss": 0.1911,
      "step": 107800
    },
    {
      "epoch": 0.7008996719607652,
      "grad_norm": 1.62460458278656,
      "learning_rate": 4.573521058239962e-05,
      "loss": 0.2026,
      "step": 107900
    },
    {
      "epoch": 0.701549254603917,
      "grad_norm": 2.0369811058044434,
      "learning_rate": 4.5728300123973627e-05,
      "loss": 0.1911,
      "step": 108000
    },
    {
      "epoch": 0.7021988372470688,
      "grad_norm": 1.3508957624435425,
      "learning_rate": 4.5721389665547636e-05,
      "loss": 0.1852,
      "step": 108100
    },
    {
      "epoch": 0.7028484198902205,
      "grad_norm": 1.4976388216018677,
      "learning_rate": 4.5714479207121646e-05,
      "loss": 0.1933,
      "step": 108200
    },
    {
      "epoch": 0.7034980025333724,
      "grad_norm": 1.5303773880004883,
      "learning_rate": 4.570756874869565e-05,
      "loss": 0.1895,
      "step": 108300
    },
    {
      "epoch": 0.7041475851765241,
      "grad_norm": 1.508109450340271,
      "learning_rate": 4.570065829026966e-05,
      "loss": 0.2044,
      "step": 108400
    },
    {
      "epoch": 0.7047971678196758,
      "grad_norm": 1.5188599824905396,
      "learning_rate": 4.569374783184367e-05,
      "loss": 0.1984,
      "step": 108500
    },
    {
      "epoch": 0.7054467504628277,
      "grad_norm": 1.5972399711608887,
      "learning_rate": 4.568683737341768e-05,
      "loss": 0.1967,
      "step": 108600
    },
    {
      "epoch": 0.7060963331059794,
      "grad_norm": 1.5598609447479248,
      "learning_rate": 4.567992691499169e-05,
      "loss": 0.2049,
      "step": 108700
    },
    {
      "epoch": 0.7067459157491311,
      "grad_norm": 1.4414339065551758,
      "learning_rate": 4.56730164565657e-05,
      "loss": 0.1913,
      "step": 108800
    },
    {
      "epoch": 0.707395498392283,
      "grad_norm": 1.3765459060668945,
      "learning_rate": 4.566610599813971e-05,
      "loss": 0.1965,
      "step": 108900
    },
    {
      "epoch": 0.7080450810354347,
      "grad_norm": 1.672554612159729,
      "learning_rate": 4.565919553971372e-05,
      "loss": 0.1997,
      "step": 109000
    },
    {
      "epoch": 0.7086946636785865,
      "grad_norm": 1.0559970140457153,
      "learning_rate": 4.565228508128773e-05,
      "loss": 0.1991,
      "step": 109100
    },
    {
      "epoch": 0.7093442463217383,
      "grad_norm": 1.3711721897125244,
      "learning_rate": 4.564537462286174e-05,
      "loss": 0.1976,
      "step": 109200
    },
    {
      "epoch": 0.70999382896489,
      "grad_norm": 1.735076665878296,
      "learning_rate": 4.563846416443574e-05,
      "loss": 0.194,
      "step": 109300
    },
    {
      "epoch": 0.7106434116080418,
      "grad_norm": 1.5411065816879272,
      "learning_rate": 4.563155370600975e-05,
      "loss": 0.1945,
      "step": 109400
    },
    {
      "epoch": 0.7112929942511936,
      "grad_norm": 2.1083176136016846,
      "learning_rate": 4.562464324758376e-05,
      "loss": 0.196,
      "step": 109500
    },
    {
      "epoch": 0.7119425768943454,
      "grad_norm": 1.620845913887024,
      "learning_rate": 4.561773278915777e-05,
      "loss": 0.1941,
      "step": 109600
    },
    {
      "epoch": 0.7125921595374971,
      "grad_norm": 1.6644611358642578,
      "learning_rate": 4.561082233073178e-05,
      "loss": 0.1999,
      "step": 109700
    },
    {
      "epoch": 0.713241742180649,
      "grad_norm": 1.5805526971817017,
      "learning_rate": 4.560391187230579e-05,
      "loss": 0.1955,
      "step": 109800
    },
    {
      "epoch": 0.7138913248238007,
      "grad_norm": 1.8578146696090698,
      "learning_rate": 4.55970014138798e-05,
      "loss": 0.1982,
      "step": 109900
    },
    {
      "epoch": 0.7145409074669525,
      "grad_norm": 2.2826180458068848,
      "learning_rate": 4.559009095545381e-05,
      "loss": 0.2006,
      "step": 110000
    },
    {
      "epoch": 0.7151904901101043,
      "grad_norm": 1.7424064874649048,
      "learning_rate": 4.558318049702781e-05,
      "loss": 0.189,
      "step": 110100
    },
    {
      "epoch": 0.715840072753256,
      "grad_norm": 1.4475750923156738,
      "learning_rate": 4.557627003860182e-05,
      "loss": 0.1853,
      "step": 110200
    },
    {
      "epoch": 0.7164896553964079,
      "grad_norm": 1.845632553100586,
      "learning_rate": 4.556935958017583e-05,
      "loss": 0.1919,
      "step": 110300
    },
    {
      "epoch": 0.7171392380395596,
      "grad_norm": 1.4285269975662231,
      "learning_rate": 4.556244912174984e-05,
      "loss": 0.1965,
      "step": 110400
    },
    {
      "epoch": 0.7177888206827113,
      "grad_norm": 1.3244563341140747,
      "learning_rate": 4.5555538663323846e-05,
      "loss": 0.1861,
      "step": 110500
    },
    {
      "epoch": 0.7184384033258632,
      "grad_norm": 1.4590187072753906,
      "learning_rate": 4.5548628204897856e-05,
      "loss": 0.1846,
      "step": 110600
    },
    {
      "epoch": 0.7190879859690149,
      "grad_norm": 1.4848756790161133,
      "learning_rate": 4.5541717746471865e-05,
      "loss": 0.1948,
      "step": 110700
    },
    {
      "epoch": 0.7197375686121666,
      "grad_norm": 1.8005119562149048,
      "learning_rate": 4.5534807288045875e-05,
      "loss": 0.1898,
      "step": 110800
    },
    {
      "epoch": 0.7203871512553185,
      "grad_norm": 1.4879279136657715,
      "learning_rate": 4.5527896829619885e-05,
      "loss": 0.1955,
      "step": 110900
    },
    {
      "epoch": 0.7210367338984702,
      "grad_norm": 1.5720773935317993,
      "learning_rate": 4.5520986371193895e-05,
      "loss": 0.198,
      "step": 111000
    },
    {
      "epoch": 0.721686316541622,
      "grad_norm": 1.5901707410812378,
      "learning_rate": 4.55140759127679e-05,
      "loss": 0.18,
      "step": 111100
    },
    {
      "epoch": 0.7223358991847738,
      "grad_norm": 1.7160778045654297,
      "learning_rate": 4.550716545434191e-05,
      "loss": 0.194,
      "step": 111200
    },
    {
      "epoch": 0.7229854818279255,
      "grad_norm": 1.2912416458129883,
      "learning_rate": 4.550025499591592e-05,
      "loss": 0.1971,
      "step": 111300
    },
    {
      "epoch": 0.7236350644710773,
      "grad_norm": 1.1221644878387451,
      "learning_rate": 4.549334453748993e-05,
      "loss": 0.1946,
      "step": 111400
    },
    {
      "epoch": 0.7242846471142291,
      "grad_norm": 1.3603042364120483,
      "learning_rate": 4.548643407906394e-05,
      "loss": 0.186,
      "step": 111500
    },
    {
      "epoch": 0.7249342297573809,
      "grad_norm": 1.277190923690796,
      "learning_rate": 4.547952362063795e-05,
      "loss": 0.1886,
      "step": 111600
    },
    {
      "epoch": 0.7255838124005327,
      "grad_norm": 1.3515785932540894,
      "learning_rate": 4.547261316221196e-05,
      "loss": 0.1922,
      "step": 111700
    },
    {
      "epoch": 0.7262333950436844,
      "grad_norm": 1.1820542812347412,
      "learning_rate": 4.546570270378597e-05,
      "loss": 0.1941,
      "step": 111800
    },
    {
      "epoch": 0.7268829776868362,
      "grad_norm": 2.1803832054138184,
      "learning_rate": 4.545879224535998e-05,
      "loss": 0.1972,
      "step": 111900
    },
    {
      "epoch": 0.727532560329988,
      "grad_norm": 1.4392948150634766,
      "learning_rate": 4.545188178693399e-05,
      "loss": 0.1907,
      "step": 112000
    },
    {
      "epoch": 0.7281821429731398,
      "grad_norm": 1.3993550539016724,
      "learning_rate": 4.544497132850799e-05,
      "loss": 0.1884,
      "step": 112100
    },
    {
      "epoch": 0.7288317256162915,
      "grad_norm": 1.3777741193771362,
      "learning_rate": 4.5438060870082e-05,
      "loss": 0.1914,
      "step": 112200
    },
    {
      "epoch": 0.7294813082594434,
      "grad_norm": 1.033537745475769,
      "learning_rate": 4.543115041165601e-05,
      "loss": 0.1891,
      "step": 112300
    },
    {
      "epoch": 0.7301308909025951,
      "grad_norm": 1.4745986461639404,
      "learning_rate": 4.542423995323002e-05,
      "loss": 0.1892,
      "step": 112400
    },
    {
      "epoch": 0.7307804735457468,
      "grad_norm": 1.8332688808441162,
      "learning_rate": 4.541732949480403e-05,
      "loss": 0.1868,
      "step": 112500
    },
    {
      "epoch": 0.7314300561888987,
      "grad_norm": 1.4320054054260254,
      "learning_rate": 4.541041903637804e-05,
      "loss": 0.1985,
      "step": 112600
    },
    {
      "epoch": 0.7320796388320504,
      "grad_norm": 1.4166696071624756,
      "learning_rate": 4.540350857795205e-05,
      "loss": 0.1932,
      "step": 112700
    },
    {
      "epoch": 0.7327292214752021,
      "grad_norm": 1.3065739870071411,
      "learning_rate": 4.539659811952606e-05,
      "loss": 0.1898,
      "step": 112800
    },
    {
      "epoch": 0.733378804118354,
      "grad_norm": 1.6578636169433594,
      "learning_rate": 4.538968766110007e-05,
      "loss": 0.1933,
      "step": 112900
    },
    {
      "epoch": 0.7340283867615057,
      "grad_norm": 1.336243987083435,
      "learning_rate": 4.538277720267407e-05,
      "loss": 0.1887,
      "step": 113000
    },
    {
      "epoch": 0.7346779694046575,
      "grad_norm": 1.796130895614624,
      "learning_rate": 4.537586674424808e-05,
      "loss": 0.1952,
      "step": 113100
    },
    {
      "epoch": 0.7353275520478093,
      "grad_norm": 0.9389215111732483,
      "learning_rate": 4.536895628582209e-05,
      "loss": 0.1989,
      "step": 113200
    },
    {
      "epoch": 0.735977134690961,
      "grad_norm": 1.8921337127685547,
      "learning_rate": 4.53620458273961e-05,
      "loss": 0.1892,
      "step": 113300
    },
    {
      "epoch": 0.7366267173341128,
      "grad_norm": 2.4331018924713135,
      "learning_rate": 4.535513536897011e-05,
      "loss": 0.1899,
      "step": 113400
    },
    {
      "epoch": 0.7372762999772646,
      "grad_norm": 1.3887890577316284,
      "learning_rate": 4.534822491054412e-05,
      "loss": 0.1865,
      "step": 113500
    },
    {
      "epoch": 0.7379258826204164,
      "grad_norm": 1.3605622053146362,
      "learning_rate": 4.534131445211813e-05,
      "loss": 0.1902,
      "step": 113600
    },
    {
      "epoch": 0.7385754652635682,
      "grad_norm": 1.6764812469482422,
      "learning_rate": 4.5334403993692134e-05,
      "loss": 0.1888,
      "step": 113700
    },
    {
      "epoch": 0.73922504790672,
      "grad_norm": 1.7559664249420166,
      "learning_rate": 4.5327493535266144e-05,
      "loss": 0.1963,
      "step": 113800
    },
    {
      "epoch": 0.7398746305498717,
      "grad_norm": 1.566593050956726,
      "learning_rate": 4.5320583076840154e-05,
      "loss": 0.1896,
      "step": 113900
    },
    {
      "epoch": 0.7405242131930235,
      "grad_norm": 1.413488745689392,
      "learning_rate": 4.5313672618414163e-05,
      "loss": 0.1918,
      "step": 114000
    },
    {
      "epoch": 0.7411737958361753,
      "grad_norm": 1.563123345375061,
      "learning_rate": 4.5306762159988167e-05,
      "loss": 0.1901,
      "step": 114100
    },
    {
      "epoch": 0.741823378479327,
      "grad_norm": 1.0857161283493042,
      "learning_rate": 4.5299851701562176e-05,
      "loss": 0.1861,
      "step": 114200
    },
    {
      "epoch": 0.7424729611224788,
      "grad_norm": 1.5655986070632935,
      "learning_rate": 4.5292941243136186e-05,
      "loss": 0.1882,
      "step": 114300
    },
    {
      "epoch": 0.7431225437656306,
      "grad_norm": 2.0573747158050537,
      "learning_rate": 4.5286030784710196e-05,
      "loss": 0.1822,
      "step": 114400
    },
    {
      "epoch": 0.7437721264087823,
      "grad_norm": 1.5882548093795776,
      "learning_rate": 4.5279120326284206e-05,
      "loss": 0.1947,
      "step": 114500
    },
    {
      "epoch": 0.7444217090519342,
      "grad_norm": 1.4289073944091797,
      "learning_rate": 4.5272209867858216e-05,
      "loss": 0.1917,
      "step": 114600
    },
    {
      "epoch": 0.7450712916950859,
      "grad_norm": 1.1199153661727905,
      "learning_rate": 4.5265299409432226e-05,
      "loss": 0.1961,
      "step": 114700
    },
    {
      "epoch": 0.7457208743382376,
      "grad_norm": 2.145589828491211,
      "learning_rate": 4.5258388951006235e-05,
      "loss": 0.1872,
      "step": 114800
    },
    {
      "epoch": 0.7463704569813895,
      "grad_norm": 2.0722835063934326,
      "learning_rate": 4.525147849258024e-05,
      "loss": 0.186,
      "step": 114900
    },
    {
      "epoch": 0.7470200396245412,
      "grad_norm": 1.499427318572998,
      "learning_rate": 4.524456803415425e-05,
      "loss": 0.1901,
      "step": 115000
    },
    {
      "epoch": 0.747669622267693,
      "grad_norm": 1.974426507949829,
      "learning_rate": 4.523765757572826e-05,
      "loss": 0.1954,
      "step": 115100
    },
    {
      "epoch": 0.7483192049108448,
      "grad_norm": 1.3513175249099731,
      "learning_rate": 4.523074711730227e-05,
      "loss": 0.1976,
      "step": 115200
    },
    {
      "epoch": 0.7489687875539965,
      "grad_norm": 1.6006656885147095,
      "learning_rate": 4.522383665887628e-05,
      "loss": 0.1889,
      "step": 115300
    },
    {
      "epoch": 0.7496183701971484,
      "grad_norm": 1.5608422756195068,
      "learning_rate": 4.521692620045029e-05,
      "loss": 0.1912,
      "step": 115400
    },
    {
      "epoch": 0.7502679528403001,
      "grad_norm": 1.813368320465088,
      "learning_rate": 4.52100157420243e-05,
      "loss": 0.1862,
      "step": 115500
    },
    {
      "epoch": 0.7509175354834519,
      "grad_norm": 1.2922505140304565,
      "learning_rate": 4.520310528359831e-05,
      "loss": 0.1886,
      "step": 115600
    },
    {
      "epoch": 0.7515671181266037,
      "grad_norm": 1.4137097597122192,
      "learning_rate": 4.519619482517232e-05,
      "loss": 0.191,
      "step": 115700
    },
    {
      "epoch": 0.7522167007697554,
      "grad_norm": 1.882009506225586,
      "learning_rate": 4.518928436674633e-05,
      "loss": 0.1982,
      "step": 115800
    },
    {
      "epoch": 0.7528662834129072,
      "grad_norm": 1.5661637783050537,
      "learning_rate": 4.518237390832033e-05,
      "loss": 0.188,
      "step": 115900
    },
    {
      "epoch": 0.753515866056059,
      "grad_norm": 1.3101444244384766,
      "learning_rate": 4.517546344989434e-05,
      "loss": 0.1877,
      "step": 116000
    },
    {
      "epoch": 0.7541654486992108,
      "grad_norm": 1.310462236404419,
      "learning_rate": 4.516855299146835e-05,
      "loss": 0.1877,
      "step": 116100
    },
    {
      "epoch": 0.7548150313423625,
      "grad_norm": 1.159184455871582,
      "learning_rate": 4.516164253304236e-05,
      "loss": 0.1868,
      "step": 116200
    },
    {
      "epoch": 0.7554646139855143,
      "grad_norm": 1.5193347930908203,
      "learning_rate": 4.515473207461637e-05,
      "loss": 0.1853,
      "step": 116300
    },
    {
      "epoch": 0.7561141966286661,
      "grad_norm": 1.6542794704437256,
      "learning_rate": 4.514782161619038e-05,
      "loss": 0.1875,
      "step": 116400
    },
    {
      "epoch": 0.7567637792718178,
      "grad_norm": 1.4196875095367432,
      "learning_rate": 4.514091115776439e-05,
      "loss": 0.1864,
      "step": 116500
    },
    {
      "epoch": 0.7574133619149697,
      "grad_norm": 1.5197421312332153,
      "learning_rate": 4.51340006993384e-05,
      "loss": 0.1916,
      "step": 116600
    },
    {
      "epoch": 0.7580629445581214,
      "grad_norm": 1.3187406063079834,
      "learning_rate": 4.512709024091241e-05,
      "loss": 0.1883,
      "step": 116700
    },
    {
      "epoch": 0.7587125272012731,
      "grad_norm": 1.0934545993804932,
      "learning_rate": 4.512017978248641e-05,
      "loss": 0.1862,
      "step": 116800
    },
    {
      "epoch": 0.759362109844425,
      "grad_norm": 1.2300899028778076,
      "learning_rate": 4.511326932406042e-05,
      "loss": 0.1852,
      "step": 116900
    },
    {
      "epoch": 0.7600116924875767,
      "grad_norm": 1.543483018875122,
      "learning_rate": 4.510635886563443e-05,
      "loss": 0.188,
      "step": 117000
    },
    {
      "epoch": 0.7606612751307285,
      "grad_norm": 1.6984193325042725,
      "learning_rate": 4.509944840720844e-05,
      "loss": 0.1904,
      "step": 117100
    },
    {
      "epoch": 0.7613108577738803,
      "grad_norm": 1.1213178634643555,
      "learning_rate": 4.509253794878245e-05,
      "loss": 0.1823,
      "step": 117200
    },
    {
      "epoch": 0.761960440417032,
      "grad_norm": 1.3136770725250244,
      "learning_rate": 4.5085627490356455e-05,
      "loss": 0.2013,
      "step": 117300
    },
    {
      "epoch": 0.7626100230601839,
      "grad_norm": 1.2097890377044678,
      "learning_rate": 4.5078717031930465e-05,
      "loss": 0.1874,
      "step": 117400
    },
    {
      "epoch": 0.7632596057033356,
      "grad_norm": 1.3143401145935059,
      "learning_rate": 4.5071806573504474e-05,
      "loss": 0.1901,
      "step": 117500
    },
    {
      "epoch": 0.7639091883464874,
      "grad_norm": 1.606265664100647,
      "learning_rate": 4.5064896115078484e-05,
      "loss": 0.188,
      "step": 117600
    },
    {
      "epoch": 0.7645587709896392,
      "grad_norm": 1.2135542631149292,
      "learning_rate": 4.505798565665249e-05,
      "loss": 0.1858,
      "step": 117700
    },
    {
      "epoch": 0.7652083536327909,
      "grad_norm": 1.2256211042404175,
      "learning_rate": 4.50510751982265e-05,
      "loss": 0.1981,
      "step": 117800
    },
    {
      "epoch": 0.7658579362759427,
      "grad_norm": 1.4810600280761719,
      "learning_rate": 4.504416473980051e-05,
      "loss": 0.1806,
      "step": 117900
    },
    {
      "epoch": 0.7665075189190945,
      "grad_norm": 1.5719090700149536,
      "learning_rate": 4.503725428137452e-05,
      "loss": 0.1821,
      "step": 118000
    },
    {
      "epoch": 0.7671571015622463,
      "grad_norm": 1.5038938522338867,
      "learning_rate": 4.503034382294853e-05,
      "loss": 0.1853,
      "step": 118100
    },
    {
      "epoch": 0.767806684205398,
      "grad_norm": 1.142907977104187,
      "learning_rate": 4.5023433364522537e-05,
      "loss": 0.1923,
      "step": 118200
    },
    {
      "epoch": 0.7684562668485498,
      "grad_norm": 1.6589810848236084,
      "learning_rate": 4.5016522906096546e-05,
      "loss": 0.1888,
      "step": 118300
    },
    {
      "epoch": 0.7691058494917016,
      "grad_norm": 1.1214194297790527,
      "learning_rate": 4.5009612447670556e-05,
      "loss": 0.1818,
      "step": 118400
    },
    {
      "epoch": 0.7697554321348533,
      "grad_norm": 1.5842351913452148,
      "learning_rate": 4.5002701989244566e-05,
      "loss": 0.1858,
      "step": 118500
    },
    {
      "epoch": 0.7704050147780052,
      "grad_norm": 1.0533242225646973,
      "learning_rate": 4.4995791530818576e-05,
      "loss": 0.1952,
      "step": 118600
    },
    {
      "epoch": 0.7710545974211569,
      "grad_norm": 1.8566614389419556,
      "learning_rate": 4.498888107239258e-05,
      "loss": 0.1929,
      "step": 118700
    },
    {
      "epoch": 0.7717041800643086,
      "grad_norm": 1.4190987348556519,
      "learning_rate": 4.498197061396659e-05,
      "loss": 0.1868,
      "step": 118800
    },
    {
      "epoch": 0.7723537627074605,
      "grad_norm": 1.3495899438858032,
      "learning_rate": 4.49750601555406e-05,
      "loss": 0.1931,
      "step": 118900
    },
    {
      "epoch": 0.7730033453506122,
      "grad_norm": 1.5387071371078491,
      "learning_rate": 4.496814969711461e-05,
      "loss": 0.1819,
      "step": 119000
    },
    {
      "epoch": 0.7736529279937641,
      "grad_norm": 1.9086716175079346,
      "learning_rate": 4.496123923868862e-05,
      "loss": 0.1893,
      "step": 119100
    },
    {
      "epoch": 0.7743025106369158,
      "grad_norm": 1.2000576257705688,
      "learning_rate": 4.495432878026263e-05,
      "loss": 0.1878,
      "step": 119200
    },
    {
      "epoch": 0.7749520932800675,
      "grad_norm": 1.7552752494812012,
      "learning_rate": 4.494741832183664e-05,
      "loss": 0.1855,
      "step": 119300
    },
    {
      "epoch": 0.7756016759232194,
      "grad_norm": 1.918915867805481,
      "learning_rate": 4.494050786341065e-05,
      "loss": 0.1825,
      "step": 119400
    },
    {
      "epoch": 0.7762512585663711,
      "grad_norm": 2.0780515670776367,
      "learning_rate": 4.493359740498466e-05,
      "loss": 0.1889,
      "step": 119500
    },
    {
      "epoch": 0.7769008412095229,
      "grad_norm": 1.115082859992981,
      "learning_rate": 4.492668694655866e-05,
      "loss": 0.1759,
      "step": 119600
    },
    {
      "epoch": 0.7775504238526747,
      "grad_norm": 1.4090476036071777,
      "learning_rate": 4.491977648813267e-05,
      "loss": 0.1792,
      "step": 119700
    },
    {
      "epoch": 0.7782000064958264,
      "grad_norm": 1.9922930002212524,
      "learning_rate": 4.491286602970668e-05,
      "loss": 0.187,
      "step": 119800
    },
    {
      "epoch": 0.7788495891389782,
      "grad_norm": 1.2176761627197266,
      "learning_rate": 4.490595557128069e-05,
      "loss": 0.1786,
      "step": 119900
    },
    {
      "epoch": 0.77949917178213,
      "grad_norm": 1.1860289573669434,
      "learning_rate": 4.48990451128547e-05,
      "loss": 0.1801,
      "step": 120000
    },
    {
      "epoch": 0.7801487544252818,
      "grad_norm": 1.1848145723342896,
      "learning_rate": 4.489213465442871e-05,
      "loss": 0.1815,
      "step": 120100
    },
    {
      "epoch": 0.7807983370684335,
      "grad_norm": 1.1225802898406982,
      "learning_rate": 4.488522419600272e-05,
      "loss": 0.1966,
      "step": 120200
    },
    {
      "epoch": 0.7814479197115853,
      "grad_norm": 1.5076826810836792,
      "learning_rate": 4.487831373757673e-05,
      "loss": 0.1822,
      "step": 120300
    },
    {
      "epoch": 0.7820975023547371,
      "grad_norm": 1.2893112897872925,
      "learning_rate": 4.487140327915074e-05,
      "loss": 0.1871,
      "step": 120400
    },
    {
      "epoch": 0.7827470849978888,
      "grad_norm": 1.632398247718811,
      "learning_rate": 4.486449282072474e-05,
      "loss": 0.1872,
      "step": 120500
    },
    {
      "epoch": 0.7833966676410407,
      "grad_norm": 1.2475147247314453,
      "learning_rate": 4.485758236229875e-05,
      "loss": 0.1861,
      "step": 120600
    },
    {
      "epoch": 0.7840462502841924,
      "grad_norm": 1.6342999935150146,
      "learning_rate": 4.485067190387276e-05,
      "loss": 0.1872,
      "step": 120700
    },
    {
      "epoch": 0.7846958329273441,
      "grad_norm": 1.4257164001464844,
      "learning_rate": 4.484376144544677e-05,
      "loss": 0.1904,
      "step": 120800
    },
    {
      "epoch": 0.785345415570496,
      "grad_norm": 1.9462275505065918,
      "learning_rate": 4.4836850987020775e-05,
      "loss": 0.1889,
      "step": 120900
    },
    {
      "epoch": 0.7859949982136477,
      "grad_norm": 1.9168579578399658,
      "learning_rate": 4.4829940528594785e-05,
      "loss": 0.1797,
      "step": 121000
    },
    {
      "epoch": 0.7866445808567996,
      "grad_norm": 1.7219464778900146,
      "learning_rate": 4.4823030070168795e-05,
      "loss": 0.1813,
      "step": 121100
    },
    {
      "epoch": 0.7872941634999513,
      "grad_norm": 2.073092460632324,
      "learning_rate": 4.4816119611742805e-05,
      "loss": 0.1946,
      "step": 121200
    },
    {
      "epoch": 0.787943746143103,
      "grad_norm": 1.1725821495056152,
      "learning_rate": 4.4809209153316815e-05,
      "loss": 0.1763,
      "step": 121300
    },
    {
      "epoch": 0.7885933287862549,
      "grad_norm": 1.1967929601669312,
      "learning_rate": 4.4802298694890825e-05,
      "loss": 0.1924,
      "step": 121400
    },
    {
      "epoch": 0.7892429114294066,
      "grad_norm": 1.1790878772735596,
      "learning_rate": 4.479538823646483e-05,
      "loss": 0.1798,
      "step": 121500
    },
    {
      "epoch": 0.7898924940725583,
      "grad_norm": 1.4158902168273926,
      "learning_rate": 4.478847777803884e-05,
      "loss": 0.173,
      "step": 121600
    },
    {
      "epoch": 0.7905420767157102,
      "grad_norm": 1.5591861009597778,
      "learning_rate": 4.478156731961285e-05,
      "loss": 0.1814,
      "step": 121700
    },
    {
      "epoch": 0.7911916593588619,
      "grad_norm": 0.9906998872756958,
      "learning_rate": 4.477465686118686e-05,
      "loss": 0.1909,
      "step": 121800
    },
    {
      "epoch": 0.7918412420020137,
      "grad_norm": 1.4914621114730835,
      "learning_rate": 4.476774640276087e-05,
      "loss": 0.181,
      "step": 121900
    },
    {
      "epoch": 0.7924908246451655,
      "grad_norm": 1.4088108539581299,
      "learning_rate": 4.476083594433488e-05,
      "loss": 0.1878,
      "step": 122000
    },
    {
      "epoch": 0.7931404072883173,
      "grad_norm": 1.4444884061813354,
      "learning_rate": 4.475392548590889e-05,
      "loss": 0.1835,
      "step": 122100
    },
    {
      "epoch": 0.793789989931469,
      "grad_norm": 1.7634955644607544,
      "learning_rate": 4.47470150274829e-05,
      "loss": 0.1825,
      "step": 122200
    },
    {
      "epoch": 0.7944395725746208,
      "grad_norm": 1.4087231159210205,
      "learning_rate": 4.474010456905691e-05,
      "loss": 0.1848,
      "step": 122300
    },
    {
      "epoch": 0.7950891552177726,
      "grad_norm": 1.361026644706726,
      "learning_rate": 4.473319411063091e-05,
      "loss": 0.1797,
      "step": 122400
    },
    {
      "epoch": 0.7957387378609243,
      "grad_norm": 1.4220479726791382,
      "learning_rate": 4.472628365220492e-05,
      "loss": 0.186,
      "step": 122500
    },
    {
      "epoch": 0.7963883205040762,
      "grad_norm": 1.542637586593628,
      "learning_rate": 4.471937319377893e-05,
      "loss": 0.1889,
      "step": 122600
    },
    {
      "epoch": 0.7970379031472279,
      "grad_norm": 1.4251567125320435,
      "learning_rate": 4.471246273535294e-05,
      "loss": 0.1832,
      "step": 122700
    },
    {
      "epoch": 0.7976874857903797,
      "grad_norm": 1.562647819519043,
      "learning_rate": 4.470555227692695e-05,
      "loss": 0.1739,
      "step": 122800
    },
    {
      "epoch": 0.7983370684335315,
      "grad_norm": 0.9121967554092407,
      "learning_rate": 4.469864181850096e-05,
      "loss": 0.1836,
      "step": 122900
    },
    {
      "epoch": 0.7989866510766832,
      "grad_norm": 1.2911872863769531,
      "learning_rate": 4.469173136007497e-05,
      "loss": 0.1868,
      "step": 123000
    },
    {
      "epoch": 0.799636233719835,
      "grad_norm": 1.6356624364852905,
      "learning_rate": 4.468482090164898e-05,
      "loss": 0.1853,
      "step": 123100
    },
    {
      "epoch": 0.8002858163629868,
      "grad_norm": 1.2973145246505737,
      "learning_rate": 4.467791044322299e-05,
      "loss": 0.1835,
      "step": 123200
    },
    {
      "epoch": 0.8009353990061385,
      "grad_norm": 1.018563151359558,
      "learning_rate": 4.4670999984797e-05,
      "loss": 0.1826,
      "step": 123300
    },
    {
      "epoch": 0.8015849816492904,
      "grad_norm": 1.4517163038253784,
      "learning_rate": 4.4664089526371e-05,
      "loss": 0.1873,
      "step": 123400
    },
    {
      "epoch": 0.8022345642924421,
      "grad_norm": 1.4642748832702637,
      "learning_rate": 4.465717906794501e-05,
      "loss": 0.185,
      "step": 123500
    },
    {
      "epoch": 0.8028841469355938,
      "grad_norm": 1.7686078548431396,
      "learning_rate": 4.465026860951902e-05,
      "loss": 0.1802,
      "step": 123600
    },
    {
      "epoch": 0.8035337295787457,
      "grad_norm": 1.616703987121582,
      "learning_rate": 4.464335815109303e-05,
      "loss": 0.1842,
      "step": 123700
    },
    {
      "epoch": 0.8041833122218974,
      "grad_norm": 1.0304832458496094,
      "learning_rate": 4.463644769266704e-05,
      "loss": 0.1777,
      "step": 123800
    },
    {
      "epoch": 0.8048328948650492,
      "grad_norm": 1.48712158203125,
      "learning_rate": 4.462953723424105e-05,
      "loss": 0.1877,
      "step": 123900
    },
    {
      "epoch": 0.805482477508201,
      "grad_norm": 1.716176986694336,
      "learning_rate": 4.462262677581506e-05,
      "loss": 0.1745,
      "step": 124000
    },
    {
      "epoch": 0.8061320601513527,
      "grad_norm": 1.4637200832366943,
      "learning_rate": 4.4615716317389064e-05,
      "loss": 0.1843,
      "step": 124100
    },
    {
      "epoch": 0.8067816427945045,
      "grad_norm": 1.1670969724655151,
      "learning_rate": 4.4608805858963073e-05,
      "loss": 0.183,
      "step": 124200
    },
    {
      "epoch": 0.8074312254376563,
      "grad_norm": 1.2386572360992432,
      "learning_rate": 4.460189540053708e-05,
      "loss": 0.1847,
      "step": 124300
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 1.114962100982666,
      "learning_rate": 4.459498494211109e-05,
      "loss": 0.1818,
      "step": 124400
    },
    {
      "epoch": 0.8087303907239598,
      "grad_norm": 1.5118098258972168,
      "learning_rate": 4.4588074483685096e-05,
      "loss": 0.1859,
      "step": 124500
    },
    {
      "epoch": 0.8093799733671116,
      "grad_norm": 1.418755292892456,
      "learning_rate": 4.4581164025259106e-05,
      "loss": 0.1788,
      "step": 124600
    },
    {
      "epoch": 0.8100295560102634,
      "grad_norm": 0.93397057056427,
      "learning_rate": 4.4574253566833116e-05,
      "loss": 0.1852,
      "step": 124700
    },
    {
      "epoch": 0.8106791386534152,
      "grad_norm": 1.909981608390808,
      "learning_rate": 4.4567343108407126e-05,
      "loss": 0.1823,
      "step": 124800
    },
    {
      "epoch": 0.811328721296567,
      "grad_norm": 1.2081772089004517,
      "learning_rate": 4.4560432649981136e-05,
      "loss": 0.1713,
      "step": 124900
    },
    {
      "epoch": 0.8119783039397187,
      "grad_norm": 1.5846575498580933,
      "learning_rate": 4.4553522191555146e-05,
      "loss": 0.1774,
      "step": 125000
    },
    {
      "epoch": 0.8126278865828706,
      "grad_norm": 1.5535086393356323,
      "learning_rate": 4.4546611733129155e-05,
      "loss": 0.1796,
      "step": 125100
    },
    {
      "epoch": 0.8132774692260223,
      "grad_norm": 1.1397377252578735,
      "learning_rate": 4.4539701274703165e-05,
      "loss": 0.1777,
      "step": 125200
    },
    {
      "epoch": 0.813927051869174,
      "grad_norm": 1.3270487785339355,
      "learning_rate": 4.453279081627717e-05,
      "loss": 0.1838,
      "step": 125300
    },
    {
      "epoch": 0.8145766345123259,
      "grad_norm": 1.2416328191757202,
      "learning_rate": 4.452588035785118e-05,
      "loss": 0.1722,
      "step": 125400
    },
    {
      "epoch": 0.8152262171554776,
      "grad_norm": 1.6202274560928345,
      "learning_rate": 4.451896989942519e-05,
      "loss": 0.1953,
      "step": 125500
    },
    {
      "epoch": 0.8158757997986293,
      "grad_norm": 1.4807649850845337,
      "learning_rate": 4.45120594409992e-05,
      "loss": 0.1798,
      "step": 125600
    },
    {
      "epoch": 0.8165253824417812,
      "grad_norm": 1.5234766006469727,
      "learning_rate": 4.450514898257321e-05,
      "loss": 0.1803,
      "step": 125700
    },
    {
      "epoch": 0.8171749650849329,
      "grad_norm": 1.2311102151870728,
      "learning_rate": 4.449823852414722e-05,
      "loss": 0.1874,
      "step": 125800
    },
    {
      "epoch": 0.8178245477280847,
      "grad_norm": 1.3954609632492065,
      "learning_rate": 4.449132806572123e-05,
      "loss": 0.1833,
      "step": 125900
    },
    {
      "epoch": 0.8184741303712365,
      "grad_norm": 1.3378942012786865,
      "learning_rate": 4.448441760729524e-05,
      "loss": 0.1908,
      "step": 126000
    },
    {
      "epoch": 0.8191237130143882,
      "grad_norm": 1.600186824798584,
      "learning_rate": 4.447750714886925e-05,
      "loss": 0.1721,
      "step": 126100
    },
    {
      "epoch": 0.81977329565754,
      "grad_norm": 1.805591344833374,
      "learning_rate": 4.447059669044325e-05,
      "loss": 0.1764,
      "step": 126200
    },
    {
      "epoch": 0.8204228783006918,
      "grad_norm": 1.4864901304244995,
      "learning_rate": 4.446368623201726e-05,
      "loss": 0.1848,
      "step": 126300
    },
    {
      "epoch": 0.8210724609438436,
      "grad_norm": 1.6748980283737183,
      "learning_rate": 4.445677577359127e-05,
      "loss": 0.1949,
      "step": 126400
    },
    {
      "epoch": 0.8217220435869954,
      "grad_norm": 0.9098340272903442,
      "learning_rate": 4.444986531516528e-05,
      "loss": 0.1749,
      "step": 126500
    },
    {
      "epoch": 0.8223716262301471,
      "grad_norm": 1.085138201713562,
      "learning_rate": 4.444295485673929e-05,
      "loss": 0.1791,
      "step": 126600
    },
    {
      "epoch": 0.8230212088732989,
      "grad_norm": 1.4325631856918335,
      "learning_rate": 4.44360443983133e-05,
      "loss": 0.1774,
      "step": 126700
    },
    {
      "epoch": 0.8236707915164507,
      "grad_norm": 1.3356225490570068,
      "learning_rate": 4.442913393988731e-05,
      "loss": 0.1795,
      "step": 126800
    },
    {
      "epoch": 0.8243203741596025,
      "grad_norm": 1.2154909372329712,
      "learning_rate": 4.442222348146132e-05,
      "loss": 0.1748,
      "step": 126900
    },
    {
      "epoch": 0.8249699568027542,
      "grad_norm": 1.752272367477417,
      "learning_rate": 4.441531302303533e-05,
      "loss": 0.185,
      "step": 127000
    },
    {
      "epoch": 0.825619539445906,
      "grad_norm": 1.6125088930130005,
      "learning_rate": 4.440840256460933e-05,
      "loss": 0.177,
      "step": 127100
    },
    {
      "epoch": 0.8262691220890578,
      "grad_norm": 1.6592776775360107,
      "learning_rate": 4.440149210618334e-05,
      "loss": 0.1843,
      "step": 127200
    },
    {
      "epoch": 0.8269187047322095,
      "grad_norm": 1.2743043899536133,
      "learning_rate": 4.439458164775735e-05,
      "loss": 0.1801,
      "step": 127300
    },
    {
      "epoch": 0.8275682873753614,
      "grad_norm": 1.9505972862243652,
      "learning_rate": 4.438767118933136e-05,
      "loss": 0.1858,
      "step": 127400
    },
    {
      "epoch": 0.8282178700185131,
      "grad_norm": 1.590743899345398,
      "learning_rate": 4.438076073090537e-05,
      "loss": 0.1728,
      "step": 127500
    },
    {
      "epoch": 0.8288674526616648,
      "grad_norm": 1.538735270500183,
      "learning_rate": 4.437385027247938e-05,
      "loss": 0.1729,
      "step": 127600
    },
    {
      "epoch": 0.8295170353048167,
      "grad_norm": 1.972784399986267,
      "learning_rate": 4.4366939814053384e-05,
      "loss": 0.1809,
      "step": 127700
    },
    {
      "epoch": 0.8301666179479684,
      "grad_norm": 1.3835359811782837,
      "learning_rate": 4.4360029355627394e-05,
      "loss": 0.1779,
      "step": 127800
    },
    {
      "epoch": 0.8308162005911202,
      "grad_norm": 1.354958176612854,
      "learning_rate": 4.4353118897201404e-05,
      "loss": 0.1756,
      "step": 127900
    },
    {
      "epoch": 0.831465783234272,
      "grad_norm": 1.5175480842590332,
      "learning_rate": 4.4346208438775414e-05,
      "loss": 0.1779,
      "step": 128000
    },
    {
      "epoch": 0.8321153658774237,
      "grad_norm": 1.21819007396698,
      "learning_rate": 4.433929798034942e-05,
      "loss": 0.1818,
      "step": 128100
    },
    {
      "epoch": 0.8327649485205755,
      "grad_norm": 1.7971832752227783,
      "learning_rate": 4.433238752192343e-05,
      "loss": 0.1739,
      "step": 128200
    },
    {
      "epoch": 0.8334145311637273,
      "grad_norm": 1.538629412651062,
      "learning_rate": 4.432547706349744e-05,
      "loss": 0.1738,
      "step": 128300
    },
    {
      "epoch": 0.8340641138068791,
      "grad_norm": 1.6590063571929932,
      "learning_rate": 4.4318566605071447e-05,
      "loss": 0.1743,
      "step": 128400
    },
    {
      "epoch": 0.8347136964500309,
      "grad_norm": 1.2037540674209595,
      "learning_rate": 4.4311656146645456e-05,
      "loss": 0.1794,
      "step": 128500
    },
    {
      "epoch": 0.8353632790931826,
      "grad_norm": 1.5152407884597778,
      "learning_rate": 4.4304745688219466e-05,
      "loss": 0.1778,
      "step": 128600
    },
    {
      "epoch": 0.8360128617363344,
      "grad_norm": 1.9178959131240845,
      "learning_rate": 4.4297835229793476e-05,
      "loss": 0.1766,
      "step": 128700
    },
    {
      "epoch": 0.8366624443794862,
      "grad_norm": 1.5935428142547607,
      "learning_rate": 4.4290924771367486e-05,
      "loss": 0.1822,
      "step": 128800
    },
    {
      "epoch": 0.837312027022638,
      "grad_norm": 1.8072583675384521,
      "learning_rate": 4.4284014312941496e-05,
      "loss": 0.1803,
      "step": 128900
    },
    {
      "epoch": 0.8379616096657897,
      "grad_norm": 1.474141001701355,
      "learning_rate": 4.42771038545155e-05,
      "loss": 0.1672,
      "step": 129000
    },
    {
      "epoch": 0.8386111923089415,
      "grad_norm": 2.151707887649536,
      "learning_rate": 4.427019339608951e-05,
      "loss": 0.1813,
      "step": 129100
    },
    {
      "epoch": 0.8392607749520933,
      "grad_norm": 1.4652714729309082,
      "learning_rate": 4.426328293766352e-05,
      "loss": 0.1849,
      "step": 129200
    },
    {
      "epoch": 0.839910357595245,
      "grad_norm": 1.1986624002456665,
      "learning_rate": 4.425637247923753e-05,
      "loss": 0.1803,
      "step": 129300
    },
    {
      "epoch": 0.8405599402383969,
      "grad_norm": 1.416619062423706,
      "learning_rate": 4.424946202081154e-05,
      "loss": 0.1823,
      "step": 129400
    },
    {
      "epoch": 0.8412095228815486,
      "grad_norm": 1.1928669214248657,
      "learning_rate": 4.424255156238555e-05,
      "loss": 0.1812,
      "step": 129500
    },
    {
      "epoch": 0.8418591055247003,
      "grad_norm": 1.4876199960708618,
      "learning_rate": 4.423564110395956e-05,
      "loss": 0.1724,
      "step": 129600
    },
    {
      "epoch": 0.8425086881678522,
      "grad_norm": 1.5405210256576538,
      "learning_rate": 4.422873064553357e-05,
      "loss": 0.1803,
      "step": 129700
    },
    {
      "epoch": 0.8431582708110039,
      "grad_norm": 1.5449130535125732,
      "learning_rate": 4.422182018710758e-05,
      "loss": 0.1749,
      "step": 129800
    },
    {
      "epoch": 0.8438078534541557,
      "grad_norm": 1.6680902242660522,
      "learning_rate": 4.421490972868159e-05,
      "loss": 0.1814,
      "step": 129900
    },
    {
      "epoch": 0.8444574360973075,
      "grad_norm": 1.68876314163208,
      "learning_rate": 4.420799927025559e-05,
      "loss": 0.1767,
      "step": 130000
    },
    {
      "epoch": 0.8451070187404592,
      "grad_norm": 1.4748891592025757,
      "learning_rate": 4.42010888118296e-05,
      "loss": 0.1751,
      "step": 130100
    },
    {
      "epoch": 0.8457566013836111,
      "grad_norm": 1.1627076864242554,
      "learning_rate": 4.419417835340361e-05,
      "loss": 0.1737,
      "step": 130200
    },
    {
      "epoch": 0.8464061840267628,
      "grad_norm": 1.5185030698776245,
      "learning_rate": 4.418726789497762e-05,
      "loss": 0.1921,
      "step": 130300
    },
    {
      "epoch": 0.8470557666699146,
      "grad_norm": 1.445383071899414,
      "learning_rate": 4.418035743655163e-05,
      "loss": 0.1839,
      "step": 130400
    },
    {
      "epoch": 0.8477053493130664,
      "grad_norm": 1.696845293045044,
      "learning_rate": 4.417344697812564e-05,
      "loss": 0.1801,
      "step": 130500
    },
    {
      "epoch": 0.8483549319562181,
      "grad_norm": 1.6365554332733154,
      "learning_rate": 4.416653651969965e-05,
      "loss": 0.176,
      "step": 130600
    },
    {
      "epoch": 0.8490045145993699,
      "grad_norm": 1.3635079860687256,
      "learning_rate": 4.415962606127366e-05,
      "loss": 0.1735,
      "step": 130700
    },
    {
      "epoch": 0.8496540972425217,
      "grad_norm": 1.0335392951965332,
      "learning_rate": 4.415271560284767e-05,
      "loss": 0.1782,
      "step": 130800
    },
    {
      "epoch": 0.8503036798856735,
      "grad_norm": 2.1032047271728516,
      "learning_rate": 4.414580514442167e-05,
      "loss": 0.1738,
      "step": 130900
    },
    {
      "epoch": 0.8509532625288252,
      "grad_norm": 1.042677640914917,
      "learning_rate": 4.413889468599568e-05,
      "loss": 0.1687,
      "step": 131000
    },
    {
      "epoch": 0.851602845171977,
      "grad_norm": 1.4585288763046265,
      "learning_rate": 4.413198422756969e-05,
      "loss": 0.1766,
      "step": 131100
    },
    {
      "epoch": 0.8522524278151288,
      "grad_norm": 1.5549426078796387,
      "learning_rate": 4.41250737691437e-05,
      "loss": 0.1836,
      "step": 131200
    },
    {
      "epoch": 0.8529020104582805,
      "grad_norm": 1.5911712646484375,
      "learning_rate": 4.4118163310717705e-05,
      "loss": 0.1714,
      "step": 131300
    },
    {
      "epoch": 0.8535515931014324,
      "grad_norm": 1.512233018875122,
      "learning_rate": 4.4111252852291715e-05,
      "loss": 0.1839,
      "step": 131400
    },
    {
      "epoch": 0.8542011757445841,
      "grad_norm": 1.4952458143234253,
      "learning_rate": 4.4104342393865725e-05,
      "loss": 0.1858,
      "step": 131500
    },
    {
      "epoch": 0.8548507583877358,
      "grad_norm": 1.3919204473495483,
      "learning_rate": 4.4097431935439735e-05,
      "loss": 0.1815,
      "step": 131600
    },
    {
      "epoch": 0.8555003410308877,
      "grad_norm": 1.2734805345535278,
      "learning_rate": 4.4090521477013745e-05,
      "loss": 0.1782,
      "step": 131700
    },
    {
      "epoch": 0.8561499236740394,
      "grad_norm": 1.3962664604187012,
      "learning_rate": 4.408361101858775e-05,
      "loss": 0.1762,
      "step": 131800
    },
    {
      "epoch": 0.8567995063171912,
      "grad_norm": 1.4923994541168213,
      "learning_rate": 4.407670056016176e-05,
      "loss": 0.174,
      "step": 131900
    },
    {
      "epoch": 0.857449088960343,
      "grad_norm": 1.2470663785934448,
      "learning_rate": 4.406979010173577e-05,
      "loss": 0.1804,
      "step": 132000
    },
    {
      "epoch": 0.8580986716034947,
      "grad_norm": 1.5794544219970703,
      "learning_rate": 4.406287964330978e-05,
      "loss": 0.171,
      "step": 132100
    },
    {
      "epoch": 0.8587482542466466,
      "grad_norm": 1.2756544351577759,
      "learning_rate": 4.405596918488379e-05,
      "loss": 0.1752,
      "step": 132200
    },
    {
      "epoch": 0.8593978368897983,
      "grad_norm": 1.2904177904129028,
      "learning_rate": 4.40490587264578e-05,
      "loss": 0.1752,
      "step": 132300
    },
    {
      "epoch": 0.86004741953295,
      "grad_norm": 1.0853992700576782,
      "learning_rate": 4.404214826803181e-05,
      "loss": 0.1793,
      "step": 132400
    },
    {
      "epoch": 0.8606970021761019,
      "grad_norm": 1.8005303144454956,
      "learning_rate": 4.403523780960582e-05,
      "loss": 0.1816,
      "step": 132500
    },
    {
      "epoch": 0.8613465848192536,
      "grad_norm": 1.5719765424728394,
      "learning_rate": 4.4028327351179827e-05,
      "loss": 0.1819,
      "step": 132600
    },
    {
      "epoch": 0.8619961674624054,
      "grad_norm": 1.2061339616775513,
      "learning_rate": 4.4021416892753836e-05,
      "loss": 0.1796,
      "step": 132700
    },
    {
      "epoch": 0.8626457501055572,
      "grad_norm": 1.3794775009155273,
      "learning_rate": 4.401450643432784e-05,
      "loss": 0.1774,
      "step": 132800
    },
    {
      "epoch": 0.863295332748709,
      "grad_norm": 1.4954030513763428,
      "learning_rate": 4.400759597590185e-05,
      "loss": 0.1747,
      "step": 132900
    },
    {
      "epoch": 0.8639449153918607,
      "grad_norm": 1.2195953130722046,
      "learning_rate": 4.400068551747586e-05,
      "loss": 0.179,
      "step": 133000
    },
    {
      "epoch": 0.8645944980350125,
      "grad_norm": 1.5701082944869995,
      "learning_rate": 4.399377505904987e-05,
      "loss": 0.1725,
      "step": 133100
    },
    {
      "epoch": 0.8652440806781643,
      "grad_norm": 1.4909167289733887,
      "learning_rate": 4.398686460062388e-05,
      "loss": 0.1685,
      "step": 133200
    },
    {
      "epoch": 0.865893663321316,
      "grad_norm": 1.9857336282730103,
      "learning_rate": 4.397995414219789e-05,
      "loss": 0.1793,
      "step": 133300
    },
    {
      "epoch": 0.8665432459644679,
      "grad_norm": 1.9501742124557495,
      "learning_rate": 4.39730436837719e-05,
      "loss": 0.179,
      "step": 133400
    },
    {
      "epoch": 0.8671928286076196,
      "grad_norm": 0.955092191696167,
      "learning_rate": 4.396613322534591e-05,
      "loss": 0.1722,
      "step": 133500
    },
    {
      "epoch": 0.8678424112507713,
      "grad_norm": 1.4918251037597656,
      "learning_rate": 4.395922276691992e-05,
      "loss": 0.1781,
      "step": 133600
    },
    {
      "epoch": 0.8684919938939232,
      "grad_norm": 1.193329095840454,
      "learning_rate": 4.395231230849392e-05,
      "loss": 0.1733,
      "step": 133700
    },
    {
      "epoch": 0.8691415765370749,
      "grad_norm": 1.484208106994629,
      "learning_rate": 4.394540185006793e-05,
      "loss": 0.1745,
      "step": 133800
    },
    {
      "epoch": 0.8697911591802268,
      "grad_norm": 1.666428565979004,
      "learning_rate": 4.393849139164194e-05,
      "loss": 0.172,
      "step": 133900
    },
    {
      "epoch": 0.8704407418233785,
      "grad_norm": 1.4182873964309692,
      "learning_rate": 4.393158093321595e-05,
      "loss": 0.1759,
      "step": 134000
    },
    {
      "epoch": 0.8710903244665302,
      "grad_norm": 1.7941280603408813,
      "learning_rate": 4.392467047478996e-05,
      "loss": 0.1761,
      "step": 134100
    },
    {
      "epoch": 0.8717399071096821,
      "grad_norm": 1.41727876663208,
      "learning_rate": 4.391776001636397e-05,
      "loss": 0.1757,
      "step": 134200
    },
    {
      "epoch": 0.8723894897528338,
      "grad_norm": 1.335458517074585,
      "learning_rate": 4.391084955793798e-05,
      "loss": 0.1803,
      "step": 134300
    },
    {
      "epoch": 0.8730390723959855,
      "grad_norm": 1.089991569519043,
      "learning_rate": 4.390393909951199e-05,
      "loss": 0.183,
      "step": 134400
    },
    {
      "epoch": 0.8736886550391374,
      "grad_norm": 1.388725996017456,
      "learning_rate": 4.389702864108599e-05,
      "loss": 0.1698,
      "step": 134500
    },
    {
      "epoch": 0.8743382376822891,
      "grad_norm": 1.2984943389892578,
      "learning_rate": 4.389011818266e-05,
      "loss": 0.176,
      "step": 134600
    },
    {
      "epoch": 0.8749878203254409,
      "grad_norm": 1.7298914194107056,
      "learning_rate": 4.388320772423401e-05,
      "loss": 0.1799,
      "step": 134700
    },
    {
      "epoch": 0.8756374029685927,
      "grad_norm": 1.5463279485702515,
      "learning_rate": 4.387629726580802e-05,
      "loss": 0.1747,
      "step": 134800
    },
    {
      "epoch": 0.8762869856117445,
      "grad_norm": 1.6103140115737915,
      "learning_rate": 4.3869386807382026e-05,
      "loss": 0.1753,
      "step": 134900
    },
    {
      "epoch": 0.8769365682548962,
      "grad_norm": 1.7693626880645752,
      "learning_rate": 4.3862476348956036e-05,
      "loss": 0.1709,
      "step": 135000
    },
    {
      "epoch": 0.877586150898048,
      "grad_norm": 1.6580801010131836,
      "learning_rate": 4.3855565890530046e-05,
      "loss": 0.1753,
      "step": 135100
    },
    {
      "epoch": 0.8782357335411998,
      "grad_norm": 1.4591624736785889,
      "learning_rate": 4.3848655432104056e-05,
      "loss": 0.1694,
      "step": 135200
    },
    {
      "epoch": 0.8788853161843515,
      "grad_norm": 1.5379369258880615,
      "learning_rate": 4.3841744973678065e-05,
      "loss": 0.1719,
      "step": 135300
    },
    {
      "epoch": 0.8795348988275034,
      "grad_norm": 1.1937364339828491,
      "learning_rate": 4.3834834515252075e-05,
      "loss": 0.1714,
      "step": 135400
    },
    {
      "epoch": 0.8801844814706551,
      "grad_norm": 1.4738309383392334,
      "learning_rate": 4.3827924056826085e-05,
      "loss": 0.1651,
      "step": 135500
    },
    {
      "epoch": 0.8808340641138068,
      "grad_norm": 1.5881892442703247,
      "learning_rate": 4.382101359840009e-05,
      "loss": 0.169,
      "step": 135600
    },
    {
      "epoch": 0.8814836467569587,
      "grad_norm": 1.741869330406189,
      "learning_rate": 4.38141031399741e-05,
      "loss": 0.1822,
      "step": 135700
    },
    {
      "epoch": 0.8821332294001104,
      "grad_norm": 1.266081690788269,
      "learning_rate": 4.380719268154811e-05,
      "loss": 0.1776,
      "step": 135800
    },
    {
      "epoch": 0.8827828120432623,
      "grad_norm": 1.302151083946228,
      "learning_rate": 4.380028222312212e-05,
      "loss": 0.1748,
      "step": 135900
    },
    {
      "epoch": 0.883432394686414,
      "grad_norm": 1.539236307144165,
      "learning_rate": 4.379337176469613e-05,
      "loss": 0.1767,
      "step": 136000
    },
    {
      "epoch": 0.8840819773295657,
      "grad_norm": 1.663916826248169,
      "learning_rate": 4.378646130627014e-05,
      "loss": 0.1754,
      "step": 136100
    },
    {
      "epoch": 0.8847315599727176,
      "grad_norm": 1.1992758512496948,
      "learning_rate": 4.377955084784415e-05,
      "loss": 0.172,
      "step": 136200
    },
    {
      "epoch": 0.8853811426158693,
      "grad_norm": 1.2430399656295776,
      "learning_rate": 4.377264038941816e-05,
      "loss": 0.1777,
      "step": 136300
    },
    {
      "epoch": 0.886030725259021,
      "grad_norm": 1.1617931127548218,
      "learning_rate": 4.376572993099217e-05,
      "loss": 0.1765,
      "step": 136400
    },
    {
      "epoch": 0.8866803079021729,
      "grad_norm": 1.7992067337036133,
      "learning_rate": 4.375881947256617e-05,
      "loss": 0.1742,
      "step": 136500
    },
    {
      "epoch": 0.8873298905453246,
      "grad_norm": 1.3517532348632812,
      "learning_rate": 4.375190901414018e-05,
      "loss": 0.1836,
      "step": 136600
    },
    {
      "epoch": 0.8879794731884764,
      "grad_norm": 1.5375699996948242,
      "learning_rate": 4.374499855571419e-05,
      "loss": 0.1752,
      "step": 136700
    },
    {
      "epoch": 0.8886290558316282,
      "grad_norm": 1.223276138305664,
      "learning_rate": 4.37380880972882e-05,
      "loss": 0.1761,
      "step": 136800
    },
    {
      "epoch": 0.88927863847478,
      "grad_norm": 1.5212455987930298,
      "learning_rate": 4.373117763886221e-05,
      "loss": 0.1753,
      "step": 136900
    },
    {
      "epoch": 0.8899282211179317,
      "grad_norm": 1.5559005737304688,
      "learning_rate": 4.372426718043622e-05,
      "loss": 0.1797,
      "step": 137000
    },
    {
      "epoch": 0.8905778037610835,
      "grad_norm": 1.4280258417129517,
      "learning_rate": 4.371735672201023e-05,
      "loss": 0.1765,
      "step": 137100
    },
    {
      "epoch": 0.8912273864042353,
      "grad_norm": 1.3732523918151855,
      "learning_rate": 4.371044626358424e-05,
      "loss": 0.1766,
      "step": 137200
    },
    {
      "epoch": 0.891876969047387,
      "grad_norm": 1.1058549880981445,
      "learning_rate": 4.370353580515825e-05,
      "loss": 0.1714,
      "step": 137300
    },
    {
      "epoch": 0.8925265516905388,
      "grad_norm": 1.0790612697601318,
      "learning_rate": 4.369662534673226e-05,
      "loss": 0.1711,
      "step": 137400
    },
    {
      "epoch": 0.8931761343336906,
      "grad_norm": 1.556707739830017,
      "learning_rate": 4.368971488830626e-05,
      "loss": 0.1686,
      "step": 137500
    },
    {
      "epoch": 0.8938257169768424,
      "grad_norm": 1.8259835243225098,
      "learning_rate": 4.368280442988027e-05,
      "loss": 0.1803,
      "step": 137600
    },
    {
      "epoch": 0.8944752996199942,
      "grad_norm": 1.1069952249526978,
      "learning_rate": 4.367589397145428e-05,
      "loss": 0.1727,
      "step": 137700
    },
    {
      "epoch": 0.8951248822631459,
      "grad_norm": 1.406540870666504,
      "learning_rate": 4.366898351302829e-05,
      "loss": 0.1672,
      "step": 137800
    },
    {
      "epoch": 0.8957744649062978,
      "grad_norm": 1.5924233198165894,
      "learning_rate": 4.36620730546023e-05,
      "loss": 0.1706,
      "step": 137900
    },
    {
      "epoch": 0.8964240475494495,
      "grad_norm": 1.0996453762054443,
      "learning_rate": 4.365516259617631e-05,
      "loss": 0.1683,
      "step": 138000
    },
    {
      "epoch": 0.8970736301926012,
      "grad_norm": 1.5999138355255127,
      "learning_rate": 4.3648252137750314e-05,
      "loss": 0.1696,
      "step": 138100
    },
    {
      "epoch": 0.8977232128357531,
      "grad_norm": 1.346989393234253,
      "learning_rate": 4.3641341679324324e-05,
      "loss": 0.1672,
      "step": 138200
    },
    {
      "epoch": 0.8983727954789048,
      "grad_norm": 1.2611137628555298,
      "learning_rate": 4.3634431220898334e-05,
      "loss": 0.1687,
      "step": 138300
    },
    {
      "epoch": 0.8990223781220565,
      "grad_norm": 1.4958771467208862,
      "learning_rate": 4.3627520762472344e-05,
      "loss": 0.1761,
      "step": 138400
    },
    {
      "epoch": 0.8996719607652084,
      "grad_norm": 1.3153260946273804,
      "learning_rate": 4.362061030404635e-05,
      "loss": 0.1686,
      "step": 138500
    },
    {
      "epoch": 0.9003215434083601,
      "grad_norm": 1.603724718093872,
      "learning_rate": 4.361369984562036e-05,
      "loss": 0.1796,
      "step": 138600
    },
    {
      "epoch": 0.9009711260515119,
      "grad_norm": 1.2549350261688232,
      "learning_rate": 4.3606789387194366e-05,
      "loss": 0.1671,
      "step": 138700
    },
    {
      "epoch": 0.9016207086946637,
      "grad_norm": 1.7647794485092163,
      "learning_rate": 4.3599878928768376e-05,
      "loss": 0.1736,
      "step": 138800
    },
    {
      "epoch": 0.9022702913378154,
      "grad_norm": 1.6357640027999878,
      "learning_rate": 4.3592968470342386e-05,
      "loss": 0.1745,
      "step": 138900
    },
    {
      "epoch": 0.9029198739809672,
      "grad_norm": 1.042960286140442,
      "learning_rate": 4.3586058011916396e-05,
      "loss": 0.1686,
      "step": 139000
    },
    {
      "epoch": 0.903569456624119,
      "grad_norm": 1.6262388229370117,
      "learning_rate": 4.3579147553490406e-05,
      "loss": 0.1738,
      "step": 139100
    },
    {
      "epoch": 0.9042190392672708,
      "grad_norm": 1.3417205810546875,
      "learning_rate": 4.3572237095064416e-05,
      "loss": 0.1727,
      "step": 139200
    },
    {
      "epoch": 0.9048686219104225,
      "grad_norm": 1.0083222389221191,
      "learning_rate": 4.3565326636638426e-05,
      "loss": 0.1634,
      "step": 139300
    },
    {
      "epoch": 0.9055182045535743,
      "grad_norm": 1.6464143991470337,
      "learning_rate": 4.355841617821243e-05,
      "loss": 0.1713,
      "step": 139400
    },
    {
      "epoch": 0.9061677871967261,
      "grad_norm": 1.4879664182662964,
      "learning_rate": 4.355150571978644e-05,
      "loss": 0.1697,
      "step": 139500
    },
    {
      "epoch": 0.9068173698398779,
      "grad_norm": 1.4526616334915161,
      "learning_rate": 4.354459526136045e-05,
      "loss": 0.1715,
      "step": 139600
    },
    {
      "epoch": 0.9074669524830297,
      "grad_norm": 1.5730791091918945,
      "learning_rate": 4.353768480293446e-05,
      "loss": 0.1753,
      "step": 139700
    },
    {
      "epoch": 0.9081165351261814,
      "grad_norm": 1.4293190240859985,
      "learning_rate": 4.353077434450847e-05,
      "loss": 0.1712,
      "step": 139800
    },
    {
      "epoch": 0.9087661177693332,
      "grad_norm": 1.3337688446044922,
      "learning_rate": 4.352386388608248e-05,
      "loss": 0.1665,
      "step": 139900
    },
    {
      "epoch": 0.909415700412485,
      "grad_norm": 1.4270540475845337,
      "learning_rate": 4.351695342765649e-05,
      "loss": 0.169,
      "step": 140000
    },
    {
      "epoch": 0.9100652830556367,
      "grad_norm": 1.3402916193008423,
      "learning_rate": 4.35100429692305e-05,
      "loss": 0.1716,
      "step": 140100
    },
    {
      "epoch": 0.9107148656987886,
      "grad_norm": 1.17219877243042,
      "learning_rate": 4.350313251080451e-05,
      "loss": 0.1622,
      "step": 140200
    },
    {
      "epoch": 0.9113644483419403,
      "grad_norm": 1.3029838800430298,
      "learning_rate": 4.349622205237851e-05,
      "loss": 0.1643,
      "step": 140300
    },
    {
      "epoch": 0.912014030985092,
      "grad_norm": 2.006976366043091,
      "learning_rate": 4.348931159395252e-05,
      "loss": 0.1684,
      "step": 140400
    },
    {
      "epoch": 0.9126636136282439,
      "grad_norm": 1.4703854322433472,
      "learning_rate": 4.348240113552653e-05,
      "loss": 0.1719,
      "step": 140500
    },
    {
      "epoch": 0.9133131962713956,
      "grad_norm": 1.6983803510665894,
      "learning_rate": 4.347549067710054e-05,
      "loss": 0.1689,
      "step": 140600
    },
    {
      "epoch": 0.9139627789145474,
      "grad_norm": 1.328250527381897,
      "learning_rate": 4.346858021867455e-05,
      "loss": 0.1717,
      "step": 140700
    },
    {
      "epoch": 0.9146123615576992,
      "grad_norm": 2.5097556114196777,
      "learning_rate": 4.346166976024856e-05,
      "loss": 0.1727,
      "step": 140800
    },
    {
      "epoch": 0.9152619442008509,
      "grad_norm": 1.5105078220367432,
      "learning_rate": 4.345475930182257e-05,
      "loss": 0.1663,
      "step": 140900
    },
    {
      "epoch": 0.9159115268440027,
      "grad_norm": 1.2343788146972656,
      "learning_rate": 4.344784884339658e-05,
      "loss": 0.1756,
      "step": 141000
    },
    {
      "epoch": 0.9165611094871545,
      "grad_norm": 1.2338203191757202,
      "learning_rate": 4.344093838497059e-05,
      "loss": 0.1773,
      "step": 141100
    },
    {
      "epoch": 0.9172106921303063,
      "grad_norm": 1.2443101406097412,
      "learning_rate": 4.34340279265446e-05,
      "loss": 0.1726,
      "step": 141200
    },
    {
      "epoch": 0.9178602747734581,
      "grad_norm": 1.8523361682891846,
      "learning_rate": 4.34271174681186e-05,
      "loss": 0.1669,
      "step": 141300
    },
    {
      "epoch": 0.9185098574166098,
      "grad_norm": 1.0344713926315308,
      "learning_rate": 4.342020700969261e-05,
      "loss": 0.1723,
      "step": 141400
    },
    {
      "epoch": 0.9191594400597616,
      "grad_norm": 1.3721551895141602,
      "learning_rate": 4.341329655126662e-05,
      "loss": 0.1752,
      "step": 141500
    },
    {
      "epoch": 0.9198090227029134,
      "grad_norm": 1.1043339967727661,
      "learning_rate": 4.340638609284063e-05,
      "loss": 0.1752,
      "step": 141600
    },
    {
      "epoch": 0.9204586053460652,
      "grad_norm": 1.4592238664627075,
      "learning_rate": 4.3399475634414635e-05,
      "loss": 0.1622,
      "step": 141700
    },
    {
      "epoch": 0.9211081879892169,
      "grad_norm": 2.038898229598999,
      "learning_rate": 4.3392565175988645e-05,
      "loss": 0.1735,
      "step": 141800
    },
    {
      "epoch": 0.9217577706323687,
      "grad_norm": 1.2465335130691528,
      "learning_rate": 4.3385654717562655e-05,
      "loss": 0.1766,
      "step": 141900
    },
    {
      "epoch": 0.9224073532755205,
      "grad_norm": 1.4189300537109375,
      "learning_rate": 4.3378744259136664e-05,
      "loss": 0.1716,
      "step": 142000
    },
    {
      "epoch": 0.9230569359186722,
      "grad_norm": 1.5783377885818481,
      "learning_rate": 4.3371833800710674e-05,
      "loss": 0.169,
      "step": 142100
    },
    {
      "epoch": 0.9237065185618241,
      "grad_norm": 1.61380934715271,
      "learning_rate": 4.336492334228468e-05,
      "loss": 0.1665,
      "step": 142200
    },
    {
      "epoch": 0.9243561012049758,
      "grad_norm": 1.344033122062683,
      "learning_rate": 4.335801288385869e-05,
      "loss": 0.1679,
      "step": 142300
    },
    {
      "epoch": 0.9250056838481275,
      "grad_norm": 1.6827150583267212,
      "learning_rate": 4.33511024254327e-05,
      "loss": 0.1674,
      "step": 142400
    },
    {
      "epoch": 0.9256552664912794,
      "grad_norm": 1.1410921812057495,
      "learning_rate": 4.334419196700671e-05,
      "loss": 0.1648,
      "step": 142500
    },
    {
      "epoch": 0.9263048491344311,
      "grad_norm": 1.1203171014785767,
      "learning_rate": 4.333728150858072e-05,
      "loss": 0.169,
      "step": 142600
    },
    {
      "epoch": 0.9269544317775829,
      "grad_norm": 0.9907063841819763,
      "learning_rate": 4.333037105015473e-05,
      "loss": 0.1704,
      "step": 142700
    },
    {
      "epoch": 0.9276040144207347,
      "grad_norm": 1.3428339958190918,
      "learning_rate": 4.3323460591728737e-05,
      "loss": 0.1725,
      "step": 142800
    },
    {
      "epoch": 0.9282535970638864,
      "grad_norm": 1.3969695568084717,
      "learning_rate": 4.3316550133302746e-05,
      "loss": 0.171,
      "step": 142900
    },
    {
      "epoch": 0.9289031797070382,
      "grad_norm": 1.1928826570510864,
      "learning_rate": 4.3309639674876756e-05,
      "loss": 0.1674,
      "step": 143000
    },
    {
      "epoch": 0.92955276235019,
      "grad_norm": 1.4756499528884888,
      "learning_rate": 4.330272921645076e-05,
      "loss": 0.1655,
      "step": 143100
    },
    {
      "epoch": 0.9302023449933418,
      "grad_norm": 1.6357240676879883,
      "learning_rate": 4.329581875802477e-05,
      "loss": 0.1816,
      "step": 143200
    },
    {
      "epoch": 0.9308519276364936,
      "grad_norm": 1.6405701637268066,
      "learning_rate": 4.328890829959878e-05,
      "loss": 0.167,
      "step": 143300
    },
    {
      "epoch": 0.9315015102796453,
      "grad_norm": 1.358625888824463,
      "learning_rate": 4.328199784117279e-05,
      "loss": 0.1699,
      "step": 143400
    },
    {
      "epoch": 0.9321510929227971,
      "grad_norm": 0.9986377358436584,
      "learning_rate": 4.32750873827468e-05,
      "loss": 0.1768,
      "step": 143500
    },
    {
      "epoch": 0.9328006755659489,
      "grad_norm": 1.152999997138977,
      "learning_rate": 4.326817692432081e-05,
      "loss": 0.1725,
      "step": 143600
    },
    {
      "epoch": 0.9334502582091007,
      "grad_norm": 1.2949689626693726,
      "learning_rate": 4.326126646589482e-05,
      "loss": 0.1646,
      "step": 143700
    },
    {
      "epoch": 0.9340998408522524,
      "grad_norm": 1.5806190967559814,
      "learning_rate": 4.325435600746883e-05,
      "loss": 0.1725,
      "step": 143800
    },
    {
      "epoch": 0.9347494234954042,
      "grad_norm": 1.6910288333892822,
      "learning_rate": 4.324744554904284e-05,
      "loss": 0.1744,
      "step": 143900
    },
    {
      "epoch": 0.935399006138556,
      "grad_norm": 1.473999261856079,
      "learning_rate": 4.324053509061685e-05,
      "loss": 0.1745,
      "step": 144000
    },
    {
      "epoch": 0.9360485887817077,
      "grad_norm": 1.2137930393218994,
      "learning_rate": 4.323362463219085e-05,
      "loss": 0.1735,
      "step": 144100
    },
    {
      "epoch": 0.9366981714248596,
      "grad_norm": 1.58351469039917,
      "learning_rate": 4.322671417376486e-05,
      "loss": 0.1682,
      "step": 144200
    },
    {
      "epoch": 0.9373477540680113,
      "grad_norm": 1.7422692775726318,
      "learning_rate": 4.321980371533887e-05,
      "loss": 0.1718,
      "step": 144300
    },
    {
      "epoch": 0.937997336711163,
      "grad_norm": 1.0820496082305908,
      "learning_rate": 4.321289325691288e-05,
      "loss": 0.1664,
      "step": 144400
    },
    {
      "epoch": 0.9386469193543149,
      "grad_norm": 1.5724035501480103,
      "learning_rate": 4.320598279848689e-05,
      "loss": 0.1691,
      "step": 144500
    },
    {
      "epoch": 0.9392965019974666,
      "grad_norm": 1.6601760387420654,
      "learning_rate": 4.31990723400609e-05,
      "loss": 0.1687,
      "step": 144600
    },
    {
      "epoch": 0.9399460846406184,
      "grad_norm": 1.4487388134002686,
      "learning_rate": 4.319216188163491e-05,
      "loss": 0.1697,
      "step": 144700
    },
    {
      "epoch": 0.9405956672837702,
      "grad_norm": 1.7529951333999634,
      "learning_rate": 4.318525142320892e-05,
      "loss": 0.1696,
      "step": 144800
    },
    {
      "epoch": 0.9412452499269219,
      "grad_norm": 0.9213592410087585,
      "learning_rate": 4.317834096478292e-05,
      "loss": 0.1694,
      "step": 144900
    },
    {
      "epoch": 0.9418948325700738,
      "grad_norm": 1.6004606485366821,
      "learning_rate": 4.317143050635693e-05,
      "loss": 0.1678,
      "step": 145000
    },
    {
      "epoch": 0.9425444152132255,
      "grad_norm": 1.2694013118743896,
      "learning_rate": 4.316452004793094e-05,
      "loss": 0.1654,
      "step": 145100
    },
    {
      "epoch": 0.9431939978563773,
      "grad_norm": 1.3064545392990112,
      "learning_rate": 4.3157609589504946e-05,
      "loss": 0.1636,
      "step": 145200
    },
    {
      "epoch": 0.9438435804995291,
      "grad_norm": 1.6670033931732178,
      "learning_rate": 4.3150699131078956e-05,
      "loss": 0.1674,
      "step": 145300
    },
    {
      "epoch": 0.9444931631426808,
      "grad_norm": 1.9834058284759521,
      "learning_rate": 4.3143788672652966e-05,
      "loss": 0.1681,
      "step": 145400
    },
    {
      "epoch": 0.9451427457858326,
      "grad_norm": 1.6037967205047607,
      "learning_rate": 4.3136878214226975e-05,
      "loss": 0.1723,
      "step": 145500
    },
    {
      "epoch": 0.9457923284289844,
      "grad_norm": 1.1356146335601807,
      "learning_rate": 4.3129967755800985e-05,
      "loss": 0.1674,
      "step": 145600
    },
    {
      "epoch": 0.9464419110721362,
      "grad_norm": 1.842963457107544,
      "learning_rate": 4.3123057297374995e-05,
      "loss": 0.1627,
      "step": 145700
    },
    {
      "epoch": 0.9470914937152879,
      "grad_norm": 1.2723051309585571,
      "learning_rate": 4.3116146838949005e-05,
      "loss": 0.1626,
      "step": 145800
    },
    {
      "epoch": 0.9477410763584397,
      "grad_norm": 1.8062620162963867,
      "learning_rate": 4.3109236380523015e-05,
      "loss": 0.1705,
      "step": 145900
    },
    {
      "epoch": 0.9483906590015915,
      "grad_norm": 1.4100170135498047,
      "learning_rate": 4.310232592209702e-05,
      "loss": 0.1617,
      "step": 146000
    },
    {
      "epoch": 0.9490402416447432,
      "grad_norm": 1.6224535703659058,
      "learning_rate": 4.309541546367103e-05,
      "loss": 0.1624,
      "step": 146100
    },
    {
      "epoch": 0.9496898242878951,
      "grad_norm": 1.3050795793533325,
      "learning_rate": 4.308850500524504e-05,
      "loss": 0.1727,
      "step": 146200
    },
    {
      "epoch": 0.9503394069310468,
      "grad_norm": 1.4006646871566772,
      "learning_rate": 4.308159454681905e-05,
      "loss": 0.1584,
      "step": 146300
    },
    {
      "epoch": 0.9509889895741985,
      "grad_norm": 1.9529151916503906,
      "learning_rate": 4.307468408839306e-05,
      "loss": 0.1699,
      "step": 146400
    },
    {
      "epoch": 0.9516385722173504,
      "grad_norm": 1.3459057807922363,
      "learning_rate": 4.306777362996707e-05,
      "loss": 0.1675,
      "step": 146500
    },
    {
      "epoch": 0.9522881548605021,
      "grad_norm": 1.5892398357391357,
      "learning_rate": 4.306086317154108e-05,
      "loss": 0.1715,
      "step": 146600
    },
    {
      "epoch": 0.9529377375036538,
      "grad_norm": 1.6869670152664185,
      "learning_rate": 4.305395271311509e-05,
      "loss": 0.1751,
      "step": 146700
    },
    {
      "epoch": 0.9535873201468057,
      "grad_norm": 1.8946272134780884,
      "learning_rate": 4.30470422546891e-05,
      "loss": 0.1716,
      "step": 146800
    },
    {
      "epoch": 0.9542369027899574,
      "grad_norm": 0.8381455540657043,
      "learning_rate": 4.30401317962631e-05,
      "loss": 0.169,
      "step": 146900
    },
    {
      "epoch": 0.9548864854331093,
      "grad_norm": 1.5294501781463623,
      "learning_rate": 4.303322133783711e-05,
      "loss": 0.1598,
      "step": 147000
    },
    {
      "epoch": 0.955536068076261,
      "grad_norm": 1.1952790021896362,
      "learning_rate": 4.302631087941112e-05,
      "loss": 0.1705,
      "step": 147100
    },
    {
      "epoch": 0.9561856507194127,
      "grad_norm": 1.1726638078689575,
      "learning_rate": 4.301940042098513e-05,
      "loss": 0.1732,
      "step": 147200
    },
    {
      "epoch": 0.9568352333625646,
      "grad_norm": 1.6095484495162964,
      "learning_rate": 4.301248996255914e-05,
      "loss": 0.1631,
      "step": 147300
    },
    {
      "epoch": 0.9574848160057163,
      "grad_norm": 1.4284250736236572,
      "learning_rate": 4.300557950413315e-05,
      "loss": 0.1689,
      "step": 147400
    },
    {
      "epoch": 0.9581343986488681,
      "grad_norm": 1.3704792261123657,
      "learning_rate": 4.299866904570716e-05,
      "loss": 0.1752,
      "step": 147500
    },
    {
      "epoch": 0.9587839812920199,
      "grad_norm": 1.4960649013519287,
      "learning_rate": 4.299175858728117e-05,
      "loss": 0.1633,
      "step": 147600
    },
    {
      "epoch": 0.9594335639351717,
      "grad_norm": 1.0774214267730713,
      "learning_rate": 4.298484812885518e-05,
      "loss": 0.1606,
      "step": 147700
    },
    {
      "epoch": 0.9600831465783234,
      "grad_norm": 1.918212652206421,
      "learning_rate": 4.297793767042918e-05,
      "loss": 0.1638,
      "step": 147800
    },
    {
      "epoch": 0.9607327292214752,
      "grad_norm": 1.2258175611495972,
      "learning_rate": 4.297102721200319e-05,
      "loss": 0.1702,
      "step": 147900
    },
    {
      "epoch": 0.961382311864627,
      "grad_norm": 0.897953987121582,
      "learning_rate": 4.29641167535772e-05,
      "loss": 0.1667,
      "step": 148000
    },
    {
      "epoch": 0.9620318945077787,
      "grad_norm": 1.8412240743637085,
      "learning_rate": 4.295720629515121e-05,
      "loss": 0.1771,
      "step": 148100
    },
    {
      "epoch": 0.9626814771509306,
      "grad_norm": 1.6073689460754395,
      "learning_rate": 4.295029583672522e-05,
      "loss": 0.1637,
      "step": 148200
    },
    {
      "epoch": 0.9633310597940823,
      "grad_norm": 1.3961198329925537,
      "learning_rate": 4.294338537829923e-05,
      "loss": 0.168,
      "step": 148300
    },
    {
      "epoch": 0.963980642437234,
      "grad_norm": 1.5249797105789185,
      "learning_rate": 4.293647491987324e-05,
      "loss": 0.1585,
      "step": 148400
    },
    {
      "epoch": 0.9646302250803859,
      "grad_norm": 1.3317649364471436,
      "learning_rate": 4.2929564461447244e-05,
      "loss": 0.1751,
      "step": 148500
    },
    {
      "epoch": 0.9652798077235376,
      "grad_norm": 1.3519691228866577,
      "learning_rate": 4.2922654003021254e-05,
      "loss": 0.1679,
      "step": 148600
    },
    {
      "epoch": 0.9659293903666895,
      "grad_norm": 1.4893783330917358,
      "learning_rate": 4.2915743544595264e-05,
      "loss": 0.1626,
      "step": 148700
    },
    {
      "epoch": 0.9665789730098412,
      "grad_norm": 1.3653843402862549,
      "learning_rate": 4.290883308616927e-05,
      "loss": 0.1627,
      "step": 148800
    },
    {
      "epoch": 0.9672285556529929,
      "grad_norm": 1.3105188608169556,
      "learning_rate": 4.2901922627743277e-05,
      "loss": 0.1658,
      "step": 148900
    },
    {
      "epoch": 0.9678781382961448,
      "grad_norm": 1.1672443151474,
      "learning_rate": 4.2895012169317286e-05,
      "loss": 0.1771,
      "step": 149000
    },
    {
      "epoch": 0.9685277209392965,
      "grad_norm": 1.5155500173568726,
      "learning_rate": 4.2888101710891296e-05,
      "loss": 0.1574,
      "step": 149100
    },
    {
      "epoch": 0.9691773035824482,
      "grad_norm": 1.6441246271133423,
      "learning_rate": 4.2881191252465306e-05,
      "loss": 0.1629,
      "step": 149200
    },
    {
      "epoch": 0.9698268862256001,
      "grad_norm": 1.0958422422409058,
      "learning_rate": 4.2874280794039316e-05,
      "loss": 0.1693,
      "step": 149300
    },
    {
      "epoch": 0.9704764688687518,
      "grad_norm": 1.3267631530761719,
      "learning_rate": 4.2867370335613326e-05,
      "loss": 0.1726,
      "step": 149400
    },
    {
      "epoch": 0.9711260515119036,
      "grad_norm": 1.2190897464752197,
      "learning_rate": 4.2860459877187336e-05,
      "loss": 0.1684,
      "step": 149500
    },
    {
      "epoch": 0.9717756341550554,
      "grad_norm": 1.3681544065475464,
      "learning_rate": 4.2853549418761345e-05,
      "loss": 0.1665,
      "step": 149600
    },
    {
      "epoch": 0.9724252167982071,
      "grad_norm": 1.5277589559555054,
      "learning_rate": 4.284663896033535e-05,
      "loss": 0.1636,
      "step": 149700
    },
    {
      "epoch": 0.9730747994413589,
      "grad_norm": 1.9991801977157593,
      "learning_rate": 4.283972850190936e-05,
      "loss": 0.1655,
      "step": 149800
    },
    {
      "epoch": 0.9737243820845107,
      "grad_norm": 1.5956699848175049,
      "learning_rate": 4.283281804348337e-05,
      "loss": 0.1661,
      "step": 149900
    },
    {
      "epoch": 0.9743739647276625,
      "grad_norm": 1.4506142139434814,
      "learning_rate": 4.282590758505738e-05,
      "loss": 0.1705,
      "step": 150000
    },
    {
      "epoch": 0.9750235473708142,
      "grad_norm": 1.5380605459213257,
      "learning_rate": 4.281899712663139e-05,
      "loss": 0.1691,
      "step": 150100
    },
    {
      "epoch": 0.975673130013966,
      "grad_norm": 0.9583351612091064,
      "learning_rate": 4.28120866682054e-05,
      "loss": 0.1664,
      "step": 150200
    },
    {
      "epoch": 0.9763227126571178,
      "grad_norm": 1.2746940851211548,
      "learning_rate": 4.280517620977941e-05,
      "loss": 0.1614,
      "step": 150300
    },
    {
      "epoch": 0.9769722953002695,
      "grad_norm": 1.3045244216918945,
      "learning_rate": 4.279826575135342e-05,
      "loss": 0.167,
      "step": 150400
    },
    {
      "epoch": 0.9776218779434214,
      "grad_norm": 1.584122657775879,
      "learning_rate": 4.279135529292743e-05,
      "loss": 0.1747,
      "step": 150500
    },
    {
      "epoch": 0.9782714605865731,
      "grad_norm": 1.4912998676300049,
      "learning_rate": 4.278444483450144e-05,
      "loss": 0.1658,
      "step": 150600
    },
    {
      "epoch": 0.978921043229725,
      "grad_norm": 1.4104275703430176,
      "learning_rate": 4.277753437607544e-05,
      "loss": 0.1685,
      "step": 150700
    },
    {
      "epoch": 0.9795706258728767,
      "grad_norm": 1.5703157186508179,
      "learning_rate": 4.277062391764945e-05,
      "loss": 0.1648,
      "step": 150800
    },
    {
      "epoch": 0.9802202085160284,
      "grad_norm": 1.5125510692596436,
      "learning_rate": 4.276371345922346e-05,
      "loss": 0.1603,
      "step": 150900
    },
    {
      "epoch": 0.9808697911591803,
      "grad_norm": 1.849138617515564,
      "learning_rate": 4.275680300079747e-05,
      "loss": 0.1485,
      "step": 151000
    },
    {
      "epoch": 0.981519373802332,
      "grad_norm": 1.4198870658874512,
      "learning_rate": 4.274989254237148e-05,
      "loss": 0.1602,
      "step": 151100
    },
    {
      "epoch": 0.9821689564454837,
      "grad_norm": 1.601399540901184,
      "learning_rate": 4.274298208394549e-05,
      "loss": 0.1698,
      "step": 151200
    },
    {
      "epoch": 0.9828185390886356,
      "grad_norm": 1.4355089664459229,
      "learning_rate": 4.27360716255195e-05,
      "loss": 0.1686,
      "step": 151300
    },
    {
      "epoch": 0.9834681217317873,
      "grad_norm": 1.2601052522659302,
      "learning_rate": 4.272916116709351e-05,
      "loss": 0.1684,
      "step": 151400
    },
    {
      "epoch": 0.9841177043749391,
      "grad_norm": 1.2176281213760376,
      "learning_rate": 4.272225070866752e-05,
      "loss": 0.161,
      "step": 151500
    },
    {
      "epoch": 0.9847672870180909,
      "grad_norm": 1.011656403541565,
      "learning_rate": 4.271534025024152e-05,
      "loss": 0.1638,
      "step": 151600
    },
    {
      "epoch": 0.9854168696612426,
      "grad_norm": 1.3270304203033447,
      "learning_rate": 4.270842979181553e-05,
      "loss": 0.1774,
      "step": 151700
    },
    {
      "epoch": 0.9860664523043944,
      "grad_norm": 1.2316248416900635,
      "learning_rate": 4.270151933338954e-05,
      "loss": 0.1699,
      "step": 151800
    },
    {
      "epoch": 0.9867160349475462,
      "grad_norm": 1.1668832302093506,
      "learning_rate": 4.269460887496355e-05,
      "loss": 0.1673,
      "step": 151900
    },
    {
      "epoch": 0.987365617590698,
      "grad_norm": 1.391981840133667,
      "learning_rate": 4.2687698416537555e-05,
      "loss": 0.1696,
      "step": 152000
    },
    {
      "epoch": 0.9880152002338497,
      "grad_norm": 1.5988575220108032,
      "learning_rate": 4.2680787958111565e-05,
      "loss": 0.1683,
      "step": 152100
    },
    {
      "epoch": 0.9886647828770015,
      "grad_norm": 2.1219420433044434,
      "learning_rate": 4.2673877499685575e-05,
      "loss": 0.1612,
      "step": 152200
    },
    {
      "epoch": 0.9893143655201533,
      "grad_norm": 1.2095980644226074,
      "learning_rate": 4.2666967041259584e-05,
      "loss": 0.1681,
      "step": 152300
    },
    {
      "epoch": 0.9899639481633051,
      "grad_norm": 1.3834445476531982,
      "learning_rate": 4.2660056582833594e-05,
      "loss": 0.1714,
      "step": 152400
    },
    {
      "epoch": 0.9906135308064569,
      "grad_norm": 2.0278117656707764,
      "learning_rate": 4.26531461244076e-05,
      "loss": 0.1628,
      "step": 152500
    },
    {
      "epoch": 0.9912631134496086,
      "grad_norm": 1.2552180290222168,
      "learning_rate": 4.264623566598161e-05,
      "loss": 0.1667,
      "step": 152600
    },
    {
      "epoch": 0.9919126960927604,
      "grad_norm": 1.4156320095062256,
      "learning_rate": 4.263932520755562e-05,
      "loss": 0.1706,
      "step": 152700
    },
    {
      "epoch": 0.9925622787359122,
      "grad_norm": 1.3523918390274048,
      "learning_rate": 4.263241474912963e-05,
      "loss": 0.1719,
      "step": 152800
    },
    {
      "epoch": 0.9932118613790639,
      "grad_norm": 1.154470682144165,
      "learning_rate": 4.262550429070364e-05,
      "loss": 0.1662,
      "step": 152900
    },
    {
      "epoch": 0.9938614440222158,
      "grad_norm": 1.5293585062026978,
      "learning_rate": 4.2618593832277647e-05,
      "loss": 0.1675,
      "step": 153000
    },
    {
      "epoch": 0.9945110266653675,
      "grad_norm": 1.4389309883117676,
      "learning_rate": 4.2611683373851656e-05,
      "loss": 0.1586,
      "step": 153100
    },
    {
      "epoch": 0.9951606093085192,
      "grad_norm": 1.364861011505127,
      "learning_rate": 4.2604772915425666e-05,
      "loss": 0.1584,
      "step": 153200
    },
    {
      "epoch": 0.9958101919516711,
      "grad_norm": 1.5176640748977661,
      "learning_rate": 4.2597862456999676e-05,
      "loss": 0.1642,
      "step": 153300
    },
    {
      "epoch": 0.9964597745948228,
      "grad_norm": 1.7272067070007324,
      "learning_rate": 4.2590951998573686e-05,
      "loss": 0.1743,
      "step": 153400
    },
    {
      "epoch": 0.9971093572379746,
      "grad_norm": 1.5589993000030518,
      "learning_rate": 4.258404154014769e-05,
      "loss": 0.1656,
      "step": 153500
    },
    {
      "epoch": 0.9977589398811264,
      "grad_norm": 1.3199681043624878,
      "learning_rate": 4.25771310817217e-05,
      "loss": 0.1634,
      "step": 153600
    },
    {
      "epoch": 0.9984085225242781,
      "grad_norm": 1.4430087804794312,
      "learning_rate": 4.257022062329571e-05,
      "loss": 0.161,
      "step": 153700
    },
    {
      "epoch": 0.9990581051674299,
      "grad_norm": 1.0545488595962524,
      "learning_rate": 4.256331016486972e-05,
      "loss": 0.1672,
      "step": 153800
    },
    {
      "epoch": 0.9997076878105817,
      "grad_norm": 1.7367568016052246,
      "learning_rate": 4.255639970644373e-05,
      "loss": 0.166,
      "step": 153900
    },
    {
      "epoch": 1.0003572704537336,
      "grad_norm": 1.4185808897018433,
      "learning_rate": 4.254948924801774e-05,
      "loss": 0.164,
      "step": 154000
    },
    {
      "epoch": 1.0010068530968852,
      "grad_norm": 1.863992691040039,
      "learning_rate": 4.254257878959175e-05,
      "loss": 0.1642,
      "step": 154100
    },
    {
      "epoch": 1.001656435740037,
      "grad_norm": 1.2149736881256104,
      "learning_rate": 4.253566833116576e-05,
      "loss": 0.16,
      "step": 154200
    },
    {
      "epoch": 1.002306018383189,
      "grad_norm": 1.3583115339279175,
      "learning_rate": 4.252875787273977e-05,
      "loss": 0.1578,
      "step": 154300
    },
    {
      "epoch": 1.0029556010263405,
      "grad_norm": 1.0673747062683105,
      "learning_rate": 4.252184741431377e-05,
      "loss": 0.1581,
      "step": 154400
    },
    {
      "epoch": 1.0036051836694924,
      "grad_norm": 1.4400436878204346,
      "learning_rate": 4.251493695588778e-05,
      "loss": 0.1593,
      "step": 154500
    },
    {
      "epoch": 1.0042547663126442,
      "grad_norm": 1.3993664979934692,
      "learning_rate": 4.250802649746179e-05,
      "loss": 0.1665,
      "step": 154600
    },
    {
      "epoch": 1.0049043489557958,
      "grad_norm": 1.5128635168075562,
      "learning_rate": 4.25011160390358e-05,
      "loss": 0.1634,
      "step": 154700
    },
    {
      "epoch": 1.0055539315989477,
      "grad_norm": 2.029183864593506,
      "learning_rate": 4.249420558060981e-05,
      "loss": 0.1626,
      "step": 154800
    },
    {
      "epoch": 1.0062035142420995,
      "grad_norm": 1.1670247316360474,
      "learning_rate": 4.248729512218382e-05,
      "loss": 0.167,
      "step": 154900
    },
    {
      "epoch": 1.0068530968852512,
      "grad_norm": 1.004373550415039,
      "learning_rate": 4.248038466375783e-05,
      "loss": 0.1616,
      "step": 155000
    },
    {
      "epoch": 1.007502679528403,
      "grad_norm": 1.1898720264434814,
      "learning_rate": 4.247347420533184e-05,
      "loss": 0.1601,
      "step": 155100
    },
    {
      "epoch": 1.0081522621715548,
      "grad_norm": 1.1664148569107056,
      "learning_rate": 4.246656374690585e-05,
      "loss": 0.1678,
      "step": 155200
    },
    {
      "epoch": 1.0088018448147065,
      "grad_norm": 1.2206398248672485,
      "learning_rate": 4.245965328847985e-05,
      "loss": 0.1698,
      "step": 155300
    },
    {
      "epoch": 1.0094514274578583,
      "grad_norm": 1.5866626501083374,
      "learning_rate": 4.245274283005386e-05,
      "loss": 0.1503,
      "step": 155400
    },
    {
      "epoch": 1.0101010101010102,
      "grad_norm": 1.4782377481460571,
      "learning_rate": 4.244583237162787e-05,
      "loss": 0.1695,
      "step": 155500
    },
    {
      "epoch": 1.0107505927441618,
      "grad_norm": 1.6619442701339722,
      "learning_rate": 4.2438921913201876e-05,
      "loss": 0.1669,
      "step": 155600
    },
    {
      "epoch": 1.0114001753873136,
      "grad_norm": 1.1515593528747559,
      "learning_rate": 4.2432011454775885e-05,
      "loss": 0.16,
      "step": 155700
    },
    {
      "epoch": 1.0120497580304655,
      "grad_norm": 1.644728660583496,
      "learning_rate": 4.2425100996349895e-05,
      "loss": 0.1627,
      "step": 155800
    },
    {
      "epoch": 1.012699340673617,
      "grad_norm": 1.5925463438034058,
      "learning_rate": 4.2418190537923905e-05,
      "loss": 0.1601,
      "step": 155900
    },
    {
      "epoch": 1.013348923316769,
      "grad_norm": 1.0055652856826782,
      "learning_rate": 4.2411280079497915e-05,
      "loss": 0.1635,
      "step": 156000
    },
    {
      "epoch": 1.0139985059599208,
      "grad_norm": 0.7836414575576782,
      "learning_rate": 4.2404369621071925e-05,
      "loss": 0.1662,
      "step": 156100
    },
    {
      "epoch": 1.0146480886030724,
      "grad_norm": 1.0043482780456543,
      "learning_rate": 4.2397459162645935e-05,
      "loss": 0.166,
      "step": 156200
    },
    {
      "epoch": 1.0152976712462243,
      "grad_norm": 1.4528675079345703,
      "learning_rate": 4.239054870421994e-05,
      "loss": 0.1558,
      "step": 156300
    },
    {
      "epoch": 1.0159472538893761,
      "grad_norm": 1.316595435142517,
      "learning_rate": 4.238363824579395e-05,
      "loss": 0.1702,
      "step": 156400
    },
    {
      "epoch": 1.0165968365325277,
      "grad_norm": 1.6075115203857422,
      "learning_rate": 4.237672778736796e-05,
      "loss": 0.162,
      "step": 156500
    },
    {
      "epoch": 1.0172464191756796,
      "grad_norm": 1.3659182786941528,
      "learning_rate": 4.236981732894197e-05,
      "loss": 0.1692,
      "step": 156600
    },
    {
      "epoch": 1.0178960018188314,
      "grad_norm": 1.5344375371932983,
      "learning_rate": 4.236290687051598e-05,
      "loss": 0.1625,
      "step": 156700
    },
    {
      "epoch": 1.018545584461983,
      "grad_norm": 1.2931818962097168,
      "learning_rate": 4.235599641208999e-05,
      "loss": 0.164,
      "step": 156800
    },
    {
      "epoch": 1.019195167105135,
      "grad_norm": 1.3084771633148193,
      "learning_rate": 4.2349085953664e-05,
      "loss": 0.1663,
      "step": 156900
    },
    {
      "epoch": 1.0198447497482868,
      "grad_norm": 1.364586353302002,
      "learning_rate": 4.234217549523801e-05,
      "loss": 0.165,
      "step": 157000
    },
    {
      "epoch": 1.0204943323914386,
      "grad_norm": 1.2788347005844116,
      "learning_rate": 4.233526503681202e-05,
      "loss": 0.16,
      "step": 157100
    },
    {
      "epoch": 1.0211439150345902,
      "grad_norm": 1.293689250946045,
      "learning_rate": 4.232835457838602e-05,
      "loss": 0.1662,
      "step": 157200
    },
    {
      "epoch": 1.021793497677742,
      "grad_norm": 1.4263958930969238,
      "learning_rate": 4.232144411996003e-05,
      "loss": 0.1668,
      "step": 157300
    },
    {
      "epoch": 1.022443080320894,
      "grad_norm": 1.2643882036209106,
      "learning_rate": 4.231453366153404e-05,
      "loss": 0.1663,
      "step": 157400
    },
    {
      "epoch": 1.0230926629640456,
      "grad_norm": 1.2528198957443237,
      "learning_rate": 4.230762320310805e-05,
      "loss": 0.1664,
      "step": 157500
    },
    {
      "epoch": 1.0237422456071974,
      "grad_norm": 1.1317849159240723,
      "learning_rate": 4.230071274468206e-05,
      "loss": 0.1634,
      "step": 157600
    },
    {
      "epoch": 1.0243918282503492,
      "grad_norm": 1.2472933530807495,
      "learning_rate": 4.229380228625607e-05,
      "loss": 0.1611,
      "step": 157700
    },
    {
      "epoch": 1.0250414108935009,
      "grad_norm": 1.5606913566589355,
      "learning_rate": 4.228689182783008e-05,
      "loss": 0.1604,
      "step": 157800
    },
    {
      "epoch": 1.0256909935366527,
      "grad_norm": 1.462340235710144,
      "learning_rate": 4.227998136940409e-05,
      "loss": 0.1564,
      "step": 157900
    },
    {
      "epoch": 1.0263405761798046,
      "grad_norm": 1.0225369930267334,
      "learning_rate": 4.22730709109781e-05,
      "loss": 0.1628,
      "step": 158000
    },
    {
      "epoch": 1.0269901588229562,
      "grad_norm": 1.305971384048462,
      "learning_rate": 4.226616045255211e-05,
      "loss": 0.1623,
      "step": 158100
    },
    {
      "epoch": 1.027639741466108,
      "grad_norm": 1.4908286333084106,
      "learning_rate": 4.225924999412611e-05,
      "loss": 0.1566,
      "step": 158200
    },
    {
      "epoch": 1.0282893241092599,
      "grad_norm": 1.682464838027954,
      "learning_rate": 4.225233953570012e-05,
      "loss": 0.1683,
      "step": 158300
    },
    {
      "epoch": 1.0289389067524115,
      "grad_norm": 1.4134703874588013,
      "learning_rate": 4.224542907727413e-05,
      "loss": 0.1625,
      "step": 158400
    },
    {
      "epoch": 1.0295884893955634,
      "grad_norm": 1.32402765750885,
      "learning_rate": 4.223851861884814e-05,
      "loss": 0.159,
      "step": 158500
    },
    {
      "epoch": 1.0302380720387152,
      "grad_norm": 1.728236198425293,
      "learning_rate": 4.223160816042215e-05,
      "loss": 0.1548,
      "step": 158600
    },
    {
      "epoch": 1.0308876546818668,
      "grad_norm": 1.362109899520874,
      "learning_rate": 4.222469770199616e-05,
      "loss": 0.1654,
      "step": 158700
    },
    {
      "epoch": 1.0315372373250187,
      "grad_norm": 1.500074028968811,
      "learning_rate": 4.2217787243570164e-05,
      "loss": 0.1653,
      "step": 158800
    },
    {
      "epoch": 1.0321868199681705,
      "grad_norm": 1.2504483461380005,
      "learning_rate": 4.2210876785144174e-05,
      "loss": 0.1678,
      "step": 158900
    },
    {
      "epoch": 1.0328364026113221,
      "grad_norm": 1.9773029088974,
      "learning_rate": 4.2203966326718183e-05,
      "loss": 0.155,
      "step": 159000
    },
    {
      "epoch": 1.033485985254474,
      "grad_norm": 1.4616880416870117,
      "learning_rate": 4.219705586829219e-05,
      "loss": 0.1673,
      "step": 159100
    },
    {
      "epoch": 1.0341355678976258,
      "grad_norm": 1.0058881044387817,
      "learning_rate": 4.2190145409866196e-05,
      "loss": 0.1618,
      "step": 159200
    },
    {
      "epoch": 1.0347851505407775,
      "grad_norm": 1.4833091497421265,
      "learning_rate": 4.2183234951440206e-05,
      "loss": 0.1576,
      "step": 159300
    },
    {
      "epoch": 1.0354347331839293,
      "grad_norm": 0.9145375490188599,
      "learning_rate": 4.2176324493014216e-05,
      "loss": 0.1663,
      "step": 159400
    },
    {
      "epoch": 1.0360843158270812,
      "grad_norm": 1.555192232131958,
      "learning_rate": 4.2169414034588226e-05,
      "loss": 0.1574,
      "step": 159500
    },
    {
      "epoch": 1.0367338984702328,
      "grad_norm": 1.6766314506530762,
      "learning_rate": 4.2162503576162236e-05,
      "loss": 0.1523,
      "step": 159600
    },
    {
      "epoch": 1.0373834811133846,
      "grad_norm": 1.3759779930114746,
      "learning_rate": 4.2155593117736246e-05,
      "loss": 0.1662,
      "step": 159700
    },
    {
      "epoch": 1.0380330637565365,
      "grad_norm": 1.4833588600158691,
      "learning_rate": 4.2148682659310256e-05,
      "loss": 0.1637,
      "step": 159800
    },
    {
      "epoch": 1.038682646399688,
      "grad_norm": 1.9085794687271118,
      "learning_rate": 4.2141772200884265e-05,
      "loss": 0.1535,
      "step": 159900
    },
    {
      "epoch": 1.03933222904284,
      "grad_norm": 1.2318700551986694,
      "learning_rate": 4.2134861742458275e-05,
      "loss": 0.1579,
      "step": 160000
    },
    {
      "epoch": 1.0399818116859918,
      "grad_norm": 1.4831620454788208,
      "learning_rate": 4.212795128403228e-05,
      "loss": 0.1626,
      "step": 160100
    },
    {
      "epoch": 1.0406313943291434,
      "grad_norm": 1.319084644317627,
      "learning_rate": 4.212104082560629e-05,
      "loss": 0.1633,
      "step": 160200
    },
    {
      "epoch": 1.0412809769722953,
      "grad_norm": 1.087786078453064,
      "learning_rate": 4.21141303671803e-05,
      "loss": 0.1538,
      "step": 160300
    },
    {
      "epoch": 1.0419305596154471,
      "grad_norm": 1.2122654914855957,
      "learning_rate": 4.210721990875431e-05,
      "loss": 0.1589,
      "step": 160400
    },
    {
      "epoch": 1.0425801422585987,
      "grad_norm": 1.0591094493865967,
      "learning_rate": 4.210030945032832e-05,
      "loss": 0.1644,
      "step": 160500
    },
    {
      "epoch": 1.0432297249017506,
      "grad_norm": 1.3183928728103638,
      "learning_rate": 4.209339899190233e-05,
      "loss": 0.1651,
      "step": 160600
    },
    {
      "epoch": 1.0438793075449024,
      "grad_norm": 1.5596116781234741,
      "learning_rate": 4.208648853347634e-05,
      "loss": 0.1641,
      "step": 160700
    },
    {
      "epoch": 1.0445288901880543,
      "grad_norm": 1.616100549697876,
      "learning_rate": 4.207957807505035e-05,
      "loss": 0.1566,
      "step": 160800
    },
    {
      "epoch": 1.045178472831206,
      "grad_norm": 1.6623784303665161,
      "learning_rate": 4.207266761662436e-05,
      "loss": 0.1558,
      "step": 160900
    },
    {
      "epoch": 1.0458280554743578,
      "grad_norm": 1.4109818935394287,
      "learning_rate": 4.206575715819836e-05,
      "loss": 0.1565,
      "step": 161000
    },
    {
      "epoch": 1.0464776381175096,
      "grad_norm": 2.033996105194092,
      "learning_rate": 4.205884669977237e-05,
      "loss": 0.1593,
      "step": 161100
    },
    {
      "epoch": 1.0471272207606612,
      "grad_norm": 1.4226258993148804,
      "learning_rate": 4.205193624134638e-05,
      "loss": 0.1649,
      "step": 161200
    },
    {
      "epoch": 1.047776803403813,
      "grad_norm": 0.8602616786956787,
      "learning_rate": 4.204502578292039e-05,
      "loss": 0.153,
      "step": 161300
    },
    {
      "epoch": 1.048426386046965,
      "grad_norm": 1.176107406616211,
      "learning_rate": 4.20381153244944e-05,
      "loss": 0.1654,
      "step": 161400
    },
    {
      "epoch": 1.0490759686901165,
      "grad_norm": 1.3936342000961304,
      "learning_rate": 4.203120486606841e-05,
      "loss": 0.159,
      "step": 161500
    },
    {
      "epoch": 1.0497255513332684,
      "grad_norm": 1.5063058137893677,
      "learning_rate": 4.202429440764242e-05,
      "loss": 0.1653,
      "step": 161600
    },
    {
      "epoch": 1.0503751339764202,
      "grad_norm": 1.7066963911056519,
      "learning_rate": 4.201738394921643e-05,
      "loss": 0.1624,
      "step": 161700
    },
    {
      "epoch": 1.0510247166195719,
      "grad_norm": 1.0004879236221313,
      "learning_rate": 4.201047349079044e-05,
      "loss": 0.164,
      "step": 161800
    },
    {
      "epoch": 1.0516742992627237,
      "grad_norm": 1.015434741973877,
      "learning_rate": 4.200356303236444e-05,
      "loss": 0.1578,
      "step": 161900
    },
    {
      "epoch": 1.0523238819058756,
      "grad_norm": 1.4135606288909912,
      "learning_rate": 4.199665257393845e-05,
      "loss": 0.1637,
      "step": 162000
    },
    {
      "epoch": 1.0529734645490272,
      "grad_norm": 1.4079951047897339,
      "learning_rate": 4.198974211551246e-05,
      "loss": 0.162,
      "step": 162100
    },
    {
      "epoch": 1.053623047192179,
      "grad_norm": 1.3124035596847534,
      "learning_rate": 4.198283165708647e-05,
      "loss": 0.1648,
      "step": 162200
    },
    {
      "epoch": 1.0542726298353309,
      "grad_norm": 1.2948977947235107,
      "learning_rate": 4.197592119866048e-05,
      "loss": 0.1581,
      "step": 162300
    },
    {
      "epoch": 1.0549222124784825,
      "grad_norm": 1.7109997272491455,
      "learning_rate": 4.1969010740234485e-05,
      "loss": 0.1468,
      "step": 162400
    },
    {
      "epoch": 1.0555717951216343,
      "grad_norm": 1.2611713409423828,
      "learning_rate": 4.1962100281808494e-05,
      "loss": 0.1623,
      "step": 162500
    },
    {
      "epoch": 1.0562213777647862,
      "grad_norm": 1.6031798124313354,
      "learning_rate": 4.1955189823382504e-05,
      "loss": 0.1579,
      "step": 162600
    },
    {
      "epoch": 1.0568709604079378,
      "grad_norm": 1.0919591188430786,
      "learning_rate": 4.1948279364956514e-05,
      "loss": 0.1695,
      "step": 162700
    },
    {
      "epoch": 1.0575205430510897,
      "grad_norm": 1.3013622760772705,
      "learning_rate": 4.1941368906530524e-05,
      "loss": 0.1659,
      "step": 162800
    },
    {
      "epoch": 1.0581701256942415,
      "grad_norm": 1.0175857543945312,
      "learning_rate": 4.193445844810453e-05,
      "loss": 0.1575,
      "step": 162900
    },
    {
      "epoch": 1.0588197083373931,
      "grad_norm": 1.3341807126998901,
      "learning_rate": 4.192754798967854e-05,
      "loss": 0.1671,
      "step": 163000
    },
    {
      "epoch": 1.059469290980545,
      "grad_norm": 1.406334638595581,
      "learning_rate": 4.192063753125255e-05,
      "loss": 0.1583,
      "step": 163100
    },
    {
      "epoch": 1.0601188736236968,
      "grad_norm": 1.5115411281585693,
      "learning_rate": 4.1913727072826557e-05,
      "loss": 0.1582,
      "step": 163200
    },
    {
      "epoch": 1.0607684562668485,
      "grad_norm": 1.2395023107528687,
      "learning_rate": 4.1906816614400566e-05,
      "loss": 0.1658,
      "step": 163300
    },
    {
      "epoch": 1.0614180389100003,
      "grad_norm": 1.413316249847412,
      "learning_rate": 4.1899906155974576e-05,
      "loss": 0.1612,
      "step": 163400
    },
    {
      "epoch": 1.0620676215531522,
      "grad_norm": 1.38804292678833,
      "learning_rate": 4.1892995697548586e-05,
      "loss": 0.159,
      "step": 163500
    },
    {
      "epoch": 1.0627172041963038,
      "grad_norm": 1.1783558130264282,
      "learning_rate": 4.1886085239122596e-05,
      "loss": 0.1532,
      "step": 163600
    },
    {
      "epoch": 1.0633667868394556,
      "grad_norm": 1.4263943433761597,
      "learning_rate": 4.1879174780696606e-05,
      "loss": 0.1562,
      "step": 163700
    },
    {
      "epoch": 1.0640163694826075,
      "grad_norm": 1.157721996307373,
      "learning_rate": 4.187226432227061e-05,
      "loss": 0.1571,
      "step": 163800
    },
    {
      "epoch": 1.064665952125759,
      "grad_norm": 1.1886625289916992,
      "learning_rate": 4.186535386384462e-05,
      "loss": 0.1608,
      "step": 163900
    },
    {
      "epoch": 1.065315534768911,
      "grad_norm": 1.3699932098388672,
      "learning_rate": 4.185844340541863e-05,
      "loss": 0.1546,
      "step": 164000
    },
    {
      "epoch": 1.0659651174120628,
      "grad_norm": 1.5011581182479858,
      "learning_rate": 4.185153294699264e-05,
      "loss": 0.1575,
      "step": 164100
    },
    {
      "epoch": 1.0666147000552146,
      "grad_norm": 1.6110717058181763,
      "learning_rate": 4.184462248856665e-05,
      "loss": 0.1529,
      "step": 164200
    },
    {
      "epoch": 1.0672642826983663,
      "grad_norm": 1.4771612882614136,
      "learning_rate": 4.183771203014066e-05,
      "loss": 0.1597,
      "step": 164300
    },
    {
      "epoch": 1.067913865341518,
      "grad_norm": 1.5322014093399048,
      "learning_rate": 4.183080157171467e-05,
      "loss": 0.1644,
      "step": 164400
    },
    {
      "epoch": 1.06856344798467,
      "grad_norm": 1.7252556085586548,
      "learning_rate": 4.182389111328868e-05,
      "loss": 0.1555,
      "step": 164500
    },
    {
      "epoch": 1.0692130306278216,
      "grad_norm": 1.5995830297470093,
      "learning_rate": 4.181698065486269e-05,
      "loss": 0.1543,
      "step": 164600
    },
    {
      "epoch": 1.0698626132709734,
      "grad_norm": 1.1311482191085815,
      "learning_rate": 4.18100701964367e-05,
      "loss": 0.1645,
      "step": 164700
    },
    {
      "epoch": 1.0705121959141253,
      "grad_norm": 1.8914735317230225,
      "learning_rate": 4.18031597380107e-05,
      "loss": 0.1548,
      "step": 164800
    },
    {
      "epoch": 1.071161778557277,
      "grad_norm": 1.5518856048583984,
      "learning_rate": 4.179624927958471e-05,
      "loss": 0.1604,
      "step": 164900
    },
    {
      "epoch": 1.0718113612004287,
      "grad_norm": 2.2870752811431885,
      "learning_rate": 4.178933882115872e-05,
      "loss": 0.1607,
      "step": 165000
    },
    {
      "epoch": 1.0724609438435806,
      "grad_norm": 1.3115233182907104,
      "learning_rate": 4.178242836273273e-05,
      "loss": 0.1606,
      "step": 165100
    },
    {
      "epoch": 1.0731105264867322,
      "grad_norm": 1.1122344732284546,
      "learning_rate": 4.177551790430674e-05,
      "loss": 0.1635,
      "step": 165200
    },
    {
      "epoch": 1.073760109129884,
      "grad_norm": 1.2343120574951172,
      "learning_rate": 4.176860744588075e-05,
      "loss": 0.1627,
      "step": 165300
    },
    {
      "epoch": 1.074409691773036,
      "grad_norm": 1.4226932525634766,
      "learning_rate": 4.176169698745476e-05,
      "loss": 0.1615,
      "step": 165400
    },
    {
      "epoch": 1.0750592744161875,
      "grad_norm": 1.6143935918807983,
      "learning_rate": 4.175478652902877e-05,
      "loss": 0.1569,
      "step": 165500
    },
    {
      "epoch": 1.0757088570593394,
      "grad_norm": 1.607783555984497,
      "learning_rate": 4.174787607060277e-05,
      "loss": 0.1583,
      "step": 165600
    },
    {
      "epoch": 1.0763584397024912,
      "grad_norm": 1.212295651435852,
      "learning_rate": 4.174096561217678e-05,
      "loss": 0.1679,
      "step": 165700
    },
    {
      "epoch": 1.0770080223456429,
      "grad_norm": 1.73772132396698,
      "learning_rate": 4.173405515375079e-05,
      "loss": 0.158,
      "step": 165800
    },
    {
      "epoch": 1.0776576049887947,
      "grad_norm": 1.6799829006195068,
      "learning_rate": 4.17271446953248e-05,
      "loss": 0.1539,
      "step": 165900
    },
    {
      "epoch": 1.0783071876319466,
      "grad_norm": 1.113050103187561,
      "learning_rate": 4.1720234236898805e-05,
      "loss": 0.1589,
      "step": 166000
    },
    {
      "epoch": 1.0789567702750982,
      "grad_norm": 1.6051034927368164,
      "learning_rate": 4.1713323778472815e-05,
      "loss": 0.1621,
      "step": 166100
    },
    {
      "epoch": 1.07960635291825,
      "grad_norm": 1.194210171699524,
      "learning_rate": 4.1706413320046825e-05,
      "loss": 0.1565,
      "step": 166200
    },
    {
      "epoch": 1.0802559355614019,
      "grad_norm": 1.8577539920806885,
      "learning_rate": 4.1699502861620835e-05,
      "loss": 0.1574,
      "step": 166300
    },
    {
      "epoch": 1.0809055182045535,
      "grad_norm": 1.6397452354431152,
      "learning_rate": 4.1692592403194845e-05,
      "loss": 0.162,
      "step": 166400
    },
    {
      "epoch": 1.0815551008477053,
      "grad_norm": 1.2920658588409424,
      "learning_rate": 4.1685681944768855e-05,
      "loss": 0.1607,
      "step": 166500
    },
    {
      "epoch": 1.0822046834908572,
      "grad_norm": 1.7706279754638672,
      "learning_rate": 4.1678771486342864e-05,
      "loss": 0.1549,
      "step": 166600
    },
    {
      "epoch": 1.0828542661340088,
      "grad_norm": 1.3543068170547485,
      "learning_rate": 4.167186102791687e-05,
      "loss": 0.1564,
      "step": 166700
    },
    {
      "epoch": 1.0835038487771607,
      "grad_norm": 1.0678645372390747,
      "learning_rate": 4.166495056949088e-05,
      "loss": 0.1587,
      "step": 166800
    },
    {
      "epoch": 1.0841534314203125,
      "grad_norm": 1.462764859199524,
      "learning_rate": 4.165804011106489e-05,
      "loss": 0.1521,
      "step": 166900
    },
    {
      "epoch": 1.0848030140634641,
      "grad_norm": 1.4455604553222656,
      "learning_rate": 4.16511296526389e-05,
      "loss": 0.1588,
      "step": 167000
    },
    {
      "epoch": 1.085452596706616,
      "grad_norm": 1.4061013460159302,
      "learning_rate": 4.164421919421291e-05,
      "loss": 0.1643,
      "step": 167100
    },
    {
      "epoch": 1.0861021793497678,
      "grad_norm": 0.9987573027610779,
      "learning_rate": 4.163730873578692e-05,
      "loss": 0.1591,
      "step": 167200
    },
    {
      "epoch": 1.0867517619929195,
      "grad_norm": 1.532031536102295,
      "learning_rate": 4.163039827736093e-05,
      "loss": 0.1536,
      "step": 167300
    },
    {
      "epoch": 1.0874013446360713,
      "grad_norm": 1.6786987781524658,
      "learning_rate": 4.1623487818934936e-05,
      "loss": 0.1592,
      "step": 167400
    },
    {
      "epoch": 1.0880509272792231,
      "grad_norm": 0.8703023791313171,
      "learning_rate": 4.1616577360508946e-05,
      "loss": 0.1557,
      "step": 167500
    },
    {
      "epoch": 1.0887005099223748,
      "grad_norm": 1.2054433822631836,
      "learning_rate": 4.160966690208295e-05,
      "loss": 0.163,
      "step": 167600
    },
    {
      "epoch": 1.0893500925655266,
      "grad_norm": 1.425356149673462,
      "learning_rate": 4.160275644365696e-05,
      "loss": 0.1645,
      "step": 167700
    },
    {
      "epoch": 1.0899996752086785,
      "grad_norm": 1.1557705402374268,
      "learning_rate": 4.159584598523097e-05,
      "loss": 0.1584,
      "step": 167800
    },
    {
      "epoch": 1.09064925785183,
      "grad_norm": 1.6452480554580688,
      "learning_rate": 4.158893552680498e-05,
      "loss": 0.1659,
      "step": 167900
    },
    {
      "epoch": 1.091298840494982,
      "grad_norm": 1.2435219287872314,
      "learning_rate": 4.158202506837899e-05,
      "loss": 0.1666,
      "step": 168000
    },
    {
      "epoch": 1.0919484231381338,
      "grad_norm": 1.7904130220413208,
      "learning_rate": 4.1575114609953e-05,
      "loss": 0.1577,
      "step": 168100
    },
    {
      "epoch": 1.0925980057812854,
      "grad_norm": 1.970249056816101,
      "learning_rate": 4.156820415152701e-05,
      "loss": 0.1664,
      "step": 168200
    },
    {
      "epoch": 1.0932475884244373,
      "grad_norm": 1.1016846895217896,
      "learning_rate": 4.156129369310102e-05,
      "loss": 0.1582,
      "step": 168300
    },
    {
      "epoch": 1.093897171067589,
      "grad_norm": 1.3307924270629883,
      "learning_rate": 4.155438323467503e-05,
      "loss": 0.1528,
      "step": 168400
    },
    {
      "epoch": 1.094546753710741,
      "grad_norm": 1.8634599447250366,
      "learning_rate": 4.154747277624903e-05,
      "loss": 0.154,
      "step": 168500
    },
    {
      "epoch": 1.0951963363538926,
      "grad_norm": 1.3274739980697632,
      "learning_rate": 4.154056231782304e-05,
      "loss": 0.1608,
      "step": 168600
    },
    {
      "epoch": 1.0958459189970444,
      "grad_norm": 1.3281846046447754,
      "learning_rate": 4.153365185939705e-05,
      "loss": 0.161,
      "step": 168700
    },
    {
      "epoch": 1.0964955016401963,
      "grad_norm": 0.8720128536224365,
      "learning_rate": 4.152674140097106e-05,
      "loss": 0.1525,
      "step": 168800
    },
    {
      "epoch": 1.097145084283348,
      "grad_norm": 1.24750554561615,
      "learning_rate": 4.151983094254507e-05,
      "loss": 0.1517,
      "step": 168900
    },
    {
      "epoch": 1.0977946669264997,
      "grad_norm": 1.0907936096191406,
      "learning_rate": 4.151292048411908e-05,
      "loss": 0.1525,
      "step": 169000
    },
    {
      "epoch": 1.0984442495696516,
      "grad_norm": 1.457344651222229,
      "learning_rate": 4.150601002569309e-05,
      "loss": 0.1589,
      "step": 169100
    },
    {
      "epoch": 1.0990938322128032,
      "grad_norm": 1.291520357131958,
      "learning_rate": 4.1499099567267094e-05,
      "loss": 0.1676,
      "step": 169200
    },
    {
      "epoch": 1.099743414855955,
      "grad_norm": 1.367438793182373,
      "learning_rate": 4.14921891088411e-05,
      "loss": 0.1609,
      "step": 169300
    },
    {
      "epoch": 1.100392997499107,
      "grad_norm": 1.272462248802185,
      "learning_rate": 4.148527865041511e-05,
      "loss": 0.1675,
      "step": 169400
    },
    {
      "epoch": 1.1010425801422585,
      "grad_norm": 1.7491130828857422,
      "learning_rate": 4.147836819198912e-05,
      "loss": 0.1559,
      "step": 169500
    },
    {
      "epoch": 1.1016921627854104,
      "grad_norm": 1.4366834163665771,
      "learning_rate": 4.1471457733563126e-05,
      "loss": 0.1641,
      "step": 169600
    },
    {
      "epoch": 1.1023417454285622,
      "grad_norm": 1.0243865251541138,
      "learning_rate": 4.1464547275137136e-05,
      "loss": 0.159,
      "step": 169700
    },
    {
      "epoch": 1.1029913280717139,
      "grad_norm": 1.012986660003662,
      "learning_rate": 4.1457636816711146e-05,
      "loss": 0.153,
      "step": 169800
    },
    {
      "epoch": 1.1036409107148657,
      "grad_norm": 1.1656516790390015,
      "learning_rate": 4.1450726358285156e-05,
      "loss": 0.1542,
      "step": 169900
    },
    {
      "epoch": 1.1042904933580175,
      "grad_norm": 1.785400629043579,
      "learning_rate": 4.1443815899859166e-05,
      "loss": 0.1538,
      "step": 170000
    },
    {
      "epoch": 1.1049400760011692,
      "grad_norm": 1.2941622734069824,
      "learning_rate": 4.1436905441433175e-05,
      "loss": 0.1555,
      "step": 170100
    },
    {
      "epoch": 1.105589658644321,
      "grad_norm": 0.9440014362335205,
      "learning_rate": 4.1429994983007185e-05,
      "loss": 0.1542,
      "step": 170200
    },
    {
      "epoch": 1.1062392412874729,
      "grad_norm": 1.4136669635772705,
      "learning_rate": 4.1423084524581195e-05,
      "loss": 0.1602,
      "step": 170300
    },
    {
      "epoch": 1.1068888239306245,
      "grad_norm": 1.6189401149749756,
      "learning_rate": 4.14161740661552e-05,
      "loss": 0.1611,
      "step": 170400
    },
    {
      "epoch": 1.1075384065737763,
      "grad_norm": 1.4363393783569336,
      "learning_rate": 4.140926360772921e-05,
      "loss": 0.1603,
      "step": 170500
    },
    {
      "epoch": 1.1081879892169282,
      "grad_norm": 1.3200474977493286,
      "learning_rate": 4.140235314930322e-05,
      "loss": 0.1566,
      "step": 170600
    },
    {
      "epoch": 1.1088375718600798,
      "grad_norm": 1.091308355331421,
      "learning_rate": 4.139544269087723e-05,
      "loss": 0.1522,
      "step": 170700
    },
    {
      "epoch": 1.1094871545032317,
      "grad_norm": 1.1111671924591064,
      "learning_rate": 4.138853223245124e-05,
      "loss": 0.1551,
      "step": 170800
    },
    {
      "epoch": 1.1101367371463835,
      "grad_norm": 1.3267390727996826,
      "learning_rate": 4.138162177402525e-05,
      "loss": 0.1518,
      "step": 170900
    },
    {
      "epoch": 1.1107863197895351,
      "grad_norm": 1.4558004140853882,
      "learning_rate": 4.137471131559926e-05,
      "loss": 0.1581,
      "step": 171000
    },
    {
      "epoch": 1.111435902432687,
      "grad_norm": 1.4231078624725342,
      "learning_rate": 4.136780085717327e-05,
      "loss": 0.1554,
      "step": 171100
    },
    {
      "epoch": 1.1120854850758388,
      "grad_norm": 1.1097897291183472,
      "learning_rate": 4.136089039874728e-05,
      "loss": 0.1582,
      "step": 171200
    },
    {
      "epoch": 1.1127350677189904,
      "grad_norm": 1.3717916011810303,
      "learning_rate": 4.135397994032129e-05,
      "loss": 0.1543,
      "step": 171300
    },
    {
      "epoch": 1.1133846503621423,
      "grad_norm": 1.3251721858978271,
      "learning_rate": 4.134706948189529e-05,
      "loss": 0.1549,
      "step": 171400
    },
    {
      "epoch": 1.1140342330052941,
      "grad_norm": 1.544813632965088,
      "learning_rate": 4.13401590234693e-05,
      "loss": 0.1522,
      "step": 171500
    },
    {
      "epoch": 1.114683815648446,
      "grad_norm": 1.8221739530563354,
      "learning_rate": 4.133324856504331e-05,
      "loss": 0.1571,
      "step": 171600
    },
    {
      "epoch": 1.1153333982915976,
      "grad_norm": 1.1020349264144897,
      "learning_rate": 4.132633810661732e-05,
      "loss": 0.1569,
      "step": 171700
    },
    {
      "epoch": 1.1159829809347495,
      "grad_norm": 1.1876020431518555,
      "learning_rate": 4.131942764819133e-05,
      "loss": 0.1548,
      "step": 171800
    },
    {
      "epoch": 1.1166325635779013,
      "grad_norm": 1.9056956768035889,
      "learning_rate": 4.131251718976534e-05,
      "loss": 0.1524,
      "step": 171900
    },
    {
      "epoch": 1.117282146221053,
      "grad_norm": 1.3482944965362549,
      "learning_rate": 4.130560673133935e-05,
      "loss": 0.1555,
      "step": 172000
    },
    {
      "epoch": 1.1179317288642048,
      "grad_norm": 1.1067266464233398,
      "learning_rate": 4.129869627291336e-05,
      "loss": 0.1478,
      "step": 172100
    },
    {
      "epoch": 1.1185813115073566,
      "grad_norm": 1.4593271017074585,
      "learning_rate": 4.129178581448737e-05,
      "loss": 0.1583,
      "step": 172200
    },
    {
      "epoch": 1.1192308941505082,
      "grad_norm": 1.3949350118637085,
      "learning_rate": 4.128487535606137e-05,
      "loss": 0.1529,
      "step": 172300
    },
    {
      "epoch": 1.11988047679366,
      "grad_norm": 0.8507141470909119,
      "learning_rate": 4.127796489763538e-05,
      "loss": 0.1549,
      "step": 172400
    },
    {
      "epoch": 1.120530059436812,
      "grad_norm": 1.0800954103469849,
      "learning_rate": 4.127105443920939e-05,
      "loss": 0.1499,
      "step": 172500
    },
    {
      "epoch": 1.1211796420799636,
      "grad_norm": 1.5197023153305054,
      "learning_rate": 4.12641439807834e-05,
      "loss": 0.1601,
      "step": 172600
    },
    {
      "epoch": 1.1218292247231154,
      "grad_norm": 1.7418034076690674,
      "learning_rate": 4.125723352235741e-05,
      "loss": 0.1523,
      "step": 172700
    },
    {
      "epoch": 1.1224788073662673,
      "grad_norm": 1.3028465509414673,
      "learning_rate": 4.1250323063931414e-05,
      "loss": 0.1526,
      "step": 172800
    },
    {
      "epoch": 1.1231283900094189,
      "grad_norm": 1.7799041271209717,
      "learning_rate": 4.1243412605505424e-05,
      "loss": 0.1566,
      "step": 172900
    },
    {
      "epoch": 1.1237779726525707,
      "grad_norm": 1.2639288902282715,
      "learning_rate": 4.1236502147079434e-05,
      "loss": 0.16,
      "step": 173000
    },
    {
      "epoch": 1.1244275552957226,
      "grad_norm": 1.1562267541885376,
      "learning_rate": 4.1229591688653444e-05,
      "loss": 0.159,
      "step": 173100
    },
    {
      "epoch": 1.1250771379388742,
      "grad_norm": 1.0555624961853027,
      "learning_rate": 4.122268123022745e-05,
      "loss": 0.157,
      "step": 173200
    },
    {
      "epoch": 1.125726720582026,
      "grad_norm": 1.345483660697937,
      "learning_rate": 4.121577077180146e-05,
      "loss": 0.1566,
      "step": 173300
    },
    {
      "epoch": 1.126376303225178,
      "grad_norm": 0.7721764445304871,
      "learning_rate": 4.120886031337547e-05,
      "loss": 0.156,
      "step": 173400
    },
    {
      "epoch": 1.1270258858683295,
      "grad_norm": 1.2787851095199585,
      "learning_rate": 4.1201949854949476e-05,
      "loss": 0.1629,
      "step": 173500
    },
    {
      "epoch": 1.1276754685114814,
      "grad_norm": 1.6295526027679443,
      "learning_rate": 4.1195039396523486e-05,
      "loss": 0.1445,
      "step": 173600
    },
    {
      "epoch": 1.1283250511546332,
      "grad_norm": 1.4843666553497314,
      "learning_rate": 4.1188128938097496e-05,
      "loss": 0.1497,
      "step": 173700
    },
    {
      "epoch": 1.1289746337977848,
      "grad_norm": 1.6989219188690186,
      "learning_rate": 4.1181218479671506e-05,
      "loss": 0.1588,
      "step": 173800
    },
    {
      "epoch": 1.1296242164409367,
      "grad_norm": 1.4151029586791992,
      "learning_rate": 4.1174308021245516e-05,
      "loss": 0.1533,
      "step": 173900
    },
    {
      "epoch": 1.1302737990840885,
      "grad_norm": 1.5030837059020996,
      "learning_rate": 4.1167397562819526e-05,
      "loss": 0.1585,
      "step": 174000
    },
    {
      "epoch": 1.1309233817272402,
      "grad_norm": 1.7453043460845947,
      "learning_rate": 4.1160487104393536e-05,
      "loss": 0.163,
      "step": 174100
    },
    {
      "epoch": 1.131572964370392,
      "grad_norm": 1.2162423133850098,
      "learning_rate": 4.115357664596754e-05,
      "loss": 0.1547,
      "step": 174200
    },
    {
      "epoch": 1.1322225470135439,
      "grad_norm": 1.4373600482940674,
      "learning_rate": 4.114666618754155e-05,
      "loss": 0.1504,
      "step": 174300
    },
    {
      "epoch": 1.1328721296566955,
      "grad_norm": 1.3721532821655273,
      "learning_rate": 4.113975572911556e-05,
      "loss": 0.1513,
      "step": 174400
    },
    {
      "epoch": 1.1335217122998473,
      "grad_norm": 1.542325735092163,
      "learning_rate": 4.113284527068957e-05,
      "loss": 0.1576,
      "step": 174500
    },
    {
      "epoch": 1.1341712949429992,
      "grad_norm": 1.343624234199524,
      "learning_rate": 4.112593481226358e-05,
      "loss": 0.1546,
      "step": 174600
    },
    {
      "epoch": 1.1348208775861508,
      "grad_norm": 0.9170345664024353,
      "learning_rate": 4.111902435383759e-05,
      "loss": 0.1569,
      "step": 174700
    },
    {
      "epoch": 1.1354704602293026,
      "grad_norm": 1.4137848615646362,
      "learning_rate": 4.11121138954116e-05,
      "loss": 0.149,
      "step": 174800
    },
    {
      "epoch": 1.1361200428724545,
      "grad_norm": 1.8592517375946045,
      "learning_rate": 4.110520343698561e-05,
      "loss": 0.1574,
      "step": 174900
    },
    {
      "epoch": 1.1367696255156061,
      "grad_norm": 1.3618628978729248,
      "learning_rate": 4.109829297855962e-05,
      "loss": 0.1519,
      "step": 175000
    },
    {
      "epoch": 1.137419208158758,
      "grad_norm": 1.5494015216827393,
      "learning_rate": 4.109138252013362e-05,
      "loss": 0.1601,
      "step": 175100
    },
    {
      "epoch": 1.1380687908019098,
      "grad_norm": 1.2940877676010132,
      "learning_rate": 4.108447206170763e-05,
      "loss": 0.157,
      "step": 175200
    },
    {
      "epoch": 1.1387183734450614,
      "grad_norm": 1.0106638669967651,
      "learning_rate": 4.107756160328164e-05,
      "loss": 0.1544,
      "step": 175300
    },
    {
      "epoch": 1.1393679560882133,
      "grad_norm": 1.0251208543777466,
      "learning_rate": 4.107065114485565e-05,
      "loss": 0.1523,
      "step": 175400
    },
    {
      "epoch": 1.1400175387313651,
      "grad_norm": 1.378434658050537,
      "learning_rate": 4.106374068642966e-05,
      "loss": 0.154,
      "step": 175500
    },
    {
      "epoch": 1.1406671213745168,
      "grad_norm": 1.1633578538894653,
      "learning_rate": 4.105683022800367e-05,
      "loss": 0.1516,
      "step": 175600
    },
    {
      "epoch": 1.1413167040176686,
      "grad_norm": 1.373732566833496,
      "learning_rate": 4.104991976957768e-05,
      "loss": 0.157,
      "step": 175700
    },
    {
      "epoch": 1.1419662866608205,
      "grad_norm": 1.3763750791549683,
      "learning_rate": 4.104300931115169e-05,
      "loss": 0.159,
      "step": 175800
    },
    {
      "epoch": 1.142615869303972,
      "grad_norm": 1.977167010307312,
      "learning_rate": 4.10360988527257e-05,
      "loss": 0.154,
      "step": 175900
    },
    {
      "epoch": 1.143265451947124,
      "grad_norm": 1.5221915245056152,
      "learning_rate": 4.10291883942997e-05,
      "loss": 0.1568,
      "step": 176000
    },
    {
      "epoch": 1.1439150345902758,
      "grad_norm": 0.9339096546173096,
      "learning_rate": 4.102227793587371e-05,
      "loss": 0.1566,
      "step": 176100
    },
    {
      "epoch": 1.1445646172334276,
      "grad_norm": 1.4514905214309692,
      "learning_rate": 4.101536747744772e-05,
      "loss": 0.1595,
      "step": 176200
    },
    {
      "epoch": 1.1452141998765792,
      "grad_norm": 1.4946554899215698,
      "learning_rate": 4.100845701902173e-05,
      "loss": 0.159,
      "step": 176300
    },
    {
      "epoch": 1.145863782519731,
      "grad_norm": 1.475050449371338,
      "learning_rate": 4.1001546560595735e-05,
      "loss": 0.1553,
      "step": 176400
    },
    {
      "epoch": 1.146513365162883,
      "grad_norm": 1.068634033203125,
      "learning_rate": 4.0994636102169745e-05,
      "loss": 0.1576,
      "step": 176500
    },
    {
      "epoch": 1.1471629478060346,
      "grad_norm": 1.5531355142593384,
      "learning_rate": 4.0987725643743755e-05,
      "loss": 0.1597,
      "step": 176600
    },
    {
      "epoch": 1.1478125304491864,
      "grad_norm": 1.0896148681640625,
      "learning_rate": 4.0980815185317765e-05,
      "loss": 0.1537,
      "step": 176700
    },
    {
      "epoch": 1.1484621130923383,
      "grad_norm": 1.7912826538085938,
      "learning_rate": 4.0973904726891774e-05,
      "loss": 0.1439,
      "step": 176800
    },
    {
      "epoch": 1.1491116957354899,
      "grad_norm": 1.1946898698806763,
      "learning_rate": 4.0966994268465784e-05,
      "loss": 0.1566,
      "step": 176900
    },
    {
      "epoch": 1.1497612783786417,
      "grad_norm": 1.5222198963165283,
      "learning_rate": 4.096008381003979e-05,
      "loss": 0.1496,
      "step": 177000
    },
    {
      "epoch": 1.1504108610217936,
      "grad_norm": 1.4036051034927368,
      "learning_rate": 4.09531733516138e-05,
      "loss": 0.1588,
      "step": 177100
    },
    {
      "epoch": 1.1510604436649452,
      "grad_norm": 1.1036006212234497,
      "learning_rate": 4.094626289318781e-05,
      "loss": 0.1594,
      "step": 177200
    },
    {
      "epoch": 1.151710026308097,
      "grad_norm": 1.2798779010772705,
      "learning_rate": 4.093935243476182e-05,
      "loss": 0.1567,
      "step": 177300
    },
    {
      "epoch": 1.152359608951249,
      "grad_norm": 0.9611701369285583,
      "learning_rate": 4.093244197633583e-05,
      "loss": 0.1584,
      "step": 177400
    },
    {
      "epoch": 1.1530091915944005,
      "grad_norm": 1.077530026435852,
      "learning_rate": 4.092553151790984e-05,
      "loss": 0.1566,
      "step": 177500
    },
    {
      "epoch": 1.1536587742375524,
      "grad_norm": 1.3962663412094116,
      "learning_rate": 4.0918621059483847e-05,
      "loss": 0.1568,
      "step": 177600
    },
    {
      "epoch": 1.1543083568807042,
      "grad_norm": 1.301932692527771,
      "learning_rate": 4.0911710601057856e-05,
      "loss": 0.1524,
      "step": 177700
    },
    {
      "epoch": 1.1549579395238558,
      "grad_norm": 1.3896770477294922,
      "learning_rate": 4.0904800142631866e-05,
      "loss": 0.1498,
      "step": 177800
    },
    {
      "epoch": 1.1556075221670077,
      "grad_norm": 0.9121149778366089,
      "learning_rate": 4.089788968420587e-05,
      "loss": 0.1581,
      "step": 177900
    },
    {
      "epoch": 1.1562571048101595,
      "grad_norm": 1.3151384592056274,
      "learning_rate": 4.089097922577988e-05,
      "loss": 0.1562,
      "step": 178000
    },
    {
      "epoch": 1.1569066874533112,
      "grad_norm": 1.4873019456863403,
      "learning_rate": 4.088406876735389e-05,
      "loss": 0.1523,
      "step": 178100
    },
    {
      "epoch": 1.157556270096463,
      "grad_norm": 1.1516127586364746,
      "learning_rate": 4.08771583089279e-05,
      "loss": 0.1479,
      "step": 178200
    },
    {
      "epoch": 1.1582058527396148,
      "grad_norm": 1.4644428491592407,
      "learning_rate": 4.087024785050191e-05,
      "loss": 0.1541,
      "step": 178300
    },
    {
      "epoch": 1.1588554353827667,
      "grad_norm": 1.9116958379745483,
      "learning_rate": 4.086333739207592e-05,
      "loss": 0.1598,
      "step": 178400
    },
    {
      "epoch": 1.1595050180259183,
      "grad_norm": 1.298805594444275,
      "learning_rate": 4.085642693364993e-05,
      "loss": 0.1563,
      "step": 178500
    },
    {
      "epoch": 1.1601546006690702,
      "grad_norm": 1.3329583406448364,
      "learning_rate": 4.084951647522394e-05,
      "loss": 0.1564,
      "step": 178600
    },
    {
      "epoch": 1.160804183312222,
      "grad_norm": 1.4550731182098389,
      "learning_rate": 4.084260601679795e-05,
      "loss": 0.1602,
      "step": 178700
    },
    {
      "epoch": 1.1614537659553736,
      "grad_norm": 1.1588053703308105,
      "learning_rate": 4.083569555837196e-05,
      "loss": 0.1611,
      "step": 178800
    },
    {
      "epoch": 1.1621033485985255,
      "grad_norm": 1.2851699590682983,
      "learning_rate": 4.082878509994596e-05,
      "loss": 0.1524,
      "step": 178900
    },
    {
      "epoch": 1.1627529312416773,
      "grad_norm": 1.1645185947418213,
      "learning_rate": 4.082187464151997e-05,
      "loss": 0.1545,
      "step": 179000
    },
    {
      "epoch": 1.163402513884829,
      "grad_norm": 1.4027711153030396,
      "learning_rate": 4.081496418309398e-05,
      "loss": 0.1544,
      "step": 179100
    },
    {
      "epoch": 1.1640520965279808,
      "grad_norm": 1.3898588418960571,
      "learning_rate": 4.080805372466799e-05,
      "loss": 0.148,
      "step": 179200
    },
    {
      "epoch": 1.1647016791711327,
      "grad_norm": 1.4423789978027344,
      "learning_rate": 4.0801143266242e-05,
      "loss": 0.1497,
      "step": 179300
    },
    {
      "epoch": 1.1653512618142843,
      "grad_norm": 1.211031198501587,
      "learning_rate": 4.079423280781601e-05,
      "loss": 0.1566,
      "step": 179400
    },
    {
      "epoch": 1.1660008444574361,
      "grad_norm": 1.1529638767242432,
      "learning_rate": 4.078732234939002e-05,
      "loss": 0.164,
      "step": 179500
    },
    {
      "epoch": 1.166650427100588,
      "grad_norm": 1.736410140991211,
      "learning_rate": 4.078041189096402e-05,
      "loss": 0.1535,
      "step": 179600
    },
    {
      "epoch": 1.1673000097437396,
      "grad_norm": 1.1437278985977173,
      "learning_rate": 4.077350143253803e-05,
      "loss": 0.1427,
      "step": 179700
    },
    {
      "epoch": 1.1679495923868914,
      "grad_norm": 1.6743134260177612,
      "learning_rate": 4.076659097411204e-05,
      "loss": 0.1564,
      "step": 179800
    },
    {
      "epoch": 1.1685991750300433,
      "grad_norm": 1.6016212701797485,
      "learning_rate": 4.075968051568605e-05,
      "loss": 0.152,
      "step": 179900
    },
    {
      "epoch": 1.169248757673195,
      "grad_norm": 1.3499915599822998,
      "learning_rate": 4.0752770057260056e-05,
      "loss": 0.1513,
      "step": 180000
    },
    {
      "epoch": 1.1698983403163468,
      "grad_norm": 1.2894527912139893,
      "learning_rate": 4.0745859598834066e-05,
      "loss": 0.1546,
      "step": 180100
    },
    {
      "epoch": 1.1705479229594986,
      "grad_norm": 1.1594429016113281,
      "learning_rate": 4.0738949140408076e-05,
      "loss": 0.1439,
      "step": 180200
    },
    {
      "epoch": 1.1711975056026502,
      "grad_norm": 1.3706468343734741,
      "learning_rate": 4.0732038681982085e-05,
      "loss": 0.1512,
      "step": 180300
    },
    {
      "epoch": 1.171847088245802,
      "grad_norm": 1.2347254753112793,
      "learning_rate": 4.0725128223556095e-05,
      "loss": 0.1553,
      "step": 180400
    },
    {
      "epoch": 1.172496670888954,
      "grad_norm": 1.398201823234558,
      "learning_rate": 4.0718217765130105e-05,
      "loss": 0.1559,
      "step": 180500
    },
    {
      "epoch": 1.1731462535321056,
      "grad_norm": 1.0469281673431396,
      "learning_rate": 4.0711307306704115e-05,
      "loss": 0.1528,
      "step": 180600
    },
    {
      "epoch": 1.1737958361752574,
      "grad_norm": 1.5309540033340454,
      "learning_rate": 4.0704396848278125e-05,
      "loss": 0.1567,
      "step": 180700
    },
    {
      "epoch": 1.1744454188184092,
      "grad_norm": 1.643654227256775,
      "learning_rate": 4.069748638985213e-05,
      "loss": 0.1496,
      "step": 180800
    },
    {
      "epoch": 1.1750950014615609,
      "grad_norm": 1.2594302892684937,
      "learning_rate": 4.069057593142614e-05,
      "loss": 0.1481,
      "step": 180900
    },
    {
      "epoch": 1.1757445841047127,
      "grad_norm": 1.1468322277069092,
      "learning_rate": 4.068366547300015e-05,
      "loss": 0.1532,
      "step": 181000
    },
    {
      "epoch": 1.1763941667478646,
      "grad_norm": 1.4418836832046509,
      "learning_rate": 4.067675501457416e-05,
      "loss": 0.1504,
      "step": 181100
    },
    {
      "epoch": 1.1770437493910162,
      "grad_norm": 1.402575135231018,
      "learning_rate": 4.066984455614817e-05,
      "loss": 0.1472,
      "step": 181200
    },
    {
      "epoch": 1.177693332034168,
      "grad_norm": 1.457547903060913,
      "learning_rate": 4.066293409772218e-05,
      "loss": 0.1487,
      "step": 181300
    },
    {
      "epoch": 1.1783429146773199,
      "grad_norm": 1.6139376163482666,
      "learning_rate": 4.065602363929619e-05,
      "loss": 0.1524,
      "step": 181400
    },
    {
      "epoch": 1.1789924973204715,
      "grad_norm": 1.1471256017684937,
      "learning_rate": 4.06491131808702e-05,
      "loss": 0.1465,
      "step": 181500
    },
    {
      "epoch": 1.1796420799636234,
      "grad_norm": 1.5177643299102783,
      "learning_rate": 4.064220272244421e-05,
      "loss": 0.1628,
      "step": 181600
    },
    {
      "epoch": 1.1802916626067752,
      "grad_norm": 1.9900609254837036,
      "learning_rate": 4.063529226401821e-05,
      "loss": 0.1552,
      "step": 181700
    },
    {
      "epoch": 1.1809412452499268,
      "grad_norm": 1.2858778238296509,
      "learning_rate": 4.062838180559222e-05,
      "loss": 0.1484,
      "step": 181800
    },
    {
      "epoch": 1.1815908278930787,
      "grad_norm": 1.23874032497406,
      "learning_rate": 4.062147134716623e-05,
      "loss": 0.1487,
      "step": 181900
    },
    {
      "epoch": 1.1822404105362305,
      "grad_norm": 1.254987359046936,
      "learning_rate": 4.061456088874024e-05,
      "loss": 0.1536,
      "step": 182000
    },
    {
      "epoch": 1.1828899931793821,
      "grad_norm": 1.757228136062622,
      "learning_rate": 4.060765043031425e-05,
      "loss": 0.1552,
      "step": 182100
    },
    {
      "epoch": 1.183539575822534,
      "grad_norm": 1.4210050106048584,
      "learning_rate": 4.060073997188826e-05,
      "loss": 0.1543,
      "step": 182200
    },
    {
      "epoch": 1.1841891584656858,
      "grad_norm": 1.06307053565979,
      "learning_rate": 4.059382951346227e-05,
      "loss": 0.1434,
      "step": 182300
    },
    {
      "epoch": 1.1848387411088375,
      "grad_norm": 1.121673345565796,
      "learning_rate": 4.058691905503628e-05,
      "loss": 0.1483,
      "step": 182400
    },
    {
      "epoch": 1.1854883237519893,
      "grad_norm": 1.4659010171890259,
      "learning_rate": 4.058000859661029e-05,
      "loss": 0.1517,
      "step": 182500
    },
    {
      "epoch": 1.1861379063951412,
      "grad_norm": 1.6905258893966675,
      "learning_rate": 4.057309813818429e-05,
      "loss": 0.1581,
      "step": 182600
    },
    {
      "epoch": 1.1867874890382928,
      "grad_norm": 1.3361647129058838,
      "learning_rate": 4.05661876797583e-05,
      "loss": 0.1503,
      "step": 182700
    },
    {
      "epoch": 1.1874370716814446,
      "grad_norm": 1.2352079153060913,
      "learning_rate": 4.055927722133231e-05,
      "loss": 0.1579,
      "step": 182800
    },
    {
      "epoch": 1.1880866543245965,
      "grad_norm": 1.2194470167160034,
      "learning_rate": 4.055236676290632e-05,
      "loss": 0.1505,
      "step": 182900
    },
    {
      "epoch": 1.188736236967748,
      "grad_norm": 1.558764100074768,
      "learning_rate": 4.054545630448033e-05,
      "loss": 0.1521,
      "step": 183000
    },
    {
      "epoch": 1.1893858196109,
      "grad_norm": 1.337669849395752,
      "learning_rate": 4.053854584605434e-05,
      "loss": 0.1584,
      "step": 183100
    },
    {
      "epoch": 1.1900354022540518,
      "grad_norm": 1.303666591644287,
      "learning_rate": 4.0531635387628344e-05,
      "loss": 0.1553,
      "step": 183200
    },
    {
      "epoch": 1.1906849848972034,
      "grad_norm": 1.9409202337265015,
      "learning_rate": 4.0524724929202354e-05,
      "loss": 0.1539,
      "step": 183300
    },
    {
      "epoch": 1.1913345675403553,
      "grad_norm": 1.4555658102035522,
      "learning_rate": 4.0517814470776364e-05,
      "loss": 0.1528,
      "step": 183400
    },
    {
      "epoch": 1.1919841501835071,
      "grad_norm": 1.3068701028823853,
      "learning_rate": 4.0510904012350374e-05,
      "loss": 0.1485,
      "step": 183500
    },
    {
      "epoch": 1.192633732826659,
      "grad_norm": 1.0923477411270142,
      "learning_rate": 4.050399355392438e-05,
      "loss": 0.1517,
      "step": 183600
    },
    {
      "epoch": 1.1932833154698106,
      "grad_norm": 1.3211156129837036,
      "learning_rate": 4.0497083095498387e-05,
      "loss": 0.1537,
      "step": 183700
    },
    {
      "epoch": 1.1939328981129624,
      "grad_norm": 1.2670321464538574,
      "learning_rate": 4.0490172637072396e-05,
      "loss": 0.1543,
      "step": 183800
    },
    {
      "epoch": 1.1945824807561143,
      "grad_norm": 0.8836894035339355,
      "learning_rate": 4.0483262178646406e-05,
      "loss": 0.1479,
      "step": 183900
    },
    {
      "epoch": 1.195232063399266,
      "grad_norm": 0.726771354675293,
      "learning_rate": 4.0476351720220416e-05,
      "loss": 0.1528,
      "step": 184000
    },
    {
      "epoch": 1.1958816460424178,
      "grad_norm": 1.450979471206665,
      "learning_rate": 4.0469441261794426e-05,
      "loss": 0.1535,
      "step": 184100
    },
    {
      "epoch": 1.1965312286855696,
      "grad_norm": 1.0932152271270752,
      "learning_rate": 4.0462530803368436e-05,
      "loss": 0.1535,
      "step": 184200
    },
    {
      "epoch": 1.1971808113287212,
      "grad_norm": 1.0937412977218628,
      "learning_rate": 4.0455620344942446e-05,
      "loss": 0.1488,
      "step": 184300
    },
    {
      "epoch": 1.197830393971873,
      "grad_norm": 1.0883712768554688,
      "learning_rate": 4.0448709886516455e-05,
      "loss": 0.1487,
      "step": 184400
    },
    {
      "epoch": 1.198479976615025,
      "grad_norm": 1.9887498617172241,
      "learning_rate": 4.044179942809046e-05,
      "loss": 0.1532,
      "step": 184500
    },
    {
      "epoch": 1.1991295592581765,
      "grad_norm": 1.1577503681182861,
      "learning_rate": 4.043488896966447e-05,
      "loss": 0.1453,
      "step": 184600
    },
    {
      "epoch": 1.1997791419013284,
      "grad_norm": 1.7434383630752563,
      "learning_rate": 4.042797851123848e-05,
      "loss": 0.1526,
      "step": 184700
    },
    {
      "epoch": 1.2004287245444802,
      "grad_norm": 1.108537197113037,
      "learning_rate": 4.042106805281249e-05,
      "loss": 0.1487,
      "step": 184800
    },
    {
      "epoch": 1.2010783071876319,
      "grad_norm": 1.4972562789916992,
      "learning_rate": 4.04141575943865e-05,
      "loss": 0.1485,
      "step": 184900
    },
    {
      "epoch": 1.2017278898307837,
      "grad_norm": 1.5862623453140259,
      "learning_rate": 4.040724713596051e-05,
      "loss": 0.1536,
      "step": 185000
    },
    {
      "epoch": 1.2023774724739356,
      "grad_norm": 0.8758138418197632,
      "learning_rate": 4.040033667753452e-05,
      "loss": 0.1448,
      "step": 185100
    },
    {
      "epoch": 1.2030270551170872,
      "grad_norm": 1.49397611618042,
      "learning_rate": 4.039342621910853e-05,
      "loss": 0.1559,
      "step": 185200
    },
    {
      "epoch": 1.203676637760239,
      "grad_norm": 1.532050609588623,
      "learning_rate": 4.038651576068254e-05,
      "loss": 0.151,
      "step": 185300
    },
    {
      "epoch": 1.2043262204033909,
      "grad_norm": 1.655236840248108,
      "learning_rate": 4.037960530225655e-05,
      "loss": 0.1513,
      "step": 185400
    },
    {
      "epoch": 1.2049758030465425,
      "grad_norm": 1.302799105644226,
      "learning_rate": 4.037269484383055e-05,
      "loss": 0.1482,
      "step": 185500
    },
    {
      "epoch": 1.2056253856896944,
      "grad_norm": 1.3389002084732056,
      "learning_rate": 4.036578438540456e-05,
      "loss": 0.1553,
      "step": 185600
    },
    {
      "epoch": 1.2062749683328462,
      "grad_norm": 1.127278208732605,
      "learning_rate": 4.035887392697857e-05,
      "loss": 0.1522,
      "step": 185700
    },
    {
      "epoch": 1.206924550975998,
      "grad_norm": 1.7762876749038696,
      "learning_rate": 4.035196346855258e-05,
      "loss": 0.1534,
      "step": 185800
    },
    {
      "epoch": 1.2075741336191497,
      "grad_norm": 0.9424074292182922,
      "learning_rate": 4.034505301012659e-05,
      "loss": 0.1461,
      "step": 185900
    },
    {
      "epoch": 1.2082237162623015,
      "grad_norm": 1.7598974704742432,
      "learning_rate": 4.03381425517006e-05,
      "loss": 0.1508,
      "step": 186000
    },
    {
      "epoch": 1.2088732989054534,
      "grad_norm": 1.2279789447784424,
      "learning_rate": 4.033123209327461e-05,
      "loss": 0.1523,
      "step": 186100
    },
    {
      "epoch": 1.209522881548605,
      "grad_norm": 1.3617323637008667,
      "learning_rate": 4.032432163484862e-05,
      "loss": 0.1537,
      "step": 186200
    },
    {
      "epoch": 1.2101724641917568,
      "grad_norm": 1.2751235961914062,
      "learning_rate": 4.031741117642263e-05,
      "loss": 0.1578,
      "step": 186300
    },
    {
      "epoch": 1.2108220468349087,
      "grad_norm": 1.35518217086792,
      "learning_rate": 4.031050071799663e-05,
      "loss": 0.1515,
      "step": 186400
    },
    {
      "epoch": 1.2114716294780603,
      "grad_norm": 0.9033479690551758,
      "learning_rate": 4.030359025957064e-05,
      "loss": 0.1534,
      "step": 186500
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 1.781865119934082,
      "learning_rate": 4.029667980114465e-05,
      "loss": 0.1511,
      "step": 186600
    },
    {
      "epoch": 1.212770794764364,
      "grad_norm": 1.1134752035140991,
      "learning_rate": 4.028976934271866e-05,
      "loss": 0.1425,
      "step": 186700
    },
    {
      "epoch": 1.2134203774075156,
      "grad_norm": 1.598732829093933,
      "learning_rate": 4.0282858884292665e-05,
      "loss": 0.1521,
      "step": 186800
    },
    {
      "epoch": 1.2140699600506675,
      "grad_norm": 1.753735065460205,
      "learning_rate": 4.0275948425866675e-05,
      "loss": 0.1495,
      "step": 186900
    },
    {
      "epoch": 1.2147195426938193,
      "grad_norm": 1.62802255153656,
      "learning_rate": 4.0269037967440685e-05,
      "loss": 0.1548,
      "step": 187000
    },
    {
      "epoch": 1.215369125336971,
      "grad_norm": 1.961068868637085,
      "learning_rate": 4.0262127509014694e-05,
      "loss": 0.1516,
      "step": 187100
    },
    {
      "epoch": 1.2160187079801228,
      "grad_norm": 1.322646141052246,
      "learning_rate": 4.0255217050588704e-05,
      "loss": 0.1551,
      "step": 187200
    },
    {
      "epoch": 1.2166682906232746,
      "grad_norm": 1.3385971784591675,
      "learning_rate": 4.024830659216271e-05,
      "loss": 0.1482,
      "step": 187300
    },
    {
      "epoch": 1.2173178732664263,
      "grad_norm": 1.0228569507598877,
      "learning_rate": 4.024139613373672e-05,
      "loss": 0.1462,
      "step": 187400
    },
    {
      "epoch": 1.2179674559095781,
      "grad_norm": 1.0402228832244873,
      "learning_rate": 4.023448567531073e-05,
      "loss": 0.1473,
      "step": 187500
    },
    {
      "epoch": 1.21861703855273,
      "grad_norm": 1.4210487604141235,
      "learning_rate": 4.022757521688474e-05,
      "loss": 0.1485,
      "step": 187600
    },
    {
      "epoch": 1.2192666211958816,
      "grad_norm": 1.0654208660125732,
      "learning_rate": 4.022066475845875e-05,
      "loss": 0.1467,
      "step": 187700
    },
    {
      "epoch": 1.2199162038390334,
      "grad_norm": 1.6078236103057861,
      "learning_rate": 4.0213754300032757e-05,
      "loss": 0.1489,
      "step": 187800
    },
    {
      "epoch": 1.2205657864821853,
      "grad_norm": 1.4473984241485596,
      "learning_rate": 4.0206843841606766e-05,
      "loss": 0.1467,
      "step": 187900
    },
    {
      "epoch": 1.221215369125337,
      "grad_norm": 1.4517226219177246,
      "learning_rate": 4.0199933383180776e-05,
      "loss": 0.152,
      "step": 188000
    },
    {
      "epoch": 1.2218649517684887,
      "grad_norm": 1.2411593198776245,
      "learning_rate": 4.0193022924754786e-05,
      "loss": 0.1496,
      "step": 188100
    },
    {
      "epoch": 1.2225145344116406,
      "grad_norm": 0.7334876656532288,
      "learning_rate": 4.0186112466328796e-05,
      "loss": 0.155,
      "step": 188200
    },
    {
      "epoch": 1.2231641170547922,
      "grad_norm": 1.4726574420928955,
      "learning_rate": 4.01792020079028e-05,
      "loss": 0.1534,
      "step": 188300
    },
    {
      "epoch": 1.223813699697944,
      "grad_norm": 1.2824465036392212,
      "learning_rate": 4.017229154947681e-05,
      "loss": 0.1514,
      "step": 188400
    },
    {
      "epoch": 1.224463282341096,
      "grad_norm": 1.2371424436569214,
      "learning_rate": 4.016538109105082e-05,
      "loss": 0.1574,
      "step": 188500
    },
    {
      "epoch": 1.2251128649842475,
      "grad_norm": 1.160705804824829,
      "learning_rate": 4.015847063262483e-05,
      "loss": 0.1501,
      "step": 188600
    },
    {
      "epoch": 1.2257624476273994,
      "grad_norm": 1.1634280681610107,
      "learning_rate": 4.015156017419884e-05,
      "loss": 0.1502,
      "step": 188700
    },
    {
      "epoch": 1.2264120302705512,
      "grad_norm": 1.3629429340362549,
      "learning_rate": 4.014464971577285e-05,
      "loss": 0.1544,
      "step": 188800
    },
    {
      "epoch": 1.2270616129137029,
      "grad_norm": 1.3043690919876099,
      "learning_rate": 4.013773925734686e-05,
      "loss": 0.1494,
      "step": 188900
    },
    {
      "epoch": 1.2277111955568547,
      "grad_norm": 1.430375099182129,
      "learning_rate": 4.013082879892087e-05,
      "loss": 0.1509,
      "step": 189000
    },
    {
      "epoch": 1.2283607782000066,
      "grad_norm": 1.1485949754714966,
      "learning_rate": 4.012391834049488e-05,
      "loss": 0.1468,
      "step": 189100
    },
    {
      "epoch": 1.2290103608431582,
      "grad_norm": 0.9203810691833496,
      "learning_rate": 4.011700788206888e-05,
      "loss": 0.1501,
      "step": 189200
    },
    {
      "epoch": 1.22965994348631,
      "grad_norm": 1.1070917844772339,
      "learning_rate": 4.011009742364289e-05,
      "loss": 0.1487,
      "step": 189300
    },
    {
      "epoch": 1.2303095261294619,
      "grad_norm": 1.7672171592712402,
      "learning_rate": 4.01031869652169e-05,
      "loss": 0.1501,
      "step": 189400
    },
    {
      "epoch": 1.2309591087726135,
      "grad_norm": 1.434633731842041,
      "learning_rate": 4.009627650679091e-05,
      "loss": 0.144,
      "step": 189500
    },
    {
      "epoch": 1.2316086914157653,
      "grad_norm": 1.5986089706420898,
      "learning_rate": 4.008936604836492e-05,
      "loss": 0.1495,
      "step": 189600
    },
    {
      "epoch": 1.2322582740589172,
      "grad_norm": 1.0798635482788086,
      "learning_rate": 4.008245558993893e-05,
      "loss": 0.1504,
      "step": 189700
    },
    {
      "epoch": 1.2329078567020688,
      "grad_norm": 1.1862512826919556,
      "learning_rate": 4.007554513151294e-05,
      "loss": 0.1462,
      "step": 189800
    },
    {
      "epoch": 1.2335574393452207,
      "grad_norm": 1.2911430597305298,
      "learning_rate": 4.006863467308695e-05,
      "loss": 0.1438,
      "step": 189900
    },
    {
      "epoch": 1.2342070219883725,
      "grad_norm": 1.0289636850357056,
      "learning_rate": 4.006172421466095e-05,
      "loss": 0.144,
      "step": 190000
    },
    {
      "epoch": 1.2348566046315241,
      "grad_norm": 1.2134066820144653,
      "learning_rate": 4.005481375623496e-05,
      "loss": 0.1453,
      "step": 190100
    },
    {
      "epoch": 1.235506187274676,
      "grad_norm": 1.1855758428573608,
      "learning_rate": 4.004790329780897e-05,
      "loss": 0.1435,
      "step": 190200
    },
    {
      "epoch": 1.2361557699178278,
      "grad_norm": 1.1197603940963745,
      "learning_rate": 4.004099283938298e-05,
      "loss": 0.1463,
      "step": 190300
    },
    {
      "epoch": 1.2368053525609795,
      "grad_norm": 1.3321681022644043,
      "learning_rate": 4.0034082380956986e-05,
      "loss": 0.1504,
      "step": 190400
    },
    {
      "epoch": 1.2374549352041313,
      "grad_norm": 1.2824636697769165,
      "learning_rate": 4.0027171922530995e-05,
      "loss": 0.1465,
      "step": 190500
    },
    {
      "epoch": 1.2381045178472831,
      "grad_norm": 1.4778923988342285,
      "learning_rate": 4.0020261464105005e-05,
      "loss": 0.1529,
      "step": 190600
    },
    {
      "epoch": 1.2387541004904348,
      "grad_norm": 1.109447956085205,
      "learning_rate": 4.0013351005679015e-05,
      "loss": 0.1491,
      "step": 190700
    },
    {
      "epoch": 1.2394036831335866,
      "grad_norm": 1.614442229270935,
      "learning_rate": 4.0006440547253025e-05,
      "loss": 0.1517,
      "step": 190800
    },
    {
      "epoch": 1.2400532657767385,
      "grad_norm": 1.5548440217971802,
      "learning_rate": 3.9999530088827035e-05,
      "loss": 0.1484,
      "step": 190900
    },
    {
      "epoch": 1.2407028484198903,
      "grad_norm": 1.555300235748291,
      "learning_rate": 3.9992619630401045e-05,
      "loss": 0.1491,
      "step": 191000
    },
    {
      "epoch": 1.241352431063042,
      "grad_norm": 1.2192330360412598,
      "learning_rate": 3.998570917197505e-05,
      "loss": 0.1434,
      "step": 191100
    },
    {
      "epoch": 1.2420020137061938,
      "grad_norm": 1.0950714349746704,
      "learning_rate": 3.997879871354906e-05,
      "loss": 0.1386,
      "step": 191200
    },
    {
      "epoch": 1.2426515963493456,
      "grad_norm": 1.6069034337997437,
      "learning_rate": 3.997188825512307e-05,
      "loss": 0.1533,
      "step": 191300
    },
    {
      "epoch": 1.2433011789924973,
      "grad_norm": 1.1065168380737305,
      "learning_rate": 3.996497779669708e-05,
      "loss": 0.1512,
      "step": 191400
    },
    {
      "epoch": 1.243950761635649,
      "grad_norm": 1.3703197240829468,
      "learning_rate": 3.995806733827109e-05,
      "loss": 0.1419,
      "step": 191500
    },
    {
      "epoch": 1.244600344278801,
      "grad_norm": 1.0885648727416992,
      "learning_rate": 3.99511568798451e-05,
      "loss": 0.1458,
      "step": 191600
    },
    {
      "epoch": 1.2452499269219526,
      "grad_norm": 1.3166825771331787,
      "learning_rate": 3.994424642141911e-05,
      "loss": 0.1536,
      "step": 191700
    },
    {
      "epoch": 1.2458995095651044,
      "grad_norm": 1.2299903631210327,
      "learning_rate": 3.993733596299312e-05,
      "loss": 0.1473,
      "step": 191800
    },
    {
      "epoch": 1.2465490922082563,
      "grad_norm": 1.069393515586853,
      "learning_rate": 3.9930425504567127e-05,
      "loss": 0.148,
      "step": 191900
    },
    {
      "epoch": 1.247198674851408,
      "grad_norm": 0.9228580594062805,
      "learning_rate": 3.9923515046141136e-05,
      "loss": 0.1477,
      "step": 192000
    },
    {
      "epoch": 1.2478482574945597,
      "grad_norm": 1.1282975673675537,
      "learning_rate": 3.991660458771514e-05,
      "loss": 0.1498,
      "step": 192100
    },
    {
      "epoch": 1.2484978401377116,
      "grad_norm": 1.379356861114502,
      "learning_rate": 3.990969412928915e-05,
      "loss": 0.1474,
      "step": 192200
    },
    {
      "epoch": 1.2491474227808632,
      "grad_norm": 1.1988294124603271,
      "learning_rate": 3.990278367086316e-05,
      "loss": 0.1485,
      "step": 192300
    },
    {
      "epoch": 1.249797005424015,
      "grad_norm": 1.6584925651550293,
      "learning_rate": 3.989587321243717e-05,
      "loss": 0.1493,
      "step": 192400
    },
    {
      "epoch": 1.250446588067167,
      "grad_norm": 1.1392637491226196,
      "learning_rate": 3.988896275401118e-05,
      "loss": 0.1412,
      "step": 192500
    },
    {
      "epoch": 1.2510961707103188,
      "grad_norm": 1.4210197925567627,
      "learning_rate": 3.988205229558519e-05,
      "loss": 0.156,
      "step": 192600
    },
    {
      "epoch": 1.2517457533534704,
      "grad_norm": 0.9817412495613098,
      "learning_rate": 3.98751418371592e-05,
      "loss": 0.1515,
      "step": 192700
    },
    {
      "epoch": 1.2523953359966222,
      "grad_norm": 1.4021267890930176,
      "learning_rate": 3.986823137873321e-05,
      "loss": 0.1541,
      "step": 192800
    },
    {
      "epoch": 1.253044918639774,
      "grad_norm": 1.7080378532409668,
      "learning_rate": 3.986132092030722e-05,
      "loss": 0.1445,
      "step": 192900
    },
    {
      "epoch": 1.2536945012829257,
      "grad_norm": 1.214365005493164,
      "learning_rate": 3.985441046188122e-05,
      "loss": 0.1434,
      "step": 193000
    },
    {
      "epoch": 1.2543440839260775,
      "grad_norm": 1.1596161127090454,
      "learning_rate": 3.984750000345523e-05,
      "loss": 0.1514,
      "step": 193100
    },
    {
      "epoch": 1.2549936665692294,
      "grad_norm": 1.4894670248031616,
      "learning_rate": 3.984058954502924e-05,
      "loss": 0.1463,
      "step": 193200
    },
    {
      "epoch": 1.255643249212381,
      "grad_norm": 1.463025689125061,
      "learning_rate": 3.983367908660325e-05,
      "loss": 0.1458,
      "step": 193300
    },
    {
      "epoch": 1.2562928318555329,
      "grad_norm": 1.3046575784683228,
      "learning_rate": 3.982676862817726e-05,
      "loss": 0.1437,
      "step": 193400
    },
    {
      "epoch": 1.2569424144986847,
      "grad_norm": 1.5821253061294556,
      "learning_rate": 3.981985816975127e-05,
      "loss": 0.1527,
      "step": 193500
    },
    {
      "epoch": 1.2575919971418363,
      "grad_norm": 1.2886219024658203,
      "learning_rate": 3.9812947711325274e-05,
      "loss": 0.1448,
      "step": 193600
    },
    {
      "epoch": 1.2582415797849882,
      "grad_norm": 1.2393946647644043,
      "learning_rate": 3.9806037252899284e-05,
      "loss": 0.1485,
      "step": 193700
    },
    {
      "epoch": 1.25889116242814,
      "grad_norm": 1.438740849494934,
      "learning_rate": 3.9799126794473293e-05,
      "loss": 0.1401,
      "step": 193800
    },
    {
      "epoch": 1.2595407450712917,
      "grad_norm": 1.099218726158142,
      "learning_rate": 3.97922163360473e-05,
      "loss": 0.1518,
      "step": 193900
    },
    {
      "epoch": 1.2601903277144435,
      "grad_norm": 1.165507435798645,
      "learning_rate": 3.9785305877621306e-05,
      "loss": 0.1536,
      "step": 194000
    },
    {
      "epoch": 1.2608399103575954,
      "grad_norm": 1.2871909141540527,
      "learning_rate": 3.9778395419195316e-05,
      "loss": 0.1482,
      "step": 194100
    },
    {
      "epoch": 1.261489493000747,
      "grad_norm": 1.3612765073776245,
      "learning_rate": 3.9771484960769326e-05,
      "loss": 0.1518,
      "step": 194200
    },
    {
      "epoch": 1.2621390756438988,
      "grad_norm": 1.2563973665237427,
      "learning_rate": 3.9764574502343336e-05,
      "loss": 0.1512,
      "step": 194300
    },
    {
      "epoch": 1.2627886582870507,
      "grad_norm": 1.5802034139633179,
      "learning_rate": 3.9757664043917346e-05,
      "loss": 0.1514,
      "step": 194400
    },
    {
      "epoch": 1.2634382409302023,
      "grad_norm": 1.4818215370178223,
      "learning_rate": 3.9750753585491356e-05,
      "loss": 0.1608,
      "step": 194500
    },
    {
      "epoch": 1.2640878235733541,
      "grad_norm": 1.32076096534729,
      "learning_rate": 3.9743843127065365e-05,
      "loss": 0.1454,
      "step": 194600
    },
    {
      "epoch": 1.264737406216506,
      "grad_norm": 1.335048794746399,
      "learning_rate": 3.9736932668639375e-05,
      "loss": 0.1589,
      "step": 194700
    },
    {
      "epoch": 1.2653869888596576,
      "grad_norm": 1.1504733562469482,
      "learning_rate": 3.9730022210213385e-05,
      "loss": 0.1507,
      "step": 194800
    },
    {
      "epoch": 1.2660365715028095,
      "grad_norm": 1.2435580492019653,
      "learning_rate": 3.972311175178739e-05,
      "loss": 0.1534,
      "step": 194900
    },
    {
      "epoch": 1.2666861541459613,
      "grad_norm": 1.174111247062683,
      "learning_rate": 3.97162012933614e-05,
      "loss": 0.1474,
      "step": 195000
    },
    {
      "epoch": 1.267335736789113,
      "grad_norm": 1.7016699314117432,
      "learning_rate": 3.970929083493541e-05,
      "loss": 0.1454,
      "step": 195100
    },
    {
      "epoch": 1.2679853194322648,
      "grad_norm": 1.385425090789795,
      "learning_rate": 3.970238037650942e-05,
      "loss": 0.1463,
      "step": 195200
    },
    {
      "epoch": 1.2686349020754166,
      "grad_norm": 1.34474515914917,
      "learning_rate": 3.969546991808343e-05,
      "loss": 0.1502,
      "step": 195300
    },
    {
      "epoch": 1.2692844847185683,
      "grad_norm": 1.032571792602539,
      "learning_rate": 3.968855945965744e-05,
      "loss": 0.1426,
      "step": 195400
    },
    {
      "epoch": 1.26993406736172,
      "grad_norm": 1.5201618671417236,
      "learning_rate": 3.968164900123145e-05,
      "loss": 0.1514,
      "step": 195500
    },
    {
      "epoch": 1.270583650004872,
      "grad_norm": 1.7773255109786987,
      "learning_rate": 3.967473854280546e-05,
      "loss": 0.1449,
      "step": 195600
    },
    {
      "epoch": 1.2712332326480236,
      "grad_norm": 1.5878337621688843,
      "learning_rate": 3.966782808437947e-05,
      "loss": 0.1463,
      "step": 195700
    },
    {
      "epoch": 1.2718828152911754,
      "grad_norm": 0.9345765709877014,
      "learning_rate": 3.966091762595347e-05,
      "loss": 0.1504,
      "step": 195800
    },
    {
      "epoch": 1.2725323979343273,
      "grad_norm": 1.304478406906128,
      "learning_rate": 3.965400716752748e-05,
      "loss": 0.1451,
      "step": 195900
    },
    {
      "epoch": 1.2731819805774789,
      "grad_norm": 1.7298237085342407,
      "learning_rate": 3.964709670910149e-05,
      "loss": 0.146,
      "step": 196000
    },
    {
      "epoch": 1.2738315632206307,
      "grad_norm": 1.3402305841445923,
      "learning_rate": 3.96401862506755e-05,
      "loss": 0.1491,
      "step": 196100
    },
    {
      "epoch": 1.2744811458637826,
      "grad_norm": 1.3216685056686401,
      "learning_rate": 3.963327579224951e-05,
      "loss": 0.1477,
      "step": 196200
    },
    {
      "epoch": 1.2751307285069342,
      "grad_norm": 1.6625819206237793,
      "learning_rate": 3.962636533382352e-05,
      "loss": 0.1478,
      "step": 196300
    },
    {
      "epoch": 1.275780311150086,
      "grad_norm": 1.071567416191101,
      "learning_rate": 3.961945487539753e-05,
      "loss": 0.1489,
      "step": 196400
    },
    {
      "epoch": 1.276429893793238,
      "grad_norm": 1.3254891633987427,
      "learning_rate": 3.961254441697154e-05,
      "loss": 0.1482,
      "step": 196500
    },
    {
      "epoch": 1.2770794764363895,
      "grad_norm": 1.832655906677246,
      "learning_rate": 3.960563395854555e-05,
      "loss": 0.142,
      "step": 196600
    },
    {
      "epoch": 1.2777290590795414,
      "grad_norm": 1.263765573501587,
      "learning_rate": 3.959872350011956e-05,
      "loss": 0.1489,
      "step": 196700
    },
    {
      "epoch": 1.2783786417226932,
      "grad_norm": 1.7977594137191772,
      "learning_rate": 3.959181304169356e-05,
      "loss": 0.1495,
      "step": 196800
    },
    {
      "epoch": 1.2790282243658448,
      "grad_norm": 0.9972555637359619,
      "learning_rate": 3.958490258326757e-05,
      "loss": 0.143,
      "step": 196900
    },
    {
      "epoch": 1.2796778070089967,
      "grad_norm": 0.7936591506004333,
      "learning_rate": 3.957799212484158e-05,
      "loss": 0.1555,
      "step": 197000
    },
    {
      "epoch": 1.2803273896521485,
      "grad_norm": 1.1905943155288696,
      "learning_rate": 3.957108166641559e-05,
      "loss": 0.1411,
      "step": 197100
    },
    {
      "epoch": 1.2809769722953002,
      "grad_norm": 1.100326418876648,
      "learning_rate": 3.9564171207989595e-05,
      "loss": 0.1437,
      "step": 197200
    },
    {
      "epoch": 1.281626554938452,
      "grad_norm": 1.1879562139511108,
      "learning_rate": 3.9557260749563604e-05,
      "loss": 0.151,
      "step": 197300
    },
    {
      "epoch": 1.2822761375816039,
      "grad_norm": 1.3197365999221802,
      "learning_rate": 3.9550350291137614e-05,
      "loss": 0.1491,
      "step": 197400
    },
    {
      "epoch": 1.2829257202247555,
      "grad_norm": 1.200280785560608,
      "learning_rate": 3.9543439832711624e-05,
      "loss": 0.1521,
      "step": 197500
    },
    {
      "epoch": 1.2835753028679073,
      "grad_norm": 1.7538129091262817,
      "learning_rate": 3.9536529374285634e-05,
      "loss": 0.1423,
      "step": 197600
    },
    {
      "epoch": 1.2842248855110592,
      "grad_norm": 1.491033911705017,
      "learning_rate": 3.952961891585964e-05,
      "loss": 0.1483,
      "step": 197700
    },
    {
      "epoch": 1.2848744681542108,
      "grad_norm": 1.1054726839065552,
      "learning_rate": 3.952270845743365e-05,
      "loss": 0.1437,
      "step": 197800
    },
    {
      "epoch": 1.2855240507973626,
      "grad_norm": 1.5531710386276245,
      "learning_rate": 3.951579799900766e-05,
      "loss": 0.1524,
      "step": 197900
    },
    {
      "epoch": 1.2861736334405145,
      "grad_norm": 1.5387462377548218,
      "learning_rate": 3.9508887540581667e-05,
      "loss": 0.1431,
      "step": 198000
    },
    {
      "epoch": 1.2868232160836661,
      "grad_norm": 1.4598844051361084,
      "learning_rate": 3.9501977082155676e-05,
      "loss": 0.1415,
      "step": 198100
    },
    {
      "epoch": 1.287472798726818,
      "grad_norm": 1.0943716764450073,
      "learning_rate": 3.9495066623729686e-05,
      "loss": 0.1541,
      "step": 198200
    },
    {
      "epoch": 1.2881223813699698,
      "grad_norm": 1.0686649084091187,
      "learning_rate": 3.9488156165303696e-05,
      "loss": 0.1513,
      "step": 198300
    },
    {
      "epoch": 1.2887719640131214,
      "grad_norm": 1.1943237781524658,
      "learning_rate": 3.9481245706877706e-05,
      "loss": 0.1548,
      "step": 198400
    },
    {
      "epoch": 1.2894215466562733,
      "grad_norm": 1.2025941610336304,
      "learning_rate": 3.9474335248451716e-05,
      "loss": 0.1439,
      "step": 198500
    },
    {
      "epoch": 1.2900711292994251,
      "grad_norm": 1.1520010232925415,
      "learning_rate": 3.946742479002572e-05,
      "loss": 0.1521,
      "step": 198600
    },
    {
      "epoch": 1.2907207119425768,
      "grad_norm": 1.5337042808532715,
      "learning_rate": 3.946051433159973e-05,
      "loss": 0.1381,
      "step": 198700
    },
    {
      "epoch": 1.2913702945857286,
      "grad_norm": 1.2037432193756104,
      "learning_rate": 3.945360387317374e-05,
      "loss": 0.1453,
      "step": 198800
    },
    {
      "epoch": 1.2920198772288805,
      "grad_norm": 1.003947138786316,
      "learning_rate": 3.944669341474775e-05,
      "loss": 0.151,
      "step": 198900
    },
    {
      "epoch": 1.2926694598720323,
      "grad_norm": 1.5706186294555664,
      "learning_rate": 3.943978295632176e-05,
      "loss": 0.1468,
      "step": 199000
    },
    {
      "epoch": 1.293319042515184,
      "grad_norm": 1.6271001100540161,
      "learning_rate": 3.943287249789577e-05,
      "loss": 0.1457,
      "step": 199100
    },
    {
      "epoch": 1.2939686251583358,
      "grad_norm": 1.4693262577056885,
      "learning_rate": 3.942596203946978e-05,
      "loss": 0.1438,
      "step": 199200
    },
    {
      "epoch": 1.2946182078014876,
      "grad_norm": 1.722752571105957,
      "learning_rate": 3.941905158104379e-05,
      "loss": 0.1448,
      "step": 199300
    },
    {
      "epoch": 1.2952677904446392,
      "grad_norm": 0.8003622889518738,
      "learning_rate": 3.94121411226178e-05,
      "loss": 0.1447,
      "step": 199400
    },
    {
      "epoch": 1.295917373087791,
      "grad_norm": 1.2971152067184448,
      "learning_rate": 3.940523066419181e-05,
      "loss": 0.1459,
      "step": 199500
    },
    {
      "epoch": 1.296566955730943,
      "grad_norm": 1.264765977859497,
      "learning_rate": 3.939832020576581e-05,
      "loss": 0.1433,
      "step": 199600
    },
    {
      "epoch": 1.2972165383740946,
      "grad_norm": 1.0534619092941284,
      "learning_rate": 3.939140974733982e-05,
      "loss": 0.1479,
      "step": 199700
    },
    {
      "epoch": 1.2978661210172464,
      "grad_norm": 1.4239981174468994,
      "learning_rate": 3.938449928891383e-05,
      "loss": 0.1493,
      "step": 199800
    },
    {
      "epoch": 1.2985157036603983,
      "grad_norm": 1.2644778490066528,
      "learning_rate": 3.937758883048784e-05,
      "loss": 0.1436,
      "step": 199900
    },
    {
      "epoch": 1.29916528630355,
      "grad_norm": 1.8234940767288208,
      "learning_rate": 3.937067837206185e-05,
      "loss": 0.1465,
      "step": 200000
    },
    {
      "epoch": 1.2998148689467017,
      "grad_norm": 2.0224180221557617,
      "learning_rate": 3.936376791363586e-05,
      "loss": 0.1435,
      "step": 200100
    },
    {
      "epoch": 1.3004644515898536,
      "grad_norm": 1.387840747833252,
      "learning_rate": 3.935685745520987e-05,
      "loss": 0.1436,
      "step": 200200
    },
    {
      "epoch": 1.3011140342330054,
      "grad_norm": 1.342923641204834,
      "learning_rate": 3.934994699678388e-05,
      "loss": 0.1411,
      "step": 200300
    },
    {
      "epoch": 1.301763616876157,
      "grad_norm": 1.1820762157440186,
      "learning_rate": 3.934303653835788e-05,
      "loss": 0.1483,
      "step": 200400
    },
    {
      "epoch": 1.302413199519309,
      "grad_norm": 1.3346420526504517,
      "learning_rate": 3.933612607993189e-05,
      "loss": 0.1462,
      "step": 200500
    },
    {
      "epoch": 1.3030627821624607,
      "grad_norm": 1.495097279548645,
      "learning_rate": 3.93292156215059e-05,
      "loss": 0.1477,
      "step": 200600
    },
    {
      "epoch": 1.3037123648056124,
      "grad_norm": 1.1380959749221802,
      "learning_rate": 3.9322305163079905e-05,
      "loss": 0.1451,
      "step": 200700
    },
    {
      "epoch": 1.3043619474487642,
      "grad_norm": 1.6057212352752686,
      "learning_rate": 3.9315394704653915e-05,
      "loss": 0.1426,
      "step": 200800
    },
    {
      "epoch": 1.305011530091916,
      "grad_norm": 1.6679658889770508,
      "learning_rate": 3.9308484246227925e-05,
      "loss": 0.1402,
      "step": 200900
    },
    {
      "epoch": 1.3056611127350677,
      "grad_norm": 1.2324974536895752,
      "learning_rate": 3.9301573787801935e-05,
      "loss": 0.1423,
      "step": 201000
    },
    {
      "epoch": 1.3063106953782195,
      "grad_norm": 1.0284662246704102,
      "learning_rate": 3.9294663329375945e-05,
      "loss": 0.143,
      "step": 201100
    },
    {
      "epoch": 1.3069602780213714,
      "grad_norm": 1.4056168794631958,
      "learning_rate": 3.9287752870949955e-05,
      "loss": 0.1432,
      "step": 201200
    },
    {
      "epoch": 1.307609860664523,
      "grad_norm": 1.8983691930770874,
      "learning_rate": 3.9280842412523965e-05,
      "loss": 0.1374,
      "step": 201300
    },
    {
      "epoch": 1.3082594433076749,
      "grad_norm": 1.1477266550064087,
      "learning_rate": 3.9273931954097974e-05,
      "loss": 0.1492,
      "step": 201400
    },
    {
      "epoch": 1.3089090259508267,
      "grad_norm": 0.9876846671104431,
      "learning_rate": 3.926702149567198e-05,
      "loss": 0.143,
      "step": 201500
    },
    {
      "epoch": 1.3095586085939783,
      "grad_norm": 1.128652811050415,
      "learning_rate": 3.926011103724599e-05,
      "loss": 0.141,
      "step": 201600
    },
    {
      "epoch": 1.3102081912371302,
      "grad_norm": 1.21562659740448,
      "learning_rate": 3.925320057882e-05,
      "loss": 0.1442,
      "step": 201700
    },
    {
      "epoch": 1.310857773880282,
      "grad_norm": 0.9764229655265808,
      "learning_rate": 3.924629012039401e-05,
      "loss": 0.1567,
      "step": 201800
    },
    {
      "epoch": 1.3115073565234336,
      "grad_norm": 1.8906903266906738,
      "learning_rate": 3.923937966196802e-05,
      "loss": 0.1492,
      "step": 201900
    },
    {
      "epoch": 1.3121569391665855,
      "grad_norm": 0.8073344230651855,
      "learning_rate": 3.923246920354203e-05,
      "loss": 0.1445,
      "step": 202000
    },
    {
      "epoch": 1.3128065218097373,
      "grad_norm": 1.2696878910064697,
      "learning_rate": 3.922555874511604e-05,
      "loss": 0.1436,
      "step": 202100
    },
    {
      "epoch": 1.313456104452889,
      "grad_norm": 1.664184808731079,
      "learning_rate": 3.9218648286690046e-05,
      "loss": 0.1458,
      "step": 202200
    },
    {
      "epoch": 1.3141056870960408,
      "grad_norm": 1.4775350093841553,
      "learning_rate": 3.9211737828264056e-05,
      "loss": 0.1538,
      "step": 202300
    },
    {
      "epoch": 1.3147552697391927,
      "grad_norm": 1.1706047058105469,
      "learning_rate": 3.920482736983806e-05,
      "loss": 0.148,
      "step": 202400
    },
    {
      "epoch": 1.3154048523823443,
      "grad_norm": 1.1940730810165405,
      "learning_rate": 3.919791691141207e-05,
      "loss": 0.1482,
      "step": 202500
    },
    {
      "epoch": 1.3160544350254961,
      "grad_norm": 1.5435082912445068,
      "learning_rate": 3.919100645298608e-05,
      "loss": 0.1439,
      "step": 202600
    },
    {
      "epoch": 1.316704017668648,
      "grad_norm": 1.3644267320632935,
      "learning_rate": 3.918409599456009e-05,
      "loss": 0.1507,
      "step": 202700
    },
    {
      "epoch": 1.3173536003117996,
      "grad_norm": 1.1372994184494019,
      "learning_rate": 3.91771855361341e-05,
      "loss": 0.1433,
      "step": 202800
    },
    {
      "epoch": 1.3180031829549514,
      "grad_norm": 1.6720657348632812,
      "learning_rate": 3.917027507770811e-05,
      "loss": 0.1474,
      "step": 202900
    },
    {
      "epoch": 1.3186527655981033,
      "grad_norm": 0.593555748462677,
      "learning_rate": 3.916336461928212e-05,
      "loss": 0.1466,
      "step": 203000
    },
    {
      "epoch": 1.319302348241255,
      "grad_norm": 1.3155872821807861,
      "learning_rate": 3.915645416085613e-05,
      "loss": 0.1458,
      "step": 203100
    },
    {
      "epoch": 1.3199519308844068,
      "grad_norm": 1.2698804140090942,
      "learning_rate": 3.914954370243014e-05,
      "loss": 0.1477,
      "step": 203200
    },
    {
      "epoch": 1.3206015135275586,
      "grad_norm": 1.1066596508026123,
      "learning_rate": 3.914263324400414e-05,
      "loss": 0.156,
      "step": 203300
    },
    {
      "epoch": 1.3212510961707102,
      "grad_norm": 1.0720994472503662,
      "learning_rate": 3.913572278557815e-05,
      "loss": 0.1444,
      "step": 203400
    },
    {
      "epoch": 1.321900678813862,
      "grad_norm": 1.2453495264053345,
      "learning_rate": 3.912881232715216e-05,
      "loss": 0.1449,
      "step": 203500
    },
    {
      "epoch": 1.322550261457014,
      "grad_norm": 1.6299262046813965,
      "learning_rate": 3.912190186872617e-05,
      "loss": 0.137,
      "step": 203600
    },
    {
      "epoch": 1.3231998441001656,
      "grad_norm": 1.4414989948272705,
      "learning_rate": 3.911499141030018e-05,
      "loss": 0.144,
      "step": 203700
    },
    {
      "epoch": 1.3238494267433174,
      "grad_norm": 1.1002414226531982,
      "learning_rate": 3.910808095187419e-05,
      "loss": 0.1425,
      "step": 203800
    },
    {
      "epoch": 1.3244990093864693,
      "grad_norm": 1.187897801399231,
      "learning_rate": 3.91011704934482e-05,
      "loss": 0.1448,
      "step": 203900
    },
    {
      "epoch": 1.3251485920296209,
      "grad_norm": 1.3522881269454956,
      "learning_rate": 3.9094260035022203e-05,
      "loss": 0.1479,
      "step": 204000
    },
    {
      "epoch": 1.3257981746727727,
      "grad_norm": 1.7851290702819824,
      "learning_rate": 3.908734957659621e-05,
      "loss": 0.1384,
      "step": 204100
    },
    {
      "epoch": 1.3264477573159246,
      "grad_norm": 0.9800494313240051,
      "learning_rate": 3.908043911817022e-05,
      "loss": 0.1509,
      "step": 204200
    },
    {
      "epoch": 1.3270973399590762,
      "grad_norm": 1.411415457725525,
      "learning_rate": 3.9073528659744226e-05,
      "loss": 0.1395,
      "step": 204300
    },
    {
      "epoch": 1.327746922602228,
      "grad_norm": 0.9498138427734375,
      "learning_rate": 3.9066618201318236e-05,
      "loss": 0.1425,
      "step": 204400
    },
    {
      "epoch": 1.3283965052453799,
      "grad_norm": 1.1965197324752808,
      "learning_rate": 3.9059707742892246e-05,
      "loss": 0.1453,
      "step": 204500
    },
    {
      "epoch": 1.3290460878885315,
      "grad_norm": 1.2593250274658203,
      "learning_rate": 3.9052797284466256e-05,
      "loss": 0.1405,
      "step": 204600
    },
    {
      "epoch": 1.3296956705316834,
      "grad_norm": 0.9413178563117981,
      "learning_rate": 3.9045886826040266e-05,
      "loss": 0.1412,
      "step": 204700
    },
    {
      "epoch": 1.3303452531748352,
      "grad_norm": 1.8094488382339478,
      "learning_rate": 3.9038976367614276e-05,
      "loss": 0.1438,
      "step": 204800
    },
    {
      "epoch": 1.3309948358179868,
      "grad_norm": 1.0065102577209473,
      "learning_rate": 3.9032065909188285e-05,
      "loss": 0.1513,
      "step": 204900
    },
    {
      "epoch": 1.3316444184611387,
      "grad_norm": 1.6237497329711914,
      "learning_rate": 3.9025155450762295e-05,
      "loss": 0.1439,
      "step": 205000
    },
    {
      "epoch": 1.3322940011042905,
      "grad_norm": 1.287240982055664,
      "learning_rate": 3.9018244992336305e-05,
      "loss": 0.1431,
      "step": 205100
    },
    {
      "epoch": 1.3329435837474422,
      "grad_norm": 1.3190529346466064,
      "learning_rate": 3.901133453391031e-05,
      "loss": 0.1486,
      "step": 205200
    },
    {
      "epoch": 1.333593166390594,
      "grad_norm": 1.3498843908309937,
      "learning_rate": 3.900442407548432e-05,
      "loss": 0.1445,
      "step": 205300
    },
    {
      "epoch": 1.3342427490337458,
      "grad_norm": 1.8770743608474731,
      "learning_rate": 3.899751361705833e-05,
      "loss": 0.1463,
      "step": 205400
    },
    {
      "epoch": 1.3348923316768975,
      "grad_norm": 1.6747602224349976,
      "learning_rate": 3.899060315863234e-05,
      "loss": 0.1433,
      "step": 205500
    },
    {
      "epoch": 1.3355419143200493,
      "grad_norm": 1.259382963180542,
      "learning_rate": 3.898369270020635e-05,
      "loss": 0.1421,
      "step": 205600
    },
    {
      "epoch": 1.3361914969632012,
      "grad_norm": 0.7375331521034241,
      "learning_rate": 3.897678224178036e-05,
      "loss": 0.1426,
      "step": 205700
    },
    {
      "epoch": 1.3368410796063528,
      "grad_norm": 1.6569952964782715,
      "learning_rate": 3.896987178335437e-05,
      "loss": 0.14,
      "step": 205800
    },
    {
      "epoch": 1.3374906622495046,
      "grad_norm": 1.5096018314361572,
      "learning_rate": 3.896296132492838e-05,
      "loss": 0.1492,
      "step": 205900
    },
    {
      "epoch": 1.3381402448926565,
      "grad_norm": 1.7433078289031982,
      "learning_rate": 3.895605086650239e-05,
      "loss": 0.1444,
      "step": 206000
    },
    {
      "epoch": 1.338789827535808,
      "grad_norm": 1.3754830360412598,
      "learning_rate": 3.89491404080764e-05,
      "loss": 0.1504,
      "step": 206100
    },
    {
      "epoch": 1.33943941017896,
      "grad_norm": 1.1026989221572876,
      "learning_rate": 3.89422299496504e-05,
      "loss": 0.1523,
      "step": 206200
    },
    {
      "epoch": 1.3400889928221118,
      "grad_norm": 1.132856011390686,
      "learning_rate": 3.893531949122441e-05,
      "loss": 0.145,
      "step": 206300
    },
    {
      "epoch": 1.3407385754652636,
      "grad_norm": 1.4986188411712646,
      "learning_rate": 3.892840903279842e-05,
      "loss": 0.1398,
      "step": 206400
    },
    {
      "epoch": 1.3413881581084153,
      "grad_norm": 1.2111374139785767,
      "learning_rate": 3.892149857437243e-05,
      "loss": 0.1477,
      "step": 206500
    },
    {
      "epoch": 1.3420377407515671,
      "grad_norm": 0.9079937934875488,
      "learning_rate": 3.891458811594644e-05,
      "loss": 0.1429,
      "step": 206600
    },
    {
      "epoch": 1.342687323394719,
      "grad_norm": 1.2609703540802002,
      "learning_rate": 3.890767765752045e-05,
      "loss": 0.1476,
      "step": 206700
    },
    {
      "epoch": 1.3433369060378706,
      "grad_norm": 1.4666420221328735,
      "learning_rate": 3.890076719909446e-05,
      "loss": 0.1407,
      "step": 206800
    },
    {
      "epoch": 1.3439864886810224,
      "grad_norm": 1.1384285688400269,
      "learning_rate": 3.889385674066847e-05,
      "loss": 0.1438,
      "step": 206900
    },
    {
      "epoch": 1.3446360713241743,
      "grad_norm": 1.8004711866378784,
      "learning_rate": 3.888694628224248e-05,
      "loss": 0.1444,
      "step": 207000
    },
    {
      "epoch": 1.345285653967326,
      "grad_norm": 1.4905673265457153,
      "learning_rate": 3.888003582381648e-05,
      "loss": 0.1412,
      "step": 207100
    },
    {
      "epoch": 1.3459352366104778,
      "grad_norm": 2.3260397911071777,
      "learning_rate": 3.887312536539049e-05,
      "loss": 0.1507,
      "step": 207200
    },
    {
      "epoch": 1.3465848192536296,
      "grad_norm": 1.0797358751296997,
      "learning_rate": 3.88662149069645e-05,
      "loss": 0.147,
      "step": 207300
    },
    {
      "epoch": 1.3472344018967815,
      "grad_norm": 1.0563833713531494,
      "learning_rate": 3.885930444853851e-05,
      "loss": 0.1432,
      "step": 207400
    },
    {
      "epoch": 1.347883984539933,
      "grad_norm": 1.1490341424942017,
      "learning_rate": 3.8852393990112514e-05,
      "loss": 0.1456,
      "step": 207500
    },
    {
      "epoch": 1.348533567183085,
      "grad_norm": 1.4303977489471436,
      "learning_rate": 3.8845483531686524e-05,
      "loss": 0.145,
      "step": 207600
    },
    {
      "epoch": 1.3491831498262368,
      "grad_norm": 1.4124573469161987,
      "learning_rate": 3.8838573073260534e-05,
      "loss": 0.1459,
      "step": 207700
    },
    {
      "epoch": 1.3498327324693884,
      "grad_norm": 1.4890530109405518,
      "learning_rate": 3.8831662614834544e-05,
      "loss": 0.1427,
      "step": 207800
    },
    {
      "epoch": 1.3504823151125402,
      "grad_norm": 1.1553008556365967,
      "learning_rate": 3.8824752156408554e-05,
      "loss": 0.1331,
      "step": 207900
    },
    {
      "epoch": 1.351131897755692,
      "grad_norm": 1.250490665435791,
      "learning_rate": 3.881784169798256e-05,
      "loss": 0.1336,
      "step": 208000
    },
    {
      "epoch": 1.3517814803988437,
      "grad_norm": 1.1280046701431274,
      "learning_rate": 3.881093123955657e-05,
      "loss": 0.1471,
      "step": 208100
    },
    {
      "epoch": 1.3524310630419956,
      "grad_norm": 1.5996136665344238,
      "learning_rate": 3.8804020781130577e-05,
      "loss": 0.1452,
      "step": 208200
    },
    {
      "epoch": 1.3530806456851474,
      "grad_norm": 0.9840705990791321,
      "learning_rate": 3.8797110322704586e-05,
      "loss": 0.144,
      "step": 208300
    },
    {
      "epoch": 1.353730228328299,
      "grad_norm": 1.1466224193572998,
      "learning_rate": 3.8790199864278596e-05,
      "loss": 0.1459,
      "step": 208400
    },
    {
      "epoch": 1.3543798109714509,
      "grad_norm": 1.505479335784912,
      "learning_rate": 3.8783289405852606e-05,
      "loss": 0.1449,
      "step": 208500
    },
    {
      "epoch": 1.3550293936146027,
      "grad_norm": 1.321629285812378,
      "learning_rate": 3.8776378947426616e-05,
      "loss": 0.1344,
      "step": 208600
    },
    {
      "epoch": 1.3556789762577544,
      "grad_norm": 1.3347301483154297,
      "learning_rate": 3.8769468489000626e-05,
      "loss": 0.1494,
      "step": 208700
    },
    {
      "epoch": 1.3563285589009062,
      "grad_norm": 1.8938400745391846,
      "learning_rate": 3.8762558030574636e-05,
      "loss": 0.1402,
      "step": 208800
    },
    {
      "epoch": 1.356978141544058,
      "grad_norm": 0.9654029011726379,
      "learning_rate": 3.8755647572148646e-05,
      "loss": 0.1479,
      "step": 208900
    },
    {
      "epoch": 1.3576277241872097,
      "grad_norm": 1.0581895112991333,
      "learning_rate": 3.874873711372265e-05,
      "loss": 0.1466,
      "step": 209000
    },
    {
      "epoch": 1.3582773068303615,
      "grad_norm": 1.291479468345642,
      "learning_rate": 3.874182665529666e-05,
      "loss": 0.1364,
      "step": 209100
    },
    {
      "epoch": 1.3589268894735134,
      "grad_norm": 1.3710271120071411,
      "learning_rate": 3.873491619687067e-05,
      "loss": 0.1435,
      "step": 209200
    },
    {
      "epoch": 1.359576472116665,
      "grad_norm": 1.8966678380966187,
      "learning_rate": 3.872800573844468e-05,
      "loss": 0.1499,
      "step": 209300
    },
    {
      "epoch": 1.3602260547598168,
      "grad_norm": 1.5441502332687378,
      "learning_rate": 3.872109528001869e-05,
      "loss": 0.1382,
      "step": 209400
    },
    {
      "epoch": 1.3608756374029687,
      "grad_norm": 1.0440587997436523,
      "learning_rate": 3.87141848215927e-05,
      "loss": 0.1394,
      "step": 209500
    },
    {
      "epoch": 1.3615252200461203,
      "grad_norm": 1.363538384437561,
      "learning_rate": 3.870727436316671e-05,
      "loss": 0.1405,
      "step": 209600
    },
    {
      "epoch": 1.3621748026892722,
      "grad_norm": 1.29616117477417,
      "learning_rate": 3.870036390474072e-05,
      "loss": 0.1354,
      "step": 209700
    },
    {
      "epoch": 1.362824385332424,
      "grad_norm": 0.9838395118713379,
      "learning_rate": 3.869345344631473e-05,
      "loss": 0.1374,
      "step": 209800
    },
    {
      "epoch": 1.3634739679755756,
      "grad_norm": 1.232768177986145,
      "learning_rate": 3.868654298788873e-05,
      "loss": 0.1427,
      "step": 209900
    },
    {
      "epoch": 1.3641235506187275,
      "grad_norm": 1.6046535968780518,
      "learning_rate": 3.867963252946274e-05,
      "loss": 0.1436,
      "step": 210000
    },
    {
      "epoch": 1.3647731332618793,
      "grad_norm": 1.1290472745895386,
      "learning_rate": 3.867272207103675e-05,
      "loss": 0.1437,
      "step": 210100
    },
    {
      "epoch": 1.365422715905031,
      "grad_norm": 1.4541901350021362,
      "learning_rate": 3.866581161261076e-05,
      "loss": 0.1409,
      "step": 210200
    },
    {
      "epoch": 1.3660722985481828,
      "grad_norm": 1.4558223485946655,
      "learning_rate": 3.865890115418477e-05,
      "loss": 0.1407,
      "step": 210300
    },
    {
      "epoch": 1.3667218811913346,
      "grad_norm": 1.4423693418502808,
      "learning_rate": 3.865199069575878e-05,
      "loss": 0.1432,
      "step": 210400
    },
    {
      "epoch": 1.3673714638344863,
      "grad_norm": 1.1814312934875488,
      "learning_rate": 3.864508023733279e-05,
      "loss": 0.1489,
      "step": 210500
    },
    {
      "epoch": 1.3680210464776381,
      "grad_norm": 0.8492742776870728,
      "learning_rate": 3.86381697789068e-05,
      "loss": 0.1417,
      "step": 210600
    },
    {
      "epoch": 1.36867062912079,
      "grad_norm": 1.2738428115844727,
      "learning_rate": 3.863125932048081e-05,
      "loss": 0.1495,
      "step": 210700
    },
    {
      "epoch": 1.3693202117639416,
      "grad_norm": 1.8737950325012207,
      "learning_rate": 3.862434886205481e-05,
      "loss": 0.1486,
      "step": 210800
    },
    {
      "epoch": 1.3699697944070934,
      "grad_norm": 1.2264124155044556,
      "learning_rate": 3.861743840362882e-05,
      "loss": 0.1387,
      "step": 210900
    },
    {
      "epoch": 1.3706193770502453,
      "grad_norm": 1.4425462484359741,
      "learning_rate": 3.861052794520283e-05,
      "loss": 0.1477,
      "step": 211000
    },
    {
      "epoch": 1.371268959693397,
      "grad_norm": 1.1907671689987183,
      "learning_rate": 3.8603617486776835e-05,
      "loss": 0.142,
      "step": 211100
    },
    {
      "epoch": 1.3719185423365488,
      "grad_norm": 1.3138808012008667,
      "learning_rate": 3.8596707028350845e-05,
      "loss": 0.1454,
      "step": 211200
    },
    {
      "epoch": 1.3725681249797006,
      "grad_norm": 1.7164034843444824,
      "learning_rate": 3.8589796569924855e-05,
      "loss": 0.1465,
      "step": 211300
    },
    {
      "epoch": 1.3732177076228522,
      "grad_norm": 1.27226984500885,
      "learning_rate": 3.8582886111498865e-05,
      "loss": 0.1438,
      "step": 211400
    },
    {
      "epoch": 1.373867290266004,
      "grad_norm": 1.0966668128967285,
      "learning_rate": 3.8575975653072875e-05,
      "loss": 0.1429,
      "step": 211500
    },
    {
      "epoch": 1.374516872909156,
      "grad_norm": 1.1962233781814575,
      "learning_rate": 3.8569065194646884e-05,
      "loss": 0.1368,
      "step": 211600
    },
    {
      "epoch": 1.3751664555523075,
      "grad_norm": 2.014193058013916,
      "learning_rate": 3.8562154736220894e-05,
      "loss": 0.1384,
      "step": 211700
    },
    {
      "epoch": 1.3758160381954594,
      "grad_norm": 1.1002471446990967,
      "learning_rate": 3.85552442777949e-05,
      "loss": 0.154,
      "step": 211800
    },
    {
      "epoch": 1.3764656208386112,
      "grad_norm": 1.3024492263793945,
      "learning_rate": 3.854833381936891e-05,
      "loss": 0.1449,
      "step": 211900
    },
    {
      "epoch": 1.3771152034817629,
      "grad_norm": 1.4856231212615967,
      "learning_rate": 3.854142336094292e-05,
      "loss": 0.141,
      "step": 212000
    },
    {
      "epoch": 1.3777647861249147,
      "grad_norm": 1.1605042219161987,
      "learning_rate": 3.853451290251693e-05,
      "loss": 0.1408,
      "step": 212100
    },
    {
      "epoch": 1.3784143687680666,
      "grad_norm": 1.1718039512634277,
      "learning_rate": 3.852760244409094e-05,
      "loss": 0.1427,
      "step": 212200
    },
    {
      "epoch": 1.3790639514112182,
      "grad_norm": 1.8526452779769897,
      "learning_rate": 3.852069198566495e-05,
      "loss": 0.1515,
      "step": 212300
    },
    {
      "epoch": 1.37971353405437,
      "grad_norm": 1.0521266460418701,
      "learning_rate": 3.8513781527238957e-05,
      "loss": 0.1406,
      "step": 212400
    },
    {
      "epoch": 1.3803631166975219,
      "grad_norm": 1.1960992813110352,
      "learning_rate": 3.8506871068812966e-05,
      "loss": 0.1394,
      "step": 212500
    },
    {
      "epoch": 1.3810126993406735,
      "grad_norm": 1.5361690521240234,
      "learning_rate": 3.8499960610386976e-05,
      "loss": 0.1351,
      "step": 212600
    },
    {
      "epoch": 1.3816622819838253,
      "grad_norm": 1.5339192152023315,
      "learning_rate": 3.8493050151960986e-05,
      "loss": 0.1491,
      "step": 212700
    },
    {
      "epoch": 1.3823118646269772,
      "grad_norm": 1.410110354423523,
      "learning_rate": 3.848613969353499e-05,
      "loss": 0.144,
      "step": 212800
    },
    {
      "epoch": 1.3829614472701288,
      "grad_norm": 1.5148506164550781,
      "learning_rate": 3.8479229235109e-05,
      "loss": 0.1445,
      "step": 212900
    },
    {
      "epoch": 1.3836110299132807,
      "grad_norm": 1.0156190395355225,
      "learning_rate": 3.847231877668301e-05,
      "loss": 0.1398,
      "step": 213000
    },
    {
      "epoch": 1.3842606125564325,
      "grad_norm": 1.6348470449447632,
      "learning_rate": 3.846540831825702e-05,
      "loss": 0.1485,
      "step": 213100
    },
    {
      "epoch": 1.3849101951995841,
      "grad_norm": 1.2207461595535278,
      "learning_rate": 3.845849785983103e-05,
      "loss": 0.1416,
      "step": 213200
    },
    {
      "epoch": 1.385559777842736,
      "grad_norm": 1.0998164415359497,
      "learning_rate": 3.845158740140504e-05,
      "loss": 0.1407,
      "step": 213300
    },
    {
      "epoch": 1.3862093604858878,
      "grad_norm": 1.4122018814086914,
      "learning_rate": 3.844467694297905e-05,
      "loss": 0.1363,
      "step": 213400
    },
    {
      "epoch": 1.3868589431290395,
      "grad_norm": 1.3108752965927124,
      "learning_rate": 3.843776648455306e-05,
      "loss": 0.1398,
      "step": 213500
    },
    {
      "epoch": 1.3875085257721913,
      "grad_norm": 1.3546196222305298,
      "learning_rate": 3.843085602612707e-05,
      "loss": 0.1349,
      "step": 213600
    },
    {
      "epoch": 1.3881581084153432,
      "grad_norm": 1.2255887985229492,
      "learning_rate": 3.842394556770107e-05,
      "loss": 0.134,
      "step": 213700
    },
    {
      "epoch": 1.388807691058495,
      "grad_norm": 1.225986123085022,
      "learning_rate": 3.841703510927508e-05,
      "loss": 0.141,
      "step": 213800
    },
    {
      "epoch": 1.3894572737016466,
      "grad_norm": 1.1707874536514282,
      "learning_rate": 3.841012465084909e-05,
      "loss": 0.1482,
      "step": 213900
    },
    {
      "epoch": 1.3901068563447985,
      "grad_norm": 1.1988487243652344,
      "learning_rate": 3.84032141924231e-05,
      "loss": 0.143,
      "step": 214000
    },
    {
      "epoch": 1.3907564389879503,
      "grad_norm": 1.3927464485168457,
      "learning_rate": 3.839630373399711e-05,
      "loss": 0.1429,
      "step": 214100
    },
    {
      "epoch": 1.391406021631102,
      "grad_norm": 1.8642643690109253,
      "learning_rate": 3.838939327557112e-05,
      "loss": 0.1418,
      "step": 214200
    },
    {
      "epoch": 1.3920556042742538,
      "grad_norm": 1.147970199584961,
      "learning_rate": 3.838248281714512e-05,
      "loss": 0.1431,
      "step": 214300
    },
    {
      "epoch": 1.3927051869174056,
      "grad_norm": 1.3834744691848755,
      "learning_rate": 3.837557235871913e-05,
      "loss": 0.136,
      "step": 214400
    },
    {
      "epoch": 1.3933547695605573,
      "grad_norm": 1.6063803434371948,
      "learning_rate": 3.836866190029314e-05,
      "loss": 0.1467,
      "step": 214500
    },
    {
      "epoch": 1.394004352203709,
      "grad_norm": 1.540042519569397,
      "learning_rate": 3.836175144186715e-05,
      "loss": 0.1323,
      "step": 214600
    },
    {
      "epoch": 1.394653934846861,
      "grad_norm": 1.155821681022644,
      "learning_rate": 3.8354840983441156e-05,
      "loss": 0.1377,
      "step": 214700
    },
    {
      "epoch": 1.3953035174900128,
      "grad_norm": 1.0770926475524902,
      "learning_rate": 3.8347930525015166e-05,
      "loss": 0.1493,
      "step": 214800
    },
    {
      "epoch": 1.3959531001331644,
      "grad_norm": 1.5647509098052979,
      "learning_rate": 3.8341020066589176e-05,
      "loss": 0.1423,
      "step": 214900
    },
    {
      "epoch": 1.3966026827763163,
      "grad_norm": 1.5167940855026245,
      "learning_rate": 3.8334109608163186e-05,
      "loss": 0.1433,
      "step": 215000
    },
    {
      "epoch": 1.3972522654194681,
      "grad_norm": 1.4984967708587646,
      "learning_rate": 3.8327199149737195e-05,
      "loss": 0.1532,
      "step": 215100
    },
    {
      "epoch": 1.3979018480626197,
      "grad_norm": 1.1992132663726807,
      "learning_rate": 3.8320288691311205e-05,
      "loss": 0.1394,
      "step": 215200
    },
    {
      "epoch": 1.3985514307057716,
      "grad_norm": 0.8645550608634949,
      "learning_rate": 3.8313378232885215e-05,
      "loss": 0.1381,
      "step": 215300
    },
    {
      "epoch": 1.3992010133489234,
      "grad_norm": 1.4543465375900269,
      "learning_rate": 3.8306467774459225e-05,
      "loss": 0.1384,
      "step": 215400
    },
    {
      "epoch": 1.399850595992075,
      "grad_norm": 0.9280794262886047,
      "learning_rate": 3.8299557316033235e-05,
      "loss": 0.1456,
      "step": 215500
    },
    {
      "epoch": 1.400500178635227,
      "grad_norm": 1.579627513885498,
      "learning_rate": 3.829264685760724e-05,
      "loss": 0.1403,
      "step": 215600
    },
    {
      "epoch": 1.4011497612783788,
      "grad_norm": 0.8800545930862427,
      "learning_rate": 3.828573639918125e-05,
      "loss": 0.1377,
      "step": 215700
    },
    {
      "epoch": 1.4017993439215304,
      "grad_norm": 1.6177740097045898,
      "learning_rate": 3.827882594075526e-05,
      "loss": 0.144,
      "step": 215800
    },
    {
      "epoch": 1.4024489265646822,
      "grad_norm": 1.6240475177764893,
      "learning_rate": 3.827191548232927e-05,
      "loss": 0.1439,
      "step": 215900
    },
    {
      "epoch": 1.403098509207834,
      "grad_norm": 1.8097378015518188,
      "learning_rate": 3.826500502390328e-05,
      "loss": 0.1482,
      "step": 216000
    },
    {
      "epoch": 1.4037480918509857,
      "grad_norm": 1.4535497426986694,
      "learning_rate": 3.825809456547729e-05,
      "loss": 0.14,
      "step": 216100
    },
    {
      "epoch": 1.4043976744941375,
      "grad_norm": 1.0069271326065063,
      "learning_rate": 3.82511841070513e-05,
      "loss": 0.1449,
      "step": 216200
    },
    {
      "epoch": 1.4050472571372894,
      "grad_norm": 1.1635313034057617,
      "learning_rate": 3.824427364862531e-05,
      "loss": 0.1347,
      "step": 216300
    },
    {
      "epoch": 1.405696839780441,
      "grad_norm": 1.4665510654449463,
      "learning_rate": 3.823736319019932e-05,
      "loss": 0.1422,
      "step": 216400
    },
    {
      "epoch": 1.4063464224235929,
      "grad_norm": 1.428128719329834,
      "learning_rate": 3.823045273177332e-05,
      "loss": 0.1483,
      "step": 216500
    },
    {
      "epoch": 1.4069960050667447,
      "grad_norm": 1.340622067451477,
      "learning_rate": 3.822354227334733e-05,
      "loss": 0.1379,
      "step": 216600
    },
    {
      "epoch": 1.4076455877098963,
      "grad_norm": 0.9891457557678223,
      "learning_rate": 3.821663181492134e-05,
      "loss": 0.142,
      "step": 216700
    },
    {
      "epoch": 1.4082951703530482,
      "grad_norm": 0.997592568397522,
      "learning_rate": 3.820972135649535e-05,
      "loss": 0.1391,
      "step": 216800
    },
    {
      "epoch": 1.4089447529962,
      "grad_norm": 1.5625813007354736,
      "learning_rate": 3.820281089806936e-05,
      "loss": 0.1462,
      "step": 216900
    },
    {
      "epoch": 1.4095943356393517,
      "grad_norm": 2.0147414207458496,
      "learning_rate": 3.819590043964337e-05,
      "loss": 0.1465,
      "step": 217000
    },
    {
      "epoch": 1.4102439182825035,
      "grad_norm": 1.3884724378585815,
      "learning_rate": 3.818898998121738e-05,
      "loss": 0.146,
      "step": 217100
    },
    {
      "epoch": 1.4108935009256554,
      "grad_norm": 1.4990322589874268,
      "learning_rate": 3.818207952279139e-05,
      "loss": 0.1408,
      "step": 217200
    },
    {
      "epoch": 1.411543083568807,
      "grad_norm": 1.1514085531234741,
      "learning_rate": 3.81751690643654e-05,
      "loss": 0.1392,
      "step": 217300
    },
    {
      "epoch": 1.4121926662119588,
      "grad_norm": 1.2813994884490967,
      "learning_rate": 3.816825860593941e-05,
      "loss": 0.1395,
      "step": 217400
    },
    {
      "epoch": 1.4128422488551107,
      "grad_norm": 1.6228997707366943,
      "learning_rate": 3.816134814751341e-05,
      "loss": 0.1324,
      "step": 217500
    },
    {
      "epoch": 1.4134918314982623,
      "grad_norm": 1.516884684562683,
      "learning_rate": 3.815443768908742e-05,
      "loss": 0.137,
      "step": 217600
    },
    {
      "epoch": 1.4141414141414141,
      "grad_norm": 1.107694149017334,
      "learning_rate": 3.814752723066143e-05,
      "loss": 0.1404,
      "step": 217700
    },
    {
      "epoch": 1.414790996784566,
      "grad_norm": 1.4506839513778687,
      "learning_rate": 3.814061677223544e-05,
      "loss": 0.144,
      "step": 217800
    },
    {
      "epoch": 1.4154405794277176,
      "grad_norm": 1.7842463254928589,
      "learning_rate": 3.8133706313809444e-05,
      "loss": 0.1347,
      "step": 217900
    },
    {
      "epoch": 1.4160901620708695,
      "grad_norm": 1.0019885301589966,
      "learning_rate": 3.8126795855383454e-05,
      "loss": 0.1458,
      "step": 218000
    },
    {
      "epoch": 1.4167397447140213,
      "grad_norm": 1.1722044944763184,
      "learning_rate": 3.8119885396957464e-05,
      "loss": 0.1346,
      "step": 218100
    },
    {
      "epoch": 1.417389327357173,
      "grad_norm": 0.9749017357826233,
      "learning_rate": 3.8112974938531474e-05,
      "loss": 0.1463,
      "step": 218200
    },
    {
      "epoch": 1.4180389100003248,
      "grad_norm": 1.5908397436141968,
      "learning_rate": 3.8106064480105484e-05,
      "loss": 0.1387,
      "step": 218300
    },
    {
      "epoch": 1.4186884926434766,
      "grad_norm": 1.487542748451233,
      "learning_rate": 3.809915402167949e-05,
      "loss": 0.1373,
      "step": 218400
    },
    {
      "epoch": 1.4193380752866283,
      "grad_norm": 1.0790568590164185,
      "learning_rate": 3.8092243563253496e-05,
      "loss": 0.1347,
      "step": 218500
    },
    {
      "epoch": 1.41998765792978,
      "grad_norm": 1.203325867652893,
      "learning_rate": 3.8085333104827506e-05,
      "loss": 0.1418,
      "step": 218600
    },
    {
      "epoch": 1.420637240572932,
      "grad_norm": 0.7836726903915405,
      "learning_rate": 3.8078422646401516e-05,
      "loss": 0.1338,
      "step": 218700
    },
    {
      "epoch": 1.4212868232160836,
      "grad_norm": 0.9510213136672974,
      "learning_rate": 3.8071512187975526e-05,
      "loss": 0.1396,
      "step": 218800
    },
    {
      "epoch": 1.4219364058592354,
      "grad_norm": 1.2178384065628052,
      "learning_rate": 3.8064601729549536e-05,
      "loss": 0.1436,
      "step": 218900
    },
    {
      "epoch": 1.4225859885023873,
      "grad_norm": 1.0330852270126343,
      "learning_rate": 3.8057691271123546e-05,
      "loss": 0.1327,
      "step": 219000
    },
    {
      "epoch": 1.423235571145539,
      "grad_norm": 1.2039724588394165,
      "learning_rate": 3.8050780812697556e-05,
      "loss": 0.1347,
      "step": 219100
    },
    {
      "epoch": 1.4238851537886907,
      "grad_norm": 1.7288818359375,
      "learning_rate": 3.8043870354271565e-05,
      "loss": 0.1366,
      "step": 219200
    },
    {
      "epoch": 1.4245347364318426,
      "grad_norm": 1.1886801719665527,
      "learning_rate": 3.803695989584557e-05,
      "loss": 0.1432,
      "step": 219300
    },
    {
      "epoch": 1.4251843190749942,
      "grad_norm": 1.1052039861679077,
      "learning_rate": 3.803004943741958e-05,
      "loss": 0.1351,
      "step": 219400
    },
    {
      "epoch": 1.425833901718146,
      "grad_norm": 1.3447322845458984,
      "learning_rate": 3.802313897899359e-05,
      "loss": 0.1404,
      "step": 219500
    },
    {
      "epoch": 1.426483484361298,
      "grad_norm": 1.1511235237121582,
      "learning_rate": 3.80162285205676e-05,
      "loss": 0.1382,
      "step": 219600
    },
    {
      "epoch": 1.4271330670044495,
      "grad_norm": 1.204728126525879,
      "learning_rate": 3.800931806214161e-05,
      "loss": 0.142,
      "step": 219700
    },
    {
      "epoch": 1.4277826496476014,
      "grad_norm": 1.441595196723938,
      "learning_rate": 3.800240760371562e-05,
      "loss": 0.1355,
      "step": 219800
    },
    {
      "epoch": 1.4284322322907532,
      "grad_norm": 1.6688307523727417,
      "learning_rate": 3.799549714528963e-05,
      "loss": 0.1456,
      "step": 219900
    },
    {
      "epoch": 1.4290818149339048,
      "grad_norm": 1.5970516204833984,
      "learning_rate": 3.798858668686364e-05,
      "loss": 0.1435,
      "step": 220000
    },
    {
      "epoch": 1.4297313975770567,
      "grad_norm": 2.0912094116210938,
      "learning_rate": 3.798167622843765e-05,
      "loss": 0.1432,
      "step": 220100
    },
    {
      "epoch": 1.4303809802202085,
      "grad_norm": 1.2302846908569336,
      "learning_rate": 3.797476577001166e-05,
      "loss": 0.1369,
      "step": 220200
    },
    {
      "epoch": 1.4310305628633602,
      "grad_norm": 1.0788071155548096,
      "learning_rate": 3.796785531158566e-05,
      "loss": 0.1411,
      "step": 220300
    },
    {
      "epoch": 1.431680145506512,
      "grad_norm": 1.6081738471984863,
      "learning_rate": 3.796094485315967e-05,
      "loss": 0.1349,
      "step": 220400
    },
    {
      "epoch": 1.4323297281496639,
      "grad_norm": 0.9766061305999756,
      "learning_rate": 3.795403439473368e-05,
      "loss": 0.1393,
      "step": 220500
    },
    {
      "epoch": 1.4329793107928155,
      "grad_norm": 1.3926749229431152,
      "learning_rate": 3.794712393630769e-05,
      "loss": 0.1421,
      "step": 220600
    },
    {
      "epoch": 1.4336288934359673,
      "grad_norm": 1.449205756187439,
      "learning_rate": 3.79402134778817e-05,
      "loss": 0.1379,
      "step": 220700
    },
    {
      "epoch": 1.4342784760791192,
      "grad_norm": 1.6294007301330566,
      "learning_rate": 3.793330301945571e-05,
      "loss": 0.1439,
      "step": 220800
    },
    {
      "epoch": 1.4349280587222708,
      "grad_norm": 1.2232820987701416,
      "learning_rate": 3.792639256102972e-05,
      "loss": 0.136,
      "step": 220900
    },
    {
      "epoch": 1.4355776413654227,
      "grad_norm": 1.2417023181915283,
      "learning_rate": 3.791948210260373e-05,
      "loss": 0.1377,
      "step": 221000
    },
    {
      "epoch": 1.4362272240085745,
      "grad_norm": 1.1996408700942993,
      "learning_rate": 3.791257164417773e-05,
      "loss": 0.1362,
      "step": 221100
    },
    {
      "epoch": 1.4368768066517263,
      "grad_norm": 1.9903897047042847,
      "learning_rate": 3.790566118575174e-05,
      "loss": 0.1402,
      "step": 221200
    },
    {
      "epoch": 1.437526389294878,
      "grad_norm": 0.8463209271430969,
      "learning_rate": 3.789875072732575e-05,
      "loss": 0.1354,
      "step": 221300
    },
    {
      "epoch": 1.4381759719380298,
      "grad_norm": 1.6782467365264893,
      "learning_rate": 3.789184026889976e-05,
      "loss": 0.1407,
      "step": 221400
    },
    {
      "epoch": 1.4388255545811817,
      "grad_norm": 1.5039561986923218,
      "learning_rate": 3.7884929810473765e-05,
      "loss": 0.144,
      "step": 221500
    },
    {
      "epoch": 1.4394751372243333,
      "grad_norm": 1.306210994720459,
      "learning_rate": 3.7878019352047775e-05,
      "loss": 0.1462,
      "step": 221600
    },
    {
      "epoch": 1.4401247198674851,
      "grad_norm": 1.1221026182174683,
      "learning_rate": 3.7871108893621785e-05,
      "loss": 0.1457,
      "step": 221700
    },
    {
      "epoch": 1.440774302510637,
      "grad_norm": 1.7310693264007568,
      "learning_rate": 3.7864198435195795e-05,
      "loss": 0.1389,
      "step": 221800
    },
    {
      "epoch": 1.4414238851537886,
      "grad_norm": 1.174131155014038,
      "learning_rate": 3.7857287976769804e-05,
      "loss": 0.1391,
      "step": 221900
    },
    {
      "epoch": 1.4420734677969405,
      "grad_norm": 1.3099945783615112,
      "learning_rate": 3.7850377518343814e-05,
      "loss": 0.1373,
      "step": 222000
    },
    {
      "epoch": 1.4427230504400923,
      "grad_norm": 1.1834880113601685,
      "learning_rate": 3.7843467059917824e-05,
      "loss": 0.1415,
      "step": 222100
    },
    {
      "epoch": 1.4433726330832441,
      "grad_norm": 1.5141124725341797,
      "learning_rate": 3.783655660149183e-05,
      "loss": 0.1374,
      "step": 222200
    },
    {
      "epoch": 1.4440222157263958,
      "grad_norm": 0.8188226819038391,
      "learning_rate": 3.782964614306584e-05,
      "loss": 0.1419,
      "step": 222300
    },
    {
      "epoch": 1.4446717983695476,
      "grad_norm": 1.4313112497329712,
      "learning_rate": 3.782273568463985e-05,
      "loss": 0.1399,
      "step": 222400
    },
    {
      "epoch": 1.4453213810126995,
      "grad_norm": 1.2400760650634766,
      "learning_rate": 3.781582522621386e-05,
      "loss": 0.1358,
      "step": 222500
    },
    {
      "epoch": 1.445970963655851,
      "grad_norm": 1.0496704578399658,
      "learning_rate": 3.7808914767787867e-05,
      "loss": 0.1382,
      "step": 222600
    },
    {
      "epoch": 1.446620546299003,
      "grad_norm": 1.4555938243865967,
      "learning_rate": 3.7802004309361876e-05,
      "loss": 0.1412,
      "step": 222700
    },
    {
      "epoch": 1.4472701289421548,
      "grad_norm": 1.3275631666183472,
      "learning_rate": 3.7795093850935886e-05,
      "loss": 0.1421,
      "step": 222800
    },
    {
      "epoch": 1.4479197115853064,
      "grad_norm": 1.4447389841079712,
      "learning_rate": 3.7788183392509896e-05,
      "loss": 0.1386,
      "step": 222900
    },
    {
      "epoch": 1.4485692942284583,
      "grad_norm": 1.2774988412857056,
      "learning_rate": 3.7781272934083906e-05,
      "loss": 0.1327,
      "step": 223000
    },
    {
      "epoch": 1.44921887687161,
      "grad_norm": 1.369249701499939,
      "learning_rate": 3.777436247565791e-05,
      "loss": 0.1347,
      "step": 223100
    },
    {
      "epoch": 1.4498684595147617,
      "grad_norm": 1.2275536060333252,
      "learning_rate": 3.776745201723192e-05,
      "loss": 0.141,
      "step": 223200
    },
    {
      "epoch": 1.4505180421579136,
      "grad_norm": 1.076716423034668,
      "learning_rate": 3.776054155880593e-05,
      "loss": 0.1395,
      "step": 223300
    },
    {
      "epoch": 1.4511676248010654,
      "grad_norm": 1.2274274826049805,
      "learning_rate": 3.775363110037994e-05,
      "loss": 0.1393,
      "step": 223400
    },
    {
      "epoch": 1.451817207444217,
      "grad_norm": 1.0395973920822144,
      "learning_rate": 3.774672064195395e-05,
      "loss": 0.1413,
      "step": 223500
    },
    {
      "epoch": 1.452466790087369,
      "grad_norm": 1.4347796440124512,
      "learning_rate": 3.773981018352796e-05,
      "loss": 0.1422,
      "step": 223600
    },
    {
      "epoch": 1.4531163727305207,
      "grad_norm": 1.5124986171722412,
      "learning_rate": 3.773289972510197e-05,
      "loss": 0.1467,
      "step": 223700
    },
    {
      "epoch": 1.4537659553736724,
      "grad_norm": 0.8854040503501892,
      "learning_rate": 3.772598926667598e-05,
      "loss": 0.1418,
      "step": 223800
    },
    {
      "epoch": 1.4544155380168242,
      "grad_norm": 1.3023083209991455,
      "learning_rate": 3.771907880824999e-05,
      "loss": 0.139,
      "step": 223900
    },
    {
      "epoch": 1.455065120659976,
      "grad_norm": 1.6752759218215942,
      "learning_rate": 3.771216834982399e-05,
      "loss": 0.1386,
      "step": 224000
    },
    {
      "epoch": 1.4557147033031277,
      "grad_norm": 1.0376819372177124,
      "learning_rate": 3.7705257891398e-05,
      "loss": 0.1333,
      "step": 224100
    },
    {
      "epoch": 1.4563642859462795,
      "grad_norm": 1.3484022617340088,
      "learning_rate": 3.769834743297201e-05,
      "loss": 0.1424,
      "step": 224200
    },
    {
      "epoch": 1.4570138685894314,
      "grad_norm": 1.7535549402236938,
      "learning_rate": 3.769143697454602e-05,
      "loss": 0.143,
      "step": 224300
    },
    {
      "epoch": 1.457663451232583,
      "grad_norm": 1.191282033920288,
      "learning_rate": 3.768452651612003e-05,
      "loss": 0.1481,
      "step": 224400
    },
    {
      "epoch": 1.4583130338757349,
      "grad_norm": 1.155587077140808,
      "learning_rate": 3.767761605769404e-05,
      "loss": 0.1391,
      "step": 224500
    },
    {
      "epoch": 1.4589626165188867,
      "grad_norm": 1.1045007705688477,
      "learning_rate": 3.767070559926805e-05,
      "loss": 0.1407,
      "step": 224600
    },
    {
      "epoch": 1.4596121991620383,
      "grad_norm": 0.9545802474021912,
      "learning_rate": 3.766379514084205e-05,
      "loss": 0.14,
      "step": 224700
    },
    {
      "epoch": 1.4602617818051902,
      "grad_norm": 1.1802750825881958,
      "learning_rate": 3.765688468241606e-05,
      "loss": 0.1418,
      "step": 224800
    },
    {
      "epoch": 1.460911364448342,
      "grad_norm": 1.1199150085449219,
      "learning_rate": 3.764997422399007e-05,
      "loss": 0.1406,
      "step": 224900
    },
    {
      "epoch": 1.4615609470914936,
      "grad_norm": 1.2298516035079956,
      "learning_rate": 3.764306376556408e-05,
      "loss": 0.1349,
      "step": 225000
    },
    {
      "epoch": 1.4622105297346455,
      "grad_norm": 1.06425142288208,
      "learning_rate": 3.7636153307138086e-05,
      "loss": 0.1326,
      "step": 225100
    },
    {
      "epoch": 1.4628601123777973,
      "grad_norm": 1.2814944982528687,
      "learning_rate": 3.7629242848712096e-05,
      "loss": 0.1431,
      "step": 225200
    },
    {
      "epoch": 1.463509695020949,
      "grad_norm": 1.1878248453140259,
      "learning_rate": 3.7622332390286105e-05,
      "loss": 0.1337,
      "step": 225300
    },
    {
      "epoch": 1.4641592776641008,
      "grad_norm": 1.6209256649017334,
      "learning_rate": 3.7615421931860115e-05,
      "loss": 0.1312,
      "step": 225400
    },
    {
      "epoch": 1.4648088603072527,
      "grad_norm": 1.1146717071533203,
      "learning_rate": 3.7608511473434125e-05,
      "loss": 0.1389,
      "step": 225500
    },
    {
      "epoch": 1.4654584429504043,
      "grad_norm": 1.2647682428359985,
      "learning_rate": 3.7601601015008135e-05,
      "loss": 0.1398,
      "step": 225600
    },
    {
      "epoch": 1.4661080255935561,
      "grad_norm": 1.2196929454803467,
      "learning_rate": 3.7594690556582145e-05,
      "loss": 0.1375,
      "step": 225700
    },
    {
      "epoch": 1.466757608236708,
      "grad_norm": 1.9814045429229736,
      "learning_rate": 3.7587780098156155e-05,
      "loss": 0.132,
      "step": 225800
    },
    {
      "epoch": 1.4674071908798596,
      "grad_norm": 1.072112798690796,
      "learning_rate": 3.758086963973016e-05,
      "loss": 0.1427,
      "step": 225900
    },
    {
      "epoch": 1.4680567735230114,
      "grad_norm": 2.0921034812927246,
      "learning_rate": 3.757395918130417e-05,
      "loss": 0.139,
      "step": 226000
    },
    {
      "epoch": 1.4687063561661633,
      "grad_norm": 1.0562812089920044,
      "learning_rate": 3.756704872287818e-05,
      "loss": 0.1334,
      "step": 226100
    },
    {
      "epoch": 1.469355938809315,
      "grad_norm": 1.0375715494155884,
      "learning_rate": 3.756013826445219e-05,
      "loss": 0.1389,
      "step": 226200
    },
    {
      "epoch": 1.4700055214524668,
      "grad_norm": 1.0450255870819092,
      "learning_rate": 3.75532278060262e-05,
      "loss": 0.1334,
      "step": 226300
    },
    {
      "epoch": 1.4706551040956186,
      "grad_norm": 1.224002718925476,
      "learning_rate": 3.754631734760021e-05,
      "loss": 0.1353,
      "step": 226400
    },
    {
      "epoch": 1.4713046867387702,
      "grad_norm": 1.2844094038009644,
      "learning_rate": 3.753940688917422e-05,
      "loss": 0.138,
      "step": 226500
    },
    {
      "epoch": 1.471954269381922,
      "grad_norm": 1.448059320449829,
      "learning_rate": 3.753249643074823e-05,
      "loss": 0.1357,
      "step": 226600
    },
    {
      "epoch": 1.472603852025074,
      "grad_norm": 1.1021491289138794,
      "learning_rate": 3.7525585972322237e-05,
      "loss": 0.1435,
      "step": 226700
    },
    {
      "epoch": 1.4732534346682256,
      "grad_norm": 1.224578857421875,
      "learning_rate": 3.7518675513896246e-05,
      "loss": 0.1376,
      "step": 226800
    },
    {
      "epoch": 1.4739030173113774,
      "grad_norm": 1.4410338401794434,
      "learning_rate": 3.751176505547025e-05,
      "loss": 0.1345,
      "step": 226900
    },
    {
      "epoch": 1.4745525999545293,
      "grad_norm": 1.1696938276290894,
      "learning_rate": 3.750485459704426e-05,
      "loss": 0.1401,
      "step": 227000
    },
    {
      "epoch": 1.4752021825976809,
      "grad_norm": 0.8908534646034241,
      "learning_rate": 3.749794413861827e-05,
      "loss": 0.1413,
      "step": 227100
    },
    {
      "epoch": 1.4758517652408327,
      "grad_norm": 1.2067914009094238,
      "learning_rate": 3.749103368019228e-05,
      "loss": 0.1431,
      "step": 227200
    },
    {
      "epoch": 1.4765013478839846,
      "grad_norm": 1.6039592027664185,
      "learning_rate": 3.748412322176629e-05,
      "loss": 0.1329,
      "step": 227300
    },
    {
      "epoch": 1.4771509305271362,
      "grad_norm": 0.840369462966919,
      "learning_rate": 3.74772127633403e-05,
      "loss": 0.1369,
      "step": 227400
    },
    {
      "epoch": 1.477800513170288,
      "grad_norm": 1.6779801845550537,
      "learning_rate": 3.747030230491431e-05,
      "loss": 0.1392,
      "step": 227500
    },
    {
      "epoch": 1.47845009581344,
      "grad_norm": 1.1696809530258179,
      "learning_rate": 3.746339184648832e-05,
      "loss": 0.1384,
      "step": 227600
    },
    {
      "epoch": 1.4790996784565915,
      "grad_norm": 0.8883970975875854,
      "learning_rate": 3.745648138806233e-05,
      "loss": 0.1378,
      "step": 227700
    },
    {
      "epoch": 1.4797492610997434,
      "grad_norm": 1.6915125846862793,
      "learning_rate": 3.744957092963633e-05,
      "loss": 0.1375,
      "step": 227800
    },
    {
      "epoch": 1.4803988437428952,
      "grad_norm": 1.3896937370300293,
      "learning_rate": 3.744266047121034e-05,
      "loss": 0.1372,
      "step": 227900
    },
    {
      "epoch": 1.4810484263860468,
      "grad_norm": 0.9192471504211426,
      "learning_rate": 3.743575001278435e-05,
      "loss": 0.1309,
      "step": 228000
    },
    {
      "epoch": 1.4816980090291987,
      "grad_norm": 1.4893018007278442,
      "learning_rate": 3.742883955435836e-05,
      "loss": 0.1318,
      "step": 228100
    },
    {
      "epoch": 1.4823475916723505,
      "grad_norm": 1.4355345964431763,
      "learning_rate": 3.742192909593237e-05,
      "loss": 0.1461,
      "step": 228200
    },
    {
      "epoch": 1.4829971743155022,
      "grad_norm": 1.0221887826919556,
      "learning_rate": 3.7415018637506374e-05,
      "loss": 0.1322,
      "step": 228300
    },
    {
      "epoch": 1.483646756958654,
      "grad_norm": 1.0511966943740845,
      "learning_rate": 3.7408108179080384e-05,
      "loss": 0.1332,
      "step": 228400
    },
    {
      "epoch": 1.4842963396018058,
      "grad_norm": 1.1211117506027222,
      "learning_rate": 3.7401197720654394e-05,
      "loss": 0.1382,
      "step": 228500
    },
    {
      "epoch": 1.4849459222449577,
      "grad_norm": 1.6388343572616577,
      "learning_rate": 3.7394287262228403e-05,
      "loss": 0.1389,
      "step": 228600
    },
    {
      "epoch": 1.4855955048881093,
      "grad_norm": 0.8851943016052246,
      "learning_rate": 3.7387376803802407e-05,
      "loss": 0.1392,
      "step": 228700
    },
    {
      "epoch": 1.4862450875312612,
      "grad_norm": 1.5145915746688843,
      "learning_rate": 3.7380466345376416e-05,
      "loss": 0.1346,
      "step": 228800
    },
    {
      "epoch": 1.486894670174413,
      "grad_norm": 1.434218406677246,
      "learning_rate": 3.7373555886950426e-05,
      "loss": 0.1397,
      "step": 228900
    },
    {
      "epoch": 1.4875442528175646,
      "grad_norm": 1.2877657413482666,
      "learning_rate": 3.7366645428524436e-05,
      "loss": 0.1427,
      "step": 229000
    },
    {
      "epoch": 1.4881938354607165,
      "grad_norm": 0.7964706420898438,
      "learning_rate": 3.7359734970098446e-05,
      "loss": 0.1394,
      "step": 229100
    },
    {
      "epoch": 1.4888434181038683,
      "grad_norm": 0.9578615427017212,
      "learning_rate": 3.7352824511672456e-05,
      "loss": 0.1437,
      "step": 229200
    },
    {
      "epoch": 1.48949300074702,
      "grad_norm": 0.9647533893585205,
      "learning_rate": 3.7345914053246466e-05,
      "loss": 0.1355,
      "step": 229300
    },
    {
      "epoch": 1.4901425833901718,
      "grad_norm": 1.114020586013794,
      "learning_rate": 3.7339003594820475e-05,
      "loss": 0.1301,
      "step": 229400
    },
    {
      "epoch": 1.4907921660333237,
      "grad_norm": 1.7777605056762695,
      "learning_rate": 3.7332093136394485e-05,
      "loss": 0.1334,
      "step": 229500
    },
    {
      "epoch": 1.4914417486764755,
      "grad_norm": 1.8886303901672363,
      "learning_rate": 3.7325182677968495e-05,
      "loss": 0.1388,
      "step": 229600
    },
    {
      "epoch": 1.4920913313196271,
      "grad_norm": 1.1353265047073364,
      "learning_rate": 3.73182722195425e-05,
      "loss": 0.1324,
      "step": 229700
    },
    {
      "epoch": 1.492740913962779,
      "grad_norm": 1.0366406440734863,
      "learning_rate": 3.731136176111651e-05,
      "loss": 0.1339,
      "step": 229800
    },
    {
      "epoch": 1.4933904966059308,
      "grad_norm": 1.4045310020446777,
      "learning_rate": 3.730445130269052e-05,
      "loss": 0.1457,
      "step": 229900
    },
    {
      "epoch": 1.4940400792490824,
      "grad_norm": 1.393271803855896,
      "learning_rate": 3.729754084426453e-05,
      "loss": 0.1453,
      "step": 230000
    },
    {
      "epoch": 1.4946896618922343,
      "grad_norm": 1.609004020690918,
      "learning_rate": 3.729063038583854e-05,
      "loss": 0.1327,
      "step": 230100
    },
    {
      "epoch": 1.4953392445353861,
      "grad_norm": 1.2119550704956055,
      "learning_rate": 3.728371992741255e-05,
      "loss": 0.1344,
      "step": 230200
    },
    {
      "epoch": 1.4959888271785378,
      "grad_norm": 1.526043176651001,
      "learning_rate": 3.727680946898656e-05,
      "loss": 0.1382,
      "step": 230300
    },
    {
      "epoch": 1.4966384098216896,
      "grad_norm": 1.1722936630249023,
      "learning_rate": 3.726989901056057e-05,
      "loss": 0.1426,
      "step": 230400
    },
    {
      "epoch": 1.4972879924648415,
      "grad_norm": 1.2766187191009521,
      "learning_rate": 3.726298855213458e-05,
      "loss": 0.13,
      "step": 230500
    },
    {
      "epoch": 1.497937575107993,
      "grad_norm": 0.8545934557914734,
      "learning_rate": 3.725607809370858e-05,
      "loss": 0.1346,
      "step": 230600
    },
    {
      "epoch": 1.498587157751145,
      "grad_norm": 0.9437162280082703,
      "learning_rate": 3.724916763528259e-05,
      "loss": 0.1304,
      "step": 230700
    },
    {
      "epoch": 1.4992367403942968,
      "grad_norm": 1.3258602619171143,
      "learning_rate": 3.72422571768566e-05,
      "loss": 0.136,
      "step": 230800
    },
    {
      "epoch": 1.4998863230374484,
      "grad_norm": 1.2609659433364868,
      "learning_rate": 3.723534671843061e-05,
      "loss": 0.1368,
      "step": 230900
    },
    {
      "epoch": 1.5005359056806002,
      "grad_norm": 1.381446361541748,
      "learning_rate": 3.722843626000462e-05,
      "loss": 0.1346,
      "step": 231000
    },
    {
      "epoch": 1.501185488323752,
      "grad_norm": 1.5420657396316528,
      "learning_rate": 3.722152580157863e-05,
      "loss": 0.1353,
      "step": 231100
    },
    {
      "epoch": 1.5018350709669037,
      "grad_norm": 1.2181180715560913,
      "learning_rate": 3.721461534315264e-05,
      "loss": 0.1408,
      "step": 231200
    },
    {
      "epoch": 1.5024846536100556,
      "grad_norm": 1.0286973714828491,
      "learning_rate": 3.720770488472665e-05,
      "loss": 0.1333,
      "step": 231300
    },
    {
      "epoch": 1.5031342362532074,
      "grad_norm": 1.130774736404419,
      "learning_rate": 3.720079442630066e-05,
      "loss": 0.1413,
      "step": 231400
    },
    {
      "epoch": 1.503783818896359,
      "grad_norm": 1.2131401300430298,
      "learning_rate": 3.719388396787466e-05,
      "loss": 0.1348,
      "step": 231500
    },
    {
      "epoch": 1.5044334015395109,
      "grad_norm": 1.0746219158172607,
      "learning_rate": 3.718697350944867e-05,
      "loss": 0.1362,
      "step": 231600
    },
    {
      "epoch": 1.5050829841826627,
      "grad_norm": 1.2771869897842407,
      "learning_rate": 3.718006305102268e-05,
      "loss": 0.1417,
      "step": 231700
    },
    {
      "epoch": 1.5057325668258144,
      "grad_norm": 1.2462252378463745,
      "learning_rate": 3.717315259259669e-05,
      "loss": 0.1377,
      "step": 231800
    },
    {
      "epoch": 1.5063821494689662,
      "grad_norm": 1.3563157320022583,
      "learning_rate": 3.7166242134170695e-05,
      "loss": 0.1416,
      "step": 231900
    },
    {
      "epoch": 1.507031732112118,
      "grad_norm": 1.5212068557739258,
      "learning_rate": 3.7159331675744705e-05,
      "loss": 0.1321,
      "step": 232000
    },
    {
      "epoch": 1.5076813147552697,
      "grad_norm": 0.9081448316574097,
      "learning_rate": 3.7152421217318714e-05,
      "loss": 0.1331,
      "step": 232100
    },
    {
      "epoch": 1.5083308973984215,
      "grad_norm": 0.7839008569717407,
      "learning_rate": 3.7145510758892724e-05,
      "loss": 0.1367,
      "step": 232200
    },
    {
      "epoch": 1.5089804800415734,
      "grad_norm": 1.3850631713867188,
      "learning_rate": 3.7138600300466734e-05,
      "loss": 0.1296,
      "step": 232300
    },
    {
      "epoch": 1.509630062684725,
      "grad_norm": 1.1902015209197998,
      "learning_rate": 3.7131689842040744e-05,
      "loss": 0.1371,
      "step": 232400
    },
    {
      "epoch": 1.5102796453278768,
      "grad_norm": 1.627253770828247,
      "learning_rate": 3.712477938361475e-05,
      "loss": 0.1338,
      "step": 232500
    },
    {
      "epoch": 1.5109292279710287,
      "grad_norm": 1.0599737167358398,
      "learning_rate": 3.711786892518876e-05,
      "loss": 0.1368,
      "step": 232600
    },
    {
      "epoch": 1.5115788106141803,
      "grad_norm": 1.127038598060608,
      "learning_rate": 3.711095846676277e-05,
      "loss": 0.1359,
      "step": 232700
    },
    {
      "epoch": 1.5122283932573322,
      "grad_norm": 1.139098882675171,
      "learning_rate": 3.7104048008336777e-05,
      "loss": 0.1423,
      "step": 232800
    },
    {
      "epoch": 1.512877975900484,
      "grad_norm": 0.9962657690048218,
      "learning_rate": 3.7097137549910786e-05,
      "loss": 0.1394,
      "step": 232900
    },
    {
      "epoch": 1.5135275585436356,
      "grad_norm": 1.542073369026184,
      "learning_rate": 3.7090227091484796e-05,
      "loss": 0.1364,
      "step": 233000
    },
    {
      "epoch": 1.5141771411867875,
      "grad_norm": 0.9997007846832275,
      "learning_rate": 3.7083316633058806e-05,
      "loss": 0.1428,
      "step": 233100
    },
    {
      "epoch": 1.5148267238299393,
      "grad_norm": 1.1167694330215454,
      "learning_rate": 3.7076406174632816e-05,
      "loss": 0.1419,
      "step": 233200
    },
    {
      "epoch": 1.515476306473091,
      "grad_norm": 1.3033528327941895,
      "learning_rate": 3.7069495716206826e-05,
      "loss": 0.1425,
      "step": 233300
    },
    {
      "epoch": 1.5161258891162428,
      "grad_norm": 1.8293440341949463,
      "learning_rate": 3.706258525778083e-05,
      "loss": 0.1382,
      "step": 233400
    },
    {
      "epoch": 1.5167754717593946,
      "grad_norm": 1.350903034210205,
      "learning_rate": 3.705567479935484e-05,
      "loss": 0.1272,
      "step": 233500
    },
    {
      "epoch": 1.5174250544025463,
      "grad_norm": 0.9704481363296509,
      "learning_rate": 3.704876434092885e-05,
      "loss": 0.1306,
      "step": 233600
    },
    {
      "epoch": 1.5180746370456981,
      "grad_norm": 1.5441974401474,
      "learning_rate": 3.704185388250286e-05,
      "loss": 0.1386,
      "step": 233700
    },
    {
      "epoch": 1.51872421968885,
      "grad_norm": 0.8510226607322693,
      "learning_rate": 3.703494342407687e-05,
      "loss": 0.1341,
      "step": 233800
    },
    {
      "epoch": 1.5193738023320016,
      "grad_norm": 1.2549017667770386,
      "learning_rate": 3.702803296565088e-05,
      "loss": 0.1407,
      "step": 233900
    },
    {
      "epoch": 1.5200233849751534,
      "grad_norm": 1.329811453819275,
      "learning_rate": 3.702112250722489e-05,
      "loss": 0.1345,
      "step": 234000
    },
    {
      "epoch": 1.5206729676183053,
      "grad_norm": 1.3826631307601929,
      "learning_rate": 3.70142120487989e-05,
      "loss": 0.1394,
      "step": 234100
    },
    {
      "epoch": 1.521322550261457,
      "grad_norm": 1.167685866355896,
      "learning_rate": 3.700730159037291e-05,
      "loss": 0.1454,
      "step": 234200
    },
    {
      "epoch": 1.5219721329046088,
      "grad_norm": 1.8241968154907227,
      "learning_rate": 3.700039113194692e-05,
      "loss": 0.1378,
      "step": 234300
    },
    {
      "epoch": 1.5226217155477606,
      "grad_norm": 1.3175076246261597,
      "learning_rate": 3.699348067352092e-05,
      "loss": 0.1335,
      "step": 234400
    },
    {
      "epoch": 1.5232712981909122,
      "grad_norm": 1.2951350212097168,
      "learning_rate": 3.698657021509493e-05,
      "loss": 0.1392,
      "step": 234500
    },
    {
      "epoch": 1.523920880834064,
      "grad_norm": 1.1036112308502197,
      "learning_rate": 3.697965975666894e-05,
      "loss": 0.1337,
      "step": 234600
    },
    {
      "epoch": 1.524570463477216,
      "grad_norm": 1.198075771331787,
      "learning_rate": 3.697274929824295e-05,
      "loss": 0.1333,
      "step": 234700
    },
    {
      "epoch": 1.5252200461203675,
      "grad_norm": 1.2935161590576172,
      "learning_rate": 3.696583883981696e-05,
      "loss": 0.1338,
      "step": 234800
    },
    {
      "epoch": 1.5258696287635194,
      "grad_norm": 1.2422276735305786,
      "learning_rate": 3.695892838139097e-05,
      "loss": 0.1455,
      "step": 234900
    },
    {
      "epoch": 1.5265192114066712,
      "grad_norm": 1.251919150352478,
      "learning_rate": 3.695201792296498e-05,
      "loss": 0.1356,
      "step": 235000
    },
    {
      "epoch": 1.5271687940498229,
      "grad_norm": 1.1862927675247192,
      "learning_rate": 3.694510746453898e-05,
      "loss": 0.1411,
      "step": 235100
    },
    {
      "epoch": 1.5278183766929747,
      "grad_norm": 1.2825381755828857,
      "learning_rate": 3.693819700611299e-05,
      "loss": 0.136,
      "step": 235200
    },
    {
      "epoch": 1.5284679593361266,
      "grad_norm": 1.5207616090774536,
      "learning_rate": 3.6931286547687e-05,
      "loss": 0.1376,
      "step": 235300
    },
    {
      "epoch": 1.5291175419792782,
      "grad_norm": 1.1720468997955322,
      "learning_rate": 3.692437608926101e-05,
      "loss": 0.1419,
      "step": 235400
    },
    {
      "epoch": 1.5297671246224303,
      "grad_norm": 1.7132567167282104,
      "learning_rate": 3.6917465630835015e-05,
      "loss": 0.1383,
      "step": 235500
    },
    {
      "epoch": 1.5304167072655819,
      "grad_norm": 1.2666715383529663,
      "learning_rate": 3.6910555172409025e-05,
      "loss": 0.1362,
      "step": 235600
    },
    {
      "epoch": 1.5310662899087335,
      "grad_norm": 1.5353022813796997,
      "learning_rate": 3.6903644713983035e-05,
      "loss": 0.1347,
      "step": 235700
    },
    {
      "epoch": 1.5317158725518856,
      "grad_norm": 1.1128774881362915,
      "learning_rate": 3.6896734255557045e-05,
      "loss": 0.1368,
      "step": 235800
    },
    {
      "epoch": 1.5323654551950372,
      "grad_norm": 1.4351228475570679,
      "learning_rate": 3.6889823797131055e-05,
      "loss": 0.1329,
      "step": 235900
    },
    {
      "epoch": 1.5330150378381888,
      "grad_norm": 1.3225626945495605,
      "learning_rate": 3.6882913338705065e-05,
      "loss": 0.1385,
      "step": 236000
    },
    {
      "epoch": 1.533664620481341,
      "grad_norm": 1.4218754768371582,
      "learning_rate": 3.6876002880279075e-05,
      "loss": 0.1371,
      "step": 236100
    },
    {
      "epoch": 1.5343142031244925,
      "grad_norm": 1.439720869064331,
      "learning_rate": 3.6869092421853084e-05,
      "loss": 0.1296,
      "step": 236200
    },
    {
      "epoch": 1.5349637857676441,
      "grad_norm": 1.1875059604644775,
      "learning_rate": 3.686218196342709e-05,
      "loss": 0.1357,
      "step": 236300
    },
    {
      "epoch": 1.5356133684107962,
      "grad_norm": 1.3026455640792847,
      "learning_rate": 3.68552715050011e-05,
      "loss": 0.1322,
      "step": 236400
    },
    {
      "epoch": 1.5362629510539478,
      "grad_norm": 0.9628017544746399,
      "learning_rate": 3.684836104657511e-05,
      "loss": 0.1321,
      "step": 236500
    },
    {
      "epoch": 1.5369125336970995,
      "grad_norm": 1.505230188369751,
      "learning_rate": 3.684145058814912e-05,
      "loss": 0.138,
      "step": 236600
    },
    {
      "epoch": 1.5375621163402515,
      "grad_norm": 1.3607947826385498,
      "learning_rate": 3.683454012972313e-05,
      "loss": 0.1343,
      "step": 236700
    },
    {
      "epoch": 1.5382116989834032,
      "grad_norm": 1.2770307064056396,
      "learning_rate": 3.682762967129714e-05,
      "loss": 0.1386,
      "step": 236800
    },
    {
      "epoch": 1.5388612816265548,
      "grad_norm": 1.2706295251846313,
      "learning_rate": 3.682071921287115e-05,
      "loss": 0.1371,
      "step": 236900
    },
    {
      "epoch": 1.5395108642697068,
      "grad_norm": 1.6673469543457031,
      "learning_rate": 3.6813808754445156e-05,
      "loss": 0.1357,
      "step": 237000
    },
    {
      "epoch": 1.5401604469128585,
      "grad_norm": 1.7523796558380127,
      "learning_rate": 3.6806898296019166e-05,
      "loss": 0.1332,
      "step": 237100
    },
    {
      "epoch": 1.5408100295560103,
      "grad_norm": 1.5121536254882812,
      "learning_rate": 3.679998783759317e-05,
      "loss": 0.1347,
      "step": 237200
    },
    {
      "epoch": 1.5414596121991622,
      "grad_norm": 1.2400542497634888,
      "learning_rate": 3.679307737916718e-05,
      "loss": 0.1281,
      "step": 237300
    },
    {
      "epoch": 1.5421091948423138,
      "grad_norm": 1.3982293605804443,
      "learning_rate": 3.678616692074119e-05,
      "loss": 0.1407,
      "step": 237400
    },
    {
      "epoch": 1.5427587774854656,
      "grad_norm": 1.2508965730667114,
      "learning_rate": 3.67792564623152e-05,
      "loss": 0.143,
      "step": 237500
    },
    {
      "epoch": 1.5434083601286175,
      "grad_norm": 1.1772276163101196,
      "learning_rate": 3.677234600388921e-05,
      "loss": 0.138,
      "step": 237600
    },
    {
      "epoch": 1.544057942771769,
      "grad_norm": 1.083258032798767,
      "learning_rate": 3.676543554546322e-05,
      "loss": 0.1343,
      "step": 237700
    },
    {
      "epoch": 1.544707525414921,
      "grad_norm": 1.0542497634887695,
      "learning_rate": 3.675852508703723e-05,
      "loss": 0.1378,
      "step": 237800
    },
    {
      "epoch": 1.5453571080580728,
      "grad_norm": 1.1343289613723755,
      "learning_rate": 3.675161462861124e-05,
      "loss": 0.129,
      "step": 237900
    },
    {
      "epoch": 1.5460066907012244,
      "grad_norm": 1.3122550249099731,
      "learning_rate": 3.674470417018525e-05,
      "loss": 0.141,
      "step": 238000
    },
    {
      "epoch": 1.5466562733443763,
      "grad_norm": 1.6584548950195312,
      "learning_rate": 3.673779371175926e-05,
      "loss": 0.1349,
      "step": 238100
    },
    {
      "epoch": 1.5473058559875281,
      "grad_norm": 1.2611312866210938,
      "learning_rate": 3.673088325333326e-05,
      "loss": 0.1275,
      "step": 238200
    },
    {
      "epoch": 1.5479554386306797,
      "grad_norm": 0.7468131184577942,
      "learning_rate": 3.672397279490727e-05,
      "loss": 0.1283,
      "step": 238300
    },
    {
      "epoch": 1.5486050212738316,
      "grad_norm": 1.0340349674224854,
      "learning_rate": 3.671706233648128e-05,
      "loss": 0.1394,
      "step": 238400
    },
    {
      "epoch": 1.5492546039169834,
      "grad_norm": 1.2143863439559937,
      "learning_rate": 3.671015187805529e-05,
      "loss": 0.1366,
      "step": 238500
    },
    {
      "epoch": 1.549904186560135,
      "grad_norm": 1.668947458267212,
      "learning_rate": 3.67032414196293e-05,
      "loss": 0.1363,
      "step": 238600
    },
    {
      "epoch": 1.550553769203287,
      "grad_norm": 1.0655385255813599,
      "learning_rate": 3.6696330961203304e-05,
      "loss": 0.1383,
      "step": 238700
    },
    {
      "epoch": 1.5512033518464388,
      "grad_norm": 1.0919259786605835,
      "learning_rate": 3.6689420502777313e-05,
      "loss": 0.1335,
      "step": 238800
    },
    {
      "epoch": 1.5518529344895904,
      "grad_norm": 1.6940022706985474,
      "learning_rate": 3.668251004435132e-05,
      "loss": 0.1403,
      "step": 238900
    },
    {
      "epoch": 1.5525025171327422,
      "grad_norm": 1.3477009534835815,
      "learning_rate": 3.667559958592533e-05,
      "loss": 0.1306,
      "step": 239000
    },
    {
      "epoch": 1.553152099775894,
      "grad_norm": 1.530989170074463,
      "learning_rate": 3.6668689127499336e-05,
      "loss": 0.1324,
      "step": 239100
    },
    {
      "epoch": 1.5538016824190457,
      "grad_norm": 1.311946153640747,
      "learning_rate": 3.6661778669073346e-05,
      "loss": 0.134,
      "step": 239200
    },
    {
      "epoch": 1.5544512650621976,
      "grad_norm": 1.025139331817627,
      "learning_rate": 3.6654868210647356e-05,
      "loss": 0.1304,
      "step": 239300
    },
    {
      "epoch": 1.5551008477053494,
      "grad_norm": 1.3044016361236572,
      "learning_rate": 3.6647957752221366e-05,
      "loss": 0.1298,
      "step": 239400
    },
    {
      "epoch": 1.555750430348501,
      "grad_norm": 0.8720110058784485,
      "learning_rate": 3.6641047293795376e-05,
      "loss": 0.1377,
      "step": 239500
    },
    {
      "epoch": 1.5564000129916529,
      "grad_norm": 1.624780297279358,
      "learning_rate": 3.6634136835369386e-05,
      "loss": 0.1299,
      "step": 239600
    },
    {
      "epoch": 1.5570495956348047,
      "grad_norm": 0.9518611431121826,
      "learning_rate": 3.6627226376943395e-05,
      "loss": 0.1295,
      "step": 239700
    },
    {
      "epoch": 1.5576991782779563,
      "grad_norm": 1.0451816320419312,
      "learning_rate": 3.6620315918517405e-05,
      "loss": 0.1335,
      "step": 239800
    },
    {
      "epoch": 1.5583487609211082,
      "grad_norm": 1.2839181423187256,
      "learning_rate": 3.6613405460091415e-05,
      "loss": 0.1304,
      "step": 239900
    },
    {
      "epoch": 1.55899834356426,
      "grad_norm": 1.3917336463928223,
      "learning_rate": 3.660649500166542e-05,
      "loss": 0.1295,
      "step": 240000
    },
    {
      "epoch": 1.5596479262074117,
      "grad_norm": 1.4498088359832764,
      "learning_rate": 3.659958454323943e-05,
      "loss": 0.1315,
      "step": 240100
    },
    {
      "epoch": 1.5602975088505635,
      "grad_norm": 1.8550848960876465,
      "learning_rate": 3.659267408481344e-05,
      "loss": 0.1472,
      "step": 240200
    },
    {
      "epoch": 1.5609470914937154,
      "grad_norm": 1.4710073471069336,
      "learning_rate": 3.658576362638745e-05,
      "loss": 0.1375,
      "step": 240300
    },
    {
      "epoch": 1.561596674136867,
      "grad_norm": 1.728683352470398,
      "learning_rate": 3.657885316796146e-05,
      "loss": 0.1311,
      "step": 240400
    },
    {
      "epoch": 1.5622462567800188,
      "grad_norm": 1.6588492393493652,
      "learning_rate": 3.657194270953547e-05,
      "loss": 0.1317,
      "step": 240500
    },
    {
      "epoch": 1.5628958394231707,
      "grad_norm": 1.6329445838928223,
      "learning_rate": 3.656503225110948e-05,
      "loss": 0.1319,
      "step": 240600
    },
    {
      "epoch": 1.5635454220663223,
      "grad_norm": 1.7058038711547852,
      "learning_rate": 3.655812179268349e-05,
      "loss": 0.138,
      "step": 240700
    },
    {
      "epoch": 1.5641950047094741,
      "grad_norm": 1.5249913930892944,
      "learning_rate": 3.65512113342575e-05,
      "loss": 0.1404,
      "step": 240800
    },
    {
      "epoch": 1.564844587352626,
      "grad_norm": 1.4725306034088135,
      "learning_rate": 3.654430087583151e-05,
      "loss": 0.1368,
      "step": 240900
    },
    {
      "epoch": 1.5654941699957776,
      "grad_norm": 2.003237247467041,
      "learning_rate": 3.653739041740551e-05,
      "loss": 0.1388,
      "step": 241000
    },
    {
      "epoch": 1.5661437526389295,
      "grad_norm": 1.2973926067352295,
      "learning_rate": 3.653047995897952e-05,
      "loss": 0.1404,
      "step": 241100
    },
    {
      "epoch": 1.5667933352820813,
      "grad_norm": 1.3625237941741943,
      "learning_rate": 3.652356950055353e-05,
      "loss": 0.1352,
      "step": 241200
    },
    {
      "epoch": 1.567442917925233,
      "grad_norm": 1.2335633039474487,
      "learning_rate": 3.651665904212754e-05,
      "loss": 0.1434,
      "step": 241300
    },
    {
      "epoch": 1.5680925005683848,
      "grad_norm": 1.0834274291992188,
      "learning_rate": 3.650974858370155e-05,
      "loss": 0.1355,
      "step": 241400
    },
    {
      "epoch": 1.5687420832115366,
      "grad_norm": 1.2506076097488403,
      "learning_rate": 3.650283812527556e-05,
      "loss": 0.1364,
      "step": 241500
    },
    {
      "epoch": 1.5693916658546883,
      "grad_norm": 0.8771901726722717,
      "learning_rate": 3.649592766684957e-05,
      "loss": 0.1343,
      "step": 241600
    },
    {
      "epoch": 1.57004124849784,
      "grad_norm": 1.3929078578948975,
      "learning_rate": 3.648901720842358e-05,
      "loss": 0.1338,
      "step": 241700
    },
    {
      "epoch": 1.570690831140992,
      "grad_norm": 2.026867628097534,
      "learning_rate": 3.648210674999759e-05,
      "loss": 0.1346,
      "step": 241800
    },
    {
      "epoch": 1.5713404137841436,
      "grad_norm": 1.3816404342651367,
      "learning_rate": 3.647519629157159e-05,
      "loss": 0.1365,
      "step": 241900
    },
    {
      "epoch": 1.5719899964272954,
      "grad_norm": 1.2970072031021118,
      "learning_rate": 3.64682858331456e-05,
      "loss": 0.1328,
      "step": 242000
    },
    {
      "epoch": 1.5726395790704473,
      "grad_norm": 1.273293137550354,
      "learning_rate": 3.646137537471961e-05,
      "loss": 0.1361,
      "step": 242100
    },
    {
      "epoch": 1.573289161713599,
      "grad_norm": 1.1063542366027832,
      "learning_rate": 3.645446491629362e-05,
      "loss": 0.1328,
      "step": 242200
    },
    {
      "epoch": 1.5739387443567507,
      "grad_norm": 2.1249125003814697,
      "learning_rate": 3.6447554457867624e-05,
      "loss": 0.1403,
      "step": 242300
    },
    {
      "epoch": 1.5745883269999026,
      "grad_norm": 1.5237762928009033,
      "learning_rate": 3.6440643999441634e-05,
      "loss": 0.1408,
      "step": 242400
    },
    {
      "epoch": 1.5752379096430542,
      "grad_norm": 1.7652549743652344,
      "learning_rate": 3.6433733541015644e-05,
      "loss": 0.1423,
      "step": 242500
    },
    {
      "epoch": 1.575887492286206,
      "grad_norm": 1.3373215198516846,
      "learning_rate": 3.6426823082589654e-05,
      "loss": 0.1289,
      "step": 242600
    },
    {
      "epoch": 1.576537074929358,
      "grad_norm": 1.2110494375228882,
      "learning_rate": 3.6419912624163664e-05,
      "loss": 0.1288,
      "step": 242700
    },
    {
      "epoch": 1.5771866575725095,
      "grad_norm": 1.1349685192108154,
      "learning_rate": 3.6413002165737674e-05,
      "loss": 0.1309,
      "step": 242800
    },
    {
      "epoch": 1.5778362402156616,
      "grad_norm": 1.2996125221252441,
      "learning_rate": 3.640609170731168e-05,
      "loss": 0.1359,
      "step": 242900
    },
    {
      "epoch": 1.5784858228588132,
      "grad_norm": 0.8392900824546814,
      "learning_rate": 3.6399181248885687e-05,
      "loss": 0.1326,
      "step": 243000
    },
    {
      "epoch": 1.5791354055019649,
      "grad_norm": 0.9290995597839355,
      "learning_rate": 3.6392270790459696e-05,
      "loss": 0.1322,
      "step": 243100
    },
    {
      "epoch": 1.579784988145117,
      "grad_norm": 1.0736312866210938,
      "learning_rate": 3.6385360332033706e-05,
      "loss": 0.1389,
      "step": 243200
    },
    {
      "epoch": 1.5804345707882685,
      "grad_norm": 1.0244430303573608,
      "learning_rate": 3.6378449873607716e-05,
      "loss": 0.1372,
      "step": 243300
    },
    {
      "epoch": 1.5810841534314202,
      "grad_norm": 0.7946394681930542,
      "learning_rate": 3.6371539415181726e-05,
      "loss": 0.1324,
      "step": 243400
    },
    {
      "epoch": 1.5817337360745722,
      "grad_norm": 0.731278657913208,
      "learning_rate": 3.6364628956755736e-05,
      "loss": 0.1328,
      "step": 243500
    },
    {
      "epoch": 1.5823833187177239,
      "grad_norm": 1.2609095573425293,
      "learning_rate": 3.6357718498329746e-05,
      "loss": 0.1355,
      "step": 243600
    },
    {
      "epoch": 1.5830329013608755,
      "grad_norm": 1.3930517435073853,
      "learning_rate": 3.6350808039903756e-05,
      "loss": 0.134,
      "step": 243700
    },
    {
      "epoch": 1.5836824840040276,
      "grad_norm": 1.6697109937667847,
      "learning_rate": 3.634389758147776e-05,
      "loss": 0.1394,
      "step": 243800
    },
    {
      "epoch": 1.5843320666471792,
      "grad_norm": 1.025413155555725,
      "learning_rate": 3.633698712305177e-05,
      "loss": 0.13,
      "step": 243900
    },
    {
      "epoch": 1.5849816492903308,
      "grad_norm": 0.922081470489502,
      "learning_rate": 3.633007666462578e-05,
      "loss": 0.1292,
      "step": 244000
    },
    {
      "epoch": 1.5856312319334829,
      "grad_norm": 1.6389031410217285,
      "learning_rate": 3.632316620619979e-05,
      "loss": 0.1339,
      "step": 244100
    },
    {
      "epoch": 1.5862808145766345,
      "grad_norm": 1.0314842462539673,
      "learning_rate": 3.63162557477738e-05,
      "loss": 0.1332,
      "step": 244200
    },
    {
      "epoch": 1.5869303972197861,
      "grad_norm": 1.3036826848983765,
      "learning_rate": 3.630934528934781e-05,
      "loss": 0.1368,
      "step": 244300
    },
    {
      "epoch": 1.5875799798629382,
      "grad_norm": 1.0009357929229736,
      "learning_rate": 3.630243483092182e-05,
      "loss": 0.1321,
      "step": 244400
    },
    {
      "epoch": 1.5882295625060898,
      "grad_norm": 1.0368531942367554,
      "learning_rate": 3.629552437249583e-05,
      "loss": 0.1376,
      "step": 244500
    },
    {
      "epoch": 1.5888791451492417,
      "grad_norm": 1.3061007261276245,
      "learning_rate": 3.628861391406984e-05,
      "loss": 0.1324,
      "step": 244600
    },
    {
      "epoch": 1.5895287277923935,
      "grad_norm": 1.0766843557357788,
      "learning_rate": 3.628170345564384e-05,
      "loss": 0.1375,
      "step": 244700
    },
    {
      "epoch": 1.5901783104355451,
      "grad_norm": 1.515425205230713,
      "learning_rate": 3.627479299721785e-05,
      "loss": 0.1299,
      "step": 244800
    },
    {
      "epoch": 1.590827893078697,
      "grad_norm": 1.2751227617263794,
      "learning_rate": 3.626788253879186e-05,
      "loss": 0.1401,
      "step": 244900
    },
    {
      "epoch": 1.5914774757218488,
      "grad_norm": 1.0339387655258179,
      "learning_rate": 3.626097208036587e-05,
      "loss": 0.135,
      "step": 245000
    },
    {
      "epoch": 1.5921270583650005,
      "grad_norm": 1.1237282752990723,
      "learning_rate": 3.625406162193988e-05,
      "loss": 0.1383,
      "step": 245100
    },
    {
      "epoch": 1.5927766410081523,
      "grad_norm": 1.1731758117675781,
      "learning_rate": 3.624715116351389e-05,
      "loss": 0.132,
      "step": 245200
    },
    {
      "epoch": 1.5934262236513042,
      "grad_norm": 1.436003565788269,
      "learning_rate": 3.62402407050879e-05,
      "loss": 0.1355,
      "step": 245300
    },
    {
      "epoch": 1.5940758062944558,
      "grad_norm": 1.1824793815612793,
      "learning_rate": 3.623333024666191e-05,
      "loss": 0.1283,
      "step": 245400
    },
    {
      "epoch": 1.5947253889376076,
      "grad_norm": 1.1921323537826538,
      "learning_rate": 3.622641978823591e-05,
      "loss": 0.1277,
      "step": 245500
    },
    {
      "epoch": 1.5953749715807595,
      "grad_norm": 1.386965036392212,
      "learning_rate": 3.621950932980992e-05,
      "loss": 0.1385,
      "step": 245600
    },
    {
      "epoch": 1.596024554223911,
      "grad_norm": 0.983816385269165,
      "learning_rate": 3.621259887138393e-05,
      "loss": 0.1373,
      "step": 245700
    },
    {
      "epoch": 1.596674136867063,
      "grad_norm": 1.073069453239441,
      "learning_rate": 3.620568841295794e-05,
      "loss": 0.1414,
      "step": 245800
    },
    {
      "epoch": 1.5973237195102148,
      "grad_norm": 1.3188111782073975,
      "learning_rate": 3.6198777954531945e-05,
      "loss": 0.1393,
      "step": 245900
    },
    {
      "epoch": 1.5979733021533664,
      "grad_norm": 1.4875283241271973,
      "learning_rate": 3.6191867496105955e-05,
      "loss": 0.127,
      "step": 246000
    },
    {
      "epoch": 1.5986228847965183,
      "grad_norm": 1.0166010856628418,
      "learning_rate": 3.6184957037679965e-05,
      "loss": 0.136,
      "step": 246100
    },
    {
      "epoch": 1.59927246743967,
      "grad_norm": 1.3064147233963013,
      "learning_rate": 3.6178046579253975e-05,
      "loss": 0.134,
      "step": 246200
    },
    {
      "epoch": 1.5999220500828217,
      "grad_norm": 0.920125424861908,
      "learning_rate": 3.6171136120827985e-05,
      "loss": 0.1352,
      "step": 246300
    },
    {
      "epoch": 1.6005716327259736,
      "grad_norm": 1.2294219732284546,
      "learning_rate": 3.6164225662401994e-05,
      "loss": 0.132,
      "step": 246400
    },
    {
      "epoch": 1.6012212153691254,
      "grad_norm": 1.9151885509490967,
      "learning_rate": 3.6157315203976004e-05,
      "loss": 0.1381,
      "step": 246500
    },
    {
      "epoch": 1.601870798012277,
      "grad_norm": 1.7021530866622925,
      "learning_rate": 3.615040474555001e-05,
      "loss": 0.1385,
      "step": 246600
    },
    {
      "epoch": 1.602520380655429,
      "grad_norm": 1.1385688781738281,
      "learning_rate": 3.614349428712402e-05,
      "loss": 0.1402,
      "step": 246700
    },
    {
      "epoch": 1.6031699632985807,
      "grad_norm": 1.2771657705307007,
      "learning_rate": 3.613658382869803e-05,
      "loss": 0.133,
      "step": 246800
    },
    {
      "epoch": 1.6038195459417324,
      "grad_norm": 1.1418888568878174,
      "learning_rate": 3.612967337027204e-05,
      "loss": 0.1392,
      "step": 246900
    },
    {
      "epoch": 1.6044691285848842,
      "grad_norm": 1.1701160669326782,
      "learning_rate": 3.612276291184605e-05,
      "loss": 0.127,
      "step": 247000
    },
    {
      "epoch": 1.605118711228036,
      "grad_norm": 0.8440539836883545,
      "learning_rate": 3.611585245342006e-05,
      "loss": 0.1335,
      "step": 247100
    },
    {
      "epoch": 1.6057682938711877,
      "grad_norm": 0.9955930709838867,
      "learning_rate": 3.6108941994994066e-05,
      "loss": 0.1371,
      "step": 247200
    },
    {
      "epoch": 1.6064178765143395,
      "grad_norm": 0.9868758916854858,
      "learning_rate": 3.6102031536568076e-05,
      "loss": 0.1316,
      "step": 247300
    },
    {
      "epoch": 1.6070674591574914,
      "grad_norm": 2.1768922805786133,
      "learning_rate": 3.6095121078142086e-05,
      "loss": 0.1373,
      "step": 247400
    },
    {
      "epoch": 1.607717041800643,
      "grad_norm": 0.8207905888557434,
      "learning_rate": 3.6088210619716096e-05,
      "loss": 0.138,
      "step": 247500
    },
    {
      "epoch": 1.6083666244437949,
      "grad_norm": 1.3918685913085938,
      "learning_rate": 3.60813001612901e-05,
      "loss": 0.1364,
      "step": 247600
    },
    {
      "epoch": 1.6090162070869467,
      "grad_norm": 1.7217950820922852,
      "learning_rate": 3.607438970286411e-05,
      "loss": 0.1342,
      "step": 247700
    },
    {
      "epoch": 1.6096657897300983,
      "grad_norm": 1.3244707584381104,
      "learning_rate": 3.606747924443812e-05,
      "loss": 0.1255,
      "step": 247800
    },
    {
      "epoch": 1.6103153723732502,
      "grad_norm": 1.2334688901901245,
      "learning_rate": 3.606056878601213e-05,
      "loss": 0.1287,
      "step": 247900
    },
    {
      "epoch": 1.610964955016402,
      "grad_norm": 1.6359889507293701,
      "learning_rate": 3.605365832758614e-05,
      "loss": 0.1352,
      "step": 248000
    },
    {
      "epoch": 1.6116145376595536,
      "grad_norm": 1.320976972579956,
      "learning_rate": 3.604674786916015e-05,
      "loss": 0.1307,
      "step": 248100
    },
    {
      "epoch": 1.6122641203027055,
      "grad_norm": 1.0980089902877808,
      "learning_rate": 3.603983741073416e-05,
      "loss": 0.1399,
      "step": 248200
    },
    {
      "epoch": 1.6129137029458573,
      "grad_norm": 1.1450212001800537,
      "learning_rate": 3.603292695230817e-05,
      "loss": 0.1306,
      "step": 248300
    },
    {
      "epoch": 1.613563285589009,
      "grad_norm": 1.631693959236145,
      "learning_rate": 3.602601649388218e-05,
      "loss": 0.124,
      "step": 248400
    },
    {
      "epoch": 1.6142128682321608,
      "grad_norm": 1.633796215057373,
      "learning_rate": 3.601910603545618e-05,
      "loss": 0.1341,
      "step": 248500
    },
    {
      "epoch": 1.6148624508753127,
      "grad_norm": 1.4755381345748901,
      "learning_rate": 3.601219557703019e-05,
      "loss": 0.1327,
      "step": 248600
    },
    {
      "epoch": 1.6155120335184643,
      "grad_norm": 1.411250352859497,
      "learning_rate": 3.60052851186042e-05,
      "loss": 0.1279,
      "step": 248700
    },
    {
      "epoch": 1.6161616161616161,
      "grad_norm": 1.1606487035751343,
      "learning_rate": 3.599837466017821e-05,
      "loss": 0.1283,
      "step": 248800
    },
    {
      "epoch": 1.616811198804768,
      "grad_norm": 1.029769778251648,
      "learning_rate": 3.599146420175222e-05,
      "loss": 0.1365,
      "step": 248900
    },
    {
      "epoch": 1.6174607814479196,
      "grad_norm": 1.5588994026184082,
      "learning_rate": 3.598455374332623e-05,
      "loss": 0.1372,
      "step": 249000
    },
    {
      "epoch": 1.6181103640910715,
      "grad_norm": 1.3120490312576294,
      "learning_rate": 3.597764328490023e-05,
      "loss": 0.1379,
      "step": 249100
    },
    {
      "epoch": 1.6187599467342233,
      "grad_norm": 1.1407318115234375,
      "learning_rate": 3.597073282647424e-05,
      "loss": 0.1328,
      "step": 249200
    },
    {
      "epoch": 1.619409529377375,
      "grad_norm": 1.555054783821106,
      "learning_rate": 3.596382236804825e-05,
      "loss": 0.1257,
      "step": 249300
    },
    {
      "epoch": 1.6200591120205268,
      "grad_norm": 1.1938132047653198,
      "learning_rate": 3.595691190962226e-05,
      "loss": 0.1375,
      "step": 249400
    },
    {
      "epoch": 1.6207086946636786,
      "grad_norm": 1.0801876783370972,
      "learning_rate": 3.5950001451196266e-05,
      "loss": 0.1358,
      "step": 249500
    },
    {
      "epoch": 1.6213582773068302,
      "grad_norm": 1.442254900932312,
      "learning_rate": 3.5943090992770276e-05,
      "loss": 0.1356,
      "step": 249600
    },
    {
      "epoch": 1.622007859949982,
      "grad_norm": 1.6511201858520508,
      "learning_rate": 3.5936180534344286e-05,
      "loss": 0.1355,
      "step": 249700
    },
    {
      "epoch": 1.622657442593134,
      "grad_norm": 1.3149906396865845,
      "learning_rate": 3.5929270075918296e-05,
      "loss": 0.1378,
      "step": 249800
    },
    {
      "epoch": 1.6233070252362856,
      "grad_norm": 0.9338388442993164,
      "learning_rate": 3.5922359617492305e-05,
      "loss": 0.1231,
      "step": 249900
    },
    {
      "epoch": 1.6239566078794374,
      "grad_norm": 1.1075756549835205,
      "learning_rate": 3.5915449159066315e-05,
      "loss": 0.1326,
      "step": 250000
    },
    {
      "epoch": 1.6246061905225893,
      "grad_norm": 1.0971194505691528,
      "learning_rate": 3.5908538700640325e-05,
      "loss": 0.131,
      "step": 250100
    },
    {
      "epoch": 1.6252557731657409,
      "grad_norm": 1.0311492681503296,
      "learning_rate": 3.5901628242214335e-05,
      "loss": 0.1328,
      "step": 250200
    },
    {
      "epoch": 1.625905355808893,
      "grad_norm": 1.636587381362915,
      "learning_rate": 3.5894717783788345e-05,
      "loss": 0.1357,
      "step": 250300
    },
    {
      "epoch": 1.6265549384520446,
      "grad_norm": 0.7206228375434875,
      "learning_rate": 3.588780732536235e-05,
      "loss": 0.1243,
      "step": 250400
    },
    {
      "epoch": 1.6272045210951962,
      "grad_norm": 1.2612378597259521,
      "learning_rate": 3.588089686693636e-05,
      "loss": 0.1278,
      "step": 250500
    },
    {
      "epoch": 1.6278541037383483,
      "grad_norm": 1.0692577362060547,
      "learning_rate": 3.587398640851037e-05,
      "loss": 0.1359,
      "step": 250600
    },
    {
      "epoch": 1.6285036863815,
      "grad_norm": 1.1769859790802002,
      "learning_rate": 3.586707595008438e-05,
      "loss": 0.1351,
      "step": 250700
    },
    {
      "epoch": 1.6291532690246515,
      "grad_norm": 1.4198410511016846,
      "learning_rate": 3.586016549165839e-05,
      "loss": 0.1394,
      "step": 250800
    },
    {
      "epoch": 1.6298028516678036,
      "grad_norm": 1.12034273147583,
      "learning_rate": 3.58532550332324e-05,
      "loss": 0.1397,
      "step": 250900
    },
    {
      "epoch": 1.6304524343109552,
      "grad_norm": 1.1018965244293213,
      "learning_rate": 3.584634457480641e-05,
      "loss": 0.1296,
      "step": 251000
    },
    {
      "epoch": 1.6311020169541068,
      "grad_norm": 1.38231360912323,
      "learning_rate": 3.583943411638042e-05,
      "loss": 0.1319,
      "step": 251100
    },
    {
      "epoch": 1.631751599597259,
      "grad_norm": 0.7840667963027954,
      "learning_rate": 3.583252365795443e-05,
      "loss": 0.1272,
      "step": 251200
    },
    {
      "epoch": 1.6324011822404105,
      "grad_norm": 1.1625839471817017,
      "learning_rate": 3.582561319952843e-05,
      "loss": 0.1225,
      "step": 251300
    },
    {
      "epoch": 1.6330507648835622,
      "grad_norm": 1.1855061054229736,
      "learning_rate": 3.581870274110244e-05,
      "loss": 0.134,
      "step": 251400
    },
    {
      "epoch": 1.6337003475267142,
      "grad_norm": 1.4997833967208862,
      "learning_rate": 3.581179228267645e-05,
      "loss": 0.1301,
      "step": 251500
    },
    {
      "epoch": 1.6343499301698658,
      "grad_norm": 1.096097469329834,
      "learning_rate": 3.580488182425046e-05,
      "loss": 0.1345,
      "step": 251600
    },
    {
      "epoch": 1.6349995128130175,
      "grad_norm": 1.2368265390396118,
      "learning_rate": 3.579797136582447e-05,
      "loss": 0.1334,
      "step": 251700
    },
    {
      "epoch": 1.6356490954561695,
      "grad_norm": 1.1627312898635864,
      "learning_rate": 3.579106090739848e-05,
      "loss": 0.1407,
      "step": 251800
    },
    {
      "epoch": 1.6362986780993212,
      "grad_norm": 0.8602669835090637,
      "learning_rate": 3.578415044897249e-05,
      "loss": 0.1356,
      "step": 251900
    },
    {
      "epoch": 1.636948260742473,
      "grad_norm": 0.9481270909309387,
      "learning_rate": 3.57772399905465e-05,
      "loss": 0.1301,
      "step": 252000
    },
    {
      "epoch": 1.6375978433856249,
      "grad_norm": 1.2433223724365234,
      "learning_rate": 3.577032953212051e-05,
      "loss": 0.1309,
      "step": 252100
    },
    {
      "epoch": 1.6382474260287765,
      "grad_norm": 1.0336742401123047,
      "learning_rate": 3.576341907369452e-05,
      "loss": 0.1382,
      "step": 252200
    },
    {
      "epoch": 1.6388970086719283,
      "grad_norm": 1.4514667987823486,
      "learning_rate": 3.575650861526852e-05,
      "loss": 0.1322,
      "step": 252300
    },
    {
      "epoch": 1.6395465913150802,
      "grad_norm": 1.2617111206054688,
      "learning_rate": 3.574959815684253e-05,
      "loss": 0.1379,
      "step": 252400
    },
    {
      "epoch": 1.6401961739582318,
      "grad_norm": 1.6445643901824951,
      "learning_rate": 3.574268769841654e-05,
      "loss": 0.1333,
      "step": 252500
    },
    {
      "epoch": 1.6408457566013837,
      "grad_norm": 1.1046007871627808,
      "learning_rate": 3.573577723999055e-05,
      "loss": 0.1323,
      "step": 252600
    },
    {
      "epoch": 1.6414953392445355,
      "grad_norm": 1.2443857192993164,
      "learning_rate": 3.5728866781564554e-05,
      "loss": 0.1306,
      "step": 252700
    },
    {
      "epoch": 1.6421449218876871,
      "grad_norm": 1.429876446723938,
      "learning_rate": 3.5721956323138564e-05,
      "loss": 0.1325,
      "step": 252800
    },
    {
      "epoch": 1.642794504530839,
      "grad_norm": 1.4121037721633911,
      "learning_rate": 3.5715045864712574e-05,
      "loss": 0.1349,
      "step": 252900
    },
    {
      "epoch": 1.6434440871739908,
      "grad_norm": 0.8635611534118652,
      "learning_rate": 3.5708135406286584e-05,
      "loss": 0.123,
      "step": 253000
    },
    {
      "epoch": 1.6440936698171424,
      "grad_norm": 1.142114520072937,
      "learning_rate": 3.5701224947860594e-05,
      "loss": 0.1366,
      "step": 253100
    },
    {
      "epoch": 1.6447432524602943,
      "grad_norm": 1.7379109859466553,
      "learning_rate": 3.56943144894346e-05,
      "loss": 0.132,
      "step": 253200
    },
    {
      "epoch": 1.6453928351034461,
      "grad_norm": 1.3614542484283447,
      "learning_rate": 3.5687404031008606e-05,
      "loss": 0.1403,
      "step": 253300
    },
    {
      "epoch": 1.6460424177465978,
      "grad_norm": 1.8147947788238525,
      "learning_rate": 3.5680493572582616e-05,
      "loss": 0.1292,
      "step": 253400
    },
    {
      "epoch": 1.6466920003897496,
      "grad_norm": 1.6669013500213623,
      "learning_rate": 3.5673583114156626e-05,
      "loss": 0.1283,
      "step": 253500
    },
    {
      "epoch": 1.6473415830329015,
      "grad_norm": 0.7778776288032532,
      "learning_rate": 3.5666672655730636e-05,
      "loss": 0.1321,
      "step": 253600
    },
    {
      "epoch": 1.647991165676053,
      "grad_norm": 0.7768985629081726,
      "learning_rate": 3.5659762197304646e-05,
      "loss": 0.1306,
      "step": 253700
    },
    {
      "epoch": 1.648640748319205,
      "grad_norm": 1.3095201253890991,
      "learning_rate": 3.5652851738878656e-05,
      "loss": 0.1312,
      "step": 253800
    },
    {
      "epoch": 1.6492903309623568,
      "grad_norm": 1.8941975831985474,
      "learning_rate": 3.5645941280452666e-05,
      "loss": 0.1322,
      "step": 253900
    },
    {
      "epoch": 1.6499399136055084,
      "grad_norm": 1.349934697151184,
      "learning_rate": 3.5639030822026675e-05,
      "loss": 0.13,
      "step": 254000
    },
    {
      "epoch": 1.6505894962486602,
      "grad_norm": 1.4556949138641357,
      "learning_rate": 3.563212036360068e-05,
      "loss": 0.1332,
      "step": 254100
    },
    {
      "epoch": 1.651239078891812,
      "grad_norm": 1.1240407228469849,
      "learning_rate": 3.562520990517469e-05,
      "loss": 0.1273,
      "step": 254200
    },
    {
      "epoch": 1.6518886615349637,
      "grad_norm": 1.3157304525375366,
      "learning_rate": 3.56182994467487e-05,
      "loss": 0.1318,
      "step": 254300
    },
    {
      "epoch": 1.6525382441781156,
      "grad_norm": 0.8358205556869507,
      "learning_rate": 3.561138898832271e-05,
      "loss": 0.1349,
      "step": 254400
    },
    {
      "epoch": 1.6531878268212674,
      "grad_norm": 0.9195310473442078,
      "learning_rate": 3.560447852989672e-05,
      "loss": 0.1346,
      "step": 254500
    },
    {
      "epoch": 1.653837409464419,
      "grad_norm": 1.5519918203353882,
      "learning_rate": 3.559756807147073e-05,
      "loss": 0.1245,
      "step": 254600
    },
    {
      "epoch": 1.6544869921075709,
      "grad_norm": 1.1703888177871704,
      "learning_rate": 3.559065761304474e-05,
      "loss": 0.1286,
      "step": 254700
    },
    {
      "epoch": 1.6551365747507227,
      "grad_norm": 1.5676789283752441,
      "learning_rate": 3.558374715461875e-05,
      "loss": 0.1371,
      "step": 254800
    },
    {
      "epoch": 1.6557861573938744,
      "grad_norm": 0.8493018746376038,
      "learning_rate": 3.557683669619276e-05,
      "loss": 0.135,
      "step": 254900
    },
    {
      "epoch": 1.6564357400370262,
      "grad_norm": 1.5231235027313232,
      "learning_rate": 3.556992623776677e-05,
      "loss": 0.1268,
      "step": 255000
    },
    {
      "epoch": 1.657085322680178,
      "grad_norm": 1.3901134729385376,
      "learning_rate": 3.556301577934077e-05,
      "loss": 0.1322,
      "step": 255100
    },
    {
      "epoch": 1.6577349053233297,
      "grad_norm": 1.452877402305603,
      "learning_rate": 3.555610532091478e-05,
      "loss": 0.1317,
      "step": 255200
    },
    {
      "epoch": 1.6583844879664815,
      "grad_norm": 1.0069612264633179,
      "learning_rate": 3.554919486248879e-05,
      "loss": 0.1289,
      "step": 255300
    },
    {
      "epoch": 1.6590340706096334,
      "grad_norm": 1.0635719299316406,
      "learning_rate": 3.55422844040628e-05,
      "loss": 0.1223,
      "step": 255400
    },
    {
      "epoch": 1.659683653252785,
      "grad_norm": 0.9986618757247925,
      "learning_rate": 3.553537394563681e-05,
      "loss": 0.1327,
      "step": 255500
    },
    {
      "epoch": 1.6603332358959368,
      "grad_norm": 0.8815069198608398,
      "learning_rate": 3.552846348721082e-05,
      "loss": 0.1267,
      "step": 255600
    },
    {
      "epoch": 1.6609828185390887,
      "grad_norm": 0.9791631102561951,
      "learning_rate": 3.552155302878483e-05,
      "loss": 0.1292,
      "step": 255700
    },
    {
      "epoch": 1.6616324011822403,
      "grad_norm": 1.0887154340744019,
      "learning_rate": 3.551464257035884e-05,
      "loss": 0.134,
      "step": 255800
    },
    {
      "epoch": 1.6622819838253922,
      "grad_norm": 1.1882174015045166,
      "learning_rate": 3.550773211193284e-05,
      "loss": 0.1274,
      "step": 255900
    },
    {
      "epoch": 1.662931566468544,
      "grad_norm": 1.221372127532959,
      "learning_rate": 3.550082165350685e-05,
      "loss": 0.125,
      "step": 256000
    },
    {
      "epoch": 1.6635811491116956,
      "grad_norm": 1.6884852647781372,
      "learning_rate": 3.549391119508086e-05,
      "loss": 0.1346,
      "step": 256100
    },
    {
      "epoch": 1.6642307317548475,
      "grad_norm": 1.2671406269073486,
      "learning_rate": 3.5487000736654865e-05,
      "loss": 0.131,
      "step": 256200
    },
    {
      "epoch": 1.6648803143979993,
      "grad_norm": 1.4763085842132568,
      "learning_rate": 3.5480090278228875e-05,
      "loss": 0.1383,
      "step": 256300
    },
    {
      "epoch": 1.665529897041151,
      "grad_norm": 1.094436526298523,
      "learning_rate": 3.5473179819802885e-05,
      "loss": 0.1333,
      "step": 256400
    },
    {
      "epoch": 1.6661794796843028,
      "grad_norm": 1.233725666999817,
      "learning_rate": 3.5466269361376895e-05,
      "loss": 0.128,
      "step": 256500
    },
    {
      "epoch": 1.6668290623274546,
      "grad_norm": 0.8455888628959656,
      "learning_rate": 3.5459358902950904e-05,
      "loss": 0.1284,
      "step": 256600
    },
    {
      "epoch": 1.6674786449706063,
      "grad_norm": 1.0283434391021729,
      "learning_rate": 3.5452448444524914e-05,
      "loss": 0.1297,
      "step": 256700
    },
    {
      "epoch": 1.6681282276137581,
      "grad_norm": 1.1127467155456543,
      "learning_rate": 3.5445537986098924e-05,
      "loss": 0.1301,
      "step": 256800
    },
    {
      "epoch": 1.66877781025691,
      "grad_norm": 1.8435945510864258,
      "learning_rate": 3.5438627527672934e-05,
      "loss": 0.1358,
      "step": 256900
    },
    {
      "epoch": 1.6694273929000616,
      "grad_norm": 2.3218460083007812,
      "learning_rate": 3.543171706924694e-05,
      "loss": 0.1299,
      "step": 257000
    },
    {
      "epoch": 1.6700769755432134,
      "grad_norm": 1.3477412462234497,
      "learning_rate": 3.542480661082095e-05,
      "loss": 0.133,
      "step": 257100
    },
    {
      "epoch": 1.6707265581863653,
      "grad_norm": 1.2630279064178467,
      "learning_rate": 3.541789615239496e-05,
      "loss": 0.1283,
      "step": 257200
    },
    {
      "epoch": 1.671376140829517,
      "grad_norm": 1.1632543802261353,
      "learning_rate": 3.541098569396897e-05,
      "loss": 0.1285,
      "step": 257300
    },
    {
      "epoch": 1.6720257234726688,
      "grad_norm": 1.315314531326294,
      "learning_rate": 3.5404075235542977e-05,
      "loss": 0.1304,
      "step": 257400
    },
    {
      "epoch": 1.6726753061158206,
      "grad_norm": 1.387970209121704,
      "learning_rate": 3.5397164777116986e-05,
      "loss": 0.1314,
      "step": 257500
    },
    {
      "epoch": 1.6733248887589722,
      "grad_norm": 0.9933998584747314,
      "learning_rate": 3.5390254318690996e-05,
      "loss": 0.1325,
      "step": 257600
    },
    {
      "epoch": 1.6739744714021243,
      "grad_norm": 1.1169527769088745,
      "learning_rate": 3.5383343860265006e-05,
      "loss": 0.1289,
      "step": 257700
    },
    {
      "epoch": 1.674624054045276,
      "grad_norm": 1.3698012828826904,
      "learning_rate": 3.5376433401839016e-05,
      "loss": 0.1298,
      "step": 257800
    },
    {
      "epoch": 1.6752736366884275,
      "grad_norm": 1.1507742404937744,
      "learning_rate": 3.536952294341302e-05,
      "loss": 0.1316,
      "step": 257900
    },
    {
      "epoch": 1.6759232193315796,
      "grad_norm": 0.7124912142753601,
      "learning_rate": 3.536261248498703e-05,
      "loss": 0.1263,
      "step": 258000
    },
    {
      "epoch": 1.6765728019747312,
      "grad_norm": 2.5742669105529785,
      "learning_rate": 3.535570202656104e-05,
      "loss": 0.1359,
      "step": 258100
    },
    {
      "epoch": 1.6772223846178829,
      "grad_norm": 1.1473212242126465,
      "learning_rate": 3.534879156813505e-05,
      "loss": 0.1333,
      "step": 258200
    },
    {
      "epoch": 1.677871967261035,
      "grad_norm": 1.7048976421356201,
      "learning_rate": 3.534188110970906e-05,
      "loss": 0.122,
      "step": 258300
    },
    {
      "epoch": 1.6785215499041866,
      "grad_norm": 1.409987449645996,
      "learning_rate": 3.533497065128307e-05,
      "loss": 0.1278,
      "step": 258400
    },
    {
      "epoch": 1.6791711325473382,
      "grad_norm": 1.2214120626449585,
      "learning_rate": 3.532806019285708e-05,
      "loss": 0.1346,
      "step": 258500
    },
    {
      "epoch": 1.6798207151904903,
      "grad_norm": 1.3557597398757935,
      "learning_rate": 3.532114973443109e-05,
      "loss": 0.1281,
      "step": 258600
    },
    {
      "epoch": 1.6804702978336419,
      "grad_norm": 1.1878880262374878,
      "learning_rate": 3.53142392760051e-05,
      "loss": 0.1282,
      "step": 258700
    },
    {
      "epoch": 1.6811198804767935,
      "grad_norm": 1.4333020448684692,
      "learning_rate": 3.53073288175791e-05,
      "loss": 0.1278,
      "step": 258800
    },
    {
      "epoch": 1.6817694631199456,
      "grad_norm": 0.9721235632896423,
      "learning_rate": 3.530041835915311e-05,
      "loss": 0.1268,
      "step": 258900
    },
    {
      "epoch": 1.6824190457630972,
      "grad_norm": 1.4502586126327515,
      "learning_rate": 3.529350790072712e-05,
      "loss": 0.1275,
      "step": 259000
    },
    {
      "epoch": 1.6830686284062488,
      "grad_norm": 0.9988304376602173,
      "learning_rate": 3.528659744230113e-05,
      "loss": 0.1322,
      "step": 259100
    },
    {
      "epoch": 1.683718211049401,
      "grad_norm": 0.9002795219421387,
      "learning_rate": 3.527968698387514e-05,
      "loss": 0.1361,
      "step": 259200
    },
    {
      "epoch": 1.6843677936925525,
      "grad_norm": 0.9488560557365417,
      "learning_rate": 3.527277652544915e-05,
      "loss": 0.1311,
      "step": 259300
    },
    {
      "epoch": 1.6850173763357044,
      "grad_norm": 1.0848753452301025,
      "learning_rate": 3.526586606702316e-05,
      "loss": 0.1319,
      "step": 259400
    },
    {
      "epoch": 1.6856669589788562,
      "grad_norm": 0.9533215165138245,
      "learning_rate": 3.525895560859716e-05,
      "loss": 0.1343,
      "step": 259500
    },
    {
      "epoch": 1.6863165416220078,
      "grad_norm": 1.0187686681747437,
      "learning_rate": 3.525204515017117e-05,
      "loss": 0.1352,
      "step": 259600
    },
    {
      "epoch": 1.6869661242651597,
      "grad_norm": 1.3248919248580933,
      "learning_rate": 3.524513469174518e-05,
      "loss": 0.1317,
      "step": 259700
    },
    {
      "epoch": 1.6876157069083115,
      "grad_norm": 1.0880446434020996,
      "learning_rate": 3.5238224233319186e-05,
      "loss": 0.1293,
      "step": 259800
    },
    {
      "epoch": 1.6882652895514632,
      "grad_norm": 0.9632980227470398,
      "learning_rate": 3.5231313774893196e-05,
      "loss": 0.1335,
      "step": 259900
    },
    {
      "epoch": 1.688914872194615,
      "grad_norm": 1.2035701274871826,
      "learning_rate": 3.5224403316467206e-05,
      "loss": 0.1306,
      "step": 260000
    },
    {
      "epoch": 1.6895644548377668,
      "grad_norm": 1.604853868484497,
      "learning_rate": 3.5217492858041215e-05,
      "loss": 0.1342,
      "step": 260100
    },
    {
      "epoch": 1.6902140374809185,
      "grad_norm": 1.1850175857543945,
      "learning_rate": 3.5210582399615225e-05,
      "loss": 0.1268,
      "step": 260200
    },
    {
      "epoch": 1.6908636201240703,
      "grad_norm": 0.8988083600997925,
      "learning_rate": 3.5203671941189235e-05,
      "loss": 0.1326,
      "step": 260300
    },
    {
      "epoch": 1.6915132027672222,
      "grad_norm": 1.5925129652023315,
      "learning_rate": 3.5196761482763245e-05,
      "loss": 0.1275,
      "step": 260400
    },
    {
      "epoch": 1.6921627854103738,
      "grad_norm": 1.4329379796981812,
      "learning_rate": 3.5189851024337255e-05,
      "loss": 0.1345,
      "step": 260500
    },
    {
      "epoch": 1.6928123680535256,
      "grad_norm": 0.7439415454864502,
      "learning_rate": 3.5182940565911265e-05,
      "loss": 0.1325,
      "step": 260600
    },
    {
      "epoch": 1.6934619506966775,
      "grad_norm": 1.2364670038223267,
      "learning_rate": 3.517603010748527e-05,
      "loss": 0.128,
      "step": 260700
    },
    {
      "epoch": 1.6941115333398291,
      "grad_norm": 1.0773885250091553,
      "learning_rate": 3.516911964905928e-05,
      "loss": 0.1291,
      "step": 260800
    },
    {
      "epoch": 1.694761115982981,
      "grad_norm": 1.272163987159729,
      "learning_rate": 3.516220919063329e-05,
      "loss": 0.1344,
      "step": 260900
    },
    {
      "epoch": 1.6954106986261328,
      "grad_norm": 2.012956142425537,
      "learning_rate": 3.51552987322073e-05,
      "loss": 0.1295,
      "step": 261000
    },
    {
      "epoch": 1.6960602812692844,
      "grad_norm": 1.121588945388794,
      "learning_rate": 3.514838827378131e-05,
      "loss": 0.1261,
      "step": 261100
    },
    {
      "epoch": 1.6967098639124363,
      "grad_norm": 1.2879892587661743,
      "learning_rate": 3.514147781535532e-05,
      "loss": 0.1305,
      "step": 261200
    },
    {
      "epoch": 1.6973594465555881,
      "grad_norm": 1.4938443899154663,
      "learning_rate": 3.513456735692933e-05,
      "loss": 0.1416,
      "step": 261300
    },
    {
      "epoch": 1.6980090291987397,
      "grad_norm": 1.0856211185455322,
      "learning_rate": 3.512765689850334e-05,
      "loss": 0.1272,
      "step": 261400
    },
    {
      "epoch": 1.6986586118418916,
      "grad_norm": 1.351436972618103,
      "learning_rate": 3.5120746440077347e-05,
      "loss": 0.1279,
      "step": 261500
    },
    {
      "epoch": 1.6993081944850434,
      "grad_norm": 1.4428457021713257,
      "learning_rate": 3.5113835981651356e-05,
      "loss": 0.1407,
      "step": 261600
    },
    {
      "epoch": 1.699957777128195,
      "grad_norm": 1.8210498094558716,
      "learning_rate": 3.510692552322536e-05,
      "loss": 0.1351,
      "step": 261700
    },
    {
      "epoch": 1.700607359771347,
      "grad_norm": 1.4102931022644043,
      "learning_rate": 3.510001506479937e-05,
      "loss": 0.1242,
      "step": 261800
    },
    {
      "epoch": 1.7012569424144988,
      "grad_norm": 1.1887145042419434,
      "learning_rate": 3.509310460637338e-05,
      "loss": 0.1291,
      "step": 261900
    },
    {
      "epoch": 1.7019065250576504,
      "grad_norm": 1.8187053203582764,
      "learning_rate": 3.508619414794739e-05,
      "loss": 0.1277,
      "step": 262000
    },
    {
      "epoch": 1.7025561077008022,
      "grad_norm": 1.4541882276535034,
      "learning_rate": 3.50792836895214e-05,
      "loss": 0.1402,
      "step": 262100
    },
    {
      "epoch": 1.703205690343954,
      "grad_norm": 1.6209003925323486,
      "learning_rate": 3.507237323109541e-05,
      "loss": 0.1349,
      "step": 262200
    },
    {
      "epoch": 1.7038552729871057,
      "grad_norm": 1.2665199041366577,
      "learning_rate": 3.506546277266942e-05,
      "loss": 0.1347,
      "step": 262300
    },
    {
      "epoch": 1.7045048556302576,
      "grad_norm": 1.5530489683151245,
      "learning_rate": 3.505855231424343e-05,
      "loss": 0.1303,
      "step": 262400
    },
    {
      "epoch": 1.7051544382734094,
      "grad_norm": 1.4337385892868042,
      "learning_rate": 3.505164185581744e-05,
      "loss": 0.1318,
      "step": 262500
    },
    {
      "epoch": 1.705804020916561,
      "grad_norm": 1.4828376770019531,
      "learning_rate": 3.504473139739144e-05,
      "loss": 0.1341,
      "step": 262600
    },
    {
      "epoch": 1.7064536035597129,
      "grad_norm": 1.1328822374343872,
      "learning_rate": 3.503782093896545e-05,
      "loss": 0.1317,
      "step": 262700
    },
    {
      "epoch": 1.7071031862028647,
      "grad_norm": 1.2058511972427368,
      "learning_rate": 3.503091048053946e-05,
      "loss": 0.1255,
      "step": 262800
    },
    {
      "epoch": 1.7077527688460163,
      "grad_norm": 1.3531088829040527,
      "learning_rate": 3.502400002211347e-05,
      "loss": 0.1302,
      "step": 262900
    },
    {
      "epoch": 1.7084023514891682,
      "grad_norm": 1.5420453548431396,
      "learning_rate": 3.5017089563687474e-05,
      "loss": 0.1306,
      "step": 263000
    },
    {
      "epoch": 1.70905193413232,
      "grad_norm": 1.5070778131484985,
      "learning_rate": 3.5010179105261484e-05,
      "loss": 0.1225,
      "step": 263100
    },
    {
      "epoch": 1.7097015167754717,
      "grad_norm": 1.535433053970337,
      "learning_rate": 3.5003268646835494e-05,
      "loss": 0.1288,
      "step": 263200
    },
    {
      "epoch": 1.7103510994186235,
      "grad_norm": 1.4200390577316284,
      "learning_rate": 3.4996358188409504e-05,
      "loss": 0.1355,
      "step": 263300
    },
    {
      "epoch": 1.7110006820617754,
      "grad_norm": 0.9742660522460938,
      "learning_rate": 3.4989447729983513e-05,
      "loss": 0.1339,
      "step": 263400
    },
    {
      "epoch": 1.711650264704927,
      "grad_norm": 1.1810754537582397,
      "learning_rate": 3.498253727155752e-05,
      "loss": 0.1271,
      "step": 263500
    },
    {
      "epoch": 1.7122998473480788,
      "grad_norm": 1.2418873310089111,
      "learning_rate": 3.4975626813131526e-05,
      "loss": 0.1365,
      "step": 263600
    },
    {
      "epoch": 1.7129494299912307,
      "grad_norm": 1.1948869228363037,
      "learning_rate": 3.4968716354705536e-05,
      "loss": 0.1281,
      "step": 263700
    },
    {
      "epoch": 1.7135990126343823,
      "grad_norm": 1.032791256904602,
      "learning_rate": 3.4961805896279546e-05,
      "loss": 0.1309,
      "step": 263800
    },
    {
      "epoch": 1.7142485952775341,
      "grad_norm": 1.7413580417633057,
      "learning_rate": 3.4954895437853556e-05,
      "loss": 0.1309,
      "step": 263900
    },
    {
      "epoch": 1.714898177920686,
      "grad_norm": 0.9705452919006348,
      "learning_rate": 3.4947984979427566e-05,
      "loss": 0.1298,
      "step": 264000
    },
    {
      "epoch": 1.7155477605638376,
      "grad_norm": 1.1876375675201416,
      "learning_rate": 3.4941074521001576e-05,
      "loss": 0.1275,
      "step": 264100
    },
    {
      "epoch": 1.7161973432069895,
      "grad_norm": 0.8979294896125793,
      "learning_rate": 3.4934164062575585e-05,
      "loss": 0.127,
      "step": 264200
    },
    {
      "epoch": 1.7168469258501413,
      "grad_norm": 1.19566011428833,
      "learning_rate": 3.4927253604149595e-05,
      "loss": 0.1315,
      "step": 264300
    },
    {
      "epoch": 1.717496508493293,
      "grad_norm": 1.194705605506897,
      "learning_rate": 3.4920343145723605e-05,
      "loss": 0.1278,
      "step": 264400
    },
    {
      "epoch": 1.7181460911364448,
      "grad_norm": 1.8822778463363647,
      "learning_rate": 3.491343268729761e-05,
      "loss": 0.1326,
      "step": 264500
    },
    {
      "epoch": 1.7187956737795966,
      "grad_norm": 1.2407584190368652,
      "learning_rate": 3.490652222887162e-05,
      "loss": 0.133,
      "step": 264600
    },
    {
      "epoch": 1.7194452564227483,
      "grad_norm": 0.8649521470069885,
      "learning_rate": 3.489961177044563e-05,
      "loss": 0.1232,
      "step": 264700
    },
    {
      "epoch": 1.7200948390659,
      "grad_norm": 1.176466941833496,
      "learning_rate": 3.489270131201964e-05,
      "loss": 0.1304,
      "step": 264800
    },
    {
      "epoch": 1.720744421709052,
      "grad_norm": 1.3070460557937622,
      "learning_rate": 3.488579085359365e-05,
      "loss": 0.1383,
      "step": 264900
    },
    {
      "epoch": 1.7213940043522036,
      "grad_norm": 1.139215350151062,
      "learning_rate": 3.487888039516766e-05,
      "loss": 0.132,
      "step": 265000
    },
    {
      "epoch": 1.7220435869953556,
      "grad_norm": 1.4187052249908447,
      "learning_rate": 3.487196993674167e-05,
      "loss": 0.1295,
      "step": 265100
    },
    {
      "epoch": 1.7226931696385073,
      "grad_norm": 1.084571361541748,
      "learning_rate": 3.486505947831568e-05,
      "loss": 0.135,
      "step": 265200
    },
    {
      "epoch": 1.723342752281659,
      "grad_norm": 1.1280877590179443,
      "learning_rate": 3.485814901988969e-05,
      "loss": 0.13,
      "step": 265300
    },
    {
      "epoch": 1.723992334924811,
      "grad_norm": 1.3907450437545776,
      "learning_rate": 3.485123856146369e-05,
      "loss": 0.1309,
      "step": 265400
    },
    {
      "epoch": 1.7246419175679626,
      "grad_norm": 0.5935511589050293,
      "learning_rate": 3.48443281030377e-05,
      "loss": 0.1338,
      "step": 265500
    },
    {
      "epoch": 1.7252915002111142,
      "grad_norm": 0.95548015832901,
      "learning_rate": 3.483741764461171e-05,
      "loss": 0.1284,
      "step": 265600
    },
    {
      "epoch": 1.7259410828542663,
      "grad_norm": 1.451306700706482,
      "learning_rate": 3.483050718618572e-05,
      "loss": 0.128,
      "step": 265700
    },
    {
      "epoch": 1.726590665497418,
      "grad_norm": 1.668078064918518,
      "learning_rate": 3.482359672775973e-05,
      "loss": 0.1325,
      "step": 265800
    },
    {
      "epoch": 1.7272402481405695,
      "grad_norm": 0.9414410591125488,
      "learning_rate": 3.481668626933374e-05,
      "loss": 0.1394,
      "step": 265900
    },
    {
      "epoch": 1.7278898307837216,
      "grad_norm": 1.1818724870681763,
      "learning_rate": 3.480977581090775e-05,
      "loss": 0.1267,
      "step": 266000
    },
    {
      "epoch": 1.7285394134268732,
      "grad_norm": 1.0963455438613892,
      "learning_rate": 3.480286535248176e-05,
      "loss": 0.1351,
      "step": 266100
    },
    {
      "epoch": 1.7291889960700249,
      "grad_norm": 1.235176682472229,
      "learning_rate": 3.479595489405577e-05,
      "loss": 0.1295,
      "step": 266200
    },
    {
      "epoch": 1.729838578713177,
      "grad_norm": 0.9516214728355408,
      "learning_rate": 3.478904443562977e-05,
      "loss": 0.1255,
      "step": 266300
    },
    {
      "epoch": 1.7304881613563285,
      "grad_norm": 1.5929774045944214,
      "learning_rate": 3.478213397720378e-05,
      "loss": 0.1298,
      "step": 266400
    },
    {
      "epoch": 1.7311377439994802,
      "grad_norm": 1.236155390739441,
      "learning_rate": 3.477522351877779e-05,
      "loss": 0.1341,
      "step": 266500
    },
    {
      "epoch": 1.7317873266426322,
      "grad_norm": 1.7787232398986816,
      "learning_rate": 3.4768313060351795e-05,
      "loss": 0.125,
      "step": 266600
    },
    {
      "epoch": 1.7324369092857839,
      "grad_norm": 1.367486834526062,
      "learning_rate": 3.4761402601925805e-05,
      "loss": 0.1274,
      "step": 266700
    },
    {
      "epoch": 1.7330864919289357,
      "grad_norm": 1.3262999057769775,
      "learning_rate": 3.4754492143499815e-05,
      "loss": 0.1299,
      "step": 266800
    },
    {
      "epoch": 1.7337360745720876,
      "grad_norm": 1.5256425142288208,
      "learning_rate": 3.4747581685073824e-05,
      "loss": 0.1244,
      "step": 266900
    },
    {
      "epoch": 1.7343856572152392,
      "grad_norm": 1.0332139730453491,
      "learning_rate": 3.4740671226647834e-05,
      "loss": 0.1314,
      "step": 267000
    },
    {
      "epoch": 1.735035239858391,
      "grad_norm": 2.0679521560668945,
      "learning_rate": 3.4733760768221844e-05,
      "loss": 0.1309,
      "step": 267100
    },
    {
      "epoch": 1.7356848225015429,
      "grad_norm": 1.5537534952163696,
      "learning_rate": 3.4726850309795854e-05,
      "loss": 0.124,
      "step": 267200
    },
    {
      "epoch": 1.7363344051446945,
      "grad_norm": 1.1135344505310059,
      "learning_rate": 3.471993985136986e-05,
      "loss": 0.1374,
      "step": 267300
    },
    {
      "epoch": 1.7369839877878464,
      "grad_norm": 1.664317011833191,
      "learning_rate": 3.471302939294387e-05,
      "loss": 0.1336,
      "step": 267400
    },
    {
      "epoch": 1.7376335704309982,
      "grad_norm": 1.0665682554244995,
      "learning_rate": 3.470611893451788e-05,
      "loss": 0.1361,
      "step": 267500
    },
    {
      "epoch": 1.7382831530741498,
      "grad_norm": 0.992601215839386,
      "learning_rate": 3.4699208476091887e-05,
      "loss": 0.1268,
      "step": 267600
    },
    {
      "epoch": 1.7389327357173017,
      "grad_norm": 1.0104044675827026,
      "learning_rate": 3.4692298017665896e-05,
      "loss": 0.1405,
      "step": 267700
    },
    {
      "epoch": 1.7395823183604535,
      "grad_norm": 1.1602983474731445,
      "learning_rate": 3.4685387559239906e-05,
      "loss": 0.1311,
      "step": 267800
    },
    {
      "epoch": 1.7402319010036051,
      "grad_norm": 1.4476104974746704,
      "learning_rate": 3.4678477100813916e-05,
      "loss": 0.1307,
      "step": 267900
    },
    {
      "epoch": 1.740881483646757,
      "grad_norm": 0.9984035491943359,
      "learning_rate": 3.4671566642387926e-05,
      "loss": 0.1282,
      "step": 268000
    },
    {
      "epoch": 1.7415310662899088,
      "grad_norm": 1.0876469612121582,
      "learning_rate": 3.4664656183961936e-05,
      "loss": 0.1273,
      "step": 268100
    },
    {
      "epoch": 1.7421806489330605,
      "grad_norm": 0.9969257712364197,
      "learning_rate": 3.4657745725535946e-05,
      "loss": 0.1338,
      "step": 268200
    },
    {
      "epoch": 1.7428302315762123,
      "grad_norm": 1.3282878398895264,
      "learning_rate": 3.465083526710995e-05,
      "loss": 0.1259,
      "step": 268300
    },
    {
      "epoch": 1.7434798142193642,
      "grad_norm": 1.037182331085205,
      "learning_rate": 3.464392480868396e-05,
      "loss": 0.1379,
      "step": 268400
    },
    {
      "epoch": 1.7441293968625158,
      "grad_norm": 0.9174938797950745,
      "learning_rate": 3.463701435025797e-05,
      "loss": 0.1217,
      "step": 268500
    },
    {
      "epoch": 1.7447789795056676,
      "grad_norm": 1.4023666381835938,
      "learning_rate": 3.463010389183198e-05,
      "loss": 0.1292,
      "step": 268600
    },
    {
      "epoch": 1.7454285621488195,
      "grad_norm": 1.2943795919418335,
      "learning_rate": 3.462319343340599e-05,
      "loss": 0.1279,
      "step": 268700
    },
    {
      "epoch": 1.746078144791971,
      "grad_norm": 1.1308284997940063,
      "learning_rate": 3.461628297498e-05,
      "loss": 0.1281,
      "step": 268800
    },
    {
      "epoch": 1.746727727435123,
      "grad_norm": 1.1212725639343262,
      "learning_rate": 3.460937251655401e-05,
      "loss": 0.1319,
      "step": 268900
    },
    {
      "epoch": 1.7473773100782748,
      "grad_norm": 1.5934760570526123,
      "learning_rate": 3.460246205812802e-05,
      "loss": 0.1284,
      "step": 269000
    },
    {
      "epoch": 1.7480268927214264,
      "grad_norm": 1.557191014289856,
      "learning_rate": 3.459555159970203e-05,
      "loss": 0.1339,
      "step": 269100
    },
    {
      "epoch": 1.7486764753645783,
      "grad_norm": 1.2709240913391113,
      "learning_rate": 3.458864114127603e-05,
      "loss": 0.1313,
      "step": 269200
    },
    {
      "epoch": 1.7493260580077301,
      "grad_norm": 1.4866973161697388,
      "learning_rate": 3.458173068285004e-05,
      "loss": 0.135,
      "step": 269300
    },
    {
      "epoch": 1.7499756406508817,
      "grad_norm": 1.600584864616394,
      "learning_rate": 3.457482022442405e-05,
      "loss": 0.1304,
      "step": 269400
    },
    {
      "epoch": 1.7506252232940336,
      "grad_norm": 0.8344740271568298,
      "learning_rate": 3.456790976599806e-05,
      "loss": 0.131,
      "step": 269500
    },
    {
      "epoch": 1.7512748059371854,
      "grad_norm": 0.9819554686546326,
      "learning_rate": 3.456099930757207e-05,
      "loss": 0.1357,
      "step": 269600
    },
    {
      "epoch": 1.751924388580337,
      "grad_norm": 2.216273784637451,
      "learning_rate": 3.455408884914608e-05,
      "loss": 0.1296,
      "step": 269700
    },
    {
      "epoch": 1.752573971223489,
      "grad_norm": 0.9926331639289856,
      "learning_rate": 3.454717839072008e-05,
      "loss": 0.1249,
      "step": 269800
    },
    {
      "epoch": 1.7532235538666407,
      "grad_norm": 1.69917893409729,
      "learning_rate": 3.454026793229409e-05,
      "loss": 0.1284,
      "step": 269900
    },
    {
      "epoch": 1.7538731365097924,
      "grad_norm": 0.9043379426002502,
      "learning_rate": 3.45333574738681e-05,
      "loss": 0.1232,
      "step": 270000
    },
    {
      "epoch": 1.7545227191529442,
      "grad_norm": 0.8592157959938049,
      "learning_rate": 3.452644701544211e-05,
      "loss": 0.1288,
      "step": 270100
    },
    {
      "epoch": 1.755172301796096,
      "grad_norm": 1.4083232879638672,
      "learning_rate": 3.4519536557016116e-05,
      "loss": 0.1272,
      "step": 270200
    },
    {
      "epoch": 1.7558218844392477,
      "grad_norm": 1.4972320795059204,
      "learning_rate": 3.4512626098590125e-05,
      "loss": 0.1334,
      "step": 270300
    },
    {
      "epoch": 1.7564714670823995,
      "grad_norm": 1.4856563806533813,
      "learning_rate": 3.4505715640164135e-05,
      "loss": 0.1282,
      "step": 270400
    },
    {
      "epoch": 1.7571210497255514,
      "grad_norm": 1.1563818454742432,
      "learning_rate": 3.4498805181738145e-05,
      "loss": 0.1306,
      "step": 270500
    },
    {
      "epoch": 1.757770632368703,
      "grad_norm": 1.3352348804473877,
      "learning_rate": 3.4491894723312155e-05,
      "loss": 0.1351,
      "step": 270600
    },
    {
      "epoch": 1.7584202150118549,
      "grad_norm": 1.0802884101867676,
      "learning_rate": 3.4484984264886165e-05,
      "loss": 0.1291,
      "step": 270700
    },
    {
      "epoch": 1.7590697976550067,
      "grad_norm": 1.526436686515808,
      "learning_rate": 3.4478073806460175e-05,
      "loss": 0.1287,
      "step": 270800
    },
    {
      "epoch": 1.7597193802981583,
      "grad_norm": 1.0311306715011597,
      "learning_rate": 3.4471163348034185e-05,
      "loss": 0.1292,
      "step": 270900
    },
    {
      "epoch": 1.7603689629413102,
      "grad_norm": 1.1810170412063599,
      "learning_rate": 3.4464252889608194e-05,
      "loss": 0.1304,
      "step": 271000
    },
    {
      "epoch": 1.761018545584462,
      "grad_norm": 1.37248694896698,
      "learning_rate": 3.44573424311822e-05,
      "loss": 0.1265,
      "step": 271100
    },
    {
      "epoch": 1.7616681282276136,
      "grad_norm": 1.4001574516296387,
      "learning_rate": 3.445043197275621e-05,
      "loss": 0.1334,
      "step": 271200
    },
    {
      "epoch": 1.7623177108707655,
      "grad_norm": 0.9630841016769409,
      "learning_rate": 3.444352151433022e-05,
      "loss": 0.1277,
      "step": 271300
    },
    {
      "epoch": 1.7629672935139173,
      "grad_norm": 1.0720187425613403,
      "learning_rate": 3.443661105590423e-05,
      "loss": 0.1254,
      "step": 271400
    },
    {
      "epoch": 1.763616876157069,
      "grad_norm": 1.2379428148269653,
      "learning_rate": 3.442970059747824e-05,
      "loss": 0.1357,
      "step": 271500
    },
    {
      "epoch": 1.7642664588002208,
      "grad_norm": 1.1952980756759644,
      "learning_rate": 3.442279013905225e-05,
      "loss": 0.1313,
      "step": 271600
    },
    {
      "epoch": 1.7649160414433727,
      "grad_norm": 1.2707853317260742,
      "learning_rate": 3.4415879680626257e-05,
      "loss": 0.1371,
      "step": 271700
    },
    {
      "epoch": 1.7655656240865243,
      "grad_norm": 1.2054872512817383,
      "learning_rate": 3.4408969222200266e-05,
      "loss": 0.1308,
      "step": 271800
    },
    {
      "epoch": 1.7662152067296761,
      "grad_norm": 1.000402808189392,
      "learning_rate": 3.4402058763774276e-05,
      "loss": 0.1229,
      "step": 271900
    },
    {
      "epoch": 1.766864789372828,
      "grad_norm": 1.6986403465270996,
      "learning_rate": 3.439514830534828e-05,
      "loss": 0.1296,
      "step": 272000
    },
    {
      "epoch": 1.7675143720159796,
      "grad_norm": 1.0509095191955566,
      "learning_rate": 3.438823784692229e-05,
      "loss": 0.1286,
      "step": 272100
    },
    {
      "epoch": 1.7681639546591315,
      "grad_norm": 1.4172159433364868,
      "learning_rate": 3.43813273884963e-05,
      "loss": 0.1282,
      "step": 272200
    },
    {
      "epoch": 1.7688135373022833,
      "grad_norm": 0.848465621471405,
      "learning_rate": 3.437441693007031e-05,
      "loss": 0.1257,
      "step": 272300
    },
    {
      "epoch": 1.769463119945435,
      "grad_norm": 1.410956621170044,
      "learning_rate": 3.436750647164432e-05,
      "loss": 0.1264,
      "step": 272400
    },
    {
      "epoch": 1.770112702588587,
      "grad_norm": 1.2822107076644897,
      "learning_rate": 3.436059601321833e-05,
      "loss": 0.1256,
      "step": 272500
    },
    {
      "epoch": 1.7707622852317386,
      "grad_norm": 1.1642909049987793,
      "learning_rate": 3.435368555479234e-05,
      "loss": 0.131,
      "step": 272600
    },
    {
      "epoch": 1.7714118678748902,
      "grad_norm": 1.337979793548584,
      "learning_rate": 3.434677509636635e-05,
      "loss": 0.1272,
      "step": 272700
    },
    {
      "epoch": 1.7720614505180423,
      "grad_norm": 1.4357606172561646,
      "learning_rate": 3.433986463794036e-05,
      "loss": 0.1273,
      "step": 272800
    },
    {
      "epoch": 1.772711033161194,
      "grad_norm": 1.2609710693359375,
      "learning_rate": 3.433295417951437e-05,
      "loss": 0.1312,
      "step": 272900
    },
    {
      "epoch": 1.7733606158043456,
      "grad_norm": 0.9770748019218445,
      "learning_rate": 3.432604372108837e-05,
      "loss": 0.126,
      "step": 273000
    },
    {
      "epoch": 1.7740101984474976,
      "grad_norm": 1.001654863357544,
      "learning_rate": 3.431913326266238e-05,
      "loss": 0.1288,
      "step": 273100
    },
    {
      "epoch": 1.7746597810906493,
      "grad_norm": 1.627440333366394,
      "learning_rate": 3.431222280423639e-05,
      "loss": 0.1308,
      "step": 273200
    },
    {
      "epoch": 1.7753093637338009,
      "grad_norm": 1.25196373462677,
      "learning_rate": 3.43053123458104e-05,
      "loss": 0.1203,
      "step": 273300
    },
    {
      "epoch": 1.775958946376953,
      "grad_norm": 1.231143593788147,
      "learning_rate": 3.4298401887384404e-05,
      "loss": 0.126,
      "step": 273400
    },
    {
      "epoch": 1.7766085290201046,
      "grad_norm": 1.2079271078109741,
      "learning_rate": 3.4291491428958414e-05,
      "loss": 0.1302,
      "step": 273500
    },
    {
      "epoch": 1.7772581116632562,
      "grad_norm": 1.5684982538223267,
      "learning_rate": 3.4284580970532423e-05,
      "loss": 0.1305,
      "step": 273600
    },
    {
      "epoch": 1.7779076943064083,
      "grad_norm": 1.061091423034668,
      "learning_rate": 3.427767051210643e-05,
      "loss": 0.1348,
      "step": 273700
    },
    {
      "epoch": 1.77855727694956,
      "grad_norm": 1.146504282951355,
      "learning_rate": 3.427076005368044e-05,
      "loss": 0.1237,
      "step": 273800
    },
    {
      "epoch": 1.7792068595927115,
      "grad_norm": 1.1368484497070312,
      "learning_rate": 3.4263849595254446e-05,
      "loss": 0.124,
      "step": 273900
    },
    {
      "epoch": 1.7798564422358636,
      "grad_norm": 1.2125067710876465,
      "learning_rate": 3.4256939136828456e-05,
      "loss": 0.1257,
      "step": 274000
    },
    {
      "epoch": 1.7805060248790152,
      "grad_norm": 0.8792715072631836,
      "learning_rate": 3.4250028678402466e-05,
      "loss": 0.1238,
      "step": 274100
    },
    {
      "epoch": 1.781155607522167,
      "grad_norm": 1.3083443641662598,
      "learning_rate": 3.4243118219976476e-05,
      "loss": 0.1341,
      "step": 274200
    },
    {
      "epoch": 1.781805190165319,
      "grad_norm": 1.5018653869628906,
      "learning_rate": 3.4236207761550486e-05,
      "loss": 0.1263,
      "step": 274300
    },
    {
      "epoch": 1.7824547728084705,
      "grad_norm": 1.285125970840454,
      "learning_rate": 3.4229297303124496e-05,
      "loss": 0.1318,
      "step": 274400
    },
    {
      "epoch": 1.7831043554516224,
      "grad_norm": 1.1574559211730957,
      "learning_rate": 3.4222386844698505e-05,
      "loss": 0.1294,
      "step": 274500
    },
    {
      "epoch": 1.7837539380947742,
      "grad_norm": 1.0773168802261353,
      "learning_rate": 3.4215476386272515e-05,
      "loss": 0.1259,
      "step": 274600
    },
    {
      "epoch": 1.7844035207379259,
      "grad_norm": 0.9070599675178528,
      "learning_rate": 3.4208565927846525e-05,
      "loss": 0.13,
      "step": 274700
    },
    {
      "epoch": 1.7850531033810777,
      "grad_norm": 1.1549372673034668,
      "learning_rate": 3.420165546942053e-05,
      "loss": 0.1338,
      "step": 274800
    },
    {
      "epoch": 1.7857026860242295,
      "grad_norm": 1.2719310522079468,
      "learning_rate": 3.419474501099454e-05,
      "loss": 0.1258,
      "step": 274900
    },
    {
      "epoch": 1.7863522686673812,
      "grad_norm": 1.5326483249664307,
      "learning_rate": 3.418783455256855e-05,
      "loss": 0.1208,
      "step": 275000
    },
    {
      "epoch": 1.787001851310533,
      "grad_norm": 1.4012407064437866,
      "learning_rate": 3.418092409414256e-05,
      "loss": 0.1231,
      "step": 275100
    },
    {
      "epoch": 1.7876514339536849,
      "grad_norm": 1.3332566022872925,
      "learning_rate": 3.417401363571657e-05,
      "loss": 0.1299,
      "step": 275200
    },
    {
      "epoch": 1.7883010165968365,
      "grad_norm": 1.1549267768859863,
      "learning_rate": 3.416710317729058e-05,
      "loss": 0.1288,
      "step": 275300
    },
    {
      "epoch": 1.7889505992399883,
      "grad_norm": 1.1670584678649902,
      "learning_rate": 3.416019271886459e-05,
      "loss": 0.1264,
      "step": 275400
    },
    {
      "epoch": 1.7896001818831402,
      "grad_norm": 1.5316071510314941,
      "learning_rate": 3.41532822604386e-05,
      "loss": 0.1256,
      "step": 275500
    },
    {
      "epoch": 1.7902497645262918,
      "grad_norm": 1.237430453300476,
      "learning_rate": 3.414637180201261e-05,
      "loss": 0.1265,
      "step": 275600
    },
    {
      "epoch": 1.7908993471694437,
      "grad_norm": 1.0752055644989014,
      "learning_rate": 3.413946134358662e-05,
      "loss": 0.1264,
      "step": 275700
    },
    {
      "epoch": 1.7915489298125955,
      "grad_norm": 0.9934261441230774,
      "learning_rate": 3.413255088516062e-05,
      "loss": 0.1294,
      "step": 275800
    },
    {
      "epoch": 1.7921985124557471,
      "grad_norm": 1.2840251922607422,
      "learning_rate": 3.412564042673463e-05,
      "loss": 0.1294,
      "step": 275900
    },
    {
      "epoch": 1.792848095098899,
      "grad_norm": 1.0770082473754883,
      "learning_rate": 3.411872996830864e-05,
      "loss": 0.1254,
      "step": 276000
    },
    {
      "epoch": 1.7934976777420508,
      "grad_norm": 1.4047101736068726,
      "learning_rate": 3.411181950988265e-05,
      "loss": 0.1368,
      "step": 276100
    },
    {
      "epoch": 1.7941472603852024,
      "grad_norm": 1.0301471948623657,
      "learning_rate": 3.410490905145666e-05,
      "loss": 0.1232,
      "step": 276200
    },
    {
      "epoch": 1.7947968430283543,
      "grad_norm": 1.2577632665634155,
      "learning_rate": 3.409799859303067e-05,
      "loss": 0.1204,
      "step": 276300
    },
    {
      "epoch": 1.7954464256715061,
      "grad_norm": 1.5464649200439453,
      "learning_rate": 3.409108813460468e-05,
      "loss": 0.131,
      "step": 276400
    },
    {
      "epoch": 1.7960960083146578,
      "grad_norm": 0.930109441280365,
      "learning_rate": 3.408417767617869e-05,
      "loss": 0.1279,
      "step": 276500
    },
    {
      "epoch": 1.7967455909578096,
      "grad_norm": 1.133017897605896,
      "learning_rate": 3.407726721775269e-05,
      "loss": 0.1227,
      "step": 276600
    },
    {
      "epoch": 1.7973951736009615,
      "grad_norm": 0.9886115789413452,
      "learning_rate": 3.40703567593267e-05,
      "loss": 0.1231,
      "step": 276700
    },
    {
      "epoch": 1.798044756244113,
      "grad_norm": 0.8396825790405273,
      "learning_rate": 3.406344630090071e-05,
      "loss": 0.1294,
      "step": 276800
    },
    {
      "epoch": 1.798694338887265,
      "grad_norm": 0.9120609760284424,
      "learning_rate": 3.405653584247472e-05,
      "loss": 0.1326,
      "step": 276900
    },
    {
      "epoch": 1.7993439215304168,
      "grad_norm": 1.5962586402893066,
      "learning_rate": 3.4049625384048725e-05,
      "loss": 0.1252,
      "step": 277000
    },
    {
      "epoch": 1.7999935041735684,
      "grad_norm": 0.9536313414573669,
      "learning_rate": 3.4042714925622734e-05,
      "loss": 0.1248,
      "step": 277100
    },
    {
      "epoch": 1.8006430868167203,
      "grad_norm": 1.6518449783325195,
      "learning_rate": 3.4035804467196744e-05,
      "loss": 0.1263,
      "step": 277200
    },
    {
      "epoch": 1.801292669459872,
      "grad_norm": 1.0122750997543335,
      "learning_rate": 3.4028894008770754e-05,
      "loss": 0.1325,
      "step": 277300
    },
    {
      "epoch": 1.8019422521030237,
      "grad_norm": 1.2695026397705078,
      "learning_rate": 3.4021983550344764e-05,
      "loss": 0.1343,
      "step": 277400
    },
    {
      "epoch": 1.8025918347461756,
      "grad_norm": 1.0880693197250366,
      "learning_rate": 3.4015073091918774e-05,
      "loss": 0.123,
      "step": 277500
    },
    {
      "epoch": 1.8032414173893274,
      "grad_norm": 1.2944092750549316,
      "learning_rate": 3.4008162633492784e-05,
      "loss": 0.1333,
      "step": 277600
    },
    {
      "epoch": 1.803891000032479,
      "grad_norm": 1.0045017004013062,
      "learning_rate": 3.400125217506679e-05,
      "loss": 0.1227,
      "step": 277700
    },
    {
      "epoch": 1.8045405826756309,
      "grad_norm": 1.5896286964416504,
      "learning_rate": 3.3994341716640797e-05,
      "loss": 0.1266,
      "step": 277800
    },
    {
      "epoch": 1.8051901653187827,
      "grad_norm": 1.5534179210662842,
      "learning_rate": 3.3987431258214806e-05,
      "loss": 0.1259,
      "step": 277900
    },
    {
      "epoch": 1.8058397479619344,
      "grad_norm": 1.0612061023712158,
      "learning_rate": 3.3980520799788816e-05,
      "loss": 0.1267,
      "step": 278000
    },
    {
      "epoch": 1.8064893306050862,
      "grad_norm": 1.946711778640747,
      "learning_rate": 3.3973610341362826e-05,
      "loss": 0.1232,
      "step": 278100
    },
    {
      "epoch": 1.807138913248238,
      "grad_norm": 1.5110321044921875,
      "learning_rate": 3.3966699882936836e-05,
      "loss": 0.128,
      "step": 278200
    },
    {
      "epoch": 1.8077884958913897,
      "grad_norm": 1.238221287727356,
      "learning_rate": 3.3959789424510846e-05,
      "loss": 0.1328,
      "step": 278300
    },
    {
      "epoch": 1.8084380785345415,
      "grad_norm": 1.4535810947418213,
      "learning_rate": 3.3952878966084856e-05,
      "loss": 0.1268,
      "step": 278400
    },
    {
      "epoch": 1.8090876611776934,
      "grad_norm": 1.1950663328170776,
      "learning_rate": 3.3945968507658866e-05,
      "loss": 0.1315,
      "step": 278500
    },
    {
      "epoch": 1.809737243820845,
      "grad_norm": 0.9999556541442871,
      "learning_rate": 3.393905804923287e-05,
      "loss": 0.1263,
      "step": 278600
    },
    {
      "epoch": 1.8103868264639968,
      "grad_norm": 1.043136477470398,
      "learning_rate": 3.393214759080688e-05,
      "loss": 0.134,
      "step": 278700
    },
    {
      "epoch": 1.8110364091071487,
      "grad_norm": 1.2929965257644653,
      "learning_rate": 3.392523713238089e-05,
      "loss": 0.1212,
      "step": 278800
    },
    {
      "epoch": 1.8116859917503003,
      "grad_norm": 1.7357548475265503,
      "learning_rate": 3.39183266739549e-05,
      "loss": 0.1227,
      "step": 278900
    },
    {
      "epoch": 1.8123355743934522,
      "grad_norm": 1.1725599765777588,
      "learning_rate": 3.391141621552891e-05,
      "loss": 0.1265,
      "step": 279000
    },
    {
      "epoch": 1.812985157036604,
      "grad_norm": 1.4015116691589355,
      "learning_rate": 3.390450575710292e-05,
      "loss": 0.1291,
      "step": 279100
    },
    {
      "epoch": 1.8136347396797556,
      "grad_norm": 0.9600167274475098,
      "learning_rate": 3.389759529867693e-05,
      "loss": 0.1275,
      "step": 279200
    },
    {
      "epoch": 1.8142843223229075,
      "grad_norm": 1.1260782480239868,
      "learning_rate": 3.389068484025094e-05,
      "loss": 0.1256,
      "step": 279300
    },
    {
      "epoch": 1.8149339049660593,
      "grad_norm": 1.1756283044815063,
      "learning_rate": 3.388377438182495e-05,
      "loss": 0.1286,
      "step": 279400
    },
    {
      "epoch": 1.815583487609211,
      "grad_norm": 1.3210114240646362,
      "learning_rate": 3.387686392339895e-05,
      "loss": 0.1321,
      "step": 279500
    },
    {
      "epoch": 1.8162330702523628,
      "grad_norm": 1.6929707527160645,
      "learning_rate": 3.386995346497296e-05,
      "loss": 0.1224,
      "step": 279600
    },
    {
      "epoch": 1.8168826528955146,
      "grad_norm": 1.7265058755874634,
      "learning_rate": 3.386304300654697e-05,
      "loss": 0.1234,
      "step": 279700
    },
    {
      "epoch": 1.8175322355386663,
      "grad_norm": 1.1811474561691284,
      "learning_rate": 3.385613254812098e-05,
      "loss": 0.1215,
      "step": 279800
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 1.0395026206970215,
      "learning_rate": 3.384922208969499e-05,
      "loss": 0.1291,
      "step": 279900
    },
    {
      "epoch": 1.81883140082497,
      "grad_norm": 0.8294277191162109,
      "learning_rate": 3.3842311631269e-05,
      "loss": 0.1288,
      "step": 280000
    },
    {
      "epoch": 1.8194809834681216,
      "grad_norm": 1.1682934761047363,
      "learning_rate": 3.383540117284301e-05,
      "loss": 0.1223,
      "step": 280100
    },
    {
      "epoch": 1.8201305661112737,
      "grad_norm": 1.1941256523132324,
      "learning_rate": 3.382849071441701e-05,
      "loss": 0.1275,
      "step": 280200
    },
    {
      "epoch": 1.8207801487544253,
      "grad_norm": 1.2053836584091187,
      "learning_rate": 3.382158025599102e-05,
      "loss": 0.1298,
      "step": 280300
    },
    {
      "epoch": 1.821429731397577,
      "grad_norm": 1.2345231771469116,
      "learning_rate": 3.381466979756503e-05,
      "loss": 0.1208,
      "step": 280400
    },
    {
      "epoch": 1.822079314040729,
      "grad_norm": 0.9972127676010132,
      "learning_rate": 3.380775933913904e-05,
      "loss": 0.132,
      "step": 280500
    },
    {
      "epoch": 1.8227288966838806,
      "grad_norm": 1.2552900314331055,
      "learning_rate": 3.3800848880713045e-05,
      "loss": 0.1261,
      "step": 280600
    },
    {
      "epoch": 1.8233784793270322,
      "grad_norm": 0.8908994197845459,
      "learning_rate": 3.3793938422287055e-05,
      "loss": 0.1239,
      "step": 280700
    },
    {
      "epoch": 1.8240280619701843,
      "grad_norm": 1.235754370689392,
      "learning_rate": 3.3787027963861065e-05,
      "loss": 0.1318,
      "step": 280800
    },
    {
      "epoch": 1.824677644613336,
      "grad_norm": 1.4651758670806885,
      "learning_rate": 3.3780117505435075e-05,
      "loss": 0.1345,
      "step": 280900
    },
    {
      "epoch": 1.8253272272564876,
      "grad_norm": 1.2074488401412964,
      "learning_rate": 3.3773207047009085e-05,
      "loss": 0.1268,
      "step": 281000
    },
    {
      "epoch": 1.8259768098996396,
      "grad_norm": 1.6913634538650513,
      "learning_rate": 3.3766296588583095e-05,
      "loss": 0.1287,
      "step": 281100
    },
    {
      "epoch": 1.8266263925427912,
      "grad_norm": 1.4307096004486084,
      "learning_rate": 3.3759386130157104e-05,
      "loss": 0.1307,
      "step": 281200
    },
    {
      "epoch": 1.8272759751859429,
      "grad_norm": 1.0284264087677002,
      "learning_rate": 3.3752475671731114e-05,
      "loss": 0.1295,
      "step": 281300
    },
    {
      "epoch": 1.827925557829095,
      "grad_norm": 1.3641650676727295,
      "learning_rate": 3.374556521330512e-05,
      "loss": 0.1308,
      "step": 281400
    },
    {
      "epoch": 1.8285751404722466,
      "grad_norm": 1.5846824645996094,
      "learning_rate": 3.373865475487913e-05,
      "loss": 0.1234,
      "step": 281500
    },
    {
      "epoch": 1.8292247231153984,
      "grad_norm": 1.3534603118896484,
      "learning_rate": 3.373174429645314e-05,
      "loss": 0.1345,
      "step": 281600
    },
    {
      "epoch": 1.8298743057585503,
      "grad_norm": 1.4109152555465698,
      "learning_rate": 3.372483383802715e-05,
      "loss": 0.13,
      "step": 281700
    },
    {
      "epoch": 1.8305238884017019,
      "grad_norm": 1.6707762479782104,
      "learning_rate": 3.371792337960116e-05,
      "loss": 0.1228,
      "step": 281800
    },
    {
      "epoch": 1.8311734710448537,
      "grad_norm": 1.003883719444275,
      "learning_rate": 3.371101292117517e-05,
      "loss": 0.1254,
      "step": 281900
    },
    {
      "epoch": 1.8318230536880056,
      "grad_norm": 1.0046606063842773,
      "learning_rate": 3.3704102462749176e-05,
      "loss": 0.1226,
      "step": 282000
    },
    {
      "epoch": 1.8324726363311572,
      "grad_norm": 1.292026162147522,
      "learning_rate": 3.3697192004323186e-05,
      "loss": 0.1266,
      "step": 282100
    },
    {
      "epoch": 1.833122218974309,
      "grad_norm": 2.1548361778259277,
      "learning_rate": 3.3690281545897196e-05,
      "loss": 0.1272,
      "step": 282200
    },
    {
      "epoch": 1.833771801617461,
      "grad_norm": 1.4129775762557983,
      "learning_rate": 3.3683371087471206e-05,
      "loss": 0.1259,
      "step": 282300
    },
    {
      "epoch": 1.8344213842606125,
      "grad_norm": 0.8045793771743774,
      "learning_rate": 3.367646062904521e-05,
      "loss": 0.1284,
      "step": 282400
    },
    {
      "epoch": 1.8350709669037644,
      "grad_norm": 1.1795763969421387,
      "learning_rate": 3.366955017061922e-05,
      "loss": 0.1284,
      "step": 282500
    },
    {
      "epoch": 1.8357205495469162,
      "grad_norm": 1.3910325765609741,
      "learning_rate": 3.366263971219323e-05,
      "loss": 0.1305,
      "step": 282600
    },
    {
      "epoch": 1.8363701321900678,
      "grad_norm": 1.2823147773742676,
      "learning_rate": 3.365572925376724e-05,
      "loss": 0.1266,
      "step": 282700
    },
    {
      "epoch": 1.8370197148332197,
      "grad_norm": 1.3902329206466675,
      "learning_rate": 3.364881879534125e-05,
      "loss": 0.1274,
      "step": 282800
    },
    {
      "epoch": 1.8376692974763715,
      "grad_norm": 1.016780138015747,
      "learning_rate": 3.364190833691526e-05,
      "loss": 0.1234,
      "step": 282900
    },
    {
      "epoch": 1.8383188801195232,
      "grad_norm": 1.7228100299835205,
      "learning_rate": 3.363499787848927e-05,
      "loss": 0.1235,
      "step": 283000
    },
    {
      "epoch": 1.838968462762675,
      "grad_norm": 1.4236725568771362,
      "learning_rate": 3.362808742006328e-05,
      "loss": 0.1302,
      "step": 283100
    },
    {
      "epoch": 1.8396180454058269,
      "grad_norm": 1.3667539358139038,
      "learning_rate": 3.362117696163729e-05,
      "loss": 0.1256,
      "step": 283200
    },
    {
      "epoch": 1.8402676280489785,
      "grad_norm": 1.2212079763412476,
      "learning_rate": 3.361426650321129e-05,
      "loss": 0.1247,
      "step": 283300
    },
    {
      "epoch": 1.8409172106921303,
      "grad_norm": 1.2181674242019653,
      "learning_rate": 3.36073560447853e-05,
      "loss": 0.1237,
      "step": 283400
    },
    {
      "epoch": 1.8415667933352822,
      "grad_norm": 1.042803406715393,
      "learning_rate": 3.360044558635931e-05,
      "loss": 0.1197,
      "step": 283500
    },
    {
      "epoch": 1.8422163759784338,
      "grad_norm": 0.8643117547035217,
      "learning_rate": 3.359353512793332e-05,
      "loss": 0.1266,
      "step": 283600
    },
    {
      "epoch": 1.8428659586215856,
      "grad_norm": 0.810644805431366,
      "learning_rate": 3.358662466950733e-05,
      "loss": 0.1255,
      "step": 283700
    },
    {
      "epoch": 1.8435155412647375,
      "grad_norm": 1.6557159423828125,
      "learning_rate": 3.3579714211081333e-05,
      "loss": 0.1194,
      "step": 283800
    },
    {
      "epoch": 1.8441651239078891,
      "grad_norm": 2.243577718734741,
      "learning_rate": 3.357280375265534e-05,
      "loss": 0.1343,
      "step": 283900
    },
    {
      "epoch": 1.844814706551041,
      "grad_norm": 1.1485929489135742,
      "learning_rate": 3.356589329422935e-05,
      "loss": 0.133,
      "step": 284000
    },
    {
      "epoch": 1.8454642891941928,
      "grad_norm": 1.3321502208709717,
      "learning_rate": 3.355898283580336e-05,
      "loss": 0.1245,
      "step": 284100
    },
    {
      "epoch": 1.8461138718373444,
      "grad_norm": 1.136047601699829,
      "learning_rate": 3.3552072377377366e-05,
      "loss": 0.1259,
      "step": 284200
    },
    {
      "epoch": 1.8467634544804963,
      "grad_norm": 0.7126206159591675,
      "learning_rate": 3.3545161918951376e-05,
      "loss": 0.1212,
      "step": 284300
    },
    {
      "epoch": 1.8474130371236481,
      "grad_norm": 0.8562908172607422,
      "learning_rate": 3.3538251460525386e-05,
      "loss": 0.1315,
      "step": 284400
    },
    {
      "epoch": 1.8480626197667998,
      "grad_norm": 1.0925897359848022,
      "learning_rate": 3.3531341002099396e-05,
      "loss": 0.1204,
      "step": 284500
    },
    {
      "epoch": 1.8487122024099516,
      "grad_norm": 1.0783593654632568,
      "learning_rate": 3.3524430543673406e-05,
      "loss": 0.1278,
      "step": 284600
    },
    {
      "epoch": 1.8493617850531034,
      "grad_norm": 1.1271545886993408,
      "learning_rate": 3.3517520085247415e-05,
      "loss": 0.1256,
      "step": 284700
    },
    {
      "epoch": 1.850011367696255,
      "grad_norm": 1.6501933336257935,
      "learning_rate": 3.3510609626821425e-05,
      "loss": 0.1276,
      "step": 284800
    },
    {
      "epoch": 1.850660950339407,
      "grad_norm": 0.9057088494300842,
      "learning_rate": 3.3503699168395435e-05,
      "loss": 0.1292,
      "step": 284900
    },
    {
      "epoch": 1.8513105329825588,
      "grad_norm": 1.0068743228912354,
      "learning_rate": 3.3496788709969445e-05,
      "loss": 0.1253,
      "step": 285000
    },
    {
      "epoch": 1.8519601156257104,
      "grad_norm": 2.137169122695923,
      "learning_rate": 3.3489878251543455e-05,
      "loss": 0.1249,
      "step": 285100
    },
    {
      "epoch": 1.8526096982688622,
      "grad_norm": 0.8198175430297852,
      "learning_rate": 3.348296779311746e-05,
      "loss": 0.118,
      "step": 285200
    },
    {
      "epoch": 1.853259280912014,
      "grad_norm": 1.4081060886383057,
      "learning_rate": 3.347605733469147e-05,
      "loss": 0.1288,
      "step": 285300
    },
    {
      "epoch": 1.8539088635551657,
      "grad_norm": 1.052826166152954,
      "learning_rate": 3.346914687626548e-05,
      "loss": 0.131,
      "step": 285400
    },
    {
      "epoch": 1.8545584461983176,
      "grad_norm": 1.0258426666259766,
      "learning_rate": 3.346223641783949e-05,
      "loss": 0.1268,
      "step": 285500
    },
    {
      "epoch": 1.8552080288414694,
      "grad_norm": 1.0112048387527466,
      "learning_rate": 3.34553259594135e-05,
      "loss": 0.1359,
      "step": 285600
    },
    {
      "epoch": 1.855857611484621,
      "grad_norm": 1.6892828941345215,
      "learning_rate": 3.344841550098751e-05,
      "loss": 0.1234,
      "step": 285700
    },
    {
      "epoch": 1.8565071941277729,
      "grad_norm": 1.0114651918411255,
      "learning_rate": 3.344150504256152e-05,
      "loss": 0.1231,
      "step": 285800
    },
    {
      "epoch": 1.8571567767709247,
      "grad_norm": 1.1742674112319946,
      "learning_rate": 3.343459458413553e-05,
      "loss": 0.1207,
      "step": 285900
    },
    {
      "epoch": 1.8578063594140763,
      "grad_norm": 0.9221954345703125,
      "learning_rate": 3.342768412570954e-05,
      "loss": 0.1296,
      "step": 286000
    },
    {
      "epoch": 1.8584559420572282,
      "grad_norm": 1.2934422492980957,
      "learning_rate": 3.342077366728354e-05,
      "loss": 0.1258,
      "step": 286100
    },
    {
      "epoch": 1.85910552470038,
      "grad_norm": 1.3359612226486206,
      "learning_rate": 3.341386320885755e-05,
      "loss": 0.1261,
      "step": 286200
    },
    {
      "epoch": 1.8597551073435317,
      "grad_norm": 1.5614848136901855,
      "learning_rate": 3.340695275043156e-05,
      "loss": 0.1289,
      "step": 286300
    },
    {
      "epoch": 1.8604046899866835,
      "grad_norm": 0.9641402959823608,
      "learning_rate": 3.340004229200557e-05,
      "loss": 0.1257,
      "step": 286400
    },
    {
      "epoch": 1.8610542726298354,
      "grad_norm": 1.01161789894104,
      "learning_rate": 3.339313183357958e-05,
      "loss": 0.1241,
      "step": 286500
    },
    {
      "epoch": 1.861703855272987,
      "grad_norm": 0.9614704847335815,
      "learning_rate": 3.338622137515359e-05,
      "loss": 0.1278,
      "step": 286600
    },
    {
      "epoch": 1.8623534379161388,
      "grad_norm": 1.4754196405410767,
      "learning_rate": 3.33793109167276e-05,
      "loss": 0.127,
      "step": 286700
    },
    {
      "epoch": 1.8630030205592907,
      "grad_norm": 1.0253586769104004,
      "learning_rate": 3.337240045830161e-05,
      "loss": 0.128,
      "step": 286800
    },
    {
      "epoch": 1.8636526032024423,
      "grad_norm": 2.0035829544067383,
      "learning_rate": 3.336548999987562e-05,
      "loss": 0.1239,
      "step": 286900
    },
    {
      "epoch": 1.8643021858455942,
      "grad_norm": 0.7991671562194824,
      "learning_rate": 3.335857954144962e-05,
      "loss": 0.1212,
      "step": 287000
    },
    {
      "epoch": 1.864951768488746,
      "grad_norm": 1.5320271253585815,
      "learning_rate": 3.335166908302363e-05,
      "loss": 0.1282,
      "step": 287100
    },
    {
      "epoch": 1.8656013511318976,
      "grad_norm": 1.4443724155426025,
      "learning_rate": 3.334475862459764e-05,
      "loss": 0.1263,
      "step": 287200
    },
    {
      "epoch": 1.8662509337750497,
      "grad_norm": 1.4834319353103638,
      "learning_rate": 3.333784816617165e-05,
      "loss": 0.1279,
      "step": 287300
    },
    {
      "epoch": 1.8669005164182013,
      "grad_norm": 1.5652344226837158,
      "learning_rate": 3.3330937707745654e-05,
      "loss": 0.123,
      "step": 287400
    },
    {
      "epoch": 1.867550099061353,
      "grad_norm": 1.6094683408737183,
      "learning_rate": 3.3324027249319664e-05,
      "loss": 0.1234,
      "step": 287500
    },
    {
      "epoch": 1.868199681704505,
      "grad_norm": 1.1879382133483887,
      "learning_rate": 3.3317116790893674e-05,
      "loss": 0.1264,
      "step": 287600
    },
    {
      "epoch": 1.8688492643476566,
      "grad_norm": 1.2326297760009766,
      "learning_rate": 3.3310206332467684e-05,
      "loss": 0.1356,
      "step": 287700
    },
    {
      "epoch": 1.8694988469908083,
      "grad_norm": 1.520180344581604,
      "learning_rate": 3.3303295874041694e-05,
      "loss": 0.1241,
      "step": 287800
    },
    {
      "epoch": 1.8701484296339603,
      "grad_norm": 0.9522805213928223,
      "learning_rate": 3.3296385415615704e-05,
      "loss": 0.1169,
      "step": 287900
    },
    {
      "epoch": 1.870798012277112,
      "grad_norm": 1.1485694646835327,
      "learning_rate": 3.328947495718971e-05,
      "loss": 0.1252,
      "step": 288000
    },
    {
      "epoch": 1.8714475949202636,
      "grad_norm": 0.8834131360054016,
      "learning_rate": 3.3282564498763716e-05,
      "loss": 0.1259,
      "step": 288100
    },
    {
      "epoch": 1.8720971775634156,
      "grad_norm": 0.8651913404464722,
      "learning_rate": 3.3275654040337726e-05,
      "loss": 0.1277,
      "step": 288200
    },
    {
      "epoch": 1.8727467602065673,
      "grad_norm": 1.1019078493118286,
      "learning_rate": 3.3268743581911736e-05,
      "loss": 0.1312,
      "step": 288300
    },
    {
      "epoch": 1.873396342849719,
      "grad_norm": 1.241346001625061,
      "learning_rate": 3.3261833123485746e-05,
      "loss": 0.1234,
      "step": 288400
    },
    {
      "epoch": 1.874045925492871,
      "grad_norm": 1.1611108779907227,
      "learning_rate": 3.3254922665059756e-05,
      "loss": 0.1254,
      "step": 288500
    },
    {
      "epoch": 1.8746955081360226,
      "grad_norm": 1.3439249992370605,
      "learning_rate": 3.3248012206633766e-05,
      "loss": 0.1236,
      "step": 288600
    },
    {
      "epoch": 1.8753450907791742,
      "grad_norm": 1.679652214050293,
      "learning_rate": 3.3241101748207776e-05,
      "loss": 0.1284,
      "step": 288700
    },
    {
      "epoch": 1.8759946734223263,
      "grad_norm": 1.1702337265014648,
      "learning_rate": 3.3234191289781785e-05,
      "loss": 0.1264,
      "step": 288800
    },
    {
      "epoch": 1.876644256065478,
      "grad_norm": 1.0613348484039307,
      "learning_rate": 3.3227280831355795e-05,
      "loss": 0.1217,
      "step": 288900
    },
    {
      "epoch": 1.8772938387086298,
      "grad_norm": 1.7101683616638184,
      "learning_rate": 3.32203703729298e-05,
      "loss": 0.1258,
      "step": 289000
    },
    {
      "epoch": 1.8779434213517816,
      "grad_norm": 0.9966170787811279,
      "learning_rate": 3.321345991450381e-05,
      "loss": 0.121,
      "step": 289100
    },
    {
      "epoch": 1.8785930039949332,
      "grad_norm": 1.050762414932251,
      "learning_rate": 3.320654945607782e-05,
      "loss": 0.1296,
      "step": 289200
    },
    {
      "epoch": 1.879242586638085,
      "grad_norm": 1.6429332494735718,
      "learning_rate": 3.319963899765183e-05,
      "loss": 0.1206,
      "step": 289300
    },
    {
      "epoch": 1.879892169281237,
      "grad_norm": 0.9565326571464539,
      "learning_rate": 3.319272853922584e-05,
      "loss": 0.1171,
      "step": 289400
    },
    {
      "epoch": 1.8805417519243885,
      "grad_norm": 1.2258795499801636,
      "learning_rate": 3.318581808079985e-05,
      "loss": 0.1273,
      "step": 289500
    },
    {
      "epoch": 1.8811913345675404,
      "grad_norm": 1.0873262882232666,
      "learning_rate": 3.317890762237386e-05,
      "loss": 0.1257,
      "step": 289600
    },
    {
      "epoch": 1.8818409172106922,
      "grad_norm": 1.0946664810180664,
      "learning_rate": 3.317199716394787e-05,
      "loss": 0.1207,
      "step": 289700
    },
    {
      "epoch": 1.8824904998538439,
      "grad_norm": 1.4674714803695679,
      "learning_rate": 3.316508670552188e-05,
      "loss": 0.1269,
      "step": 289800
    },
    {
      "epoch": 1.8831400824969957,
      "grad_norm": 1.4908467531204224,
      "learning_rate": 3.315817624709588e-05,
      "loss": 0.1271,
      "step": 289900
    },
    {
      "epoch": 1.8837896651401476,
      "grad_norm": 2.0583715438842773,
      "learning_rate": 3.315126578866989e-05,
      "loss": 0.1245,
      "step": 290000
    },
    {
      "epoch": 1.8844392477832992,
      "grad_norm": 0.9816379547119141,
      "learning_rate": 3.31443553302439e-05,
      "loss": 0.1196,
      "step": 290100
    },
    {
      "epoch": 1.885088830426451,
      "grad_norm": 1.4347466230392456,
      "learning_rate": 3.313744487181791e-05,
      "loss": 0.1231,
      "step": 290200
    },
    {
      "epoch": 1.8857384130696029,
      "grad_norm": 0.8555940985679626,
      "learning_rate": 3.313053441339192e-05,
      "loss": 0.1241,
      "step": 290300
    },
    {
      "epoch": 1.8863879957127545,
      "grad_norm": 0.5822900533676147,
      "learning_rate": 3.312362395496593e-05,
      "loss": 0.1232,
      "step": 290400
    },
    {
      "epoch": 1.8870375783559064,
      "grad_norm": 1.3594821691513062,
      "learning_rate": 3.311671349653994e-05,
      "loss": 0.124,
      "step": 290500
    },
    {
      "epoch": 1.8876871609990582,
      "grad_norm": 0.6817883849143982,
      "learning_rate": 3.310980303811394e-05,
      "loss": 0.125,
      "step": 290600
    },
    {
      "epoch": 1.8883367436422098,
      "grad_norm": 1.1553205251693726,
      "learning_rate": 3.310289257968795e-05,
      "loss": 0.125,
      "step": 290700
    },
    {
      "epoch": 1.8889863262853617,
      "grad_norm": 0.839539647102356,
      "learning_rate": 3.309598212126196e-05,
      "loss": 0.1259,
      "step": 290800
    },
    {
      "epoch": 1.8896359089285135,
      "grad_norm": 1.2962102890014648,
      "learning_rate": 3.308907166283597e-05,
      "loss": 0.1215,
      "step": 290900
    },
    {
      "epoch": 1.8902854915716651,
      "grad_norm": 0.964831531047821,
      "learning_rate": 3.3082161204409975e-05,
      "loss": 0.1284,
      "step": 291000
    },
    {
      "epoch": 1.890935074214817,
      "grad_norm": 1.0076508522033691,
      "learning_rate": 3.3075250745983985e-05,
      "loss": 0.1362,
      "step": 291100
    },
    {
      "epoch": 1.8915846568579688,
      "grad_norm": 0.9970458149909973,
      "learning_rate": 3.3068340287557995e-05,
      "loss": 0.1293,
      "step": 291200
    },
    {
      "epoch": 1.8922342395011205,
      "grad_norm": 0.7969223856925964,
      "learning_rate": 3.3061429829132005e-05,
      "loss": 0.125,
      "step": 291300
    },
    {
      "epoch": 1.8928838221442723,
      "grad_norm": 0.7665503025054932,
      "learning_rate": 3.3054519370706014e-05,
      "loss": 0.1217,
      "step": 291400
    },
    {
      "epoch": 1.8935334047874242,
      "grad_norm": 1.1072415113449097,
      "learning_rate": 3.3047608912280024e-05,
      "loss": 0.1274,
      "step": 291500
    },
    {
      "epoch": 1.8941829874305758,
      "grad_norm": 1.0856932401657104,
      "learning_rate": 3.3040698453854034e-05,
      "loss": 0.1237,
      "step": 291600
    },
    {
      "epoch": 1.8948325700737276,
      "grad_norm": 1.3887089490890503,
      "learning_rate": 3.3033787995428044e-05,
      "loss": 0.1217,
      "step": 291700
    },
    {
      "epoch": 1.8954821527168795,
      "grad_norm": 1.3755242824554443,
      "learning_rate": 3.302687753700205e-05,
      "loss": 0.1183,
      "step": 291800
    },
    {
      "epoch": 1.896131735360031,
      "grad_norm": 1.1102855205535889,
      "learning_rate": 3.301996707857606e-05,
      "loss": 0.1273,
      "step": 291900
    },
    {
      "epoch": 1.896781318003183,
      "grad_norm": 1.1318650245666504,
      "learning_rate": 3.301305662015007e-05,
      "loss": 0.127,
      "step": 292000
    },
    {
      "epoch": 1.8974309006463348,
      "grad_norm": 1.5276875495910645,
      "learning_rate": 3.300614616172408e-05,
      "loss": 0.1275,
      "step": 292100
    },
    {
      "epoch": 1.8980804832894864,
      "grad_norm": 0.8922169208526611,
      "learning_rate": 3.2999235703298087e-05,
      "loss": 0.1291,
      "step": 292200
    },
    {
      "epoch": 1.8987300659326383,
      "grad_norm": 1.113368272781372,
      "learning_rate": 3.2992325244872096e-05,
      "loss": 0.1219,
      "step": 292300
    },
    {
      "epoch": 1.8993796485757901,
      "grad_norm": 1.6275445222854614,
      "learning_rate": 3.2985414786446106e-05,
      "loss": 0.1321,
      "step": 292400
    },
    {
      "epoch": 1.9000292312189417,
      "grad_norm": 1.3988441228866577,
      "learning_rate": 3.2978504328020116e-05,
      "loss": 0.1301,
      "step": 292500
    },
    {
      "epoch": 1.9006788138620936,
      "grad_norm": 1.6233564615249634,
      "learning_rate": 3.2971593869594126e-05,
      "loss": 0.1321,
      "step": 292600
    },
    {
      "epoch": 1.9013283965052454,
      "grad_norm": 0.8630346059799194,
      "learning_rate": 3.296468341116813e-05,
      "loss": 0.1279,
      "step": 292700
    },
    {
      "epoch": 1.901977979148397,
      "grad_norm": 1.2340607643127441,
      "learning_rate": 3.295777295274214e-05,
      "loss": 0.125,
      "step": 292800
    },
    {
      "epoch": 1.902627561791549,
      "grad_norm": 1.067305564880371,
      "learning_rate": 3.295086249431615e-05,
      "loss": 0.1242,
      "step": 292900
    },
    {
      "epoch": 1.9032771444347008,
      "grad_norm": 1.0805814266204834,
      "learning_rate": 3.294395203589016e-05,
      "loss": 0.1219,
      "step": 293000
    },
    {
      "epoch": 1.9039267270778524,
      "grad_norm": 1.3342915773391724,
      "learning_rate": 3.293704157746417e-05,
      "loss": 0.1265,
      "step": 293100
    },
    {
      "epoch": 1.9045763097210042,
      "grad_norm": 1.1619174480438232,
      "learning_rate": 3.293013111903818e-05,
      "loss": 0.1202,
      "step": 293200
    },
    {
      "epoch": 1.905225892364156,
      "grad_norm": 1.2967073917388916,
      "learning_rate": 3.292322066061219e-05,
      "loss": 0.1289,
      "step": 293300
    },
    {
      "epoch": 1.9058754750073077,
      "grad_norm": 0.8705270886421204,
      "learning_rate": 3.29163102021862e-05,
      "loss": 0.1287,
      "step": 293400
    },
    {
      "epoch": 1.9065250576504595,
      "grad_norm": 1.0963350534439087,
      "learning_rate": 3.290939974376021e-05,
      "loss": 0.1195,
      "step": 293500
    },
    {
      "epoch": 1.9071746402936114,
      "grad_norm": 0.8340170383453369,
      "learning_rate": 3.290248928533422e-05,
      "loss": 0.1218,
      "step": 293600
    },
    {
      "epoch": 1.907824222936763,
      "grad_norm": 2.0207648277282715,
      "learning_rate": 3.289557882690822e-05,
      "loss": 0.1291,
      "step": 293700
    },
    {
      "epoch": 1.9084738055799149,
      "grad_norm": 1.2453596591949463,
      "learning_rate": 3.288866836848223e-05,
      "loss": 0.125,
      "step": 293800
    },
    {
      "epoch": 1.9091233882230667,
      "grad_norm": 1.2922338247299194,
      "learning_rate": 3.288175791005624e-05,
      "loss": 0.1241,
      "step": 293900
    },
    {
      "epoch": 1.9097729708662183,
      "grad_norm": 1.0045791864395142,
      "learning_rate": 3.287484745163025e-05,
      "loss": 0.1237,
      "step": 294000
    },
    {
      "epoch": 1.9104225535093702,
      "grad_norm": 1.4093677997589111,
      "learning_rate": 3.286793699320426e-05,
      "loss": 0.1225,
      "step": 294100
    },
    {
      "epoch": 1.911072136152522,
      "grad_norm": 1.1073070764541626,
      "learning_rate": 3.286102653477826e-05,
      "loss": 0.1259,
      "step": 294200
    },
    {
      "epoch": 1.9117217187956737,
      "grad_norm": 1.139812707901001,
      "learning_rate": 3.285411607635227e-05,
      "loss": 0.1226,
      "step": 294300
    },
    {
      "epoch": 1.9123713014388255,
      "grad_norm": 1.3158820867538452,
      "learning_rate": 3.284720561792628e-05,
      "loss": 0.1245,
      "step": 294400
    },
    {
      "epoch": 1.9130208840819773,
      "grad_norm": 0.5969948172569275,
      "learning_rate": 3.284029515950029e-05,
      "loss": 0.129,
      "step": 294500
    },
    {
      "epoch": 1.913670466725129,
      "grad_norm": 1.176300048828125,
      "learning_rate": 3.2833384701074296e-05,
      "loss": 0.1304,
      "step": 294600
    },
    {
      "epoch": 1.914320049368281,
      "grad_norm": 1.0964823961257935,
      "learning_rate": 3.2826474242648306e-05,
      "loss": 0.1272,
      "step": 294700
    },
    {
      "epoch": 1.9149696320114327,
      "grad_norm": 1.1492536067962646,
      "learning_rate": 3.2819563784222316e-05,
      "loss": 0.1208,
      "step": 294800
    },
    {
      "epoch": 1.9156192146545843,
      "grad_norm": 1.0169633626937866,
      "learning_rate": 3.2812653325796325e-05,
      "loss": 0.1278,
      "step": 294900
    },
    {
      "epoch": 1.9162687972977364,
      "grad_norm": 1.0135301351547241,
      "learning_rate": 3.2805742867370335e-05,
      "loss": 0.1208,
      "step": 295000
    },
    {
      "epoch": 1.916918379940888,
      "grad_norm": 1.264649510383606,
      "learning_rate": 3.2798832408944345e-05,
      "loss": 0.1238,
      "step": 295100
    },
    {
      "epoch": 1.9175679625840396,
      "grad_norm": 1.2776563167572021,
      "learning_rate": 3.2791921950518355e-05,
      "loss": 0.1261,
      "step": 295200
    },
    {
      "epoch": 1.9182175452271917,
      "grad_norm": 1.711323857307434,
      "learning_rate": 3.2785011492092365e-05,
      "loss": 0.124,
      "step": 295300
    },
    {
      "epoch": 1.9188671278703433,
      "grad_norm": 1.0908496379852295,
      "learning_rate": 3.2778101033666375e-05,
      "loss": 0.1244,
      "step": 295400
    },
    {
      "epoch": 1.919516710513495,
      "grad_norm": 1.2616984844207764,
      "learning_rate": 3.277119057524038e-05,
      "loss": 0.1307,
      "step": 295500
    },
    {
      "epoch": 1.920166293156647,
      "grad_norm": 0.91823810338974,
      "learning_rate": 3.276428011681439e-05,
      "loss": 0.1187,
      "step": 295600
    },
    {
      "epoch": 1.9208158757997986,
      "grad_norm": 0.9546362161636353,
      "learning_rate": 3.27573696583884e-05,
      "loss": 0.1271,
      "step": 295700
    },
    {
      "epoch": 1.9214654584429502,
      "grad_norm": 0.9331855177879333,
      "learning_rate": 3.275045919996241e-05,
      "loss": 0.1228,
      "step": 295800
    },
    {
      "epoch": 1.9221150410861023,
      "grad_norm": 1.5556515455245972,
      "learning_rate": 3.274354874153642e-05,
      "loss": 0.1212,
      "step": 295900
    },
    {
      "epoch": 1.922764623729254,
      "grad_norm": 1.3235834836959839,
      "learning_rate": 3.273663828311043e-05,
      "loss": 0.1292,
      "step": 296000
    },
    {
      "epoch": 1.9234142063724056,
      "grad_norm": 1.0158065557479858,
      "learning_rate": 3.272972782468444e-05,
      "loss": 0.1242,
      "step": 296100
    },
    {
      "epoch": 1.9240637890155576,
      "grad_norm": 1.0374747514724731,
      "learning_rate": 3.272281736625845e-05,
      "loss": 0.1235,
      "step": 296200
    },
    {
      "epoch": 1.9247133716587093,
      "grad_norm": 1.5832701921463013,
      "learning_rate": 3.2715906907832457e-05,
      "loss": 0.1117,
      "step": 296300
    },
    {
      "epoch": 1.925362954301861,
      "grad_norm": 1.1747866868972778,
      "learning_rate": 3.2708996449406466e-05,
      "loss": 0.1179,
      "step": 296400
    },
    {
      "epoch": 1.926012536945013,
      "grad_norm": 1.5901423692703247,
      "learning_rate": 3.270208599098047e-05,
      "loss": 0.1258,
      "step": 296500
    },
    {
      "epoch": 1.9266621195881646,
      "grad_norm": 1.2026166915893555,
      "learning_rate": 3.269517553255448e-05,
      "loss": 0.1227,
      "step": 296600
    },
    {
      "epoch": 1.9273117022313164,
      "grad_norm": 1.753780484199524,
      "learning_rate": 3.268826507412849e-05,
      "loss": 0.1225,
      "step": 296700
    },
    {
      "epoch": 1.9279612848744683,
      "grad_norm": 1.3408241271972656,
      "learning_rate": 3.26813546157025e-05,
      "loss": 0.1192,
      "step": 296800
    },
    {
      "epoch": 1.92861086751762,
      "grad_norm": 1.0608761310577393,
      "learning_rate": 3.267444415727651e-05,
      "loss": 0.1277,
      "step": 296900
    },
    {
      "epoch": 1.9292604501607717,
      "grad_norm": 1.1763981580734253,
      "learning_rate": 3.266753369885052e-05,
      "loss": 0.1256,
      "step": 297000
    },
    {
      "epoch": 1.9299100328039236,
      "grad_norm": 1.2240146398544312,
      "learning_rate": 3.266062324042453e-05,
      "loss": 0.1264,
      "step": 297100
    },
    {
      "epoch": 1.9305596154470752,
      "grad_norm": 1.071141004562378,
      "learning_rate": 3.265371278199854e-05,
      "loss": 0.1247,
      "step": 297200
    },
    {
      "epoch": 1.931209198090227,
      "grad_norm": 1.1050883531570435,
      "learning_rate": 3.264680232357255e-05,
      "loss": 0.1303,
      "step": 297300
    },
    {
      "epoch": 1.931858780733379,
      "grad_norm": 1.1565812826156616,
      "learning_rate": 3.263989186514655e-05,
      "loss": 0.1286,
      "step": 297400
    },
    {
      "epoch": 1.9325083633765305,
      "grad_norm": 1.3592971563339233,
      "learning_rate": 3.263298140672056e-05,
      "loss": 0.1235,
      "step": 297500
    },
    {
      "epoch": 1.9331579460196824,
      "grad_norm": 1.016296625137329,
      "learning_rate": 3.262607094829457e-05,
      "loss": 0.1214,
      "step": 297600
    },
    {
      "epoch": 1.9338075286628342,
      "grad_norm": 0.8674411773681641,
      "learning_rate": 3.261916048986858e-05,
      "loss": 0.1211,
      "step": 297700
    },
    {
      "epoch": 1.9344571113059859,
      "grad_norm": 1.089138388633728,
      "learning_rate": 3.2612250031442584e-05,
      "loss": 0.115,
      "step": 297800
    },
    {
      "epoch": 1.9351066939491377,
      "grad_norm": 1.284438967704773,
      "learning_rate": 3.2605339573016594e-05,
      "loss": 0.1341,
      "step": 297900
    },
    {
      "epoch": 1.9357562765922895,
      "grad_norm": 1.1100143194198608,
      "learning_rate": 3.2598429114590604e-05,
      "loss": 0.1168,
      "step": 298000
    },
    {
      "epoch": 1.9364058592354412,
      "grad_norm": 1.0087032318115234,
      "learning_rate": 3.2591518656164614e-05,
      "loss": 0.1244,
      "step": 298100
    },
    {
      "epoch": 1.937055441878593,
      "grad_norm": 1.1132632493972778,
      "learning_rate": 3.2584608197738623e-05,
      "loss": 0.1314,
      "step": 298200
    },
    {
      "epoch": 1.9377050245217449,
      "grad_norm": 0.8628320693969727,
      "learning_rate": 3.257769773931263e-05,
      "loss": 0.13,
      "step": 298300
    },
    {
      "epoch": 1.9383546071648965,
      "grad_norm": 1.2232542037963867,
      "learning_rate": 3.2570787280886636e-05,
      "loss": 0.1265,
      "step": 298400
    },
    {
      "epoch": 1.9390041898080483,
      "grad_norm": 1.251861572265625,
      "learning_rate": 3.2563876822460646e-05,
      "loss": 0.1232,
      "step": 298500
    },
    {
      "epoch": 1.9396537724512002,
      "grad_norm": 1.0266746282577515,
      "learning_rate": 3.2556966364034656e-05,
      "loss": 0.127,
      "step": 298600
    },
    {
      "epoch": 1.9403033550943518,
      "grad_norm": 0.9121211767196655,
      "learning_rate": 3.2550055905608666e-05,
      "loss": 0.1203,
      "step": 298700
    },
    {
      "epoch": 1.9409529377375037,
      "grad_norm": 0.7663824558258057,
      "learning_rate": 3.2543145447182676e-05,
      "loss": 0.1262,
      "step": 298800
    },
    {
      "epoch": 1.9416025203806555,
      "grad_norm": 1.3633846044540405,
      "learning_rate": 3.2536234988756686e-05,
      "loss": 0.1218,
      "step": 298900
    },
    {
      "epoch": 1.9422521030238071,
      "grad_norm": 0.9072057604789734,
      "learning_rate": 3.2529324530330695e-05,
      "loss": 0.1309,
      "step": 299000
    },
    {
      "epoch": 1.942901685666959,
      "grad_norm": 1.7293691635131836,
      "learning_rate": 3.2522414071904705e-05,
      "loss": 0.1206,
      "step": 299100
    },
    {
      "epoch": 1.9435512683101108,
      "grad_norm": 0.9960491061210632,
      "learning_rate": 3.2515503613478715e-05,
      "loss": 0.1264,
      "step": 299200
    },
    {
      "epoch": 1.9442008509532624,
      "grad_norm": 1.625238299369812,
      "learning_rate": 3.250859315505272e-05,
      "loss": 0.1248,
      "step": 299300
    },
    {
      "epoch": 1.9448504335964143,
      "grad_norm": 1.3648613691329956,
      "learning_rate": 3.250168269662673e-05,
      "loss": 0.1299,
      "step": 299400
    },
    {
      "epoch": 1.9455000162395661,
      "grad_norm": 1.344688057899475,
      "learning_rate": 3.249477223820074e-05,
      "loss": 0.1289,
      "step": 299500
    },
    {
      "epoch": 1.9461495988827178,
      "grad_norm": 1.1813101768493652,
      "learning_rate": 3.248786177977475e-05,
      "loss": 0.125,
      "step": 299600
    },
    {
      "epoch": 1.9467991815258696,
      "grad_norm": 1.7459150552749634,
      "learning_rate": 3.248095132134876e-05,
      "loss": 0.1178,
      "step": 299700
    },
    {
      "epoch": 1.9474487641690215,
      "grad_norm": 0.9729148149490356,
      "learning_rate": 3.247404086292277e-05,
      "loss": 0.121,
      "step": 299800
    },
    {
      "epoch": 1.948098346812173,
      "grad_norm": 0.7134274244308472,
      "learning_rate": 3.246713040449678e-05,
      "loss": 0.1221,
      "step": 299900
    },
    {
      "epoch": 1.948747929455325,
      "grad_norm": 1.4691650867462158,
      "learning_rate": 3.246021994607079e-05,
      "loss": 0.1222,
      "step": 300000
    },
    {
      "epoch": 1.9493975120984768,
      "grad_norm": 1.527846097946167,
      "learning_rate": 3.24533094876448e-05,
      "loss": 0.1264,
      "step": 300100
    },
    {
      "epoch": 1.9500470947416284,
      "grad_norm": 1.4616284370422363,
      "learning_rate": 3.24463990292188e-05,
      "loss": 0.1244,
      "step": 300200
    },
    {
      "epoch": 1.9506966773847803,
      "grad_norm": 0.655335009098053,
      "learning_rate": 3.243948857079281e-05,
      "loss": 0.1228,
      "step": 300300
    },
    {
      "epoch": 1.951346260027932,
      "grad_norm": 1.3463491201400757,
      "learning_rate": 3.243257811236682e-05,
      "loss": 0.1284,
      "step": 300400
    },
    {
      "epoch": 1.9519958426710837,
      "grad_norm": 1.9839129447937012,
      "learning_rate": 3.242566765394083e-05,
      "loss": 0.1241,
      "step": 300500
    },
    {
      "epoch": 1.9526454253142356,
      "grad_norm": 1.3951374292373657,
      "learning_rate": 3.241875719551484e-05,
      "loss": 0.1284,
      "step": 300600
    },
    {
      "epoch": 1.9532950079573874,
      "grad_norm": 0.9537239670753479,
      "learning_rate": 3.241184673708885e-05,
      "loss": 0.1293,
      "step": 300700
    },
    {
      "epoch": 1.953944590600539,
      "grad_norm": 1.0792739391326904,
      "learning_rate": 3.240493627866286e-05,
      "loss": 0.1175,
      "step": 300800
    },
    {
      "epoch": 1.954594173243691,
      "grad_norm": 1.060096263885498,
      "learning_rate": 3.239802582023687e-05,
      "loss": 0.1302,
      "step": 300900
    },
    {
      "epoch": 1.9552437558868427,
      "grad_norm": 1.7115358114242554,
      "learning_rate": 3.239111536181087e-05,
      "loss": 0.1251,
      "step": 301000
    },
    {
      "epoch": 1.9558933385299944,
      "grad_norm": 1.5128191709518433,
      "learning_rate": 3.238420490338488e-05,
      "loss": 0.1228,
      "step": 301100
    },
    {
      "epoch": 1.9565429211731462,
      "grad_norm": 1.0155692100524902,
      "learning_rate": 3.237729444495889e-05,
      "loss": 0.1196,
      "step": 301200
    },
    {
      "epoch": 1.957192503816298,
      "grad_norm": 1.1058344841003418,
      "learning_rate": 3.23703839865329e-05,
      "loss": 0.1233,
      "step": 301300
    },
    {
      "epoch": 1.9578420864594497,
      "grad_norm": 0.7390015721321106,
      "learning_rate": 3.2363473528106905e-05,
      "loss": 0.1231,
      "step": 301400
    },
    {
      "epoch": 1.9584916691026015,
      "grad_norm": 1.018782138824463,
      "learning_rate": 3.2356563069680915e-05,
      "loss": 0.1267,
      "step": 301500
    },
    {
      "epoch": 1.9591412517457534,
      "grad_norm": 1.3170173168182373,
      "learning_rate": 3.2349652611254925e-05,
      "loss": 0.1278,
      "step": 301600
    },
    {
      "epoch": 1.959790834388905,
      "grad_norm": 1.3899983167648315,
      "learning_rate": 3.2342742152828934e-05,
      "loss": 0.125,
      "step": 301700
    },
    {
      "epoch": 1.960440417032057,
      "grad_norm": 1.2785975933074951,
      "learning_rate": 3.2335831694402944e-05,
      "loss": 0.1222,
      "step": 301800
    },
    {
      "epoch": 1.9610899996752087,
      "grad_norm": 0.8363768458366394,
      "learning_rate": 3.2328921235976954e-05,
      "loss": 0.1198,
      "step": 301900
    },
    {
      "epoch": 1.9617395823183603,
      "grad_norm": 1.143232822418213,
      "learning_rate": 3.2322010777550964e-05,
      "loss": 0.1282,
      "step": 302000
    },
    {
      "epoch": 1.9623891649615124,
      "grad_norm": 1.0330302715301514,
      "learning_rate": 3.231510031912497e-05,
      "loss": 0.1215,
      "step": 302100
    },
    {
      "epoch": 1.963038747604664,
      "grad_norm": 1.105892539024353,
      "learning_rate": 3.230818986069898e-05,
      "loss": 0.129,
      "step": 302200
    },
    {
      "epoch": 1.9636883302478156,
      "grad_norm": 1.6052378416061401,
      "learning_rate": 3.230127940227299e-05,
      "loss": 0.1169,
      "step": 302300
    },
    {
      "epoch": 1.9643379128909677,
      "grad_norm": 0.9468976259231567,
      "learning_rate": 3.2294368943846997e-05,
      "loss": 0.1243,
      "step": 302400
    },
    {
      "epoch": 1.9649874955341193,
      "grad_norm": 1.1578470468521118,
      "learning_rate": 3.2287458485421006e-05,
      "loss": 0.1306,
      "step": 302500
    },
    {
      "epoch": 1.965637078177271,
      "grad_norm": 0.7045027613639832,
      "learning_rate": 3.2280548026995016e-05,
      "loss": 0.1267,
      "step": 302600
    },
    {
      "epoch": 1.966286660820423,
      "grad_norm": 0.9560788869857788,
      "learning_rate": 3.2273637568569026e-05,
      "loss": 0.1248,
      "step": 302700
    },
    {
      "epoch": 1.9669362434635747,
      "grad_norm": 0.846484899520874,
      "learning_rate": 3.2266727110143036e-05,
      "loss": 0.1281,
      "step": 302800
    },
    {
      "epoch": 1.9675858261067263,
      "grad_norm": 0.9606303572654724,
      "learning_rate": 3.2259816651717046e-05,
      "loss": 0.121,
      "step": 302900
    },
    {
      "epoch": 1.9682354087498783,
      "grad_norm": 1.1767445802688599,
      "learning_rate": 3.2252906193291056e-05,
      "loss": 0.136,
      "step": 303000
    },
    {
      "epoch": 1.96888499139303,
      "grad_norm": 1.215463638305664,
      "learning_rate": 3.224599573486506e-05,
      "loss": 0.1266,
      "step": 303100
    },
    {
      "epoch": 1.9695345740361816,
      "grad_norm": 0.8003443479537964,
      "learning_rate": 3.223908527643907e-05,
      "loss": 0.1269,
      "step": 303200
    },
    {
      "epoch": 1.9701841566793337,
      "grad_norm": 1.8869764804840088,
      "learning_rate": 3.223217481801308e-05,
      "loss": 0.1235,
      "step": 303300
    },
    {
      "epoch": 1.9708337393224853,
      "grad_norm": 1.0688272714614868,
      "learning_rate": 3.222526435958709e-05,
      "loss": 0.1189,
      "step": 303400
    },
    {
      "epoch": 1.971483321965637,
      "grad_norm": 1.2033580541610718,
      "learning_rate": 3.22183539011611e-05,
      "loss": 0.1208,
      "step": 303500
    },
    {
      "epoch": 1.972132904608789,
      "grad_norm": 1.0558363199234009,
      "learning_rate": 3.221144344273511e-05,
      "loss": 0.1217,
      "step": 303600
    },
    {
      "epoch": 1.9727824872519406,
      "grad_norm": 1.3437527418136597,
      "learning_rate": 3.220453298430912e-05,
      "loss": 0.1301,
      "step": 303700
    },
    {
      "epoch": 1.9734320698950925,
      "grad_norm": 1.4996941089630127,
      "learning_rate": 3.219762252588313e-05,
      "loss": 0.1221,
      "step": 303800
    },
    {
      "epoch": 1.9740816525382443,
      "grad_norm": 1.5780277252197266,
      "learning_rate": 3.219071206745714e-05,
      "loss": 0.1361,
      "step": 303900
    },
    {
      "epoch": 1.974731235181396,
      "grad_norm": 0.9967495203018188,
      "learning_rate": 3.218380160903114e-05,
      "loss": 0.1227,
      "step": 304000
    },
    {
      "epoch": 1.9753808178245478,
      "grad_norm": 1.6011911630630493,
      "learning_rate": 3.217689115060515e-05,
      "loss": 0.1217,
      "step": 304100
    },
    {
      "epoch": 1.9760304004676996,
      "grad_norm": 1.4337573051452637,
      "learning_rate": 3.216998069217916e-05,
      "loss": 0.1266,
      "step": 304200
    },
    {
      "epoch": 1.9766799831108512,
      "grad_norm": 1.6597036123275757,
      "learning_rate": 3.216307023375317e-05,
      "loss": 0.1227,
      "step": 304300
    },
    {
      "epoch": 1.977329565754003,
      "grad_norm": 1.709245204925537,
      "learning_rate": 3.215615977532718e-05,
      "loss": 0.1207,
      "step": 304400
    },
    {
      "epoch": 1.977979148397155,
      "grad_norm": 1.3953715562820435,
      "learning_rate": 3.214924931690119e-05,
      "loss": 0.1225,
      "step": 304500
    },
    {
      "epoch": 1.9786287310403066,
      "grad_norm": 0.9491944909095764,
      "learning_rate": 3.214233885847519e-05,
      "loss": 0.1254,
      "step": 304600
    },
    {
      "epoch": 1.9792783136834584,
      "grad_norm": 1.2975407838821411,
      "learning_rate": 3.21354284000492e-05,
      "loss": 0.1289,
      "step": 304700
    },
    {
      "epoch": 1.9799278963266103,
      "grad_norm": 1.3848481178283691,
      "learning_rate": 3.212851794162321e-05,
      "loss": 0.1282,
      "step": 304800
    },
    {
      "epoch": 1.9805774789697619,
      "grad_norm": 0.9686267971992493,
      "learning_rate": 3.212160748319722e-05,
      "loss": 0.1233,
      "step": 304900
    },
    {
      "epoch": 1.9812270616129137,
      "grad_norm": 1.3340245485305786,
      "learning_rate": 3.2114697024771226e-05,
      "loss": 0.123,
      "step": 305000
    },
    {
      "epoch": 1.9818766442560656,
      "grad_norm": 1.4033782482147217,
      "learning_rate": 3.2107786566345235e-05,
      "loss": 0.1217,
      "step": 305100
    },
    {
      "epoch": 1.9825262268992172,
      "grad_norm": 1.2241588830947876,
      "learning_rate": 3.2100876107919245e-05,
      "loss": 0.1322,
      "step": 305200
    },
    {
      "epoch": 1.983175809542369,
      "grad_norm": 0.851292610168457,
      "learning_rate": 3.2093965649493255e-05,
      "loss": 0.1136,
      "step": 305300
    },
    {
      "epoch": 1.983825392185521,
      "grad_norm": 1.2517940998077393,
      "learning_rate": 3.2087055191067265e-05,
      "loss": 0.121,
      "step": 305400
    },
    {
      "epoch": 1.9844749748286725,
      "grad_norm": 1.2758487462997437,
      "learning_rate": 3.2080144732641275e-05,
      "loss": 0.1185,
      "step": 305500
    },
    {
      "epoch": 1.9851245574718244,
      "grad_norm": 1.067962646484375,
      "learning_rate": 3.2073234274215285e-05,
      "loss": 0.1208,
      "step": 305600
    },
    {
      "epoch": 1.9857741401149762,
      "grad_norm": 0.983211874961853,
      "learning_rate": 3.2066323815789295e-05,
      "loss": 0.1223,
      "step": 305700
    },
    {
      "epoch": 1.9864237227581278,
      "grad_norm": 1.6240094900131226,
      "learning_rate": 3.2059413357363304e-05,
      "loss": 0.1159,
      "step": 305800
    },
    {
      "epoch": 1.9870733054012797,
      "grad_norm": 0.9441291689872742,
      "learning_rate": 3.205250289893731e-05,
      "loss": 0.1179,
      "step": 305900
    },
    {
      "epoch": 1.9877228880444315,
      "grad_norm": 1.2358425855636597,
      "learning_rate": 3.204559244051132e-05,
      "loss": 0.1166,
      "step": 306000
    },
    {
      "epoch": 1.9883724706875832,
      "grad_norm": 1.0242761373519897,
      "learning_rate": 3.203868198208533e-05,
      "loss": 0.1235,
      "step": 306100
    },
    {
      "epoch": 1.989022053330735,
      "grad_norm": 0.752235472202301,
      "learning_rate": 3.203177152365934e-05,
      "loss": 0.1265,
      "step": 306200
    },
    {
      "epoch": 1.9896716359738869,
      "grad_norm": 1.2162975072860718,
      "learning_rate": 3.202486106523335e-05,
      "loss": 0.1184,
      "step": 306300
    },
    {
      "epoch": 1.9903212186170385,
      "grad_norm": 0.9971446990966797,
      "learning_rate": 3.201795060680736e-05,
      "loss": 0.1224,
      "step": 306400
    },
    {
      "epoch": 1.9909708012601903,
      "grad_norm": 1.305893063545227,
      "learning_rate": 3.2011040148381367e-05,
      "loss": 0.1145,
      "step": 306500
    },
    {
      "epoch": 1.9916203839033422,
      "grad_norm": 1.3489983081817627,
      "learning_rate": 3.2004129689955376e-05,
      "loss": 0.1278,
      "step": 306600
    },
    {
      "epoch": 1.9922699665464938,
      "grad_norm": 1.0859498977661133,
      "learning_rate": 3.1997219231529386e-05,
      "loss": 0.1213,
      "step": 306700
    },
    {
      "epoch": 1.9929195491896456,
      "grad_norm": 1.0022135972976685,
      "learning_rate": 3.199030877310339e-05,
      "loss": 0.1274,
      "step": 306800
    },
    {
      "epoch": 1.9935691318327975,
      "grad_norm": 1.014685869216919,
      "learning_rate": 3.19833983146774e-05,
      "loss": 0.1185,
      "step": 306900
    },
    {
      "epoch": 1.9942187144759491,
      "grad_norm": 1.3860777616500854,
      "learning_rate": 3.197648785625141e-05,
      "loss": 0.1185,
      "step": 307000
    },
    {
      "epoch": 1.994868297119101,
      "grad_norm": 0.9073100090026855,
      "learning_rate": 3.196957739782542e-05,
      "loss": 0.1218,
      "step": 307100
    },
    {
      "epoch": 1.9955178797622528,
      "grad_norm": 1.008078694343567,
      "learning_rate": 3.196266693939943e-05,
      "loss": 0.1185,
      "step": 307200
    },
    {
      "epoch": 1.9961674624054044,
      "grad_norm": 0.8734065890312195,
      "learning_rate": 3.195575648097344e-05,
      "loss": 0.1193,
      "step": 307300
    },
    {
      "epoch": 1.9968170450485563,
      "grad_norm": 0.9946189522743225,
      "learning_rate": 3.194884602254745e-05,
      "loss": 0.1236,
      "step": 307400
    },
    {
      "epoch": 1.9974666276917081,
      "grad_norm": 1.6103084087371826,
      "learning_rate": 3.194193556412146e-05,
      "loss": 0.1242,
      "step": 307500
    },
    {
      "epoch": 1.9981162103348598,
      "grad_norm": 0.692001223564148,
      "learning_rate": 3.193502510569547e-05,
      "loss": 0.1223,
      "step": 307600
    },
    {
      "epoch": 1.9987657929780116,
      "grad_norm": 1.2157669067382812,
      "learning_rate": 3.192811464726948e-05,
      "loss": 0.1193,
      "step": 307700
    },
    {
      "epoch": 1.9994153756211634,
      "grad_norm": 1.1062042713165283,
      "learning_rate": 3.192120418884348e-05,
      "loss": 0.1216,
      "step": 307800
    },
    {
      "epoch": 2.000064958264315,
      "grad_norm": 1.1485826969146729,
      "learning_rate": 3.191429373041749e-05,
      "loss": 0.1213,
      "step": 307900
    },
    {
      "epoch": 2.000714540907467,
      "grad_norm": 1.340593934059143,
      "learning_rate": 3.19073832719915e-05,
      "loss": 0.1225,
      "step": 308000
    },
    {
      "epoch": 2.0013641235506188,
      "grad_norm": 1.146694540977478,
      "learning_rate": 3.190047281356551e-05,
      "loss": 0.1265,
      "step": 308100
    },
    {
      "epoch": 2.0020137061937704,
      "grad_norm": 0.9147036671638489,
      "learning_rate": 3.1893562355139514e-05,
      "loss": 0.128,
      "step": 308200
    },
    {
      "epoch": 2.0026632888369225,
      "grad_norm": 1.2904865741729736,
      "learning_rate": 3.1886651896713524e-05,
      "loss": 0.1146,
      "step": 308300
    },
    {
      "epoch": 2.003312871480074,
      "grad_norm": 1.0441066026687622,
      "learning_rate": 3.1879741438287533e-05,
      "loss": 0.1292,
      "step": 308400
    },
    {
      "epoch": 2.0039624541232257,
      "grad_norm": 1.957013487815857,
      "learning_rate": 3.187283097986154e-05,
      "loss": 0.1199,
      "step": 308500
    },
    {
      "epoch": 2.004612036766378,
      "grad_norm": 1.7897684574127197,
      "learning_rate": 3.186592052143555e-05,
      "loss": 0.1261,
      "step": 308600
    },
    {
      "epoch": 2.0052616194095294,
      "grad_norm": 1.0247409343719482,
      "learning_rate": 3.1859010063009556e-05,
      "loss": 0.1222,
      "step": 308700
    },
    {
      "epoch": 2.005911202052681,
      "grad_norm": 1.2251390218734741,
      "learning_rate": 3.1852099604583566e-05,
      "loss": 0.1254,
      "step": 308800
    },
    {
      "epoch": 2.006560784695833,
      "grad_norm": 1.4877060651779175,
      "learning_rate": 3.1845189146157576e-05,
      "loss": 0.1259,
      "step": 308900
    },
    {
      "epoch": 2.0072103673389847,
      "grad_norm": 1.1407665014266968,
      "learning_rate": 3.1838278687731586e-05,
      "loss": 0.1286,
      "step": 309000
    },
    {
      "epoch": 2.0078599499821363,
      "grad_norm": 1.016911268234253,
      "learning_rate": 3.1831368229305596e-05,
      "loss": 0.1269,
      "step": 309100
    },
    {
      "epoch": 2.0085095326252884,
      "grad_norm": 0.7637618780136108,
      "learning_rate": 3.1824457770879605e-05,
      "loss": 0.122,
      "step": 309200
    },
    {
      "epoch": 2.00915911526844,
      "grad_norm": 1.029913067817688,
      "learning_rate": 3.1817547312453615e-05,
      "loss": 0.1171,
      "step": 309300
    },
    {
      "epoch": 2.0098086979115917,
      "grad_norm": 1.246476173400879,
      "learning_rate": 3.1810636854027625e-05,
      "loss": 0.1207,
      "step": 309400
    },
    {
      "epoch": 2.0104582805547437,
      "grad_norm": 1.549055576324463,
      "learning_rate": 3.1803726395601635e-05,
      "loss": 0.118,
      "step": 309500
    },
    {
      "epoch": 2.0111078631978954,
      "grad_norm": 0.9014304876327515,
      "learning_rate": 3.179681593717564e-05,
      "loss": 0.121,
      "step": 309600
    },
    {
      "epoch": 2.011757445841047,
      "grad_norm": 1.8527333736419678,
      "learning_rate": 3.178990547874965e-05,
      "loss": 0.1274,
      "step": 309700
    },
    {
      "epoch": 2.012407028484199,
      "grad_norm": 1.2480409145355225,
      "learning_rate": 3.178299502032366e-05,
      "loss": 0.123,
      "step": 309800
    },
    {
      "epoch": 2.0130566111273507,
      "grad_norm": 1.3558814525604248,
      "learning_rate": 3.177608456189767e-05,
      "loss": 0.1236,
      "step": 309900
    },
    {
      "epoch": 2.0137061937705023,
      "grad_norm": 0.9630253314971924,
      "learning_rate": 3.176917410347168e-05,
      "loss": 0.1266,
      "step": 310000
    },
    {
      "epoch": 2.0143557764136544,
      "grad_norm": 1.275426983833313,
      "learning_rate": 3.176226364504569e-05,
      "loss": 0.1204,
      "step": 310100
    },
    {
      "epoch": 2.015005359056806,
      "grad_norm": 1.2350550889968872,
      "learning_rate": 3.17553531866197e-05,
      "loss": 0.1188,
      "step": 310200
    },
    {
      "epoch": 2.0156549416999576,
      "grad_norm": 1.227388858795166,
      "learning_rate": 3.174844272819371e-05,
      "loss": 0.1196,
      "step": 310300
    },
    {
      "epoch": 2.0163045243431097,
      "grad_norm": 1.2941467761993408,
      "learning_rate": 3.174153226976772e-05,
      "loss": 0.1123,
      "step": 310400
    },
    {
      "epoch": 2.0169541069862613,
      "grad_norm": 1.1392663717269897,
      "learning_rate": 3.173462181134173e-05,
      "loss": 0.1298,
      "step": 310500
    },
    {
      "epoch": 2.017603689629413,
      "grad_norm": 1.0901172161102295,
      "learning_rate": 3.172771135291573e-05,
      "loss": 0.1221,
      "step": 310600
    },
    {
      "epoch": 2.018253272272565,
      "grad_norm": 1.2353878021240234,
      "learning_rate": 3.172080089448974e-05,
      "loss": 0.1273,
      "step": 310700
    },
    {
      "epoch": 2.0189028549157166,
      "grad_norm": 1.0564370155334473,
      "learning_rate": 3.171389043606375e-05,
      "loss": 0.1203,
      "step": 310800
    },
    {
      "epoch": 2.0195524375588683,
      "grad_norm": 1.305997371673584,
      "learning_rate": 3.170697997763776e-05,
      "loss": 0.1182,
      "step": 310900
    },
    {
      "epoch": 2.0202020202020203,
      "grad_norm": 1.380165696144104,
      "learning_rate": 3.170006951921177e-05,
      "loss": 0.1219,
      "step": 311000
    },
    {
      "epoch": 2.020851602845172,
      "grad_norm": 1.3409479856491089,
      "learning_rate": 3.169315906078578e-05,
      "loss": 0.1254,
      "step": 311100
    },
    {
      "epoch": 2.0215011854883236,
      "grad_norm": 0.8426687121391296,
      "learning_rate": 3.168624860235979e-05,
      "loss": 0.1228,
      "step": 311200
    },
    {
      "epoch": 2.0221507681314757,
      "grad_norm": 1.1593241691589355,
      "learning_rate": 3.16793381439338e-05,
      "loss": 0.1171,
      "step": 311300
    },
    {
      "epoch": 2.0228003507746273,
      "grad_norm": 1.0613285303115845,
      "learning_rate": 3.16724276855078e-05,
      "loss": 0.1264,
      "step": 311400
    },
    {
      "epoch": 2.023449933417779,
      "grad_norm": 1.0347404479980469,
      "learning_rate": 3.166551722708181e-05,
      "loss": 0.118,
      "step": 311500
    },
    {
      "epoch": 2.024099516060931,
      "grad_norm": 1.0670661926269531,
      "learning_rate": 3.165860676865582e-05,
      "loss": 0.1269,
      "step": 311600
    },
    {
      "epoch": 2.0247490987040826,
      "grad_norm": 1.0278414487838745,
      "learning_rate": 3.1651696310229825e-05,
      "loss": 0.1189,
      "step": 311700
    },
    {
      "epoch": 2.025398681347234,
      "grad_norm": 0.8907491564750671,
      "learning_rate": 3.1644785851803835e-05,
      "loss": 0.118,
      "step": 311800
    },
    {
      "epoch": 2.0260482639903863,
      "grad_norm": 1.3169206380844116,
      "learning_rate": 3.1637875393377844e-05,
      "loss": 0.1226,
      "step": 311900
    },
    {
      "epoch": 2.026697846633538,
      "grad_norm": 1.3173564672470093,
      "learning_rate": 3.1630964934951854e-05,
      "loss": 0.1198,
      "step": 312000
    },
    {
      "epoch": 2.0273474292766895,
      "grad_norm": 1.5606064796447754,
      "learning_rate": 3.1624054476525864e-05,
      "loss": 0.1248,
      "step": 312100
    },
    {
      "epoch": 2.0279970119198416,
      "grad_norm": 1.0258193016052246,
      "learning_rate": 3.1617144018099874e-05,
      "loss": 0.1237,
      "step": 312200
    },
    {
      "epoch": 2.0286465945629932,
      "grad_norm": 1.0533522367477417,
      "learning_rate": 3.1610233559673884e-05,
      "loss": 0.1265,
      "step": 312300
    },
    {
      "epoch": 2.029296177206145,
      "grad_norm": 1.010603427886963,
      "learning_rate": 3.1603323101247894e-05,
      "loss": 0.1308,
      "step": 312400
    },
    {
      "epoch": 2.029945759849297,
      "grad_norm": 1.1034153699874878,
      "learning_rate": 3.15964126428219e-05,
      "loss": 0.126,
      "step": 312500
    },
    {
      "epoch": 2.0305953424924486,
      "grad_norm": 1.4488868713378906,
      "learning_rate": 3.1589502184395907e-05,
      "loss": 0.1256,
      "step": 312600
    },
    {
      "epoch": 2.0312449251356,
      "grad_norm": 0.9409574866294861,
      "learning_rate": 3.1582591725969916e-05,
      "loss": 0.1177,
      "step": 312700
    },
    {
      "epoch": 2.0318945077787522,
      "grad_norm": 0.873325526714325,
      "learning_rate": 3.1575681267543926e-05,
      "loss": 0.1232,
      "step": 312800
    },
    {
      "epoch": 2.032544090421904,
      "grad_norm": 1.2933592796325684,
      "learning_rate": 3.1568770809117936e-05,
      "loss": 0.1207,
      "step": 312900
    },
    {
      "epoch": 2.0331936730650555,
      "grad_norm": 1.3307719230651855,
      "learning_rate": 3.1561860350691946e-05,
      "loss": 0.1208,
      "step": 313000
    },
    {
      "epoch": 2.0338432557082076,
      "grad_norm": 1.1593881845474243,
      "learning_rate": 3.1554949892265956e-05,
      "loss": 0.1175,
      "step": 313100
    },
    {
      "epoch": 2.034492838351359,
      "grad_norm": 1.254146695137024,
      "learning_rate": 3.1548039433839966e-05,
      "loss": 0.1212,
      "step": 313200
    },
    {
      "epoch": 2.035142420994511,
      "grad_norm": 1.72273850440979,
      "learning_rate": 3.1541128975413976e-05,
      "loss": 0.1289,
      "step": 313300
    },
    {
      "epoch": 2.035792003637663,
      "grad_norm": 1.4709688425064087,
      "learning_rate": 3.153421851698798e-05,
      "loss": 0.1222,
      "step": 313400
    },
    {
      "epoch": 2.0364415862808145,
      "grad_norm": 1.4938366413116455,
      "learning_rate": 3.152730805856199e-05,
      "loss": 0.1306,
      "step": 313500
    },
    {
      "epoch": 2.037091168923966,
      "grad_norm": 1.2501976490020752,
      "learning_rate": 3.1520397600136e-05,
      "loss": 0.1251,
      "step": 313600
    },
    {
      "epoch": 2.037740751567118,
      "grad_norm": 1.6208813190460205,
      "learning_rate": 3.151348714171001e-05,
      "loss": 0.1242,
      "step": 313700
    },
    {
      "epoch": 2.03839033421027,
      "grad_norm": 1.5524072647094727,
      "learning_rate": 3.150657668328402e-05,
      "loss": 0.1176,
      "step": 313800
    },
    {
      "epoch": 2.0390399168534215,
      "grad_norm": 1.30814790725708,
      "learning_rate": 3.149966622485803e-05,
      "loss": 0.12,
      "step": 313900
    },
    {
      "epoch": 2.0396894994965735,
      "grad_norm": 1.3467936515808105,
      "learning_rate": 3.149275576643204e-05,
      "loss": 0.1267,
      "step": 314000
    },
    {
      "epoch": 2.040339082139725,
      "grad_norm": 1.173891544342041,
      "learning_rate": 3.148584530800605e-05,
      "loss": 0.1277,
      "step": 314100
    },
    {
      "epoch": 2.040988664782877,
      "grad_norm": 1.3675777912139893,
      "learning_rate": 3.147893484958006e-05,
      "loss": 0.1219,
      "step": 314200
    },
    {
      "epoch": 2.041638247426029,
      "grad_norm": 1.041396975517273,
      "learning_rate": 3.147202439115407e-05,
      "loss": 0.1249,
      "step": 314300
    },
    {
      "epoch": 2.0422878300691805,
      "grad_norm": 1.1212679147720337,
      "learning_rate": 3.146511393272807e-05,
      "loss": 0.1233,
      "step": 314400
    },
    {
      "epoch": 2.0429374127123325,
      "grad_norm": 1.1912970542907715,
      "learning_rate": 3.145820347430208e-05,
      "loss": 0.1238,
      "step": 314500
    },
    {
      "epoch": 2.043586995355484,
      "grad_norm": 0.8296292424201965,
      "learning_rate": 3.145129301587609e-05,
      "loss": 0.1206,
      "step": 314600
    },
    {
      "epoch": 2.044236577998636,
      "grad_norm": 1.082344651222229,
      "learning_rate": 3.14443825574501e-05,
      "loss": 0.1258,
      "step": 314700
    },
    {
      "epoch": 2.044886160641788,
      "grad_norm": 1.326676368713379,
      "learning_rate": 3.143747209902411e-05,
      "loss": 0.1206,
      "step": 314800
    },
    {
      "epoch": 2.0455357432849395,
      "grad_norm": 1.0858343839645386,
      "learning_rate": 3.143056164059812e-05,
      "loss": 0.1199,
      "step": 314900
    },
    {
      "epoch": 2.046185325928091,
      "grad_norm": 0.8669623136520386,
      "learning_rate": 3.142365118217212e-05,
      "loss": 0.1245,
      "step": 315000
    },
    {
      "epoch": 2.046834908571243,
      "grad_norm": 1.2617517709732056,
      "learning_rate": 3.141674072374613e-05,
      "loss": 0.123,
      "step": 315100
    },
    {
      "epoch": 2.047484491214395,
      "grad_norm": 1.0432775020599365,
      "learning_rate": 3.140983026532014e-05,
      "loss": 0.1205,
      "step": 315200
    },
    {
      "epoch": 2.0481340738575464,
      "grad_norm": 0.94884192943573,
      "learning_rate": 3.1402919806894145e-05,
      "loss": 0.1237,
      "step": 315300
    },
    {
      "epoch": 2.0487836565006985,
      "grad_norm": 1.3896489143371582,
      "learning_rate": 3.1396009348468155e-05,
      "loss": 0.1159,
      "step": 315400
    },
    {
      "epoch": 2.04943323914385,
      "grad_norm": 1.2119343280792236,
      "learning_rate": 3.1389098890042165e-05,
      "loss": 0.1228,
      "step": 315500
    },
    {
      "epoch": 2.0500828217870017,
      "grad_norm": 0.9717231392860413,
      "learning_rate": 3.1382188431616175e-05,
      "loss": 0.1237,
      "step": 315600
    },
    {
      "epoch": 2.050732404430154,
      "grad_norm": 1.6155866384506226,
      "learning_rate": 3.1375277973190185e-05,
      "loss": 0.1174,
      "step": 315700
    },
    {
      "epoch": 2.0513819870733054,
      "grad_norm": 1.2319560050964355,
      "learning_rate": 3.1368367514764195e-05,
      "loss": 0.124,
      "step": 315800
    },
    {
      "epoch": 2.052031569716457,
      "grad_norm": 0.8810306787490845,
      "learning_rate": 3.1361457056338205e-05,
      "loss": 0.1229,
      "step": 315900
    },
    {
      "epoch": 2.052681152359609,
      "grad_norm": 1.1173125505447388,
      "learning_rate": 3.1354546597912214e-05,
      "loss": 0.1208,
      "step": 316000
    },
    {
      "epoch": 2.0533307350027608,
      "grad_norm": 1.1330792903900146,
      "learning_rate": 3.1347636139486224e-05,
      "loss": 0.1205,
      "step": 316100
    },
    {
      "epoch": 2.0539803176459124,
      "grad_norm": 0.8095589876174927,
      "learning_rate": 3.134072568106023e-05,
      "loss": 0.1216,
      "step": 316200
    },
    {
      "epoch": 2.0546299002890644,
      "grad_norm": 1.1040818691253662,
      "learning_rate": 3.133381522263424e-05,
      "loss": 0.1202,
      "step": 316300
    },
    {
      "epoch": 2.055279482932216,
      "grad_norm": 1.285866618156433,
      "learning_rate": 3.132690476420825e-05,
      "loss": 0.1224,
      "step": 316400
    },
    {
      "epoch": 2.0559290655753677,
      "grad_norm": 1.1433743238449097,
      "learning_rate": 3.131999430578226e-05,
      "loss": 0.1226,
      "step": 316500
    },
    {
      "epoch": 2.0565786482185198,
      "grad_norm": 1.1634316444396973,
      "learning_rate": 3.131308384735627e-05,
      "loss": 0.1241,
      "step": 316600
    },
    {
      "epoch": 2.0572282308616714,
      "grad_norm": 0.7855750918388367,
      "learning_rate": 3.130617338893028e-05,
      "loss": 0.1197,
      "step": 316700
    },
    {
      "epoch": 2.057877813504823,
      "grad_norm": 0.92742520570755,
      "learning_rate": 3.1299262930504286e-05,
      "loss": 0.1255,
      "step": 316800
    },
    {
      "epoch": 2.058527396147975,
      "grad_norm": 0.915703296661377,
      "learning_rate": 3.1292352472078296e-05,
      "loss": 0.1294,
      "step": 316900
    },
    {
      "epoch": 2.0591769787911267,
      "grad_norm": 1.173962950706482,
      "learning_rate": 3.1285442013652306e-05,
      "loss": 0.1187,
      "step": 317000
    },
    {
      "epoch": 2.0598265614342783,
      "grad_norm": 1.1051419973373413,
      "learning_rate": 3.1278531555226316e-05,
      "loss": 0.1261,
      "step": 317100
    },
    {
      "epoch": 2.0604761440774304,
      "grad_norm": 1.0984214544296265,
      "learning_rate": 3.127162109680032e-05,
      "loss": 0.1122,
      "step": 317200
    },
    {
      "epoch": 2.061125726720582,
      "grad_norm": 1.1189147233963013,
      "learning_rate": 3.126471063837433e-05,
      "loss": 0.1215,
      "step": 317300
    },
    {
      "epoch": 2.0617753093637337,
      "grad_norm": 1.2540006637573242,
      "learning_rate": 3.125780017994834e-05,
      "loss": 0.1266,
      "step": 317400
    },
    {
      "epoch": 2.0624248920068857,
      "grad_norm": 1.0512943267822266,
      "learning_rate": 3.125088972152235e-05,
      "loss": 0.1178,
      "step": 317500
    },
    {
      "epoch": 2.0630744746500373,
      "grad_norm": 1.548305630683899,
      "learning_rate": 3.124397926309636e-05,
      "loss": 0.1203,
      "step": 317600
    },
    {
      "epoch": 2.063724057293189,
      "grad_norm": 0.9261839389801025,
      "learning_rate": 3.123706880467037e-05,
      "loss": 0.1188,
      "step": 317700
    },
    {
      "epoch": 2.064373639936341,
      "grad_norm": 1.3254135847091675,
      "learning_rate": 3.123015834624438e-05,
      "loss": 0.119,
      "step": 317800
    },
    {
      "epoch": 2.0650232225794927,
      "grad_norm": 1.5475245714187622,
      "learning_rate": 3.122324788781839e-05,
      "loss": 0.1181,
      "step": 317900
    },
    {
      "epoch": 2.0656728052226443,
      "grad_norm": 1.314620018005371,
      "learning_rate": 3.12163374293924e-05,
      "loss": 0.1244,
      "step": 318000
    },
    {
      "epoch": 2.0663223878657964,
      "grad_norm": 1.4283192157745361,
      "learning_rate": 3.12094269709664e-05,
      "loss": 0.1213,
      "step": 318100
    },
    {
      "epoch": 2.066971970508948,
      "grad_norm": 1.1119424104690552,
      "learning_rate": 3.120251651254041e-05,
      "loss": 0.127,
      "step": 318200
    },
    {
      "epoch": 2.0676215531520996,
      "grad_norm": 0.8688318729400635,
      "learning_rate": 3.119560605411442e-05,
      "loss": 0.1155,
      "step": 318300
    },
    {
      "epoch": 2.0682711357952517,
      "grad_norm": 1.4089502096176147,
      "learning_rate": 3.118869559568843e-05,
      "loss": 0.1202,
      "step": 318400
    },
    {
      "epoch": 2.0689207184384033,
      "grad_norm": 1.03590989112854,
      "learning_rate": 3.1181785137262434e-05,
      "loss": 0.1277,
      "step": 318500
    },
    {
      "epoch": 2.069570301081555,
      "grad_norm": 1.0538946390151978,
      "learning_rate": 3.1174874678836443e-05,
      "loss": 0.1242,
      "step": 318600
    },
    {
      "epoch": 2.070219883724707,
      "grad_norm": 1.0585265159606934,
      "learning_rate": 3.116796422041045e-05,
      "loss": 0.1159,
      "step": 318700
    },
    {
      "epoch": 2.0708694663678586,
      "grad_norm": 1.1618943214416504,
      "learning_rate": 3.116105376198446e-05,
      "loss": 0.1131,
      "step": 318800
    },
    {
      "epoch": 2.0715190490110102,
      "grad_norm": 1.051592230796814,
      "learning_rate": 3.115414330355847e-05,
      "loss": 0.1229,
      "step": 318900
    },
    {
      "epoch": 2.0721686316541623,
      "grad_norm": 0.963543176651001,
      "learning_rate": 3.114723284513248e-05,
      "loss": 0.1177,
      "step": 319000
    },
    {
      "epoch": 2.072818214297314,
      "grad_norm": 1.4145921468734741,
      "learning_rate": 3.1140322386706486e-05,
      "loss": 0.1199,
      "step": 319100
    },
    {
      "epoch": 2.0734677969404656,
      "grad_norm": 1.1460778713226318,
      "learning_rate": 3.1133411928280496e-05,
      "loss": 0.1243,
      "step": 319200
    },
    {
      "epoch": 2.0741173795836176,
      "grad_norm": 0.7346607446670532,
      "learning_rate": 3.1126501469854506e-05,
      "loss": 0.1212,
      "step": 319300
    },
    {
      "epoch": 2.0747669622267693,
      "grad_norm": 1.0869423151016235,
      "learning_rate": 3.1119591011428516e-05,
      "loss": 0.1165,
      "step": 319400
    },
    {
      "epoch": 2.075416544869921,
      "grad_norm": 0.6306309700012207,
      "learning_rate": 3.1112680553002525e-05,
      "loss": 0.1182,
      "step": 319500
    },
    {
      "epoch": 2.076066127513073,
      "grad_norm": 1.0118552446365356,
      "learning_rate": 3.1105770094576535e-05,
      "loss": 0.1286,
      "step": 319600
    },
    {
      "epoch": 2.0767157101562246,
      "grad_norm": 0.8675832152366638,
      "learning_rate": 3.1098859636150545e-05,
      "loss": 0.1257,
      "step": 319700
    },
    {
      "epoch": 2.077365292799376,
      "grad_norm": 1.120965600013733,
      "learning_rate": 3.1091949177724555e-05,
      "loss": 0.1248,
      "step": 319800
    },
    {
      "epoch": 2.0780148754425283,
      "grad_norm": 0.7799596190452576,
      "learning_rate": 3.1085038719298565e-05,
      "loss": 0.1271,
      "step": 319900
    },
    {
      "epoch": 2.07866445808568,
      "grad_norm": 0.9302660822868347,
      "learning_rate": 3.107812826087257e-05,
      "loss": 0.1223,
      "step": 320000
    },
    {
      "epoch": 2.0793140407288315,
      "grad_norm": 1.5777442455291748,
      "learning_rate": 3.107121780244658e-05,
      "loss": 0.1236,
      "step": 320100
    },
    {
      "epoch": 2.0799636233719836,
      "grad_norm": 0.9788511991500854,
      "learning_rate": 3.106430734402059e-05,
      "loss": 0.1253,
      "step": 320200
    },
    {
      "epoch": 2.080613206015135,
      "grad_norm": 0.935004711151123,
      "learning_rate": 3.10573968855946e-05,
      "loss": 0.1253,
      "step": 320300
    },
    {
      "epoch": 2.081262788658287,
      "grad_norm": 0.9825157523155212,
      "learning_rate": 3.105048642716861e-05,
      "loss": 0.119,
      "step": 320400
    },
    {
      "epoch": 2.081912371301439,
      "grad_norm": 1.0872056484222412,
      "learning_rate": 3.104357596874262e-05,
      "loss": 0.119,
      "step": 320500
    },
    {
      "epoch": 2.0825619539445905,
      "grad_norm": 1.229607343673706,
      "learning_rate": 3.103666551031663e-05,
      "loss": 0.1151,
      "step": 320600
    },
    {
      "epoch": 2.083211536587742,
      "grad_norm": 0.9666739106178284,
      "learning_rate": 3.102975505189064e-05,
      "loss": 0.1245,
      "step": 320700
    },
    {
      "epoch": 2.0838611192308942,
      "grad_norm": 1.237630844116211,
      "learning_rate": 3.102284459346465e-05,
      "loss": 0.1157,
      "step": 320800
    },
    {
      "epoch": 2.084510701874046,
      "grad_norm": 1.3210152387619019,
      "learning_rate": 3.101593413503865e-05,
      "loss": 0.1218,
      "step": 320900
    },
    {
      "epoch": 2.0851602845171975,
      "grad_norm": 1.1959823369979858,
      "learning_rate": 3.100902367661266e-05,
      "loss": 0.1227,
      "step": 321000
    },
    {
      "epoch": 2.0858098671603496,
      "grad_norm": 1.367815613746643,
      "learning_rate": 3.100211321818667e-05,
      "loss": 0.1193,
      "step": 321100
    },
    {
      "epoch": 2.086459449803501,
      "grad_norm": 1.15962553024292,
      "learning_rate": 3.099520275976068e-05,
      "loss": 0.112,
      "step": 321200
    },
    {
      "epoch": 2.0871090324466532,
      "grad_norm": 1.3458603620529175,
      "learning_rate": 3.098829230133469e-05,
      "loss": 0.1162,
      "step": 321300
    },
    {
      "epoch": 2.087758615089805,
      "grad_norm": 1.097427487373352,
      "learning_rate": 3.09813818429087e-05,
      "loss": 0.1242,
      "step": 321400
    },
    {
      "epoch": 2.0884081977329565,
      "grad_norm": 1.079990029335022,
      "learning_rate": 3.097447138448271e-05,
      "loss": 0.1164,
      "step": 321500
    },
    {
      "epoch": 2.0890577803761086,
      "grad_norm": 1.0866873264312744,
      "learning_rate": 3.096756092605672e-05,
      "loss": 0.1209,
      "step": 321600
    },
    {
      "epoch": 2.08970736301926,
      "grad_norm": 1.4995893239974976,
      "learning_rate": 3.096065046763073e-05,
      "loss": 0.1206,
      "step": 321700
    },
    {
      "epoch": 2.090356945662412,
      "grad_norm": 1.0027685165405273,
      "learning_rate": 3.095374000920473e-05,
      "loss": 0.1217,
      "step": 321800
    },
    {
      "epoch": 2.091006528305564,
      "grad_norm": 1.5090222358703613,
      "learning_rate": 3.094682955077874e-05,
      "loss": 0.1203,
      "step": 321900
    },
    {
      "epoch": 2.0916561109487155,
      "grad_norm": 1.918508768081665,
      "learning_rate": 3.093991909235275e-05,
      "loss": 0.121,
      "step": 322000
    },
    {
      "epoch": 2.092305693591867,
      "grad_norm": 1.4119586944580078,
      "learning_rate": 3.0933008633926754e-05,
      "loss": 0.1209,
      "step": 322100
    },
    {
      "epoch": 2.092955276235019,
      "grad_norm": 0.8992457389831543,
      "learning_rate": 3.0926098175500764e-05,
      "loss": 0.1178,
      "step": 322200
    },
    {
      "epoch": 2.093604858878171,
      "grad_norm": 0.7173193693161011,
      "learning_rate": 3.0919187717074774e-05,
      "loss": 0.1141,
      "step": 322300
    },
    {
      "epoch": 2.0942544415213225,
      "grad_norm": 1.1075570583343506,
      "learning_rate": 3.0912277258648784e-05,
      "loss": 0.1223,
      "step": 322400
    },
    {
      "epoch": 2.0949040241644745,
      "grad_norm": 1.2805907726287842,
      "learning_rate": 3.0905366800222794e-05,
      "loss": 0.1155,
      "step": 322500
    },
    {
      "epoch": 2.095553606807626,
      "grad_norm": 1.023730993270874,
      "learning_rate": 3.0898456341796804e-05,
      "loss": 0.1211,
      "step": 322600
    },
    {
      "epoch": 2.0962031894507778,
      "grad_norm": 1.18217933177948,
      "learning_rate": 3.0891545883370814e-05,
      "loss": 0.1179,
      "step": 322700
    },
    {
      "epoch": 2.09685277209393,
      "grad_norm": 1.0590952634811401,
      "learning_rate": 3.0884635424944817e-05,
      "loss": 0.1243,
      "step": 322800
    },
    {
      "epoch": 2.0975023547370815,
      "grad_norm": 0.8374660015106201,
      "learning_rate": 3.0877724966518826e-05,
      "loss": 0.1235,
      "step": 322900
    },
    {
      "epoch": 2.098151937380233,
      "grad_norm": 1.0819599628448486,
      "learning_rate": 3.0870814508092836e-05,
      "loss": 0.1244,
      "step": 323000
    },
    {
      "epoch": 2.098801520023385,
      "grad_norm": 1.2362549304962158,
      "learning_rate": 3.0863904049666846e-05,
      "loss": 0.1237,
      "step": 323100
    },
    {
      "epoch": 2.099451102666537,
      "grad_norm": 1.2919129133224487,
      "learning_rate": 3.0856993591240856e-05,
      "loss": 0.1181,
      "step": 323200
    },
    {
      "epoch": 2.1001006853096884,
      "grad_norm": 1.2715493440628052,
      "learning_rate": 3.0850083132814866e-05,
      "loss": 0.1204,
      "step": 323300
    },
    {
      "epoch": 2.1007502679528405,
      "grad_norm": 0.9598077535629272,
      "learning_rate": 3.0843172674388876e-05,
      "loss": 0.1134,
      "step": 323400
    },
    {
      "epoch": 2.101399850595992,
      "grad_norm": 1.2668752670288086,
      "learning_rate": 3.0836262215962886e-05,
      "loss": 0.1235,
      "step": 323500
    },
    {
      "epoch": 2.1020494332391437,
      "grad_norm": 0.7458949685096741,
      "learning_rate": 3.0829351757536895e-05,
      "loss": 0.1209,
      "step": 323600
    },
    {
      "epoch": 2.102699015882296,
      "grad_norm": 1.1691068410873413,
      "learning_rate": 3.0822441299110905e-05,
      "loss": 0.1218,
      "step": 323700
    },
    {
      "epoch": 2.1033485985254474,
      "grad_norm": 1.1093798875808716,
      "learning_rate": 3.081553084068491e-05,
      "loss": 0.1252,
      "step": 323800
    },
    {
      "epoch": 2.103998181168599,
      "grad_norm": 1.0170137882232666,
      "learning_rate": 3.080862038225892e-05,
      "loss": 0.1178,
      "step": 323900
    },
    {
      "epoch": 2.104647763811751,
      "grad_norm": 1.0381150245666504,
      "learning_rate": 3.080170992383293e-05,
      "loss": 0.1174,
      "step": 324000
    },
    {
      "epoch": 2.1052973464549027,
      "grad_norm": 0.7821545600891113,
      "learning_rate": 3.079479946540694e-05,
      "loss": 0.1247,
      "step": 324100
    },
    {
      "epoch": 2.1059469290980544,
      "grad_norm": 1.0278592109680176,
      "learning_rate": 3.078788900698095e-05,
      "loss": 0.1211,
      "step": 324200
    },
    {
      "epoch": 2.1065965117412064,
      "grad_norm": 0.939203143119812,
      "learning_rate": 3.078097854855496e-05,
      "loss": 0.1171,
      "step": 324300
    },
    {
      "epoch": 2.107246094384358,
      "grad_norm": 1.1590014696121216,
      "learning_rate": 3.077406809012897e-05,
      "loss": 0.121,
      "step": 324400
    },
    {
      "epoch": 2.1078956770275097,
      "grad_norm": 1.2315475940704346,
      "learning_rate": 3.076715763170298e-05,
      "loss": 0.1265,
      "step": 324500
    },
    {
      "epoch": 2.1085452596706618,
      "grad_norm": 1.032329797744751,
      "learning_rate": 3.076024717327699e-05,
      "loss": 0.1182,
      "step": 324600
    },
    {
      "epoch": 2.1091948423138134,
      "grad_norm": 1.5124505758285522,
      "learning_rate": 3.075333671485099e-05,
      "loss": 0.1228,
      "step": 324700
    },
    {
      "epoch": 2.109844424956965,
      "grad_norm": 1.2390660047531128,
      "learning_rate": 3.0746426256425e-05,
      "loss": 0.1213,
      "step": 324800
    },
    {
      "epoch": 2.110494007600117,
      "grad_norm": 1.4462083578109741,
      "learning_rate": 3.073951579799901e-05,
      "loss": 0.1177,
      "step": 324900
    },
    {
      "epoch": 2.1111435902432687,
      "grad_norm": 1.7353613376617432,
      "learning_rate": 3.073260533957302e-05,
      "loss": 0.1252,
      "step": 325000
    },
    {
      "epoch": 2.1117931728864203,
      "grad_norm": 1.1498852968215942,
      "learning_rate": 3.072569488114703e-05,
      "loss": 0.1217,
      "step": 325100
    },
    {
      "epoch": 2.1124427555295724,
      "grad_norm": 1.1608034372329712,
      "learning_rate": 3.071878442272104e-05,
      "loss": 0.1232,
      "step": 325200
    },
    {
      "epoch": 2.113092338172724,
      "grad_norm": 1.063429355621338,
      "learning_rate": 3.071187396429504e-05,
      "loss": 0.1263,
      "step": 325300
    },
    {
      "epoch": 2.1137419208158756,
      "grad_norm": 1.2343807220458984,
      "learning_rate": 3.070496350586905e-05,
      "loss": 0.1197,
      "step": 325400
    },
    {
      "epoch": 2.1143915034590277,
      "grad_norm": 1.3905532360076904,
      "learning_rate": 3.069805304744306e-05,
      "loss": 0.1186,
      "step": 325500
    },
    {
      "epoch": 2.1150410861021793,
      "grad_norm": 0.9847021698951721,
      "learning_rate": 3.069114258901707e-05,
      "loss": 0.1245,
      "step": 325600
    },
    {
      "epoch": 2.115690668745331,
      "grad_norm": 1.1530624628067017,
      "learning_rate": 3.0684232130591075e-05,
      "loss": 0.1218,
      "step": 325700
    },
    {
      "epoch": 2.116340251388483,
      "grad_norm": 0.7398521304130554,
      "learning_rate": 3.0677321672165085e-05,
      "loss": 0.1198,
      "step": 325800
    },
    {
      "epoch": 2.1169898340316347,
      "grad_norm": 1.3042199611663818,
      "learning_rate": 3.0670411213739095e-05,
      "loss": 0.122,
      "step": 325900
    },
    {
      "epoch": 2.1176394166747863,
      "grad_norm": 1.315364956855774,
      "learning_rate": 3.0663500755313105e-05,
      "loss": 0.1199,
      "step": 326000
    },
    {
      "epoch": 2.1182889993179383,
      "grad_norm": 1.0332815647125244,
      "learning_rate": 3.0656590296887115e-05,
      "loss": 0.1137,
      "step": 326100
    },
    {
      "epoch": 2.11893858196109,
      "grad_norm": 1.4164297580718994,
      "learning_rate": 3.0649679838461124e-05,
      "loss": 0.1189,
      "step": 326200
    },
    {
      "epoch": 2.1195881646042416,
      "grad_norm": 1.2769886255264282,
      "learning_rate": 3.0642769380035134e-05,
      "loss": 0.1188,
      "step": 326300
    },
    {
      "epoch": 2.1202377472473937,
      "grad_norm": 0.8789976239204407,
      "learning_rate": 3.0635858921609144e-05,
      "loss": 0.1167,
      "step": 326400
    },
    {
      "epoch": 2.1208873298905453,
      "grad_norm": 0.9709554314613342,
      "learning_rate": 3.0628948463183154e-05,
      "loss": 0.119,
      "step": 326500
    },
    {
      "epoch": 2.121536912533697,
      "grad_norm": 1.3538470268249512,
      "learning_rate": 3.062203800475716e-05,
      "loss": 0.1253,
      "step": 326600
    },
    {
      "epoch": 2.122186495176849,
      "grad_norm": 1.0604091882705688,
      "learning_rate": 3.061512754633117e-05,
      "loss": 0.1188,
      "step": 326700
    },
    {
      "epoch": 2.1228360778200006,
      "grad_norm": 1.0546690225601196,
      "learning_rate": 3.060821708790518e-05,
      "loss": 0.1178,
      "step": 326800
    },
    {
      "epoch": 2.1234856604631522,
      "grad_norm": 1.1139945983886719,
      "learning_rate": 3.060130662947919e-05,
      "loss": 0.1217,
      "step": 326900
    },
    {
      "epoch": 2.1241352431063043,
      "grad_norm": 1.1713143587112427,
      "learning_rate": 3.0594396171053197e-05,
      "loss": 0.1215,
      "step": 327000
    },
    {
      "epoch": 2.124784825749456,
      "grad_norm": 1.026777982711792,
      "learning_rate": 3.0587485712627206e-05,
      "loss": 0.1135,
      "step": 327100
    },
    {
      "epoch": 2.1254344083926076,
      "grad_norm": 0.9671640992164612,
      "learning_rate": 3.0580575254201216e-05,
      "loss": 0.1185,
      "step": 327200
    },
    {
      "epoch": 2.1260839910357596,
      "grad_norm": 1.1083252429962158,
      "learning_rate": 3.0573664795775226e-05,
      "loss": 0.1232,
      "step": 327300
    },
    {
      "epoch": 2.1267335736789112,
      "grad_norm": 1.2915416955947876,
      "learning_rate": 3.0566754337349236e-05,
      "loss": 0.1182,
      "step": 327400
    },
    {
      "epoch": 2.127383156322063,
      "grad_norm": 1.3071959018707275,
      "learning_rate": 3.055984387892324e-05,
      "loss": 0.116,
      "step": 327500
    },
    {
      "epoch": 2.128032738965215,
      "grad_norm": 1.2103190422058105,
      "learning_rate": 3.055293342049725e-05,
      "loss": 0.119,
      "step": 327600
    },
    {
      "epoch": 2.1286823216083666,
      "grad_norm": 1.31194269657135,
      "learning_rate": 3.054602296207126e-05,
      "loss": 0.1186,
      "step": 327700
    },
    {
      "epoch": 2.129331904251518,
      "grad_norm": 0.6923289895057678,
      "learning_rate": 3.053911250364527e-05,
      "loss": 0.1186,
      "step": 327800
    },
    {
      "epoch": 2.1299814868946703,
      "grad_norm": 1.019437551498413,
      "learning_rate": 3.053220204521928e-05,
      "loss": 0.1182,
      "step": 327900
    },
    {
      "epoch": 2.130631069537822,
      "grad_norm": 1.2573633193969727,
      "learning_rate": 3.052529158679329e-05,
      "loss": 0.1126,
      "step": 328000
    },
    {
      "epoch": 2.1312806521809735,
      "grad_norm": 1.0574768781661987,
      "learning_rate": 3.05183811283673e-05,
      "loss": 0.1159,
      "step": 328100
    },
    {
      "epoch": 2.1319302348241256,
      "grad_norm": 1.6062731742858887,
      "learning_rate": 3.0511470669941305e-05,
      "loss": 0.1146,
      "step": 328200
    },
    {
      "epoch": 2.132579817467277,
      "grad_norm": 0.6774397492408752,
      "learning_rate": 3.0504560211515314e-05,
      "loss": 0.1247,
      "step": 328300
    },
    {
      "epoch": 2.1332294001104293,
      "grad_norm": 1.310002088546753,
      "learning_rate": 3.0497649753089324e-05,
      "loss": 0.1244,
      "step": 328400
    },
    {
      "epoch": 2.133878982753581,
      "grad_norm": 1.0106793642044067,
      "learning_rate": 3.0490739294663327e-05,
      "loss": 0.1244,
      "step": 328500
    },
    {
      "epoch": 2.1345285653967325,
      "grad_norm": 0.9173144698143005,
      "learning_rate": 3.0483828836237337e-05,
      "loss": 0.1188,
      "step": 328600
    },
    {
      "epoch": 2.135178148039884,
      "grad_norm": 1.1105291843414307,
      "learning_rate": 3.0476918377811347e-05,
      "loss": 0.1171,
      "step": 328700
    },
    {
      "epoch": 2.135827730683036,
      "grad_norm": 0.7574138045310974,
      "learning_rate": 3.0470007919385357e-05,
      "loss": 0.1169,
      "step": 328800
    },
    {
      "epoch": 2.136477313326188,
      "grad_norm": 1.5571925640106201,
      "learning_rate": 3.0463097460959367e-05,
      "loss": 0.1222,
      "step": 328900
    },
    {
      "epoch": 2.13712689596934,
      "grad_norm": 1.3248505592346191,
      "learning_rate": 3.0456187002533377e-05,
      "loss": 0.1255,
      "step": 329000
    },
    {
      "epoch": 2.1377764786124915,
      "grad_norm": 1.5674407482147217,
      "learning_rate": 3.0449276544107386e-05,
      "loss": 0.1229,
      "step": 329100
    },
    {
      "epoch": 2.138426061255643,
      "grad_norm": 1.3240567445755005,
      "learning_rate": 3.0442366085681396e-05,
      "loss": 0.1207,
      "step": 329200
    },
    {
      "epoch": 2.1390756438987952,
      "grad_norm": 1.2143014669418335,
      "learning_rate": 3.0435455627255406e-05,
      "loss": 0.1129,
      "step": 329300
    },
    {
      "epoch": 2.139725226541947,
      "grad_norm": 1.406795859336853,
      "learning_rate": 3.042854516882941e-05,
      "loss": 0.1208,
      "step": 329400
    },
    {
      "epoch": 2.1403748091850985,
      "grad_norm": 1.2887309789657593,
      "learning_rate": 3.042163471040342e-05,
      "loss": 0.1133,
      "step": 329500
    },
    {
      "epoch": 2.1410243918282506,
      "grad_norm": 0.9873607158660889,
      "learning_rate": 3.041472425197743e-05,
      "loss": 0.1226,
      "step": 329600
    },
    {
      "epoch": 2.141673974471402,
      "grad_norm": 1.2933893203735352,
      "learning_rate": 3.040781379355144e-05,
      "loss": 0.118,
      "step": 329700
    },
    {
      "epoch": 2.142323557114554,
      "grad_norm": 1.0428094863891602,
      "learning_rate": 3.0400903335125445e-05,
      "loss": 0.119,
      "step": 329800
    },
    {
      "epoch": 2.142973139757706,
      "grad_norm": 1.2639557123184204,
      "learning_rate": 3.0393992876699455e-05,
      "loss": 0.1175,
      "step": 329900
    },
    {
      "epoch": 2.1436227224008575,
      "grad_norm": 0.9715728163719177,
      "learning_rate": 3.0387082418273465e-05,
      "loss": 0.1147,
      "step": 330000
    },
    {
      "epoch": 2.144272305044009,
      "grad_norm": 1.4151283502578735,
      "learning_rate": 3.0380171959847475e-05,
      "loss": 0.1187,
      "step": 330100
    },
    {
      "epoch": 2.144921887687161,
      "grad_norm": 1.0750020742416382,
      "learning_rate": 3.0373261501421485e-05,
      "loss": 0.1159,
      "step": 330200
    },
    {
      "epoch": 2.145571470330313,
      "grad_norm": 0.850269079208374,
      "learning_rate": 3.0366351042995488e-05,
      "loss": 0.1208,
      "step": 330300
    },
    {
      "epoch": 2.1462210529734644,
      "grad_norm": 1.097684383392334,
      "learning_rate": 3.0359440584569498e-05,
      "loss": 0.1241,
      "step": 330400
    },
    {
      "epoch": 2.1468706356166165,
      "grad_norm": 1.3559669256210327,
      "learning_rate": 3.0352530126143507e-05,
      "loss": 0.1156,
      "step": 330500
    },
    {
      "epoch": 2.147520218259768,
      "grad_norm": 0.9238687753677368,
      "learning_rate": 3.0345619667717517e-05,
      "loss": 0.1184,
      "step": 330600
    },
    {
      "epoch": 2.1481698009029198,
      "grad_norm": 0.8588829636573792,
      "learning_rate": 3.0338709209291527e-05,
      "loss": 0.1158,
      "step": 330700
    },
    {
      "epoch": 2.148819383546072,
      "grad_norm": 2.127662420272827,
      "learning_rate": 3.0331798750865537e-05,
      "loss": 0.1233,
      "step": 330800
    },
    {
      "epoch": 2.1494689661892235,
      "grad_norm": 0.829268753528595,
      "learning_rate": 3.0324888292439547e-05,
      "loss": 0.1184,
      "step": 330900
    },
    {
      "epoch": 2.150118548832375,
      "grad_norm": 1.1982131004333496,
      "learning_rate": 3.0317977834013557e-05,
      "loss": 0.1135,
      "step": 331000
    },
    {
      "epoch": 2.150768131475527,
      "grad_norm": 1.2230993509292603,
      "learning_rate": 3.0311067375587567e-05,
      "loss": 0.1184,
      "step": 331100
    },
    {
      "epoch": 2.1514177141186788,
      "grad_norm": 1.6562665700912476,
      "learning_rate": 3.0304156917161573e-05,
      "loss": 0.1189,
      "step": 331200
    },
    {
      "epoch": 2.1520672967618304,
      "grad_norm": 0.987740695476532,
      "learning_rate": 3.029724645873558e-05,
      "loss": 0.1132,
      "step": 331300
    },
    {
      "epoch": 2.1527168794049825,
      "grad_norm": 0.9196268916130066,
      "learning_rate": 3.029033600030959e-05,
      "loss": 0.1218,
      "step": 331400
    },
    {
      "epoch": 2.153366462048134,
      "grad_norm": 1.0975230932235718,
      "learning_rate": 3.02834255418836e-05,
      "loss": 0.1127,
      "step": 331500
    },
    {
      "epoch": 2.1540160446912857,
      "grad_norm": 1.1123405694961548,
      "learning_rate": 3.0276515083457606e-05,
      "loss": 0.1168,
      "step": 331600
    },
    {
      "epoch": 2.154665627334438,
      "grad_norm": 0.9458540678024292,
      "learning_rate": 3.0269604625031615e-05,
      "loss": 0.1204,
      "step": 331700
    },
    {
      "epoch": 2.1553152099775894,
      "grad_norm": 1.8872246742248535,
      "learning_rate": 3.0262694166605625e-05,
      "loss": 0.1152,
      "step": 331800
    },
    {
      "epoch": 2.155964792620741,
      "grad_norm": 1.1590309143066406,
      "learning_rate": 3.0255783708179635e-05,
      "loss": 0.117,
      "step": 331900
    },
    {
      "epoch": 2.156614375263893,
      "grad_norm": 1.4178577661514282,
      "learning_rate": 3.0248873249753645e-05,
      "loss": 0.1192,
      "step": 332000
    },
    {
      "epoch": 2.1572639579070447,
      "grad_norm": 1.0090569257736206,
      "learning_rate": 3.0241962791327655e-05,
      "loss": 0.1183,
      "step": 332100
    },
    {
      "epoch": 2.1579135405501964,
      "grad_norm": 1.5770564079284668,
      "learning_rate": 3.0235052332901658e-05,
      "loss": 0.1215,
      "step": 332200
    },
    {
      "epoch": 2.1585631231933484,
      "grad_norm": 1.3102519512176514,
      "learning_rate": 3.0228141874475668e-05,
      "loss": 0.1191,
      "step": 332300
    },
    {
      "epoch": 2.1592127058365,
      "grad_norm": 0.7984986901283264,
      "learning_rate": 3.0221231416049678e-05,
      "loss": 0.118,
      "step": 332400
    },
    {
      "epoch": 2.1598622884796517,
      "grad_norm": 1.3804608583450317,
      "learning_rate": 3.0214320957623688e-05,
      "loss": 0.1164,
      "step": 332500
    },
    {
      "epoch": 2.1605118711228037,
      "grad_norm": 0.871439516544342,
      "learning_rate": 3.0207410499197697e-05,
      "loss": 0.1203,
      "step": 332600
    },
    {
      "epoch": 2.1611614537659554,
      "grad_norm": 1.332802653312683,
      "learning_rate": 3.0200500040771707e-05,
      "loss": 0.1196,
      "step": 332700
    },
    {
      "epoch": 2.161811036409107,
      "grad_norm": 1.288927674293518,
      "learning_rate": 3.0193589582345717e-05,
      "loss": 0.1223,
      "step": 332800
    },
    {
      "epoch": 2.162460619052259,
      "grad_norm": 1.2942696809768677,
      "learning_rate": 3.0186679123919727e-05,
      "loss": 0.1205,
      "step": 332900
    },
    {
      "epoch": 2.1631102016954107,
      "grad_norm": 0.5420283079147339,
      "learning_rate": 3.0179768665493733e-05,
      "loss": 0.1215,
      "step": 333000
    },
    {
      "epoch": 2.1637597843385623,
      "grad_norm": 0.9310777187347412,
      "learning_rate": 3.0172858207067743e-05,
      "loss": 0.1212,
      "step": 333100
    },
    {
      "epoch": 2.1644093669817144,
      "grad_norm": 1.4139246940612793,
      "learning_rate": 3.016594774864175e-05,
      "loss": 0.1212,
      "step": 333200
    },
    {
      "epoch": 2.165058949624866,
      "grad_norm": 1.1123788356781006,
      "learning_rate": 3.015903729021576e-05,
      "loss": 0.1214,
      "step": 333300
    },
    {
      "epoch": 2.1657085322680176,
      "grad_norm": 1.2104763984680176,
      "learning_rate": 3.0152126831789766e-05,
      "loss": 0.1232,
      "step": 333400
    },
    {
      "epoch": 2.1663581149111697,
      "grad_norm": 1.0172291994094849,
      "learning_rate": 3.0145216373363776e-05,
      "loss": 0.1133,
      "step": 333500
    },
    {
      "epoch": 2.1670076975543213,
      "grad_norm": 1.0893102884292603,
      "learning_rate": 3.0138305914937786e-05,
      "loss": 0.1116,
      "step": 333600
    },
    {
      "epoch": 2.167657280197473,
      "grad_norm": 1.0290931463241577,
      "learning_rate": 3.0131395456511796e-05,
      "loss": 0.122,
      "step": 333700
    },
    {
      "epoch": 2.168306862840625,
      "grad_norm": 0.9306284785270691,
      "learning_rate": 3.0124484998085805e-05,
      "loss": 0.122,
      "step": 333800
    },
    {
      "epoch": 2.1689564454837766,
      "grad_norm": 1.302219033241272,
      "learning_rate": 3.0117574539659815e-05,
      "loss": 0.116,
      "step": 333900
    },
    {
      "epoch": 2.1696060281269283,
      "grad_norm": 1.4220964908599854,
      "learning_rate": 3.0110664081233825e-05,
      "loss": 0.1188,
      "step": 334000
    },
    {
      "epoch": 2.1702556107700803,
      "grad_norm": 0.9714597463607788,
      "learning_rate": 3.0103753622807828e-05,
      "loss": 0.1191,
      "step": 334100
    },
    {
      "epoch": 2.170905193413232,
      "grad_norm": 0.9921350479125977,
      "learning_rate": 3.0096843164381838e-05,
      "loss": 0.1181,
      "step": 334200
    },
    {
      "epoch": 2.1715547760563836,
      "grad_norm": 1.0335224866867065,
      "learning_rate": 3.0089932705955848e-05,
      "loss": 0.1128,
      "step": 334300
    },
    {
      "epoch": 2.1722043586995357,
      "grad_norm": 1.5485401153564453,
      "learning_rate": 3.0083022247529858e-05,
      "loss": 0.1177,
      "step": 334400
    },
    {
      "epoch": 2.1728539413426873,
      "grad_norm": 1.2377935647964478,
      "learning_rate": 3.0076111789103868e-05,
      "loss": 0.1138,
      "step": 334500
    },
    {
      "epoch": 2.173503523985839,
      "grad_norm": 1.0666145086288452,
      "learning_rate": 3.0069201330677877e-05,
      "loss": 0.1203,
      "step": 334600
    },
    {
      "epoch": 2.174153106628991,
      "grad_norm": 0.8593149781227112,
      "learning_rate": 3.0062290872251887e-05,
      "loss": 0.1201,
      "step": 334700
    },
    {
      "epoch": 2.1748026892721426,
      "grad_norm": 0.8661556243896484,
      "learning_rate": 3.0055380413825894e-05,
      "loss": 0.116,
      "step": 334800
    },
    {
      "epoch": 2.1754522719152942,
      "grad_norm": 1.1085056066513062,
      "learning_rate": 3.0048469955399904e-05,
      "loss": 0.1189,
      "step": 334900
    },
    {
      "epoch": 2.1761018545584463,
      "grad_norm": 0.947087287902832,
      "learning_rate": 3.004155949697391e-05,
      "loss": 0.1187,
      "step": 335000
    },
    {
      "epoch": 2.176751437201598,
      "grad_norm": 1.2158048152923584,
      "learning_rate": 3.003464903854792e-05,
      "loss": 0.1183,
      "step": 335100
    },
    {
      "epoch": 2.1774010198447495,
      "grad_norm": 1.5213322639465332,
      "learning_rate": 3.0027738580121926e-05,
      "loss": 0.1177,
      "step": 335200
    },
    {
      "epoch": 2.1780506024879016,
      "grad_norm": 1.188960075378418,
      "learning_rate": 3.0020828121695936e-05,
      "loss": 0.12,
      "step": 335300
    },
    {
      "epoch": 2.1787001851310532,
      "grad_norm": 1.197740077972412,
      "learning_rate": 3.0013917663269946e-05,
      "loss": 0.117,
      "step": 335400
    },
    {
      "epoch": 2.1793497677742053,
      "grad_norm": 0.9761002659797668,
      "learning_rate": 3.0007007204843956e-05,
      "loss": 0.1217,
      "step": 335500
    },
    {
      "epoch": 2.179999350417357,
      "grad_norm": 0.7689167857170105,
      "learning_rate": 3.0000096746417966e-05,
      "loss": 0.114,
      "step": 335600
    },
    {
      "epoch": 2.1806489330605086,
      "grad_norm": 0.8398715257644653,
      "learning_rate": 2.9993186287991976e-05,
      "loss": 0.1183,
      "step": 335700
    },
    {
      "epoch": 2.18129851570366,
      "grad_norm": 1.0493110418319702,
      "learning_rate": 2.9986275829565986e-05,
      "loss": 0.1203,
      "step": 335800
    },
    {
      "epoch": 2.1819480983468122,
      "grad_norm": 1.0488615036010742,
      "learning_rate": 2.9979365371139995e-05,
      "loss": 0.114,
      "step": 335900
    },
    {
      "epoch": 2.182597680989964,
      "grad_norm": 0.8883906006813049,
      "learning_rate": 2.9972454912714e-05,
      "loss": 0.1137,
      "step": 336000
    },
    {
      "epoch": 2.183247263633116,
      "grad_norm": 0.8385868072509766,
      "learning_rate": 2.996554445428801e-05,
      "loss": 0.1211,
      "step": 336100
    },
    {
      "epoch": 2.1838968462762676,
      "grad_norm": 0.739604651927948,
      "learning_rate": 2.9958633995862018e-05,
      "loss": 0.1236,
      "step": 336200
    },
    {
      "epoch": 2.184546428919419,
      "grad_norm": 0.9876843690872192,
      "learning_rate": 2.9951723537436028e-05,
      "loss": 0.1215,
      "step": 336300
    },
    {
      "epoch": 2.185196011562571,
      "grad_norm": 1.2181724309921265,
      "learning_rate": 2.9944813079010038e-05,
      "loss": 0.1148,
      "step": 336400
    },
    {
      "epoch": 2.185845594205723,
      "grad_norm": 2.1694202423095703,
      "learning_rate": 2.9937902620584048e-05,
      "loss": 0.1217,
      "step": 336500
    },
    {
      "epoch": 2.1864951768488745,
      "grad_norm": 1.6045185327529907,
      "learning_rate": 2.9930992162158054e-05,
      "loss": 0.1159,
      "step": 336600
    },
    {
      "epoch": 2.1871447594920266,
      "grad_norm": 1.3918935060501099,
      "learning_rate": 2.9924081703732064e-05,
      "loss": 0.1229,
      "step": 336700
    },
    {
      "epoch": 2.187794342135178,
      "grad_norm": 1.340720772743225,
      "learning_rate": 2.9917171245306074e-05,
      "loss": 0.1173,
      "step": 336800
    },
    {
      "epoch": 2.18844392477833,
      "grad_norm": 1.4642555713653564,
      "learning_rate": 2.991026078688008e-05,
      "loss": 0.1151,
      "step": 336900
    },
    {
      "epoch": 2.189093507421482,
      "grad_norm": 1.2845388650894165,
      "learning_rate": 2.9903350328454087e-05,
      "loss": 0.1248,
      "step": 337000
    },
    {
      "epoch": 2.1897430900646335,
      "grad_norm": 0.6744624376296997,
      "learning_rate": 2.9896439870028097e-05,
      "loss": 0.1225,
      "step": 337100
    },
    {
      "epoch": 2.190392672707785,
      "grad_norm": 0.745684027671814,
      "learning_rate": 2.9889529411602107e-05,
      "loss": 0.1305,
      "step": 337200
    },
    {
      "epoch": 2.191042255350937,
      "grad_norm": 1.103588342666626,
      "learning_rate": 2.9882618953176116e-05,
      "loss": 0.1178,
      "step": 337300
    },
    {
      "epoch": 2.191691837994089,
      "grad_norm": 1.814621925354004,
      "learning_rate": 2.9875708494750126e-05,
      "loss": 0.113,
      "step": 337400
    },
    {
      "epoch": 2.1923414206372405,
      "grad_norm": 0.8634632229804993,
      "learning_rate": 2.9868798036324136e-05,
      "loss": 0.1228,
      "step": 337500
    },
    {
      "epoch": 2.1929910032803925,
      "grad_norm": 1.2720270156860352,
      "learning_rate": 2.9861887577898146e-05,
      "loss": 0.1201,
      "step": 337600
    },
    {
      "epoch": 2.193640585923544,
      "grad_norm": 0.9765012860298157,
      "learning_rate": 2.9854977119472156e-05,
      "loss": 0.1193,
      "step": 337700
    },
    {
      "epoch": 2.194290168566696,
      "grad_norm": 1.1679925918579102,
      "learning_rate": 2.9848066661046166e-05,
      "loss": 0.1178,
      "step": 337800
    },
    {
      "epoch": 2.194939751209848,
      "grad_norm": 0.86247318983078,
      "learning_rate": 2.984115620262017e-05,
      "loss": 0.1182,
      "step": 337900
    },
    {
      "epoch": 2.1955893338529995,
      "grad_norm": 0.8448802828788757,
      "learning_rate": 2.983424574419418e-05,
      "loss": 0.1162,
      "step": 338000
    },
    {
      "epoch": 2.196238916496151,
      "grad_norm": 0.8503199815750122,
      "learning_rate": 2.982733528576819e-05,
      "loss": 0.1182,
      "step": 338100
    },
    {
      "epoch": 2.196888499139303,
      "grad_norm": 1.2162443399429321,
      "learning_rate": 2.9820424827342198e-05,
      "loss": 0.119,
      "step": 338200
    },
    {
      "epoch": 2.197538081782455,
      "grad_norm": 1.2768065929412842,
      "learning_rate": 2.9813514368916208e-05,
      "loss": 0.1179,
      "step": 338300
    },
    {
      "epoch": 2.1981876644256064,
      "grad_norm": 1.6509332656860352,
      "learning_rate": 2.9806603910490215e-05,
      "loss": 0.1207,
      "step": 338400
    },
    {
      "epoch": 2.1988372470687585,
      "grad_norm": 1.236708402633667,
      "learning_rate": 2.9799693452064224e-05,
      "loss": 0.1218,
      "step": 338500
    },
    {
      "epoch": 2.19948682971191,
      "grad_norm": 1.1074442863464355,
      "learning_rate": 2.9792782993638234e-05,
      "loss": 0.1223,
      "step": 338600
    },
    {
      "epoch": 2.2001364123550617,
      "grad_norm": 1.1807174682617188,
      "learning_rate": 2.9785872535212244e-05,
      "loss": 0.1158,
      "step": 338700
    },
    {
      "epoch": 2.200785994998214,
      "grad_norm": 1.1375147104263306,
      "learning_rate": 2.9778962076786247e-05,
      "loss": 0.1182,
      "step": 338800
    },
    {
      "epoch": 2.2014355776413654,
      "grad_norm": 1.110518455505371,
      "learning_rate": 2.9772051618360257e-05,
      "loss": 0.1173,
      "step": 338900
    },
    {
      "epoch": 2.202085160284517,
      "grad_norm": 1.4417117834091187,
      "learning_rate": 2.9765141159934267e-05,
      "loss": 0.1201,
      "step": 339000
    },
    {
      "epoch": 2.202734742927669,
      "grad_norm": 1.0011523962020874,
      "learning_rate": 2.9758230701508277e-05,
      "loss": 0.1189,
      "step": 339100
    },
    {
      "epoch": 2.2033843255708208,
      "grad_norm": 0.9306291937828064,
      "learning_rate": 2.9751320243082287e-05,
      "loss": 0.1215,
      "step": 339200
    },
    {
      "epoch": 2.2040339082139724,
      "grad_norm": 1.317947506904602,
      "learning_rate": 2.9744409784656296e-05,
      "loss": 0.1235,
      "step": 339300
    },
    {
      "epoch": 2.2046834908571245,
      "grad_norm": 1.0466985702514648,
      "learning_rate": 2.9737499326230306e-05,
      "loss": 0.1197,
      "step": 339400
    },
    {
      "epoch": 2.205333073500276,
      "grad_norm": 1.0044047832489014,
      "learning_rate": 2.9730588867804316e-05,
      "loss": 0.1186,
      "step": 339500
    },
    {
      "epoch": 2.2059826561434277,
      "grad_norm": 1.484885811805725,
      "learning_rate": 2.9723678409378326e-05,
      "loss": 0.118,
      "step": 339600
    },
    {
      "epoch": 2.2066322387865798,
      "grad_norm": 0.8383066654205322,
      "learning_rate": 2.9716767950952336e-05,
      "loss": 0.1194,
      "step": 339700
    },
    {
      "epoch": 2.2072818214297314,
      "grad_norm": 0.9787412881851196,
      "learning_rate": 2.970985749252634e-05,
      "loss": 0.1201,
      "step": 339800
    },
    {
      "epoch": 2.207931404072883,
      "grad_norm": 0.8714255094528198,
      "learning_rate": 2.970294703410035e-05,
      "loss": 0.1145,
      "step": 339900
    },
    {
      "epoch": 2.208580986716035,
      "grad_norm": 0.8100884556770325,
      "learning_rate": 2.969603657567436e-05,
      "loss": 0.1204,
      "step": 340000
    },
    {
      "epoch": 2.2092305693591867,
      "grad_norm": 1.1874589920043945,
      "learning_rate": 2.968912611724837e-05,
      "loss": 0.1191,
      "step": 340100
    },
    {
      "epoch": 2.2098801520023383,
      "grad_norm": 0.9573115706443787,
      "learning_rate": 2.9682215658822375e-05,
      "loss": 0.1172,
      "step": 340200
    },
    {
      "epoch": 2.2105297346454904,
      "grad_norm": 0.9140103459358215,
      "learning_rate": 2.9675305200396385e-05,
      "loss": 0.1162,
      "step": 340300
    },
    {
      "epoch": 2.211179317288642,
      "grad_norm": 1.2361878156661987,
      "learning_rate": 2.9668394741970395e-05,
      "loss": 0.1217,
      "step": 340400
    },
    {
      "epoch": 2.2118288999317937,
      "grad_norm": 0.972996711730957,
      "learning_rate": 2.9661484283544405e-05,
      "loss": 0.1216,
      "step": 340500
    },
    {
      "epoch": 2.2124784825749457,
      "grad_norm": 0.9654338359832764,
      "learning_rate": 2.9654573825118414e-05,
      "loss": 0.1174,
      "step": 340600
    },
    {
      "epoch": 2.2131280652180974,
      "grad_norm": 1.1144351959228516,
      "learning_rate": 2.9647663366692417e-05,
      "loss": 0.1213,
      "step": 340700
    },
    {
      "epoch": 2.213777647861249,
      "grad_norm": 0.9501617550849915,
      "learning_rate": 2.9640752908266427e-05,
      "loss": 0.1261,
      "step": 340800
    },
    {
      "epoch": 2.214427230504401,
      "grad_norm": 1.1016995906829834,
      "learning_rate": 2.9633842449840437e-05,
      "loss": 0.124,
      "step": 340900
    },
    {
      "epoch": 2.2150768131475527,
      "grad_norm": 0.9598181843757629,
      "learning_rate": 2.9626931991414447e-05,
      "loss": 0.1214,
      "step": 341000
    },
    {
      "epoch": 2.2157263957907043,
      "grad_norm": 1.380834698677063,
      "learning_rate": 2.9620021532988457e-05,
      "loss": 0.1213,
      "step": 341100
    },
    {
      "epoch": 2.2163759784338564,
      "grad_norm": 0.7471187710762024,
      "learning_rate": 2.9613111074562467e-05,
      "loss": 0.1173,
      "step": 341200
    },
    {
      "epoch": 2.217025561077008,
      "grad_norm": 0.783710241317749,
      "learning_rate": 2.9606200616136477e-05,
      "loss": 0.1206,
      "step": 341300
    },
    {
      "epoch": 2.2176751437201596,
      "grad_norm": 1.7329072952270508,
      "learning_rate": 2.9599290157710486e-05,
      "loss": 0.1151,
      "step": 341400
    },
    {
      "epoch": 2.2183247263633117,
      "grad_norm": 1.309853196144104,
      "learning_rate": 2.9592379699284496e-05,
      "loss": 0.12,
      "step": 341500
    },
    {
      "epoch": 2.2189743090064633,
      "grad_norm": 0.9154731631278992,
      "learning_rate": 2.95854692408585e-05,
      "loss": 0.1234,
      "step": 341600
    },
    {
      "epoch": 2.219623891649615,
      "grad_norm": 0.9143710732460022,
      "learning_rate": 2.957855878243251e-05,
      "loss": 0.1152,
      "step": 341700
    },
    {
      "epoch": 2.220273474292767,
      "grad_norm": 1.4181697368621826,
      "learning_rate": 2.957164832400652e-05,
      "loss": 0.1225,
      "step": 341800
    },
    {
      "epoch": 2.2209230569359186,
      "grad_norm": 1.1191270351409912,
      "learning_rate": 2.956473786558053e-05,
      "loss": 0.1134,
      "step": 341900
    },
    {
      "epoch": 2.2215726395790703,
      "grad_norm": 1.00449538230896,
      "learning_rate": 2.9557827407154535e-05,
      "loss": 0.1229,
      "step": 342000
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 1.3122034072875977,
      "learning_rate": 2.9550916948728545e-05,
      "loss": 0.1198,
      "step": 342100
    },
    {
      "epoch": 2.222871804865374,
      "grad_norm": 1.6762932538986206,
      "learning_rate": 2.9544006490302555e-05,
      "loss": 0.1198,
      "step": 342200
    },
    {
      "epoch": 2.2235213875085256,
      "grad_norm": 1.7431893348693848,
      "learning_rate": 2.9537096031876565e-05,
      "loss": 0.1159,
      "step": 342300
    },
    {
      "epoch": 2.2241709701516776,
      "grad_norm": 0.9389511346817017,
      "learning_rate": 2.9530185573450575e-05,
      "loss": 0.1148,
      "step": 342400
    },
    {
      "epoch": 2.2248205527948293,
      "grad_norm": 1.244979739189148,
      "learning_rate": 2.9523275115024585e-05,
      "loss": 0.1217,
      "step": 342500
    },
    {
      "epoch": 2.225470135437981,
      "grad_norm": 1.166092038154602,
      "learning_rate": 2.9516364656598588e-05,
      "loss": 0.1184,
      "step": 342600
    },
    {
      "epoch": 2.226119718081133,
      "grad_norm": 1.036135196685791,
      "learning_rate": 2.9509454198172598e-05,
      "loss": 0.116,
      "step": 342700
    },
    {
      "epoch": 2.2267693007242846,
      "grad_norm": 0.9444077014923096,
      "learning_rate": 2.9502543739746607e-05,
      "loss": 0.1223,
      "step": 342800
    },
    {
      "epoch": 2.227418883367436,
      "grad_norm": 1.0988973379135132,
      "learning_rate": 2.9495633281320617e-05,
      "loss": 0.1209,
      "step": 342900
    },
    {
      "epoch": 2.2280684660105883,
      "grad_norm": 1.349331259727478,
      "learning_rate": 2.9488722822894627e-05,
      "loss": 0.1166,
      "step": 343000
    },
    {
      "epoch": 2.22871804865374,
      "grad_norm": 1.2740144729614258,
      "learning_rate": 2.9481812364468637e-05,
      "loss": 0.1137,
      "step": 343100
    },
    {
      "epoch": 2.229367631296892,
      "grad_norm": 1.1539225578308105,
      "learning_rate": 2.9474901906042647e-05,
      "loss": 0.1202,
      "step": 343200
    },
    {
      "epoch": 2.2300172139400436,
      "grad_norm": 0.8700383305549622,
      "learning_rate": 2.9467991447616657e-05,
      "loss": 0.1188,
      "step": 343300
    },
    {
      "epoch": 2.230666796583195,
      "grad_norm": 0.9959421753883362,
      "learning_rate": 2.9461080989190663e-05,
      "loss": 0.1104,
      "step": 343400
    },
    {
      "epoch": 2.231316379226347,
      "grad_norm": 1.0083928108215332,
      "learning_rate": 2.945417053076467e-05,
      "loss": 0.1177,
      "step": 343500
    },
    {
      "epoch": 2.231965961869499,
      "grad_norm": 1.1997990608215332,
      "learning_rate": 2.944726007233868e-05,
      "loss": 0.1145,
      "step": 343600
    },
    {
      "epoch": 2.2326155445126505,
      "grad_norm": 0.7074712514877319,
      "learning_rate": 2.944034961391269e-05,
      "loss": 0.1185,
      "step": 343700
    },
    {
      "epoch": 2.2332651271558026,
      "grad_norm": 0.5947743654251099,
      "learning_rate": 2.9433439155486696e-05,
      "loss": 0.1171,
      "step": 343800
    },
    {
      "epoch": 2.2339147097989542,
      "grad_norm": 0.8425007462501526,
      "learning_rate": 2.9426528697060706e-05,
      "loss": 0.1118,
      "step": 343900
    },
    {
      "epoch": 2.234564292442106,
      "grad_norm": 0.5984640717506409,
      "learning_rate": 2.9419618238634715e-05,
      "loss": 0.1166,
      "step": 344000
    },
    {
      "epoch": 2.235213875085258,
      "grad_norm": 1.3494154214859009,
      "learning_rate": 2.9412707780208725e-05,
      "loss": 0.1169,
      "step": 344100
    },
    {
      "epoch": 2.2358634577284096,
      "grad_norm": 0.8182329535484314,
      "learning_rate": 2.9405797321782735e-05,
      "loss": 0.113,
      "step": 344200
    },
    {
      "epoch": 2.236513040371561,
      "grad_norm": 1.594956874847412,
      "learning_rate": 2.9398886863356745e-05,
      "loss": 0.1174,
      "step": 344300
    },
    {
      "epoch": 2.2371626230147132,
      "grad_norm": 0.860691249370575,
      "learning_rate": 2.9391976404930755e-05,
      "loss": 0.1174,
      "step": 344400
    },
    {
      "epoch": 2.237812205657865,
      "grad_norm": 1.6957275867462158,
      "learning_rate": 2.9385065946504758e-05,
      "loss": 0.1182,
      "step": 344500
    },
    {
      "epoch": 2.2384617883010165,
      "grad_norm": 0.9615991115570068,
      "learning_rate": 2.9378155488078768e-05,
      "loss": 0.1158,
      "step": 344600
    },
    {
      "epoch": 2.2391113709441686,
      "grad_norm": 1.819381833076477,
      "learning_rate": 2.9371245029652778e-05,
      "loss": 0.1143,
      "step": 344700
    },
    {
      "epoch": 2.23976095358732,
      "grad_norm": 1.0124162435531616,
      "learning_rate": 2.9364334571226788e-05,
      "loss": 0.1109,
      "step": 344800
    },
    {
      "epoch": 2.240410536230472,
      "grad_norm": 1.3047443628311157,
      "learning_rate": 2.9357424112800797e-05,
      "loss": 0.1146,
      "step": 344900
    },
    {
      "epoch": 2.241060118873624,
      "grad_norm": 1.2060433626174927,
      "learning_rate": 2.9350513654374807e-05,
      "loss": 0.1183,
      "step": 345000
    },
    {
      "epoch": 2.2417097015167755,
      "grad_norm": 1.1794003248214722,
      "learning_rate": 2.9343603195948817e-05,
      "loss": 0.1171,
      "step": 345100
    },
    {
      "epoch": 2.242359284159927,
      "grad_norm": 1.0569521188735962,
      "learning_rate": 2.9336692737522824e-05,
      "loss": 0.1169,
      "step": 345200
    },
    {
      "epoch": 2.243008866803079,
      "grad_norm": 1.1991972923278809,
      "learning_rate": 2.9329782279096833e-05,
      "loss": 0.1161,
      "step": 345300
    },
    {
      "epoch": 2.243658449446231,
      "grad_norm": 0.7884312868118286,
      "learning_rate": 2.932287182067084e-05,
      "loss": 0.1182,
      "step": 345400
    },
    {
      "epoch": 2.2443080320893825,
      "grad_norm": 1.0097259283065796,
      "learning_rate": 2.931596136224485e-05,
      "loss": 0.1198,
      "step": 345500
    },
    {
      "epoch": 2.2449576147325345,
      "grad_norm": 1.014866590499878,
      "learning_rate": 2.9309050903818856e-05,
      "loss": 0.1152,
      "step": 345600
    },
    {
      "epoch": 2.245607197375686,
      "grad_norm": 0.8683765530586243,
      "learning_rate": 2.9302140445392866e-05,
      "loss": 0.1138,
      "step": 345700
    },
    {
      "epoch": 2.2462567800188378,
      "grad_norm": 1.1515119075775146,
      "learning_rate": 2.9295229986966876e-05,
      "loss": 0.1124,
      "step": 345800
    },
    {
      "epoch": 2.24690636266199,
      "grad_norm": 1.0999385118484497,
      "learning_rate": 2.9288319528540886e-05,
      "loss": 0.1165,
      "step": 345900
    },
    {
      "epoch": 2.2475559453051415,
      "grad_norm": 1.4367464780807495,
      "learning_rate": 2.9281409070114896e-05,
      "loss": 0.1189,
      "step": 346000
    },
    {
      "epoch": 2.248205527948293,
      "grad_norm": 2.3333983421325684,
      "learning_rate": 2.9274498611688905e-05,
      "loss": 0.1197,
      "step": 346100
    },
    {
      "epoch": 2.248855110591445,
      "grad_norm": 1.013360619544983,
      "learning_rate": 2.9267588153262915e-05,
      "loss": 0.1228,
      "step": 346200
    },
    {
      "epoch": 2.249504693234597,
      "grad_norm": 0.9050899744033813,
      "learning_rate": 2.926067769483692e-05,
      "loss": 0.1155,
      "step": 346300
    },
    {
      "epoch": 2.2501542758777484,
      "grad_norm": 1.6841535568237305,
      "learning_rate": 2.9253767236410928e-05,
      "loss": 0.1158,
      "step": 346400
    },
    {
      "epoch": 2.2508038585209005,
      "grad_norm": 1.0313467979431152,
      "learning_rate": 2.9246856777984938e-05,
      "loss": 0.1088,
      "step": 346500
    },
    {
      "epoch": 2.251453441164052,
      "grad_norm": 0.7443056106567383,
      "learning_rate": 2.9239946319558948e-05,
      "loss": 0.1139,
      "step": 346600
    },
    {
      "epoch": 2.2521030238072037,
      "grad_norm": 1.2714906930923462,
      "learning_rate": 2.9233035861132958e-05,
      "loss": 0.1219,
      "step": 346700
    },
    {
      "epoch": 2.252752606450356,
      "grad_norm": 1.071458101272583,
      "learning_rate": 2.9226125402706968e-05,
      "loss": 0.1232,
      "step": 346800
    },
    {
      "epoch": 2.2534021890935074,
      "grad_norm": 0.9374076128005981,
      "learning_rate": 2.9219214944280977e-05,
      "loss": 0.1216,
      "step": 346900
    },
    {
      "epoch": 2.254051771736659,
      "grad_norm": 1.326938509941101,
      "learning_rate": 2.9212304485854984e-05,
      "loss": 0.1094,
      "step": 347000
    },
    {
      "epoch": 2.254701354379811,
      "grad_norm": 1.5194631814956665,
      "learning_rate": 2.9205394027428994e-05,
      "loss": 0.1179,
      "step": 347100
    },
    {
      "epoch": 2.2553509370229627,
      "grad_norm": 1.5368365049362183,
      "learning_rate": 2.9198483569003004e-05,
      "loss": 0.1126,
      "step": 347200
    },
    {
      "epoch": 2.2560005196661144,
      "grad_norm": 1.1927248239517212,
      "learning_rate": 2.919157311057701e-05,
      "loss": 0.1213,
      "step": 347300
    },
    {
      "epoch": 2.2566501023092664,
      "grad_norm": 0.8093847632408142,
      "learning_rate": 2.9184662652151017e-05,
      "loss": 0.1164,
      "step": 347400
    },
    {
      "epoch": 2.257299684952418,
      "grad_norm": 1.0139856338500977,
      "learning_rate": 2.9177752193725026e-05,
      "loss": 0.1161,
      "step": 347500
    },
    {
      "epoch": 2.2579492675955697,
      "grad_norm": 1.4228510856628418,
      "learning_rate": 2.9170841735299036e-05,
      "loss": 0.1139,
      "step": 347600
    },
    {
      "epoch": 2.2585988502387218,
      "grad_norm": 0.7406198382377625,
      "learning_rate": 2.9163931276873046e-05,
      "loss": 0.1132,
      "step": 347700
    },
    {
      "epoch": 2.2592484328818734,
      "grad_norm": 1.3114513158798218,
      "learning_rate": 2.9157020818447056e-05,
      "loss": 0.1184,
      "step": 347800
    },
    {
      "epoch": 2.259898015525025,
      "grad_norm": 0.7342653870582581,
      "learning_rate": 2.9150110360021066e-05,
      "loss": 0.1174,
      "step": 347900
    },
    {
      "epoch": 2.260547598168177,
      "grad_norm": 1.4204275608062744,
      "learning_rate": 2.9143199901595076e-05,
      "loss": 0.1188,
      "step": 348000
    },
    {
      "epoch": 2.2611971808113287,
      "grad_norm": 0.8704961538314819,
      "learning_rate": 2.9136289443169086e-05,
      "loss": 0.118,
      "step": 348100
    },
    {
      "epoch": 2.2618467634544803,
      "grad_norm": 1.346752405166626,
      "learning_rate": 2.912937898474309e-05,
      "loss": 0.1209,
      "step": 348200
    },
    {
      "epoch": 2.2624963460976324,
      "grad_norm": 1.9615700244903564,
      "learning_rate": 2.91224685263171e-05,
      "loss": 0.1194,
      "step": 348300
    },
    {
      "epoch": 2.263145928740784,
      "grad_norm": 1.1753913164138794,
      "learning_rate": 2.9115558067891108e-05,
      "loss": 0.1145,
      "step": 348400
    },
    {
      "epoch": 2.2637955113839356,
      "grad_norm": 1.6296334266662598,
      "learning_rate": 2.9108647609465118e-05,
      "loss": 0.1193,
      "step": 348500
    },
    {
      "epoch": 2.2644450940270877,
      "grad_norm": 1.1195918321609497,
      "learning_rate": 2.9101737151039128e-05,
      "loss": 0.1168,
      "step": 348600
    },
    {
      "epoch": 2.2650946766702393,
      "grad_norm": 1.216006875038147,
      "learning_rate": 2.9094826692613138e-05,
      "loss": 0.1118,
      "step": 348700
    },
    {
      "epoch": 2.265744259313391,
      "grad_norm": 0.47419771552085876,
      "learning_rate": 2.9087916234187144e-05,
      "loss": 0.1186,
      "step": 348800
    },
    {
      "epoch": 2.266393841956543,
      "grad_norm": 1.0421159267425537,
      "learning_rate": 2.9081005775761154e-05,
      "loss": 0.1123,
      "step": 348900
    },
    {
      "epoch": 2.2670434245996947,
      "grad_norm": 1.1313135623931885,
      "learning_rate": 2.9074095317335164e-05,
      "loss": 0.1155,
      "step": 349000
    },
    {
      "epoch": 2.2676930072428463,
      "grad_norm": 0.8400287628173828,
      "learning_rate": 2.9067184858909174e-05,
      "loss": 0.1124,
      "step": 349100
    },
    {
      "epoch": 2.2683425898859984,
      "grad_norm": 0.8735986351966858,
      "learning_rate": 2.9060274400483177e-05,
      "loss": 0.1151,
      "step": 349200
    },
    {
      "epoch": 2.26899217252915,
      "grad_norm": 0.9404253959655762,
      "learning_rate": 2.9053363942057187e-05,
      "loss": 0.1177,
      "step": 349300
    },
    {
      "epoch": 2.2696417551723016,
      "grad_norm": 0.9649532437324524,
      "learning_rate": 2.9046453483631197e-05,
      "loss": 0.122,
      "step": 349400
    },
    {
      "epoch": 2.2702913378154537,
      "grad_norm": 1.0982674360275269,
      "learning_rate": 2.9039543025205207e-05,
      "loss": 0.1186,
      "step": 349500
    },
    {
      "epoch": 2.2709409204586053,
      "grad_norm": 1.2346408367156982,
      "learning_rate": 2.9032632566779216e-05,
      "loss": 0.1186,
      "step": 349600
    },
    {
      "epoch": 2.2715905031017574,
      "grad_norm": 1.41617751121521,
      "learning_rate": 2.9025722108353226e-05,
      "loss": 0.1139,
      "step": 349700
    },
    {
      "epoch": 2.272240085744909,
      "grad_norm": 1.310051679611206,
      "learning_rate": 2.9018811649927236e-05,
      "loss": 0.1126,
      "step": 349800
    },
    {
      "epoch": 2.2728896683880606,
      "grad_norm": 0.8019834756851196,
      "learning_rate": 2.9011901191501246e-05,
      "loss": 0.1195,
      "step": 349900
    },
    {
      "epoch": 2.2735392510312122,
      "grad_norm": 1.2502633333206177,
      "learning_rate": 2.9004990733075256e-05,
      "loss": 0.1201,
      "step": 350000
    },
    {
      "epoch": 2.2741888336743643,
      "grad_norm": 1.2208267450332642,
      "learning_rate": 2.899808027464926e-05,
      "loss": 0.116,
      "step": 350100
    },
    {
      "epoch": 2.274838416317516,
      "grad_norm": 1.0294744968414307,
      "learning_rate": 2.899116981622327e-05,
      "loss": 0.118,
      "step": 350200
    },
    {
      "epoch": 2.275487998960668,
      "grad_norm": 1.5312484502792358,
      "learning_rate": 2.898425935779728e-05,
      "loss": 0.1191,
      "step": 350300
    },
    {
      "epoch": 2.2761375816038196,
      "grad_norm": 1.1738325357437134,
      "learning_rate": 2.897734889937129e-05,
      "loss": 0.1148,
      "step": 350400
    },
    {
      "epoch": 2.2767871642469713,
      "grad_norm": 1.0699771642684937,
      "learning_rate": 2.8970438440945298e-05,
      "loss": 0.116,
      "step": 350500
    },
    {
      "epoch": 2.277436746890123,
      "grad_norm": 1.6148828268051147,
      "learning_rate": 2.8963527982519305e-05,
      "loss": 0.113,
      "step": 350600
    },
    {
      "epoch": 2.278086329533275,
      "grad_norm": 1.5123521089553833,
      "learning_rate": 2.8956617524093315e-05,
      "loss": 0.1181,
      "step": 350700
    },
    {
      "epoch": 2.2787359121764266,
      "grad_norm": 1.2160701751708984,
      "learning_rate": 2.8949707065667324e-05,
      "loss": 0.1123,
      "step": 350800
    },
    {
      "epoch": 2.2793854948195786,
      "grad_norm": 1.0497334003448486,
      "learning_rate": 2.8942796607241334e-05,
      "loss": 0.1133,
      "step": 350900
    },
    {
      "epoch": 2.2800350774627303,
      "grad_norm": 1.1575901508331299,
      "learning_rate": 2.8935886148815337e-05,
      "loss": 0.1164,
      "step": 351000
    },
    {
      "epoch": 2.280684660105882,
      "grad_norm": 1.2351795434951782,
      "learning_rate": 2.8928975690389347e-05,
      "loss": 0.1161,
      "step": 351100
    },
    {
      "epoch": 2.2813342427490335,
      "grad_norm": 1.4199934005737305,
      "learning_rate": 2.8922065231963357e-05,
      "loss": 0.1156,
      "step": 351200
    },
    {
      "epoch": 2.2819838253921856,
      "grad_norm": 0.9090604782104492,
      "learning_rate": 2.8915154773537367e-05,
      "loss": 0.1168,
      "step": 351300
    },
    {
      "epoch": 2.282633408035337,
      "grad_norm": 1.050264835357666,
      "learning_rate": 2.8908244315111377e-05,
      "loss": 0.119,
      "step": 351400
    },
    {
      "epoch": 2.2832829906784893,
      "grad_norm": 0.7624056339263916,
      "learning_rate": 2.8901333856685387e-05,
      "loss": 0.1244,
      "step": 351500
    },
    {
      "epoch": 2.283932573321641,
      "grad_norm": 0.6205844879150391,
      "learning_rate": 2.8894423398259396e-05,
      "loss": 0.1177,
      "step": 351600
    },
    {
      "epoch": 2.2845821559647925,
      "grad_norm": 1.4656847715377808,
      "learning_rate": 2.8887512939833406e-05,
      "loss": 0.1168,
      "step": 351700
    },
    {
      "epoch": 2.285231738607944,
      "grad_norm": 0.9461056590080261,
      "learning_rate": 2.8880602481407416e-05,
      "loss": 0.1106,
      "step": 351800
    },
    {
      "epoch": 2.285881321251096,
      "grad_norm": 1.2709035873413086,
      "learning_rate": 2.8873692022981426e-05,
      "loss": 0.1132,
      "step": 351900
    },
    {
      "epoch": 2.286530903894248,
      "grad_norm": 1.2517480850219727,
      "learning_rate": 2.886678156455543e-05,
      "loss": 0.1227,
      "step": 352000
    },
    {
      "epoch": 2.2871804865374,
      "grad_norm": 1.621254801750183,
      "learning_rate": 2.885987110612944e-05,
      "loss": 0.1215,
      "step": 352100
    },
    {
      "epoch": 2.2878300691805515,
      "grad_norm": 1.141937494277954,
      "learning_rate": 2.885296064770345e-05,
      "loss": 0.1133,
      "step": 352200
    },
    {
      "epoch": 2.288479651823703,
      "grad_norm": 1.2340161800384521,
      "learning_rate": 2.884605018927746e-05,
      "loss": 0.1161,
      "step": 352300
    },
    {
      "epoch": 2.2891292344668552,
      "grad_norm": 0.816562831401825,
      "learning_rate": 2.8839139730851465e-05,
      "loss": 0.1199,
      "step": 352400
    },
    {
      "epoch": 2.289778817110007,
      "grad_norm": 1.0816280841827393,
      "learning_rate": 2.8832229272425475e-05,
      "loss": 0.1147,
      "step": 352500
    },
    {
      "epoch": 2.2904283997531585,
      "grad_norm": 1.4241869449615479,
      "learning_rate": 2.8825318813999485e-05,
      "loss": 0.1142,
      "step": 352600
    },
    {
      "epoch": 2.2910779823963106,
      "grad_norm": 1.4958232641220093,
      "learning_rate": 2.8818408355573495e-05,
      "loss": 0.1114,
      "step": 352700
    },
    {
      "epoch": 2.291727565039462,
      "grad_norm": 1.2835568189620972,
      "learning_rate": 2.8811497897147505e-05,
      "loss": 0.1177,
      "step": 352800
    },
    {
      "epoch": 2.292377147682614,
      "grad_norm": 0.9646488428115845,
      "learning_rate": 2.8804587438721508e-05,
      "loss": 0.116,
      "step": 352900
    },
    {
      "epoch": 2.293026730325766,
      "grad_norm": 1.3307976722717285,
      "learning_rate": 2.8797676980295517e-05,
      "loss": 0.1137,
      "step": 353000
    },
    {
      "epoch": 2.2936763129689175,
      "grad_norm": 0.8772334456443787,
      "learning_rate": 2.8790766521869527e-05,
      "loss": 0.1166,
      "step": 353100
    },
    {
      "epoch": 2.294325895612069,
      "grad_norm": 1.1735868453979492,
      "learning_rate": 2.8783856063443537e-05,
      "loss": 0.1195,
      "step": 353200
    },
    {
      "epoch": 2.294975478255221,
      "grad_norm": 0.8720553517341614,
      "learning_rate": 2.8776945605017547e-05,
      "loss": 0.1177,
      "step": 353300
    },
    {
      "epoch": 2.295625060898373,
      "grad_norm": 0.8878629803657532,
      "learning_rate": 2.8770035146591557e-05,
      "loss": 0.1152,
      "step": 353400
    },
    {
      "epoch": 2.2962746435415244,
      "grad_norm": 1.672106146812439,
      "learning_rate": 2.8763124688165567e-05,
      "loss": 0.1184,
      "step": 353500
    },
    {
      "epoch": 2.2969242261846765,
      "grad_norm": 1.204441785812378,
      "learning_rate": 2.8756214229739577e-05,
      "loss": 0.1134,
      "step": 353600
    },
    {
      "epoch": 2.297573808827828,
      "grad_norm": 0.6324186325073242,
      "learning_rate": 2.8749303771313586e-05,
      "loss": 0.1182,
      "step": 353700
    },
    {
      "epoch": 2.2982233914709798,
      "grad_norm": 1.181522250175476,
      "learning_rate": 2.8742393312887593e-05,
      "loss": 0.12,
      "step": 353800
    },
    {
      "epoch": 2.298872974114132,
      "grad_norm": 1.214788794517517,
      "learning_rate": 2.87354828544616e-05,
      "loss": 0.1134,
      "step": 353900
    },
    {
      "epoch": 2.2995225567572835,
      "grad_norm": 1.3849964141845703,
      "learning_rate": 2.872857239603561e-05,
      "loss": 0.1162,
      "step": 354000
    },
    {
      "epoch": 2.300172139400435,
      "grad_norm": 1.239067554473877,
      "learning_rate": 2.872166193760962e-05,
      "loss": 0.1186,
      "step": 354100
    },
    {
      "epoch": 2.300821722043587,
      "grad_norm": 1.0849422216415405,
      "learning_rate": 2.8714751479183626e-05,
      "loss": 0.115,
      "step": 354200
    },
    {
      "epoch": 2.3014713046867388,
      "grad_norm": 1.419824481010437,
      "learning_rate": 2.8707841020757635e-05,
      "loss": 0.1164,
      "step": 354300
    },
    {
      "epoch": 2.3021208873298904,
      "grad_norm": 1.3218700885772705,
      "learning_rate": 2.8700930562331645e-05,
      "loss": 0.1171,
      "step": 354400
    },
    {
      "epoch": 2.3027704699730425,
      "grad_norm": 1.154638409614563,
      "learning_rate": 2.8694020103905655e-05,
      "loss": 0.1155,
      "step": 354500
    },
    {
      "epoch": 2.303420052616194,
      "grad_norm": 1.58794105052948,
      "learning_rate": 2.8687109645479665e-05,
      "loss": 0.1206,
      "step": 354600
    },
    {
      "epoch": 2.3040696352593457,
      "grad_norm": 1.0620603561401367,
      "learning_rate": 2.8680199187053675e-05,
      "loss": 0.1145,
      "step": 354700
    },
    {
      "epoch": 2.304719217902498,
      "grad_norm": 1.2275259494781494,
      "learning_rate": 2.8673288728627678e-05,
      "loss": 0.1095,
      "step": 354800
    },
    {
      "epoch": 2.3053688005456494,
      "grad_norm": 0.9456713795661926,
      "learning_rate": 2.8666378270201688e-05,
      "loss": 0.1121,
      "step": 354900
    },
    {
      "epoch": 2.306018383188801,
      "grad_norm": 1.4862099885940552,
      "learning_rate": 2.8659467811775698e-05,
      "loss": 0.1247,
      "step": 355000
    },
    {
      "epoch": 2.306667965831953,
      "grad_norm": 1.3697048425674438,
      "learning_rate": 2.8652557353349707e-05,
      "loss": 0.1164,
      "step": 355100
    },
    {
      "epoch": 2.3073175484751047,
      "grad_norm": 1.3791110515594482,
      "learning_rate": 2.8645646894923717e-05,
      "loss": 0.1154,
      "step": 355200
    },
    {
      "epoch": 2.3079671311182564,
      "grad_norm": 0.830411434173584,
      "learning_rate": 2.8638736436497727e-05,
      "loss": 0.1228,
      "step": 355300
    },
    {
      "epoch": 2.3086167137614084,
      "grad_norm": 1.2353143692016602,
      "learning_rate": 2.8631825978071737e-05,
      "loss": 0.1176,
      "step": 355400
    },
    {
      "epoch": 2.30926629640456,
      "grad_norm": 1.453017234802246,
      "learning_rate": 2.8624915519645747e-05,
      "loss": 0.1223,
      "step": 355500
    },
    {
      "epoch": 2.3099158790477117,
      "grad_norm": 1.5059070587158203,
      "learning_rate": 2.8618005061219753e-05,
      "loss": 0.1123,
      "step": 355600
    },
    {
      "epoch": 2.3105654616908637,
      "grad_norm": 0.9022718667984009,
      "learning_rate": 2.861109460279376e-05,
      "loss": 0.1164,
      "step": 355700
    },
    {
      "epoch": 2.3112150443340154,
      "grad_norm": 0.8742977976799011,
      "learning_rate": 2.860418414436777e-05,
      "loss": 0.1185,
      "step": 355800
    },
    {
      "epoch": 2.311864626977167,
      "grad_norm": 1.1288543939590454,
      "learning_rate": 2.859727368594178e-05,
      "loss": 0.116,
      "step": 355900
    },
    {
      "epoch": 2.312514209620319,
      "grad_norm": 1.1417280435562134,
      "learning_rate": 2.8590363227515786e-05,
      "loss": 0.123,
      "step": 356000
    },
    {
      "epoch": 2.3131637922634707,
      "grad_norm": 0.8378893733024597,
      "learning_rate": 2.8583452769089796e-05,
      "loss": 0.1152,
      "step": 356100
    },
    {
      "epoch": 2.3138133749066223,
      "grad_norm": 1.045598030090332,
      "learning_rate": 2.8576542310663806e-05,
      "loss": 0.1192,
      "step": 356200
    },
    {
      "epoch": 2.3144629575497744,
      "grad_norm": 1.1416547298431396,
      "learning_rate": 2.8569631852237815e-05,
      "loss": 0.1116,
      "step": 356300
    },
    {
      "epoch": 2.315112540192926,
      "grad_norm": 1.1539183855056763,
      "learning_rate": 2.8562721393811825e-05,
      "loss": 0.113,
      "step": 356400
    },
    {
      "epoch": 2.3157621228360776,
      "grad_norm": 0.7778909206390381,
      "learning_rate": 2.8555810935385835e-05,
      "loss": 0.1133,
      "step": 356500
    },
    {
      "epoch": 2.3164117054792297,
      "grad_norm": 1.2012004852294922,
      "learning_rate": 2.8548900476959845e-05,
      "loss": 0.1178,
      "step": 356600
    },
    {
      "epoch": 2.3170612881223813,
      "grad_norm": 1.1188820600509644,
      "learning_rate": 2.8541990018533848e-05,
      "loss": 0.1221,
      "step": 356700
    },
    {
      "epoch": 2.3177108707655334,
      "grad_norm": 1.1277828216552734,
      "learning_rate": 2.8535079560107858e-05,
      "loss": 0.1174,
      "step": 356800
    },
    {
      "epoch": 2.318360453408685,
      "grad_norm": 0.7285477519035339,
      "learning_rate": 2.8528169101681868e-05,
      "loss": 0.1145,
      "step": 356900
    },
    {
      "epoch": 2.3190100360518366,
      "grad_norm": 1.1545002460479736,
      "learning_rate": 2.8521258643255878e-05,
      "loss": 0.1092,
      "step": 357000
    },
    {
      "epoch": 2.3196596186949883,
      "grad_norm": 1.499260425567627,
      "learning_rate": 2.8514348184829887e-05,
      "loss": 0.11,
      "step": 357100
    },
    {
      "epoch": 2.3203092013381403,
      "grad_norm": 0.809012234210968,
      "learning_rate": 2.8507437726403897e-05,
      "loss": 0.1185,
      "step": 357200
    },
    {
      "epoch": 2.320958783981292,
      "grad_norm": 2.025303363800049,
      "learning_rate": 2.8500527267977907e-05,
      "loss": 0.1225,
      "step": 357300
    },
    {
      "epoch": 2.321608366624444,
      "grad_norm": 1.6231143474578857,
      "learning_rate": 2.8493616809551914e-05,
      "loss": 0.1085,
      "step": 357400
    },
    {
      "epoch": 2.3222579492675957,
      "grad_norm": 1.1772494316101074,
      "learning_rate": 2.8486706351125924e-05,
      "loss": 0.1081,
      "step": 357500
    },
    {
      "epoch": 2.3229075319107473,
      "grad_norm": 0.6069200038909912,
      "learning_rate": 2.847979589269993e-05,
      "loss": 0.1144,
      "step": 357600
    },
    {
      "epoch": 2.323557114553899,
      "grad_norm": 1.1903740167617798,
      "learning_rate": 2.847288543427394e-05,
      "loss": 0.1079,
      "step": 357700
    },
    {
      "epoch": 2.324206697197051,
      "grad_norm": 1.2182645797729492,
      "learning_rate": 2.8465974975847946e-05,
      "loss": 0.1156,
      "step": 357800
    },
    {
      "epoch": 2.3248562798402026,
      "grad_norm": 1.637090802192688,
      "learning_rate": 2.8459064517421956e-05,
      "loss": 0.1104,
      "step": 357900
    },
    {
      "epoch": 2.3255058624833547,
      "grad_norm": 0.8423399925231934,
      "learning_rate": 2.8452154058995966e-05,
      "loss": 0.1119,
      "step": 358000
    },
    {
      "epoch": 2.3261554451265063,
      "grad_norm": 1.0181541442871094,
      "learning_rate": 2.8445243600569976e-05,
      "loss": 0.1161,
      "step": 358100
    },
    {
      "epoch": 2.326805027769658,
      "grad_norm": 0.8513050675392151,
      "learning_rate": 2.8438333142143986e-05,
      "loss": 0.1189,
      "step": 358200
    },
    {
      "epoch": 2.3274546104128095,
      "grad_norm": 1.3020398616790771,
      "learning_rate": 2.8431422683717996e-05,
      "loss": 0.1174,
      "step": 358300
    },
    {
      "epoch": 2.3281041930559616,
      "grad_norm": 1.1698417663574219,
      "learning_rate": 2.8424512225292005e-05,
      "loss": 0.1188,
      "step": 358400
    },
    {
      "epoch": 2.3287537756991132,
      "grad_norm": 1.0199165344238281,
      "learning_rate": 2.8417601766866015e-05,
      "loss": 0.1114,
      "step": 358500
    },
    {
      "epoch": 2.3294033583422653,
      "grad_norm": 0.8739367127418518,
      "learning_rate": 2.841069130844002e-05,
      "loss": 0.1221,
      "step": 358600
    },
    {
      "epoch": 2.330052940985417,
      "grad_norm": 1.5801564455032349,
      "learning_rate": 2.8403780850014028e-05,
      "loss": 0.1138,
      "step": 358700
    },
    {
      "epoch": 2.3307025236285686,
      "grad_norm": 1.207184076309204,
      "learning_rate": 2.8396870391588038e-05,
      "loss": 0.1193,
      "step": 358800
    },
    {
      "epoch": 2.33135210627172,
      "grad_norm": 1.2059760093688965,
      "learning_rate": 2.8389959933162048e-05,
      "loss": 0.1111,
      "step": 358900
    },
    {
      "epoch": 2.3320016889148723,
      "grad_norm": 1.409488320350647,
      "learning_rate": 2.8383049474736058e-05,
      "loss": 0.117,
      "step": 359000
    },
    {
      "epoch": 2.332651271558024,
      "grad_norm": 1.4106779098510742,
      "learning_rate": 2.8376139016310068e-05,
      "loss": 0.1149,
      "step": 359100
    },
    {
      "epoch": 2.333300854201176,
      "grad_norm": 1.1970164775848389,
      "learning_rate": 2.8369228557884074e-05,
      "loss": 0.1182,
      "step": 359200
    },
    {
      "epoch": 2.3339504368443276,
      "grad_norm": 1.0954121351242065,
      "learning_rate": 2.8362318099458084e-05,
      "loss": 0.1172,
      "step": 359300
    },
    {
      "epoch": 2.334600019487479,
      "grad_norm": 1.448310136795044,
      "learning_rate": 2.8355407641032094e-05,
      "loss": 0.1174,
      "step": 359400
    },
    {
      "epoch": 2.3352496021306313,
      "grad_norm": 1.309766411781311,
      "learning_rate": 2.83484971826061e-05,
      "loss": 0.1143,
      "step": 359500
    },
    {
      "epoch": 2.335899184773783,
      "grad_norm": 1.804036259651184,
      "learning_rate": 2.8341586724180107e-05,
      "loss": 0.1171,
      "step": 359600
    },
    {
      "epoch": 2.3365487674169345,
      "grad_norm": 1.2860561609268188,
      "learning_rate": 2.8334676265754117e-05,
      "loss": 0.1109,
      "step": 359700
    },
    {
      "epoch": 2.3371983500600866,
      "grad_norm": 0.7808642387390137,
      "learning_rate": 2.8327765807328126e-05,
      "loss": 0.1152,
      "step": 359800
    },
    {
      "epoch": 2.337847932703238,
      "grad_norm": 1.9578204154968262,
      "learning_rate": 2.8320855348902136e-05,
      "loss": 0.1161,
      "step": 359900
    },
    {
      "epoch": 2.33849751534639,
      "grad_norm": 0.9776671528816223,
      "learning_rate": 2.8313944890476146e-05,
      "loss": 0.1126,
      "step": 360000
    },
    {
      "epoch": 2.339147097989542,
      "grad_norm": 1.2301331758499146,
      "learning_rate": 2.8307034432050156e-05,
      "loss": 0.1182,
      "step": 360100
    },
    {
      "epoch": 2.3397966806326935,
      "grad_norm": 0.9627149701118469,
      "learning_rate": 2.8300123973624166e-05,
      "loss": 0.1191,
      "step": 360200
    },
    {
      "epoch": 2.340446263275845,
      "grad_norm": 1.4221570491790771,
      "learning_rate": 2.8293213515198176e-05,
      "loss": 0.1197,
      "step": 360300
    },
    {
      "epoch": 2.341095845918997,
      "grad_norm": 1.2166823148727417,
      "learning_rate": 2.828630305677218e-05,
      "loss": 0.1139,
      "step": 360400
    },
    {
      "epoch": 2.341745428562149,
      "grad_norm": 0.9193418622016907,
      "learning_rate": 2.827939259834619e-05,
      "loss": 0.1068,
      "step": 360500
    },
    {
      "epoch": 2.3423950112053005,
      "grad_norm": 1.1346217393875122,
      "learning_rate": 2.82724821399202e-05,
      "loss": 0.1153,
      "step": 360600
    },
    {
      "epoch": 2.3430445938484525,
      "grad_norm": 0.9876027703285217,
      "learning_rate": 2.8265571681494208e-05,
      "loss": 0.1143,
      "step": 360700
    },
    {
      "epoch": 2.343694176491604,
      "grad_norm": 0.9280647039413452,
      "learning_rate": 2.8258661223068218e-05,
      "loss": 0.1223,
      "step": 360800
    },
    {
      "epoch": 2.344343759134756,
      "grad_norm": 1.036372423171997,
      "learning_rate": 2.8251750764642228e-05,
      "loss": 0.1178,
      "step": 360900
    },
    {
      "epoch": 2.344993341777908,
      "grad_norm": 0.7028578519821167,
      "learning_rate": 2.8244840306216234e-05,
      "loss": 0.1122,
      "step": 361000
    },
    {
      "epoch": 2.3456429244210595,
      "grad_norm": 1.073480248451233,
      "learning_rate": 2.8237929847790244e-05,
      "loss": 0.1173,
      "step": 361100
    },
    {
      "epoch": 2.346292507064211,
      "grad_norm": 1.140927791595459,
      "learning_rate": 2.8231019389364254e-05,
      "loss": 0.1168,
      "step": 361200
    },
    {
      "epoch": 2.346942089707363,
      "grad_norm": 1.417951226234436,
      "learning_rate": 2.8224108930938264e-05,
      "loss": 0.115,
      "step": 361300
    },
    {
      "epoch": 2.347591672350515,
      "grad_norm": 1.4671614170074463,
      "learning_rate": 2.8217198472512267e-05,
      "loss": 0.1098,
      "step": 361400
    },
    {
      "epoch": 2.3482412549936664,
      "grad_norm": 1.3858884572982788,
      "learning_rate": 2.8210288014086277e-05,
      "loss": 0.1156,
      "step": 361500
    },
    {
      "epoch": 2.3488908376368185,
      "grad_norm": 1.3804818391799927,
      "learning_rate": 2.8203377555660287e-05,
      "loss": 0.1139,
      "step": 361600
    },
    {
      "epoch": 2.34954042027997,
      "grad_norm": 1.5559470653533936,
      "learning_rate": 2.8196467097234297e-05,
      "loss": 0.1169,
      "step": 361700
    },
    {
      "epoch": 2.3501900029231217,
      "grad_norm": 1.2970167398452759,
      "learning_rate": 2.8189556638808306e-05,
      "loss": 0.1134,
      "step": 361800
    },
    {
      "epoch": 2.350839585566274,
      "grad_norm": 0.771054744720459,
      "learning_rate": 2.8182646180382316e-05,
      "loss": 0.1142,
      "step": 361900
    },
    {
      "epoch": 2.3514891682094254,
      "grad_norm": 1.2215487957000732,
      "learning_rate": 2.8175735721956326e-05,
      "loss": 0.1156,
      "step": 362000
    },
    {
      "epoch": 2.352138750852577,
      "grad_norm": 0.7359899878501892,
      "learning_rate": 2.8168825263530336e-05,
      "loss": 0.1106,
      "step": 362100
    },
    {
      "epoch": 2.352788333495729,
      "grad_norm": 0.9613935947418213,
      "learning_rate": 2.8161914805104346e-05,
      "loss": 0.1077,
      "step": 362200
    },
    {
      "epoch": 2.3534379161388808,
      "grad_norm": 1.1213343143463135,
      "learning_rate": 2.815500434667835e-05,
      "loss": 0.1189,
      "step": 362300
    },
    {
      "epoch": 2.3540874987820324,
      "grad_norm": 0.8482643365859985,
      "learning_rate": 2.814809388825236e-05,
      "loss": 0.1179,
      "step": 362400
    },
    {
      "epoch": 2.3547370814251845,
      "grad_norm": 1.3041794300079346,
      "learning_rate": 2.814118342982637e-05,
      "loss": 0.1208,
      "step": 362500
    },
    {
      "epoch": 2.355386664068336,
      "grad_norm": 1.3012076616287231,
      "learning_rate": 2.813427297140038e-05,
      "loss": 0.1154,
      "step": 362600
    },
    {
      "epoch": 2.3560362467114877,
      "grad_norm": 1.350886344909668,
      "learning_rate": 2.812736251297439e-05,
      "loss": 0.116,
      "step": 362700
    },
    {
      "epoch": 2.3566858293546398,
      "grad_norm": 1.221351146697998,
      "learning_rate": 2.8120452054548395e-05,
      "loss": 0.1163,
      "step": 362800
    },
    {
      "epoch": 2.3573354119977914,
      "grad_norm": 1.1900160312652588,
      "learning_rate": 2.8113541596122405e-05,
      "loss": 0.113,
      "step": 362900
    },
    {
      "epoch": 2.357984994640943,
      "grad_norm": 0.9188217520713806,
      "learning_rate": 2.8106631137696415e-05,
      "loss": 0.1099,
      "step": 363000
    },
    {
      "epoch": 2.358634577284095,
      "grad_norm": 1.1033124923706055,
      "learning_rate": 2.8099720679270424e-05,
      "loss": 0.1124,
      "step": 363100
    },
    {
      "epoch": 2.3592841599272467,
      "grad_norm": 1.5105382204055786,
      "learning_rate": 2.8092810220844434e-05,
      "loss": 0.1188,
      "step": 363200
    },
    {
      "epoch": 2.3599337425703983,
      "grad_norm": 0.8187952637672424,
      "learning_rate": 2.8085899762418437e-05,
      "loss": 0.1161,
      "step": 363300
    },
    {
      "epoch": 2.3605833252135504,
      "grad_norm": 0.8982216715812683,
      "learning_rate": 2.8078989303992447e-05,
      "loss": 0.108,
      "step": 363400
    },
    {
      "epoch": 2.361232907856702,
      "grad_norm": 1.0416679382324219,
      "learning_rate": 2.8072078845566457e-05,
      "loss": 0.1189,
      "step": 363500
    },
    {
      "epoch": 2.3618824904998537,
      "grad_norm": 1.3690829277038574,
      "learning_rate": 2.8065168387140467e-05,
      "loss": 0.1138,
      "step": 363600
    },
    {
      "epoch": 2.3625320731430057,
      "grad_norm": 1.1752926111221313,
      "learning_rate": 2.8058257928714477e-05,
      "loss": 0.1151,
      "step": 363700
    },
    {
      "epoch": 2.3631816557861574,
      "grad_norm": 1.7522419691085815,
      "learning_rate": 2.8051347470288487e-05,
      "loss": 0.1186,
      "step": 363800
    },
    {
      "epoch": 2.363831238429309,
      "grad_norm": 0.9257369637489319,
      "learning_rate": 2.8044437011862496e-05,
      "loss": 0.1166,
      "step": 363900
    },
    {
      "epoch": 2.364480821072461,
      "grad_norm": 1.3949283361434937,
      "learning_rate": 2.8037526553436506e-05,
      "loss": 0.1171,
      "step": 364000
    },
    {
      "epoch": 2.3651304037156127,
      "grad_norm": 0.7989805340766907,
      "learning_rate": 2.8030616095010516e-05,
      "loss": 0.1196,
      "step": 364100
    },
    {
      "epoch": 2.3657799863587643,
      "grad_norm": 1.625304102897644,
      "learning_rate": 2.802370563658452e-05,
      "loss": 0.1158,
      "step": 364200
    },
    {
      "epoch": 2.3664295690019164,
      "grad_norm": 0.6724518537521362,
      "learning_rate": 2.801679517815853e-05,
      "loss": 0.1193,
      "step": 364300
    },
    {
      "epoch": 2.367079151645068,
      "grad_norm": 1.7279738187789917,
      "learning_rate": 2.800988471973254e-05,
      "loss": 0.1232,
      "step": 364400
    },
    {
      "epoch": 2.36772873428822,
      "grad_norm": 0.8643806576728821,
      "learning_rate": 2.800297426130655e-05,
      "loss": 0.1105,
      "step": 364500
    },
    {
      "epoch": 2.3683783169313717,
      "grad_norm": 0.9127917289733887,
      "learning_rate": 2.7996063802880555e-05,
      "loss": 0.1104,
      "step": 364600
    },
    {
      "epoch": 2.3690278995745233,
      "grad_norm": 1.1504217386245728,
      "learning_rate": 2.7989153344454565e-05,
      "loss": 0.1111,
      "step": 364700
    },
    {
      "epoch": 2.369677482217675,
      "grad_norm": 1.1554605960845947,
      "learning_rate": 2.7982242886028575e-05,
      "loss": 0.1108,
      "step": 364800
    },
    {
      "epoch": 2.370327064860827,
      "grad_norm": 1.091657042503357,
      "learning_rate": 2.7975332427602585e-05,
      "loss": 0.1152,
      "step": 364900
    },
    {
      "epoch": 2.3709766475039786,
      "grad_norm": 1.3501503467559814,
      "learning_rate": 2.7968421969176595e-05,
      "loss": 0.1123,
      "step": 365000
    },
    {
      "epoch": 2.3716262301471307,
      "grad_norm": 0.9312616586685181,
      "learning_rate": 2.7961511510750604e-05,
      "loss": 0.1147,
      "step": 365100
    },
    {
      "epoch": 2.3722758127902823,
      "grad_norm": 1.2005382776260376,
      "learning_rate": 2.7954601052324608e-05,
      "loss": 0.1214,
      "step": 365200
    },
    {
      "epoch": 2.372925395433434,
      "grad_norm": 1.0804280042648315,
      "learning_rate": 2.7947690593898617e-05,
      "loss": 0.1141,
      "step": 365300
    },
    {
      "epoch": 2.3735749780765856,
      "grad_norm": 1.1229345798492432,
      "learning_rate": 2.7940780135472627e-05,
      "loss": 0.1114,
      "step": 365400
    },
    {
      "epoch": 2.3742245607197376,
      "grad_norm": 0.7642452716827393,
      "learning_rate": 2.7933869677046637e-05,
      "loss": 0.1203,
      "step": 365500
    },
    {
      "epoch": 2.3748741433628893,
      "grad_norm": 0.9768989682197571,
      "learning_rate": 2.7926959218620647e-05,
      "loss": 0.1131,
      "step": 365600
    },
    {
      "epoch": 2.3755237260060413,
      "grad_norm": 1.174104928970337,
      "learning_rate": 2.7920048760194657e-05,
      "loss": 0.1203,
      "step": 365700
    },
    {
      "epoch": 2.376173308649193,
      "grad_norm": 1.4741367101669312,
      "learning_rate": 2.7913138301768667e-05,
      "loss": 0.1133,
      "step": 365800
    },
    {
      "epoch": 2.3768228912923446,
      "grad_norm": 1.145729899406433,
      "learning_rate": 2.7906227843342677e-05,
      "loss": 0.1172,
      "step": 365900
    },
    {
      "epoch": 2.377472473935496,
      "grad_norm": 0.999056339263916,
      "learning_rate": 2.7899317384916683e-05,
      "loss": 0.1158,
      "step": 366000
    },
    {
      "epoch": 2.3781220565786483,
      "grad_norm": 0.85830157995224,
      "learning_rate": 2.789240692649069e-05,
      "loss": 0.11,
      "step": 366100
    },
    {
      "epoch": 2.3787716392218,
      "grad_norm": 0.9436212778091431,
      "learning_rate": 2.78854964680647e-05,
      "loss": 0.1171,
      "step": 366200
    },
    {
      "epoch": 2.379421221864952,
      "grad_norm": 0.8481153249740601,
      "learning_rate": 2.787858600963871e-05,
      "loss": 0.1185,
      "step": 366300
    },
    {
      "epoch": 2.3800708045081036,
      "grad_norm": 1.361833095550537,
      "learning_rate": 2.7871675551212716e-05,
      "loss": 0.1135,
      "step": 366400
    },
    {
      "epoch": 2.3807203871512552,
      "grad_norm": 1.5196574926376343,
      "learning_rate": 2.7864765092786725e-05,
      "loss": 0.1202,
      "step": 366500
    },
    {
      "epoch": 2.381369969794407,
      "grad_norm": 1.5471547842025757,
      "learning_rate": 2.7857854634360735e-05,
      "loss": 0.1152,
      "step": 366600
    },
    {
      "epoch": 2.382019552437559,
      "grad_norm": 1.189090371131897,
      "learning_rate": 2.7850944175934745e-05,
      "loss": 0.1182,
      "step": 366700
    },
    {
      "epoch": 2.3826691350807105,
      "grad_norm": 0.9862112402915955,
      "learning_rate": 2.7844033717508755e-05,
      "loss": 0.1117,
      "step": 366800
    },
    {
      "epoch": 2.3833187177238626,
      "grad_norm": 1.045323133468628,
      "learning_rate": 2.7837123259082765e-05,
      "loss": 0.1136,
      "step": 366900
    },
    {
      "epoch": 2.3839683003670142,
      "grad_norm": 0.8367985486984253,
      "learning_rate": 2.7830212800656768e-05,
      "loss": 0.109,
      "step": 367000
    },
    {
      "epoch": 2.384617883010166,
      "grad_norm": 1.110594630241394,
      "learning_rate": 2.7823302342230778e-05,
      "loss": 0.1126,
      "step": 367100
    },
    {
      "epoch": 2.385267465653318,
      "grad_norm": 0.8848637342453003,
      "learning_rate": 2.7816391883804788e-05,
      "loss": 0.1158,
      "step": 367200
    },
    {
      "epoch": 2.3859170482964696,
      "grad_norm": 1.0354514122009277,
      "learning_rate": 2.7809481425378798e-05,
      "loss": 0.1113,
      "step": 367300
    },
    {
      "epoch": 2.386566630939621,
      "grad_norm": 1.0096303224563599,
      "learning_rate": 2.7802570966952807e-05,
      "loss": 0.1166,
      "step": 367400
    },
    {
      "epoch": 2.3872162135827732,
      "grad_norm": 1.3144395351409912,
      "learning_rate": 2.7795660508526817e-05,
      "loss": 0.1134,
      "step": 367500
    },
    {
      "epoch": 2.387865796225925,
      "grad_norm": 0.646776556968689,
      "learning_rate": 2.7788750050100827e-05,
      "loss": 0.11,
      "step": 367600
    },
    {
      "epoch": 2.3885153788690765,
      "grad_norm": 0.9929912090301514,
      "learning_rate": 2.7781839591674837e-05,
      "loss": 0.109,
      "step": 367700
    },
    {
      "epoch": 2.3891649615122286,
      "grad_norm": 1.5152431726455688,
      "learning_rate": 2.7774929133248843e-05,
      "loss": 0.1142,
      "step": 367800
    },
    {
      "epoch": 2.38981454415538,
      "grad_norm": 1.7666231393814087,
      "learning_rate": 2.7768018674822853e-05,
      "loss": 0.115,
      "step": 367900
    },
    {
      "epoch": 2.390464126798532,
      "grad_norm": 1.2898967266082764,
      "learning_rate": 2.776110821639686e-05,
      "loss": 0.1177,
      "step": 368000
    },
    {
      "epoch": 2.391113709441684,
      "grad_norm": 0.7575557827949524,
      "learning_rate": 2.7754197757970866e-05,
      "loss": 0.1119,
      "step": 368100
    },
    {
      "epoch": 2.3917632920848355,
      "grad_norm": 0.925132155418396,
      "learning_rate": 2.7747287299544876e-05,
      "loss": 0.1163,
      "step": 368200
    },
    {
      "epoch": 2.392412874727987,
      "grad_norm": 1.499812364578247,
      "learning_rate": 2.7740376841118886e-05,
      "loss": 0.1165,
      "step": 368300
    },
    {
      "epoch": 2.393062457371139,
      "grad_norm": 1.364917516708374,
      "learning_rate": 2.7733466382692896e-05,
      "loss": 0.1147,
      "step": 368400
    },
    {
      "epoch": 2.393712040014291,
      "grad_norm": 1.2931195497512817,
      "learning_rate": 2.7726555924266906e-05,
      "loss": 0.1124,
      "step": 368500
    },
    {
      "epoch": 2.3943616226574425,
      "grad_norm": 1.0116876363754272,
      "learning_rate": 2.7719645465840915e-05,
      "loss": 0.1164,
      "step": 368600
    },
    {
      "epoch": 2.3950112053005945,
      "grad_norm": 0.9561347961425781,
      "learning_rate": 2.7712735007414925e-05,
      "loss": 0.11,
      "step": 368700
    },
    {
      "epoch": 2.395660787943746,
      "grad_norm": 1.2938729524612427,
      "learning_rate": 2.7705824548988935e-05,
      "loss": 0.1191,
      "step": 368800
    },
    {
      "epoch": 2.3963103705868978,
      "grad_norm": 1.756937026977539,
      "learning_rate": 2.7698914090562938e-05,
      "loss": 0.1199,
      "step": 368900
    },
    {
      "epoch": 2.39695995323005,
      "grad_norm": 0.8390268087387085,
      "learning_rate": 2.7692003632136948e-05,
      "loss": 0.1128,
      "step": 369000
    },
    {
      "epoch": 2.3976095358732015,
      "grad_norm": 1.4837653636932373,
      "learning_rate": 2.7685093173710958e-05,
      "loss": 0.1152,
      "step": 369100
    },
    {
      "epoch": 2.398259118516353,
      "grad_norm": 1.667529821395874,
      "learning_rate": 2.7678182715284968e-05,
      "loss": 0.1102,
      "step": 369200
    },
    {
      "epoch": 2.398908701159505,
      "grad_norm": 1.112573266029358,
      "learning_rate": 2.7671272256858978e-05,
      "loss": 0.1158,
      "step": 369300
    },
    {
      "epoch": 2.399558283802657,
      "grad_norm": 1.0943379402160645,
      "learning_rate": 2.7664361798432987e-05,
      "loss": 0.1151,
      "step": 369400
    },
    {
      "epoch": 2.4002078664458084,
      "grad_norm": 1.7128230333328247,
      "learning_rate": 2.7657451340006997e-05,
      "loss": 0.1123,
      "step": 369500
    },
    {
      "epoch": 2.4008574490889605,
      "grad_norm": 1.4477335214614868,
      "learning_rate": 2.7650540881581004e-05,
      "loss": 0.1127,
      "step": 369600
    },
    {
      "epoch": 2.401507031732112,
      "grad_norm": 1.1464934349060059,
      "learning_rate": 2.7643630423155014e-05,
      "loss": 0.117,
      "step": 369700
    },
    {
      "epoch": 2.4021566143752637,
      "grad_norm": 1.2642672061920166,
      "learning_rate": 2.7636719964729023e-05,
      "loss": 0.1179,
      "step": 369800
    },
    {
      "epoch": 2.402806197018416,
      "grad_norm": 1.0297619104385376,
      "learning_rate": 2.7629809506303027e-05,
      "loss": 0.1115,
      "step": 369900
    },
    {
      "epoch": 2.4034557796615674,
      "grad_norm": 1.6190135478973389,
      "learning_rate": 2.7622899047877036e-05,
      "loss": 0.1105,
      "step": 370000
    },
    {
      "epoch": 2.404105362304719,
      "grad_norm": 0.7697054147720337,
      "learning_rate": 2.7615988589451046e-05,
      "loss": 0.1125,
      "step": 370100
    },
    {
      "epoch": 2.404754944947871,
      "grad_norm": 0.9550179839134216,
      "learning_rate": 2.7609078131025056e-05,
      "loss": 0.1096,
      "step": 370200
    },
    {
      "epoch": 2.4054045275910227,
      "grad_norm": 1.2022055387496948,
      "learning_rate": 2.7602167672599066e-05,
      "loss": 0.1198,
      "step": 370300
    },
    {
      "epoch": 2.4060541102341744,
      "grad_norm": 0.8198668360710144,
      "learning_rate": 2.7595257214173076e-05,
      "loss": 0.1127,
      "step": 370400
    },
    {
      "epoch": 2.4067036928773264,
      "grad_norm": 1.0149157047271729,
      "learning_rate": 2.7588346755747086e-05,
      "loss": 0.1157,
      "step": 370500
    },
    {
      "epoch": 2.407353275520478,
      "grad_norm": 1.6265051364898682,
      "learning_rate": 2.7581436297321096e-05,
      "loss": 0.1105,
      "step": 370600
    },
    {
      "epoch": 2.4080028581636297,
      "grad_norm": 0.9248876571655273,
      "learning_rate": 2.7574525838895105e-05,
      "loss": 0.1098,
      "step": 370700
    },
    {
      "epoch": 2.4086524408067818,
      "grad_norm": 1.2328673601150513,
      "learning_rate": 2.756761538046911e-05,
      "loss": 0.1202,
      "step": 370800
    },
    {
      "epoch": 2.4093020234499334,
      "grad_norm": 1.0773053169250488,
      "learning_rate": 2.756070492204312e-05,
      "loss": 0.1126,
      "step": 370900
    },
    {
      "epoch": 2.409951606093085,
      "grad_norm": 0.807266116142273,
      "learning_rate": 2.7553794463617128e-05,
      "loss": 0.1131,
      "step": 371000
    },
    {
      "epoch": 2.410601188736237,
      "grad_norm": 1.3936069011688232,
      "learning_rate": 2.7546884005191138e-05,
      "loss": 0.1147,
      "step": 371100
    },
    {
      "epoch": 2.4112507713793887,
      "grad_norm": 1.386622667312622,
      "learning_rate": 2.7539973546765148e-05,
      "loss": 0.12,
      "step": 371200
    },
    {
      "epoch": 2.4119003540225403,
      "grad_norm": 1.2764712572097778,
      "learning_rate": 2.7533063088339158e-05,
      "loss": 0.1121,
      "step": 371300
    },
    {
      "epoch": 2.4125499366656924,
      "grad_norm": 1.3378533124923706,
      "learning_rate": 2.7526152629913164e-05,
      "loss": 0.1149,
      "step": 371400
    },
    {
      "epoch": 2.413199519308844,
      "grad_norm": 0.9798773527145386,
      "learning_rate": 2.7519242171487174e-05,
      "loss": 0.1193,
      "step": 371500
    },
    {
      "epoch": 2.413849101951996,
      "grad_norm": 1.0756930112838745,
      "learning_rate": 2.7512331713061184e-05,
      "loss": 0.1106,
      "step": 371600
    },
    {
      "epoch": 2.4144986845951477,
      "grad_norm": 1.1290051937103271,
      "learning_rate": 2.7505421254635187e-05,
      "loss": 0.1118,
      "step": 371700
    },
    {
      "epoch": 2.4151482672382993,
      "grad_norm": 1.115980625152588,
      "learning_rate": 2.7498510796209197e-05,
      "loss": 0.1138,
      "step": 371800
    },
    {
      "epoch": 2.415797849881451,
      "grad_norm": 0.7460755109786987,
      "learning_rate": 2.7491600337783207e-05,
      "loss": 0.1118,
      "step": 371900
    },
    {
      "epoch": 2.416447432524603,
      "grad_norm": 0.9550734162330627,
      "learning_rate": 2.7484689879357217e-05,
      "loss": 0.1173,
      "step": 372000
    },
    {
      "epoch": 2.4170970151677547,
      "grad_norm": 0.8257817625999451,
      "learning_rate": 2.7477779420931226e-05,
      "loss": 0.1213,
      "step": 372100
    },
    {
      "epoch": 2.4177465978109067,
      "grad_norm": 1.265400767326355,
      "learning_rate": 2.7470868962505236e-05,
      "loss": 0.1192,
      "step": 372200
    },
    {
      "epoch": 2.4183961804540584,
      "grad_norm": 0.7472211122512817,
      "learning_rate": 2.7463958504079246e-05,
      "loss": 0.1092,
      "step": 372300
    },
    {
      "epoch": 2.41904576309721,
      "grad_norm": 1.216898798942566,
      "learning_rate": 2.7457048045653256e-05,
      "loss": 0.1156,
      "step": 372400
    },
    {
      "epoch": 2.4196953457403616,
      "grad_norm": 1.2024646997451782,
      "learning_rate": 2.7450137587227266e-05,
      "loss": 0.1182,
      "step": 372500
    },
    {
      "epoch": 2.4203449283835137,
      "grad_norm": 0.6961835026741028,
      "learning_rate": 2.7443227128801276e-05,
      "loss": 0.1153,
      "step": 372600
    },
    {
      "epoch": 2.4209945110266653,
      "grad_norm": 1.1922553777694702,
      "learning_rate": 2.743631667037528e-05,
      "loss": 0.1137,
      "step": 372700
    },
    {
      "epoch": 2.4216440936698174,
      "grad_norm": 0.8514546751976013,
      "learning_rate": 2.742940621194929e-05,
      "loss": 0.1208,
      "step": 372800
    },
    {
      "epoch": 2.422293676312969,
      "grad_norm": 1.1198904514312744,
      "learning_rate": 2.74224957535233e-05,
      "loss": 0.1106,
      "step": 372900
    },
    {
      "epoch": 2.4229432589561206,
      "grad_norm": 1.2544053792953491,
      "learning_rate": 2.7415585295097308e-05,
      "loss": 0.1154,
      "step": 373000
    },
    {
      "epoch": 2.4235928415992722,
      "grad_norm": 1.0613842010498047,
      "learning_rate": 2.7408674836671315e-05,
      "loss": 0.1147,
      "step": 373100
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 0.918135404586792,
      "learning_rate": 2.7401764378245325e-05,
      "loss": 0.1099,
      "step": 373200
    },
    {
      "epoch": 2.424892006885576,
      "grad_norm": 0.7335008382797241,
      "learning_rate": 2.7394853919819334e-05,
      "loss": 0.1135,
      "step": 373300
    },
    {
      "epoch": 2.425541589528728,
      "grad_norm": 1.035754680633545,
      "learning_rate": 2.7387943461393344e-05,
      "loss": 0.118,
      "step": 373400
    },
    {
      "epoch": 2.4261911721718796,
      "grad_norm": 0.907179057598114,
      "learning_rate": 2.7381033002967354e-05,
      "loss": 0.1112,
      "step": 373500
    },
    {
      "epoch": 2.4268407548150313,
      "grad_norm": 1.4309238195419312,
      "learning_rate": 2.7374122544541357e-05,
      "loss": 0.1089,
      "step": 373600
    },
    {
      "epoch": 2.427490337458183,
      "grad_norm": 0.7158011198043823,
      "learning_rate": 2.7367212086115367e-05,
      "loss": 0.1183,
      "step": 373700
    },
    {
      "epoch": 2.428139920101335,
      "grad_norm": 1.5318305492401123,
      "learning_rate": 2.7360301627689377e-05,
      "loss": 0.1128,
      "step": 373800
    },
    {
      "epoch": 2.4287895027444866,
      "grad_norm": 0.994525671005249,
      "learning_rate": 2.7353391169263387e-05,
      "loss": 0.1117,
      "step": 373900
    },
    {
      "epoch": 2.4294390853876386,
      "grad_norm": 0.8429110646247864,
      "learning_rate": 2.7346480710837397e-05,
      "loss": 0.113,
      "step": 374000
    },
    {
      "epoch": 2.4300886680307903,
      "grad_norm": 1.2653734683990479,
      "learning_rate": 2.7339570252411406e-05,
      "loss": 0.1189,
      "step": 374100
    },
    {
      "epoch": 2.430738250673942,
      "grad_norm": 1.1287723779678345,
      "learning_rate": 2.7332659793985416e-05,
      "loss": 0.1177,
      "step": 374200
    },
    {
      "epoch": 2.431387833317094,
      "grad_norm": 1.0534968376159668,
      "learning_rate": 2.7325749335559426e-05,
      "loss": 0.1144,
      "step": 374300
    },
    {
      "epoch": 2.4320374159602456,
      "grad_norm": 1.5079970359802246,
      "learning_rate": 2.7318838877133436e-05,
      "loss": 0.1152,
      "step": 374400
    },
    {
      "epoch": 2.432686998603397,
      "grad_norm": 0.8422186970710754,
      "learning_rate": 2.7311928418707446e-05,
      "loss": 0.1158,
      "step": 374500
    },
    {
      "epoch": 2.4333365812465493,
      "grad_norm": 1.156980276107788,
      "learning_rate": 2.730501796028145e-05,
      "loss": 0.1132,
      "step": 374600
    },
    {
      "epoch": 2.433986163889701,
      "grad_norm": 0.6234760880470276,
      "learning_rate": 2.729810750185546e-05,
      "loss": 0.1218,
      "step": 374700
    },
    {
      "epoch": 2.4346357465328525,
      "grad_norm": 1.2203576564788818,
      "learning_rate": 2.729119704342947e-05,
      "loss": 0.112,
      "step": 374800
    },
    {
      "epoch": 2.4352853291760046,
      "grad_norm": 1.807304859161377,
      "learning_rate": 2.7284286585003475e-05,
      "loss": 0.1158,
      "step": 374900
    },
    {
      "epoch": 2.4359349118191562,
      "grad_norm": 1.3355276584625244,
      "learning_rate": 2.7277376126577485e-05,
      "loss": 0.1148,
      "step": 375000
    },
    {
      "epoch": 2.436584494462308,
      "grad_norm": 0.8983549475669861,
      "learning_rate": 2.7270465668151495e-05,
      "loss": 0.1108,
      "step": 375100
    },
    {
      "epoch": 2.43723407710546,
      "grad_norm": 0.9699746966362,
      "learning_rate": 2.7263555209725505e-05,
      "loss": 0.1096,
      "step": 375200
    },
    {
      "epoch": 2.4378836597486115,
      "grad_norm": 1.0101850032806396,
      "learning_rate": 2.7256644751299515e-05,
      "loss": 0.1141,
      "step": 375300
    },
    {
      "epoch": 2.438533242391763,
      "grad_norm": 1.1970994472503662,
      "learning_rate": 2.7249734292873524e-05,
      "loss": 0.1204,
      "step": 375400
    },
    {
      "epoch": 2.4391828250349152,
      "grad_norm": 0.8419198393821716,
      "learning_rate": 2.7242823834447527e-05,
      "loss": 0.1227,
      "step": 375500
    },
    {
      "epoch": 2.439832407678067,
      "grad_norm": 1.0364537239074707,
      "learning_rate": 2.7235913376021537e-05,
      "loss": 0.1145,
      "step": 375600
    },
    {
      "epoch": 2.4404819903212185,
      "grad_norm": 1.4360136985778809,
      "learning_rate": 2.7229002917595547e-05,
      "loss": 0.1157,
      "step": 375700
    },
    {
      "epoch": 2.4411315729643706,
      "grad_norm": 1.0603488683700562,
      "learning_rate": 2.7222092459169557e-05,
      "loss": 0.1155,
      "step": 375800
    },
    {
      "epoch": 2.441781155607522,
      "grad_norm": 1.163869023323059,
      "learning_rate": 2.7215182000743567e-05,
      "loss": 0.1177,
      "step": 375900
    },
    {
      "epoch": 2.442430738250674,
      "grad_norm": 0.7795104384422302,
      "learning_rate": 2.7208271542317577e-05,
      "loss": 0.1164,
      "step": 376000
    },
    {
      "epoch": 2.443080320893826,
      "grad_norm": 1.1802563667297363,
      "learning_rate": 2.7201361083891587e-05,
      "loss": 0.1091,
      "step": 376100
    },
    {
      "epoch": 2.4437299035369775,
      "grad_norm": 1.0789477825164795,
      "learning_rate": 2.7194450625465596e-05,
      "loss": 0.1075,
      "step": 376200
    },
    {
      "epoch": 2.444379486180129,
      "grad_norm": 1.3382337093353271,
      "learning_rate": 2.7187540167039606e-05,
      "loss": 0.1143,
      "step": 376300
    },
    {
      "epoch": 2.445029068823281,
      "grad_norm": 1.09548819065094,
      "learning_rate": 2.718062970861361e-05,
      "loss": 0.1081,
      "step": 376400
    },
    {
      "epoch": 2.445678651466433,
      "grad_norm": 0.8974288105964661,
      "learning_rate": 2.717371925018762e-05,
      "loss": 0.116,
      "step": 376500
    },
    {
      "epoch": 2.4463282341095844,
      "grad_norm": 1.1310155391693115,
      "learning_rate": 2.716680879176163e-05,
      "loss": 0.1164,
      "step": 376600
    },
    {
      "epoch": 2.4469778167527365,
      "grad_norm": 1.0986099243164062,
      "learning_rate": 2.7159898333335636e-05,
      "loss": 0.1202,
      "step": 376700
    },
    {
      "epoch": 2.447627399395888,
      "grad_norm": 1.2195955514907837,
      "learning_rate": 2.7152987874909645e-05,
      "loss": 0.1184,
      "step": 376800
    },
    {
      "epoch": 2.4482769820390398,
      "grad_norm": 1.294654130935669,
      "learning_rate": 2.7146077416483655e-05,
      "loss": 0.1165,
      "step": 376900
    },
    {
      "epoch": 2.448926564682192,
      "grad_norm": 0.9465140700340271,
      "learning_rate": 2.7139166958057665e-05,
      "loss": 0.1146,
      "step": 377000
    },
    {
      "epoch": 2.4495761473253435,
      "grad_norm": 0.8989052176475525,
      "learning_rate": 2.7132256499631675e-05,
      "loss": 0.114,
      "step": 377100
    },
    {
      "epoch": 2.450225729968495,
      "grad_norm": 1.332829236984253,
      "learning_rate": 2.7125346041205685e-05,
      "loss": 0.1108,
      "step": 377200
    },
    {
      "epoch": 2.450875312611647,
      "grad_norm": 1.1943985223770142,
      "learning_rate": 2.7118435582779695e-05,
      "loss": 0.1159,
      "step": 377300
    },
    {
      "epoch": 2.4515248952547988,
      "grad_norm": 0.9580804705619812,
      "learning_rate": 2.7111525124353698e-05,
      "loss": 0.1161,
      "step": 377400
    },
    {
      "epoch": 2.4521744778979504,
      "grad_norm": 0.8410930633544922,
      "learning_rate": 2.7104614665927708e-05,
      "loss": 0.1169,
      "step": 377500
    },
    {
      "epoch": 2.4528240605411025,
      "grad_norm": 1.5081554651260376,
      "learning_rate": 2.7097704207501717e-05,
      "loss": 0.1125,
      "step": 377600
    },
    {
      "epoch": 2.453473643184254,
      "grad_norm": 1.1546499729156494,
      "learning_rate": 2.7090793749075727e-05,
      "loss": 0.1155,
      "step": 377700
    },
    {
      "epoch": 2.4541232258274057,
      "grad_norm": 1.270359754562378,
      "learning_rate": 2.7083883290649737e-05,
      "loss": 0.1136,
      "step": 377800
    },
    {
      "epoch": 2.454772808470558,
      "grad_norm": 1.114173173904419,
      "learning_rate": 2.7076972832223747e-05,
      "loss": 0.1123,
      "step": 377900
    },
    {
      "epoch": 2.4554223911137094,
      "grad_norm": 1.3299124240875244,
      "learning_rate": 2.7070062373797757e-05,
      "loss": 0.1114,
      "step": 378000
    },
    {
      "epoch": 2.456071973756861,
      "grad_norm": 1.4338842630386353,
      "learning_rate": 2.7063151915371767e-05,
      "loss": 0.1112,
      "step": 378100
    },
    {
      "epoch": 2.456721556400013,
      "grad_norm": 1.1364598274230957,
      "learning_rate": 2.7056241456945773e-05,
      "loss": 0.1164,
      "step": 378200
    },
    {
      "epoch": 2.4573711390431647,
      "grad_norm": 1.0504648685455322,
      "learning_rate": 2.704933099851978e-05,
      "loss": 0.1145,
      "step": 378300
    },
    {
      "epoch": 2.4580207216863164,
      "grad_norm": 1.2884082794189453,
      "learning_rate": 2.704242054009379e-05,
      "loss": 0.1139,
      "step": 378400
    },
    {
      "epoch": 2.4586703043294684,
      "grad_norm": 1.3978739976882935,
      "learning_rate": 2.7035510081667796e-05,
      "loss": 0.1124,
      "step": 378500
    },
    {
      "epoch": 2.45931988697262,
      "grad_norm": 1.155514121055603,
      "learning_rate": 2.7028599623241806e-05,
      "loss": 0.1162,
      "step": 378600
    },
    {
      "epoch": 2.4599694696157717,
      "grad_norm": 1.141254186630249,
      "learning_rate": 2.7021689164815816e-05,
      "loss": 0.1122,
      "step": 378700
    },
    {
      "epoch": 2.4606190522589237,
      "grad_norm": 1.8496618270874023,
      "learning_rate": 2.7014778706389825e-05,
      "loss": 0.1175,
      "step": 378800
    },
    {
      "epoch": 2.4612686349020754,
      "grad_norm": 1.2289583683013916,
      "learning_rate": 2.7007868247963835e-05,
      "loss": 0.1155,
      "step": 378900
    },
    {
      "epoch": 2.461918217545227,
      "grad_norm": 1.245859146118164,
      "learning_rate": 2.7000957789537845e-05,
      "loss": 0.1185,
      "step": 379000
    },
    {
      "epoch": 2.462567800188379,
      "grad_norm": 1.1648313999176025,
      "learning_rate": 2.6994047331111855e-05,
      "loss": 0.1139,
      "step": 379100
    },
    {
      "epoch": 2.4632173828315307,
      "grad_norm": 0.9016969799995422,
      "learning_rate": 2.6987136872685865e-05,
      "loss": 0.1214,
      "step": 379200
    },
    {
      "epoch": 2.4638669654746828,
      "grad_norm": 1.3962501287460327,
      "learning_rate": 2.6980226414259868e-05,
      "loss": 0.1076,
      "step": 379300
    },
    {
      "epoch": 2.4645165481178344,
      "grad_norm": 1.0652090311050415,
      "learning_rate": 2.6973315955833878e-05,
      "loss": 0.1137,
      "step": 379400
    },
    {
      "epoch": 2.465166130760986,
      "grad_norm": 1.2495251893997192,
      "learning_rate": 2.6966405497407888e-05,
      "loss": 0.1166,
      "step": 379500
    },
    {
      "epoch": 2.4658157134041376,
      "grad_norm": 1.114540696144104,
      "learning_rate": 2.6959495038981898e-05,
      "loss": 0.1084,
      "step": 379600
    },
    {
      "epoch": 2.4664652960472897,
      "grad_norm": 0.9665473103523254,
      "learning_rate": 2.6952584580555907e-05,
      "loss": 0.1184,
      "step": 379700
    },
    {
      "epoch": 2.4671148786904413,
      "grad_norm": 1.5412440299987793,
      "learning_rate": 2.6945674122129917e-05,
      "loss": 0.1093,
      "step": 379800
    },
    {
      "epoch": 2.4677644613335934,
      "grad_norm": 1.0527535676956177,
      "learning_rate": 2.6938763663703924e-05,
      "loss": 0.107,
      "step": 379900
    },
    {
      "epoch": 2.468414043976745,
      "grad_norm": 1.4731334447860718,
      "learning_rate": 2.6931853205277934e-05,
      "loss": 0.1049,
      "step": 380000
    },
    {
      "epoch": 2.4690636266198966,
      "grad_norm": 1.1004528999328613,
      "learning_rate": 2.6924942746851943e-05,
      "loss": 0.1132,
      "step": 380100
    },
    {
      "epoch": 2.4697132092630483,
      "grad_norm": 1.0513319969177246,
      "learning_rate": 2.691803228842595e-05,
      "loss": 0.1178,
      "step": 380200
    },
    {
      "epoch": 2.4703627919062003,
      "grad_norm": 0.7226352691650391,
      "learning_rate": 2.6911121829999956e-05,
      "loss": 0.1131,
      "step": 380300
    },
    {
      "epoch": 2.471012374549352,
      "grad_norm": 1.6028259992599487,
      "learning_rate": 2.6904211371573966e-05,
      "loss": 0.1175,
      "step": 380400
    },
    {
      "epoch": 2.471661957192504,
      "grad_norm": 0.9124371409416199,
      "learning_rate": 2.6897300913147976e-05,
      "loss": 0.1151,
      "step": 380500
    },
    {
      "epoch": 2.4723115398356557,
      "grad_norm": 1.4602978229522705,
      "learning_rate": 2.6890390454721986e-05,
      "loss": 0.1181,
      "step": 380600
    },
    {
      "epoch": 2.4729611224788073,
      "grad_norm": 0.7588872313499451,
      "learning_rate": 2.6883479996295996e-05,
      "loss": 0.1133,
      "step": 380700
    },
    {
      "epoch": 2.473610705121959,
      "grad_norm": 1.1965091228485107,
      "learning_rate": 2.6876569537870006e-05,
      "loss": 0.1119,
      "step": 380800
    },
    {
      "epoch": 2.474260287765111,
      "grad_norm": 1.037421464920044,
      "learning_rate": 2.6869659079444015e-05,
      "loss": 0.1147,
      "step": 380900
    },
    {
      "epoch": 2.4749098704082626,
      "grad_norm": 0.9149147272109985,
      "learning_rate": 2.6862748621018025e-05,
      "loss": 0.1129,
      "step": 381000
    },
    {
      "epoch": 2.4755594530514147,
      "grad_norm": 1.1929419040679932,
      "learning_rate": 2.685583816259203e-05,
      "loss": 0.1141,
      "step": 381100
    },
    {
      "epoch": 2.4762090356945663,
      "grad_norm": 0.8949677348136902,
      "learning_rate": 2.6848927704166038e-05,
      "loss": 0.1144,
      "step": 381200
    },
    {
      "epoch": 2.476858618337718,
      "grad_norm": 1.1737725734710693,
      "learning_rate": 2.6842017245740048e-05,
      "loss": 0.1072,
      "step": 381300
    },
    {
      "epoch": 2.4775082009808695,
      "grad_norm": 1.2822586297988892,
      "learning_rate": 2.6835106787314058e-05,
      "loss": 0.1117,
      "step": 381400
    },
    {
      "epoch": 2.4781577836240216,
      "grad_norm": 1.3708784580230713,
      "learning_rate": 2.6828196328888068e-05,
      "loss": 0.1105,
      "step": 381500
    },
    {
      "epoch": 2.4788073662671732,
      "grad_norm": 1.3479558229446411,
      "learning_rate": 2.6821285870462078e-05,
      "loss": 0.1111,
      "step": 381600
    },
    {
      "epoch": 2.4794569489103253,
      "grad_norm": 1.3054496049880981,
      "learning_rate": 2.6814375412036084e-05,
      "loss": 0.1161,
      "step": 381700
    },
    {
      "epoch": 2.480106531553477,
      "grad_norm": 1.2430273294448853,
      "learning_rate": 2.6807464953610094e-05,
      "loss": 0.1121,
      "step": 381800
    },
    {
      "epoch": 2.4807561141966286,
      "grad_norm": 1.1995439529418945,
      "learning_rate": 2.6800554495184104e-05,
      "loss": 0.1132,
      "step": 381900
    },
    {
      "epoch": 2.4814056968397806,
      "grad_norm": 1.730199933052063,
      "learning_rate": 2.6793644036758114e-05,
      "loss": 0.1074,
      "step": 382000
    },
    {
      "epoch": 2.4820552794829323,
      "grad_norm": 1.317535638809204,
      "learning_rate": 2.6786733578332117e-05,
      "loss": 0.1098,
      "step": 382100
    },
    {
      "epoch": 2.482704862126084,
      "grad_norm": 1.4366121292114258,
      "learning_rate": 2.6779823119906127e-05,
      "loss": 0.11,
      "step": 382200
    },
    {
      "epoch": 2.483354444769236,
      "grad_norm": 0.8899455666542053,
      "learning_rate": 2.6772912661480136e-05,
      "loss": 0.1122,
      "step": 382300
    },
    {
      "epoch": 2.4840040274123876,
      "grad_norm": 1.4513505697250366,
      "learning_rate": 2.6766002203054146e-05,
      "loss": 0.1107,
      "step": 382400
    },
    {
      "epoch": 2.484653610055539,
      "grad_norm": 1.226562261581421,
      "learning_rate": 2.6759091744628156e-05,
      "loss": 0.1061,
      "step": 382500
    },
    {
      "epoch": 2.4853031926986913,
      "grad_norm": 1.216174602508545,
      "learning_rate": 2.6752181286202166e-05,
      "loss": 0.1164,
      "step": 382600
    },
    {
      "epoch": 2.485952775341843,
      "grad_norm": 1.489288330078125,
      "learning_rate": 2.6745270827776176e-05,
      "loss": 0.1162,
      "step": 382700
    },
    {
      "epoch": 2.4866023579849945,
      "grad_norm": 1.0602121353149414,
      "learning_rate": 2.6738360369350186e-05,
      "loss": 0.1124,
      "step": 382800
    },
    {
      "epoch": 2.4872519406281466,
      "grad_norm": 1.1597228050231934,
      "learning_rate": 2.6731449910924196e-05,
      "loss": 0.1143,
      "step": 382900
    },
    {
      "epoch": 2.487901523271298,
      "grad_norm": 0.8465001583099365,
      "learning_rate": 2.67245394524982e-05,
      "loss": 0.1114,
      "step": 383000
    },
    {
      "epoch": 2.48855110591445,
      "grad_norm": 0.9029393196105957,
      "learning_rate": 2.671762899407221e-05,
      "loss": 0.1069,
      "step": 383100
    },
    {
      "epoch": 2.489200688557602,
      "grad_norm": 1.155930757522583,
      "learning_rate": 2.6710718535646218e-05,
      "loss": 0.1136,
      "step": 383200
    },
    {
      "epoch": 2.4898502712007535,
      "grad_norm": 1.040932297706604,
      "learning_rate": 2.6703808077220228e-05,
      "loss": 0.1128,
      "step": 383300
    },
    {
      "epoch": 2.490499853843905,
      "grad_norm": 0.9538137316703796,
      "learning_rate": 2.6696897618794238e-05,
      "loss": 0.1144,
      "step": 383400
    },
    {
      "epoch": 2.4911494364870572,
      "grad_norm": 1.0989378690719604,
      "learning_rate": 2.6689987160368244e-05,
      "loss": 0.1056,
      "step": 383500
    },
    {
      "epoch": 2.491799019130209,
      "grad_norm": 1.3815048933029175,
      "learning_rate": 2.6683076701942254e-05,
      "loss": 0.1116,
      "step": 383600
    },
    {
      "epoch": 2.4924486017733605,
      "grad_norm": 1.3777395486831665,
      "learning_rate": 2.6676166243516264e-05,
      "loss": 0.1113,
      "step": 383700
    },
    {
      "epoch": 2.4930981844165125,
      "grad_norm": 1.038090705871582,
      "learning_rate": 2.6669255785090274e-05,
      "loss": 0.1045,
      "step": 383800
    },
    {
      "epoch": 2.493747767059664,
      "grad_norm": 1.039980173110962,
      "learning_rate": 2.6662345326664284e-05,
      "loss": 0.1187,
      "step": 383900
    },
    {
      "epoch": 2.494397349702816,
      "grad_norm": 1.4352866411209106,
      "learning_rate": 2.6655434868238287e-05,
      "loss": 0.1127,
      "step": 384000
    },
    {
      "epoch": 2.495046932345968,
      "grad_norm": 1.6581568717956543,
      "learning_rate": 2.6648524409812297e-05,
      "loss": 0.106,
      "step": 384100
    },
    {
      "epoch": 2.4956965149891195,
      "grad_norm": 1.2792484760284424,
      "learning_rate": 2.6641613951386307e-05,
      "loss": 0.1153,
      "step": 384200
    },
    {
      "epoch": 2.496346097632271,
      "grad_norm": 1.3610881567001343,
      "learning_rate": 2.6634703492960316e-05,
      "loss": 0.1111,
      "step": 384300
    },
    {
      "epoch": 2.496995680275423,
      "grad_norm": 1.01810622215271,
      "learning_rate": 2.6627793034534326e-05,
      "loss": 0.111,
      "step": 384400
    },
    {
      "epoch": 2.497645262918575,
      "grad_norm": 1.0396206378936768,
      "learning_rate": 2.6620882576108336e-05,
      "loss": 0.1212,
      "step": 384500
    },
    {
      "epoch": 2.4982948455617264,
      "grad_norm": 0.7884253263473511,
      "learning_rate": 2.6613972117682346e-05,
      "loss": 0.1134,
      "step": 384600
    },
    {
      "epoch": 2.4989444282048785,
      "grad_norm": 1.1535933017730713,
      "learning_rate": 2.6607061659256356e-05,
      "loss": 0.1135,
      "step": 384700
    },
    {
      "epoch": 2.49959401084803,
      "grad_norm": 0.9427801966667175,
      "learning_rate": 2.6600151200830366e-05,
      "loss": 0.1175,
      "step": 384800
    },
    {
      "epoch": 2.5002435934911817,
      "grad_norm": 1.0176044702529907,
      "learning_rate": 2.659324074240437e-05,
      "loss": 0.1162,
      "step": 384900
    },
    {
      "epoch": 2.500893176134334,
      "grad_norm": 1.209385633468628,
      "learning_rate": 2.658633028397838e-05,
      "loss": 0.1047,
      "step": 385000
    },
    {
      "epoch": 2.5015427587774854,
      "grad_norm": 1.2267407178878784,
      "learning_rate": 2.657941982555239e-05,
      "loss": 0.1078,
      "step": 385100
    },
    {
      "epoch": 2.5021923414206375,
      "grad_norm": 0.8169118165969849,
      "learning_rate": 2.65725093671264e-05,
      "loss": 0.117,
      "step": 385200
    },
    {
      "epoch": 2.502841924063789,
      "grad_norm": 1.1137365102767944,
      "learning_rate": 2.6565598908700405e-05,
      "loss": 0.1101,
      "step": 385300
    },
    {
      "epoch": 2.5034915067069408,
      "grad_norm": 0.9329376220703125,
      "learning_rate": 2.6558688450274415e-05,
      "loss": 0.1116,
      "step": 385400
    },
    {
      "epoch": 2.5041410893500924,
      "grad_norm": 0.9450711607933044,
      "learning_rate": 2.6551777991848425e-05,
      "loss": 0.1154,
      "step": 385500
    },
    {
      "epoch": 2.5047906719932445,
      "grad_norm": 1.1849992275238037,
      "learning_rate": 2.6544867533422434e-05,
      "loss": 0.1161,
      "step": 385600
    },
    {
      "epoch": 2.505440254636396,
      "grad_norm": 1.2896819114685059,
      "learning_rate": 2.6537957074996444e-05,
      "loss": 0.1118,
      "step": 385700
    },
    {
      "epoch": 2.506089837279548,
      "grad_norm": 1.090027093887329,
      "learning_rate": 2.6531046616570454e-05,
      "loss": 0.114,
      "step": 385800
    },
    {
      "epoch": 2.5067394199226998,
      "grad_norm": 0.9446364045143127,
      "learning_rate": 2.6524136158144457e-05,
      "loss": 0.1073,
      "step": 385900
    },
    {
      "epoch": 2.5073890025658514,
      "grad_norm": 1.2122067213058472,
      "learning_rate": 2.6517225699718467e-05,
      "loss": 0.111,
      "step": 386000
    },
    {
      "epoch": 2.508038585209003,
      "grad_norm": 1.1016600131988525,
      "learning_rate": 2.6510315241292477e-05,
      "loss": 0.1144,
      "step": 386100
    },
    {
      "epoch": 2.508688167852155,
      "grad_norm": 1.2496697902679443,
      "learning_rate": 2.6503404782866487e-05,
      "loss": 0.1114,
      "step": 386200
    },
    {
      "epoch": 2.5093377504953067,
      "grad_norm": 1.7195672988891602,
      "learning_rate": 2.6496494324440497e-05,
      "loss": 0.1182,
      "step": 386300
    },
    {
      "epoch": 2.509987333138459,
      "grad_norm": 0.8243333697319031,
      "learning_rate": 2.6489583866014506e-05,
      "loss": 0.1061,
      "step": 386400
    },
    {
      "epoch": 2.5106369157816104,
      "grad_norm": 1.3227596282958984,
      "learning_rate": 2.6482673407588516e-05,
      "loss": 0.1134,
      "step": 386500
    },
    {
      "epoch": 2.511286498424762,
      "grad_norm": 1.1494734287261963,
      "learning_rate": 2.6475762949162526e-05,
      "loss": 0.1116,
      "step": 386600
    },
    {
      "epoch": 2.5119360810679137,
      "grad_norm": 1.2368015050888062,
      "learning_rate": 2.6468852490736533e-05,
      "loss": 0.1126,
      "step": 386700
    },
    {
      "epoch": 2.5125856637110657,
      "grad_norm": 1.1259666681289673,
      "learning_rate": 2.646194203231054e-05,
      "loss": 0.1084,
      "step": 386800
    },
    {
      "epoch": 2.5132352463542174,
      "grad_norm": 1.5787302255630493,
      "learning_rate": 2.645503157388455e-05,
      "loss": 0.1095,
      "step": 386900
    },
    {
      "epoch": 2.5138848289973694,
      "grad_norm": 0.8886985778808594,
      "learning_rate": 2.644812111545856e-05,
      "loss": 0.1138,
      "step": 387000
    },
    {
      "epoch": 2.514534411640521,
      "grad_norm": 1.1588048934936523,
      "learning_rate": 2.6441210657032565e-05,
      "loss": 0.1149,
      "step": 387100
    },
    {
      "epoch": 2.5151839942836727,
      "grad_norm": 1.7006956338882446,
      "learning_rate": 2.6434300198606575e-05,
      "loss": 0.1063,
      "step": 387200
    },
    {
      "epoch": 2.5158335769268243,
      "grad_norm": 1.2447487115859985,
      "learning_rate": 2.6427389740180585e-05,
      "loss": 0.1173,
      "step": 387300
    },
    {
      "epoch": 2.5164831595699764,
      "grad_norm": 0.9450350403785706,
      "learning_rate": 2.6420479281754595e-05,
      "loss": 0.1168,
      "step": 387400
    },
    {
      "epoch": 2.517132742213128,
      "grad_norm": 1.4581071138381958,
      "learning_rate": 2.6413568823328605e-05,
      "loss": 0.1131,
      "step": 387500
    },
    {
      "epoch": 2.51778232485628,
      "grad_norm": 0.9211600422859192,
      "learning_rate": 2.6406658364902615e-05,
      "loss": 0.1112,
      "step": 387600
    },
    {
      "epoch": 2.5184319074994317,
      "grad_norm": 1.1325870752334595,
      "learning_rate": 2.6399747906476618e-05,
      "loss": 0.1097,
      "step": 387700
    },
    {
      "epoch": 2.5190814901425833,
      "grad_norm": 0.8613808751106262,
      "learning_rate": 2.6392837448050627e-05,
      "loss": 0.1141,
      "step": 387800
    },
    {
      "epoch": 2.519731072785735,
      "grad_norm": 1.5270918607711792,
      "learning_rate": 2.6385926989624637e-05,
      "loss": 0.1169,
      "step": 387900
    },
    {
      "epoch": 2.520380655428887,
      "grad_norm": 0.8669672608375549,
      "learning_rate": 2.6379016531198647e-05,
      "loss": 0.1052,
      "step": 388000
    },
    {
      "epoch": 2.5210302380720386,
      "grad_norm": 1.2240877151489258,
      "learning_rate": 2.6372106072772657e-05,
      "loss": 0.1086,
      "step": 388100
    },
    {
      "epoch": 2.5216798207151907,
      "grad_norm": 0.8988316655158997,
      "learning_rate": 2.6365195614346667e-05,
      "loss": 0.1131,
      "step": 388200
    },
    {
      "epoch": 2.5223294033583423,
      "grad_norm": 1.0517455339431763,
      "learning_rate": 2.6358285155920677e-05,
      "loss": 0.11,
      "step": 388300
    },
    {
      "epoch": 2.522978986001494,
      "grad_norm": 1.1680713891983032,
      "learning_rate": 2.6351374697494687e-05,
      "loss": 0.1138,
      "step": 388400
    },
    {
      "epoch": 2.5236285686446456,
      "grad_norm": 1.0528188943862915,
      "learning_rate": 2.6344464239068693e-05,
      "loss": 0.1123,
      "step": 388500
    },
    {
      "epoch": 2.5242781512877976,
      "grad_norm": 1.393858551979065,
      "learning_rate": 2.6337553780642703e-05,
      "loss": 0.1198,
      "step": 388600
    },
    {
      "epoch": 2.5249277339309493,
      "grad_norm": 1.0471880435943604,
      "learning_rate": 2.633064332221671e-05,
      "loss": 0.1192,
      "step": 388700
    },
    {
      "epoch": 2.5255773165741013,
      "grad_norm": 0.7620964646339417,
      "learning_rate": 2.632373286379072e-05,
      "loss": 0.11,
      "step": 388800
    },
    {
      "epoch": 2.526226899217253,
      "grad_norm": 1.1669732332229614,
      "learning_rate": 2.6316822405364726e-05,
      "loss": 0.1151,
      "step": 388900
    },
    {
      "epoch": 2.5268764818604046,
      "grad_norm": 1.3032978773117065,
      "learning_rate": 2.6309911946938735e-05,
      "loss": 0.1131,
      "step": 389000
    },
    {
      "epoch": 2.527526064503556,
      "grad_norm": 1.1186848878860474,
      "learning_rate": 2.6303001488512745e-05,
      "loss": 0.1182,
      "step": 389100
    },
    {
      "epoch": 2.5281756471467083,
      "grad_norm": 1.4328117370605469,
      "learning_rate": 2.6296091030086755e-05,
      "loss": 0.1157,
      "step": 389200
    },
    {
      "epoch": 2.52882522978986,
      "grad_norm": 1.3459933996200562,
      "learning_rate": 2.6289180571660765e-05,
      "loss": 0.1126,
      "step": 389300
    },
    {
      "epoch": 2.529474812433012,
      "grad_norm": 1.0306618213653564,
      "learning_rate": 2.6282270113234775e-05,
      "loss": 0.1115,
      "step": 389400
    },
    {
      "epoch": 2.5301243950761636,
      "grad_norm": 1.6597899198532104,
      "learning_rate": 2.6275359654808785e-05,
      "loss": 0.1155,
      "step": 389500
    },
    {
      "epoch": 2.5307739777193152,
      "grad_norm": 0.9416494965553284,
      "learning_rate": 2.6268449196382788e-05,
      "loss": 0.1116,
      "step": 389600
    },
    {
      "epoch": 2.531423560362467,
      "grad_norm": 1.5352810621261597,
      "learning_rate": 2.6261538737956798e-05,
      "loss": 0.1158,
      "step": 389700
    },
    {
      "epoch": 2.532073143005619,
      "grad_norm": 0.6989672780036926,
      "learning_rate": 2.6254628279530808e-05,
      "loss": 0.1097,
      "step": 389800
    },
    {
      "epoch": 2.5327227256487705,
      "grad_norm": 1.180092453956604,
      "learning_rate": 2.6247717821104817e-05,
      "loss": 0.1181,
      "step": 389900
    },
    {
      "epoch": 2.5333723082919226,
      "grad_norm": 1.2541199922561646,
      "learning_rate": 2.6240807362678827e-05,
      "loss": 0.1122,
      "step": 390000
    },
    {
      "epoch": 2.5340218909350742,
      "grad_norm": 1.4234486818313599,
      "learning_rate": 2.6233896904252837e-05,
      "loss": 0.1075,
      "step": 390100
    },
    {
      "epoch": 2.534671473578226,
      "grad_norm": 0.916242778301239,
      "learning_rate": 2.6226986445826847e-05,
      "loss": 0.1121,
      "step": 390200
    },
    {
      "epoch": 2.5353210562213775,
      "grad_norm": 0.7691372036933899,
      "learning_rate": 2.6220075987400853e-05,
      "loss": 0.1147,
      "step": 390300
    },
    {
      "epoch": 2.5359706388645296,
      "grad_norm": 1.3662644624710083,
      "learning_rate": 2.6213165528974863e-05,
      "loss": 0.1125,
      "step": 390400
    },
    {
      "epoch": 2.536620221507681,
      "grad_norm": 1.3306856155395508,
      "learning_rate": 2.6206255070548873e-05,
      "loss": 0.107,
      "step": 390500
    },
    {
      "epoch": 2.5372698041508333,
      "grad_norm": 0.9862239956855774,
      "learning_rate": 2.619934461212288e-05,
      "loss": 0.1106,
      "step": 390600
    },
    {
      "epoch": 2.537919386793985,
      "grad_norm": 1.0258183479309082,
      "learning_rate": 2.6192434153696886e-05,
      "loss": 0.1134,
      "step": 390700
    },
    {
      "epoch": 2.5385689694371365,
      "grad_norm": 0.7806190848350525,
      "learning_rate": 2.6185523695270896e-05,
      "loss": 0.1133,
      "step": 390800
    },
    {
      "epoch": 2.5392185520802886,
      "grad_norm": 1.0381417274475098,
      "learning_rate": 2.6178613236844906e-05,
      "loss": 0.1126,
      "step": 390900
    },
    {
      "epoch": 2.53986813472344,
      "grad_norm": 0.7732764482498169,
      "learning_rate": 2.6171702778418916e-05,
      "loss": 0.1161,
      "step": 391000
    },
    {
      "epoch": 2.540517717366592,
      "grad_norm": 0.8948361873626709,
      "learning_rate": 2.6164792319992925e-05,
      "loss": 0.1107,
      "step": 391100
    },
    {
      "epoch": 2.541167300009744,
      "grad_norm": 1.1620938777923584,
      "learning_rate": 2.6157881861566935e-05,
      "loss": 0.114,
      "step": 391200
    },
    {
      "epoch": 2.5418168826528955,
      "grad_norm": 1.0173838138580322,
      "learning_rate": 2.6150971403140945e-05,
      "loss": 0.1101,
      "step": 391300
    },
    {
      "epoch": 2.542466465296047,
      "grad_norm": 2.2144296169281006,
      "learning_rate": 2.6144060944714955e-05,
      "loss": 0.1137,
      "step": 391400
    },
    {
      "epoch": 2.543116047939199,
      "grad_norm": 1.5874801874160767,
      "learning_rate": 2.6137150486288958e-05,
      "loss": 0.1135,
      "step": 391500
    },
    {
      "epoch": 2.543765630582351,
      "grad_norm": 1.0650187730789185,
      "learning_rate": 2.6130240027862968e-05,
      "loss": 0.1138,
      "step": 391600
    },
    {
      "epoch": 2.5444152132255025,
      "grad_norm": 1.0986953973770142,
      "learning_rate": 2.6123329569436978e-05,
      "loss": 0.109,
      "step": 391700
    },
    {
      "epoch": 2.5450647958686545,
      "grad_norm": 1.2416284084320068,
      "learning_rate": 2.6116419111010988e-05,
      "loss": 0.107,
      "step": 391800
    },
    {
      "epoch": 2.545714378511806,
      "grad_norm": 0.883472740650177,
      "learning_rate": 2.6109508652584997e-05,
      "loss": 0.1119,
      "step": 391900
    },
    {
      "epoch": 2.5463639611549578,
      "grad_norm": 0.9552823305130005,
      "learning_rate": 2.6102598194159007e-05,
      "loss": 0.1149,
      "step": 392000
    },
    {
      "epoch": 2.54701354379811,
      "grad_norm": 1.4702951908111572,
      "learning_rate": 2.6095687735733014e-05,
      "loss": 0.1064,
      "step": 392100
    },
    {
      "epoch": 2.5476631264412615,
      "grad_norm": 1.1624242067337036,
      "learning_rate": 2.6088777277307024e-05,
      "loss": 0.1139,
      "step": 392200
    },
    {
      "epoch": 2.5483127090844135,
      "grad_norm": 1.1415612697601318,
      "learning_rate": 2.6081866818881034e-05,
      "loss": 0.1146,
      "step": 392300
    },
    {
      "epoch": 2.548962291727565,
      "grad_norm": 0.9235137701034546,
      "learning_rate": 2.607495636045504e-05,
      "loss": 0.1164,
      "step": 392400
    },
    {
      "epoch": 2.549611874370717,
      "grad_norm": 1.1679539680480957,
      "learning_rate": 2.6068045902029046e-05,
      "loss": 0.1143,
      "step": 392500
    },
    {
      "epoch": 2.5502614570138684,
      "grad_norm": 1.110475778579712,
      "learning_rate": 2.6061135443603056e-05,
      "loss": 0.1204,
      "step": 392600
    },
    {
      "epoch": 2.5509110396570205,
      "grad_norm": 1.0841882228851318,
      "learning_rate": 2.6054224985177066e-05,
      "loss": 0.1139,
      "step": 392700
    },
    {
      "epoch": 2.551560622300172,
      "grad_norm": 1.1935679912567139,
      "learning_rate": 2.6047314526751076e-05,
      "loss": 0.1098,
      "step": 392800
    },
    {
      "epoch": 2.552210204943324,
      "grad_norm": 1.5257978439331055,
      "learning_rate": 2.6040404068325086e-05,
      "loss": 0.1082,
      "step": 392900
    },
    {
      "epoch": 2.552859787586476,
      "grad_norm": 1.0665254592895508,
      "learning_rate": 2.6033493609899096e-05,
      "loss": 0.11,
      "step": 393000
    },
    {
      "epoch": 2.5535093702296274,
      "grad_norm": 1.4538242816925049,
      "learning_rate": 2.6026583151473106e-05,
      "loss": 0.1139,
      "step": 393100
    },
    {
      "epoch": 2.554158952872779,
      "grad_norm": 1.0517195463180542,
      "learning_rate": 2.6019672693047115e-05,
      "loss": 0.1116,
      "step": 393200
    },
    {
      "epoch": 2.554808535515931,
      "grad_norm": 1.0031287670135498,
      "learning_rate": 2.6012762234621125e-05,
      "loss": 0.1119,
      "step": 393300
    },
    {
      "epoch": 2.5554581181590827,
      "grad_norm": 1.19717538356781,
      "learning_rate": 2.600585177619513e-05,
      "loss": 0.112,
      "step": 393400
    },
    {
      "epoch": 2.556107700802235,
      "grad_norm": 0.6812901496887207,
      "learning_rate": 2.5998941317769138e-05,
      "loss": 0.1139,
      "step": 393500
    },
    {
      "epoch": 2.5567572834453864,
      "grad_norm": 1.4556727409362793,
      "learning_rate": 2.5992030859343148e-05,
      "loss": 0.1102,
      "step": 393600
    },
    {
      "epoch": 2.557406866088538,
      "grad_norm": 2.313028335571289,
      "learning_rate": 2.5985120400917158e-05,
      "loss": 0.1145,
      "step": 393700
    },
    {
      "epoch": 2.5580564487316897,
      "grad_norm": 1.0786021947860718,
      "learning_rate": 2.5978209942491168e-05,
      "loss": 0.1119,
      "step": 393800
    },
    {
      "epoch": 2.5587060313748418,
      "grad_norm": 1.4572250843048096,
      "learning_rate": 2.5971299484065174e-05,
      "loss": 0.1188,
      "step": 393900
    },
    {
      "epoch": 2.5593556140179934,
      "grad_norm": 1.0059658288955688,
      "learning_rate": 2.5964389025639184e-05,
      "loss": 0.1086,
      "step": 394000
    },
    {
      "epoch": 2.5600051966611455,
      "grad_norm": 1.2788426876068115,
      "learning_rate": 2.5957478567213194e-05,
      "loss": 0.1127,
      "step": 394100
    },
    {
      "epoch": 2.560654779304297,
      "grad_norm": 0.954903244972229,
      "learning_rate": 2.5950568108787204e-05,
      "loss": 0.1088,
      "step": 394200
    },
    {
      "epoch": 2.5613043619474487,
      "grad_norm": 0.949318528175354,
      "learning_rate": 2.5943657650361207e-05,
      "loss": 0.1128,
      "step": 394300
    },
    {
      "epoch": 2.5619539445906003,
      "grad_norm": 1.0258535146713257,
      "learning_rate": 2.5936747191935217e-05,
      "loss": 0.1119,
      "step": 394400
    },
    {
      "epoch": 2.5626035272337524,
      "grad_norm": 1.0893149375915527,
      "learning_rate": 2.5929836733509227e-05,
      "loss": 0.1047,
      "step": 394500
    },
    {
      "epoch": 2.563253109876904,
      "grad_norm": 1.4102689027786255,
      "learning_rate": 2.5922926275083236e-05,
      "loss": 0.1089,
      "step": 394600
    },
    {
      "epoch": 2.563902692520056,
      "grad_norm": 1.4137930870056152,
      "learning_rate": 2.5916015816657246e-05,
      "loss": 0.1076,
      "step": 394700
    },
    {
      "epoch": 2.5645522751632077,
      "grad_norm": 0.888360857963562,
      "learning_rate": 2.5909105358231256e-05,
      "loss": 0.1049,
      "step": 394800
    },
    {
      "epoch": 2.5652018578063593,
      "grad_norm": 0.9411258101463318,
      "learning_rate": 2.5902194899805266e-05,
      "loss": 0.1107,
      "step": 394900
    },
    {
      "epoch": 2.565851440449511,
      "grad_norm": 1.3951927423477173,
      "learning_rate": 2.5895284441379276e-05,
      "loss": 0.1091,
      "step": 395000
    },
    {
      "epoch": 2.566501023092663,
      "grad_norm": 1.2917333841323853,
      "learning_rate": 2.5888373982953286e-05,
      "loss": 0.1123,
      "step": 395100
    },
    {
      "epoch": 2.5671506057358147,
      "grad_norm": 1.28523588180542,
      "learning_rate": 2.5881463524527295e-05,
      "loss": 0.1058,
      "step": 395200
    },
    {
      "epoch": 2.5678001883789667,
      "grad_norm": 1.3875641822814941,
      "learning_rate": 2.58745530661013e-05,
      "loss": 0.1082,
      "step": 395300
    },
    {
      "epoch": 2.5684497710221184,
      "grad_norm": 0.8028649091720581,
      "learning_rate": 2.586764260767531e-05,
      "loss": 0.1124,
      "step": 395400
    },
    {
      "epoch": 2.56909935366527,
      "grad_norm": 0.9986253976821899,
      "learning_rate": 2.5860732149249318e-05,
      "loss": 0.1055,
      "step": 395500
    },
    {
      "epoch": 2.5697489363084216,
      "grad_norm": 1.0907976627349854,
      "learning_rate": 2.5853821690823328e-05,
      "loss": 0.1176,
      "step": 395600
    },
    {
      "epoch": 2.5703985189515737,
      "grad_norm": 0.8774310350418091,
      "learning_rate": 2.5846911232397335e-05,
      "loss": 0.1109,
      "step": 395700
    },
    {
      "epoch": 2.5710481015947253,
      "grad_norm": 0.7210485339164734,
      "learning_rate": 2.5840000773971344e-05,
      "loss": 0.1199,
      "step": 395800
    },
    {
      "epoch": 2.5716976842378774,
      "grad_norm": 1.0918229818344116,
      "learning_rate": 2.5833090315545354e-05,
      "loss": 0.1184,
      "step": 395900
    },
    {
      "epoch": 2.572347266881029,
      "grad_norm": 0.944828987121582,
      "learning_rate": 2.5826179857119364e-05,
      "loss": 0.1084,
      "step": 396000
    },
    {
      "epoch": 2.5729968495241806,
      "grad_norm": 1.0919855833053589,
      "learning_rate": 2.5819269398693374e-05,
      "loss": 0.1109,
      "step": 396100
    },
    {
      "epoch": 2.5736464321673322,
      "grad_norm": 1.268478274345398,
      "learning_rate": 2.5812358940267377e-05,
      "loss": 0.1039,
      "step": 396200
    },
    {
      "epoch": 2.5742960148104843,
      "grad_norm": 0.616633951663971,
      "learning_rate": 2.5805448481841387e-05,
      "loss": 0.1202,
      "step": 396300
    },
    {
      "epoch": 2.574945597453636,
      "grad_norm": 0.9692588448524475,
      "learning_rate": 2.5798538023415397e-05,
      "loss": 0.1113,
      "step": 396400
    },
    {
      "epoch": 2.575595180096788,
      "grad_norm": 1.0362300872802734,
      "learning_rate": 2.5791627564989407e-05,
      "loss": 0.1155,
      "step": 396500
    },
    {
      "epoch": 2.5762447627399396,
      "grad_norm": 0.7535005807876587,
      "learning_rate": 2.5784717106563416e-05,
      "loss": 0.1166,
      "step": 396600
    },
    {
      "epoch": 2.5768943453830913,
      "grad_norm": 1.0622864961624146,
      "learning_rate": 2.5777806648137426e-05,
      "loss": 0.1102,
      "step": 396700
    },
    {
      "epoch": 2.577543928026243,
      "grad_norm": 0.9719193577766418,
      "learning_rate": 2.5770896189711436e-05,
      "loss": 0.1136,
      "step": 396800
    },
    {
      "epoch": 2.578193510669395,
      "grad_norm": 1.435510516166687,
      "learning_rate": 2.5763985731285446e-05,
      "loss": 0.1122,
      "step": 396900
    },
    {
      "epoch": 2.5788430933125466,
      "grad_norm": 0.9383102059364319,
      "learning_rate": 2.5757075272859456e-05,
      "loss": 0.1084,
      "step": 397000
    },
    {
      "epoch": 2.5794926759556986,
      "grad_norm": 1.1911839246749878,
      "learning_rate": 2.575016481443346e-05,
      "loss": 0.1074,
      "step": 397100
    },
    {
      "epoch": 2.5801422585988503,
      "grad_norm": 1.1680173873901367,
      "learning_rate": 2.574325435600747e-05,
      "loss": 0.1181,
      "step": 397200
    },
    {
      "epoch": 2.580791841242002,
      "grad_norm": 1.449109673500061,
      "learning_rate": 2.573634389758148e-05,
      "loss": 0.1139,
      "step": 397300
    },
    {
      "epoch": 2.5814414238851535,
      "grad_norm": 1.0106713771820068,
      "learning_rate": 2.572943343915549e-05,
      "loss": 0.1178,
      "step": 397400
    },
    {
      "epoch": 2.5820910065283056,
      "grad_norm": 1.4599084854125977,
      "learning_rate": 2.5722522980729495e-05,
      "loss": 0.1125,
      "step": 397500
    },
    {
      "epoch": 2.582740589171457,
      "grad_norm": 1.1004400253295898,
      "learning_rate": 2.5715612522303505e-05,
      "loss": 0.1078,
      "step": 397600
    },
    {
      "epoch": 2.5833901718146093,
      "grad_norm": 1.33198881149292,
      "learning_rate": 2.5708702063877515e-05,
      "loss": 0.1142,
      "step": 397700
    },
    {
      "epoch": 2.584039754457761,
      "grad_norm": 0.9930679202079773,
      "learning_rate": 2.5701791605451525e-05,
      "loss": 0.1147,
      "step": 397800
    },
    {
      "epoch": 2.5846893371009125,
      "grad_norm": 0.778472900390625,
      "learning_rate": 2.5694881147025534e-05,
      "loss": 0.1172,
      "step": 397900
    },
    {
      "epoch": 2.5853389197440646,
      "grad_norm": 1.4384979009628296,
      "learning_rate": 2.5687970688599544e-05,
      "loss": 0.1121,
      "step": 398000
    },
    {
      "epoch": 2.5859885023872162,
      "grad_norm": 1.0445924997329712,
      "learning_rate": 2.5681060230173547e-05,
      "loss": 0.1071,
      "step": 398100
    },
    {
      "epoch": 2.586638085030368,
      "grad_norm": 1.2049657106399536,
      "learning_rate": 2.5674149771747557e-05,
      "loss": 0.1153,
      "step": 398200
    },
    {
      "epoch": 2.58728766767352,
      "grad_norm": 1.0608346462249756,
      "learning_rate": 2.5667239313321567e-05,
      "loss": 0.1152,
      "step": 398300
    },
    {
      "epoch": 2.5879372503166715,
      "grad_norm": 1.11394202709198,
      "learning_rate": 2.5660328854895577e-05,
      "loss": 0.1118,
      "step": 398400
    },
    {
      "epoch": 2.588586832959823,
      "grad_norm": 0.8925577998161316,
      "learning_rate": 2.5653418396469587e-05,
      "loss": 0.1066,
      "step": 398500
    },
    {
      "epoch": 2.5892364156029752,
      "grad_norm": 1.1170467138290405,
      "learning_rate": 2.5646507938043597e-05,
      "loss": 0.1116,
      "step": 398600
    },
    {
      "epoch": 2.589885998246127,
      "grad_norm": 1.1521371603012085,
      "learning_rate": 2.5639597479617606e-05,
      "loss": 0.1148,
      "step": 398700
    },
    {
      "epoch": 2.5905355808892785,
      "grad_norm": 0.7815012335777283,
      "learning_rate": 2.5632687021191616e-05,
      "loss": 0.1157,
      "step": 398800
    },
    {
      "epoch": 2.5911851635324306,
      "grad_norm": 0.9168502688407898,
      "learning_rate": 2.5625776562765623e-05,
      "loss": 0.1132,
      "step": 398900
    },
    {
      "epoch": 2.591834746175582,
      "grad_norm": 1.1654348373413086,
      "learning_rate": 2.561886610433963e-05,
      "loss": 0.119,
      "step": 399000
    },
    {
      "epoch": 2.592484328818734,
      "grad_norm": 1.5313268899917603,
      "learning_rate": 2.561195564591364e-05,
      "loss": 0.1088,
      "step": 399100
    },
    {
      "epoch": 2.593133911461886,
      "grad_norm": 1.4137569665908813,
      "learning_rate": 2.560504518748765e-05,
      "loss": 0.1092,
      "step": 399200
    },
    {
      "epoch": 2.5937834941050375,
      "grad_norm": 1.613445520401001,
      "learning_rate": 2.5598134729061655e-05,
      "loss": 0.1138,
      "step": 399300
    },
    {
      "epoch": 2.594433076748189,
      "grad_norm": 1.1140098571777344,
      "learning_rate": 2.5591224270635665e-05,
      "loss": 0.1134,
      "step": 399400
    },
    {
      "epoch": 2.595082659391341,
      "grad_norm": 0.8370888829231262,
      "learning_rate": 2.5584313812209675e-05,
      "loss": 0.1076,
      "step": 399500
    },
    {
      "epoch": 2.595732242034493,
      "grad_norm": 1.4115716218948364,
      "learning_rate": 2.5577403353783685e-05,
      "loss": 0.1084,
      "step": 399600
    },
    {
      "epoch": 2.5963818246776444,
      "grad_norm": 1.3904211521148682,
      "learning_rate": 2.5570492895357695e-05,
      "loss": 0.1132,
      "step": 399700
    },
    {
      "epoch": 2.5970314073207965,
      "grad_norm": 0.9046330451965332,
      "learning_rate": 2.5563582436931705e-05,
      "loss": 0.1093,
      "step": 399800
    },
    {
      "epoch": 2.597680989963948,
      "grad_norm": 1.202103853225708,
      "learning_rate": 2.5556671978505714e-05,
      "loss": 0.1105,
      "step": 399900
    },
    {
      "epoch": 2.5983305726071,
      "grad_norm": 1.3789260387420654,
      "learning_rate": 2.5549761520079718e-05,
      "loss": 0.1117,
      "step": 400000
    },
    {
      "epoch": 2.598980155250252,
      "grad_norm": 1.5675171613693237,
      "learning_rate": 2.5542851061653727e-05,
      "loss": 0.1162,
      "step": 400100
    },
    {
      "epoch": 2.5996297378934035,
      "grad_norm": 0.7994105219841003,
      "learning_rate": 2.5535940603227737e-05,
      "loss": 0.1098,
      "step": 400200
    },
    {
      "epoch": 2.600279320536555,
      "grad_norm": 0.9683122038841248,
      "learning_rate": 2.5529030144801747e-05,
      "loss": 0.1129,
      "step": 400300
    },
    {
      "epoch": 2.600928903179707,
      "grad_norm": 1.3207615613937378,
      "learning_rate": 2.5522119686375757e-05,
      "loss": 0.1116,
      "step": 400400
    },
    {
      "epoch": 2.6015784858228588,
      "grad_norm": 1.3203521966934204,
      "learning_rate": 2.5515209227949767e-05,
      "loss": 0.1092,
      "step": 400500
    },
    {
      "epoch": 2.602228068466011,
      "grad_norm": 1.217944860458374,
      "learning_rate": 2.5508298769523777e-05,
      "loss": 0.1119,
      "step": 400600
    },
    {
      "epoch": 2.6028776511091625,
      "grad_norm": 1.014520525932312,
      "learning_rate": 2.5501388311097783e-05,
      "loss": 0.1114,
      "step": 400700
    },
    {
      "epoch": 2.603527233752314,
      "grad_norm": 1.0823744535446167,
      "learning_rate": 2.5494477852671793e-05,
      "loss": 0.1164,
      "step": 400800
    },
    {
      "epoch": 2.6041768163954657,
      "grad_norm": 1.3084193468093872,
      "learning_rate": 2.54875673942458e-05,
      "loss": 0.1156,
      "step": 400900
    },
    {
      "epoch": 2.604826399038618,
      "grad_norm": 0.6383494734764099,
      "learning_rate": 2.548065693581981e-05,
      "loss": 0.1104,
      "step": 401000
    },
    {
      "epoch": 2.6054759816817694,
      "grad_norm": 1.171027660369873,
      "learning_rate": 2.5473746477393816e-05,
      "loss": 0.1121,
      "step": 401100
    },
    {
      "epoch": 2.6061255643249215,
      "grad_norm": 1.2194842100143433,
      "learning_rate": 2.5466836018967826e-05,
      "loss": 0.1101,
      "step": 401200
    },
    {
      "epoch": 2.606775146968073,
      "grad_norm": 0.960480272769928,
      "learning_rate": 2.5459925560541835e-05,
      "loss": 0.1174,
      "step": 401300
    },
    {
      "epoch": 2.6074247296112247,
      "grad_norm": 0.9526215195655823,
      "learning_rate": 2.5453015102115845e-05,
      "loss": 0.109,
      "step": 401400
    },
    {
      "epoch": 2.6080743122543764,
      "grad_norm": 0.7828540802001953,
      "learning_rate": 2.5446104643689855e-05,
      "loss": 0.1133,
      "step": 401500
    },
    {
      "epoch": 2.6087238948975284,
      "grad_norm": 1.1050766706466675,
      "learning_rate": 2.5439194185263865e-05,
      "loss": 0.117,
      "step": 401600
    },
    {
      "epoch": 2.60937347754068,
      "grad_norm": 1.184949278831482,
      "learning_rate": 2.5432283726837875e-05,
      "loss": 0.1083,
      "step": 401700
    },
    {
      "epoch": 2.610023060183832,
      "grad_norm": 1.2012732028961182,
      "learning_rate": 2.5425373268411878e-05,
      "loss": 0.1065,
      "step": 401800
    },
    {
      "epoch": 2.6106726428269837,
      "grad_norm": 1.3679991960525513,
      "learning_rate": 2.5418462809985888e-05,
      "loss": 0.1085,
      "step": 401900
    },
    {
      "epoch": 2.6113222254701354,
      "grad_norm": 0.9016121029853821,
      "learning_rate": 2.5411552351559898e-05,
      "loss": 0.1097,
      "step": 402000
    },
    {
      "epoch": 2.611971808113287,
      "grad_norm": 1.0570406913757324,
      "learning_rate": 2.5404641893133908e-05,
      "loss": 0.1112,
      "step": 402100
    },
    {
      "epoch": 2.612621390756439,
      "grad_norm": 1.3754616975784302,
      "learning_rate": 2.5397731434707917e-05,
      "loss": 0.1102,
      "step": 402200
    },
    {
      "epoch": 2.6132709733995907,
      "grad_norm": 1.2638851404190063,
      "learning_rate": 2.5390820976281927e-05,
      "loss": 0.1138,
      "step": 402300
    },
    {
      "epoch": 2.6139205560427428,
      "grad_norm": 0.6894722580909729,
      "learning_rate": 2.5383910517855937e-05,
      "loss": 0.1132,
      "step": 402400
    },
    {
      "epoch": 2.6145701386858944,
      "grad_norm": 0.9544575214385986,
      "learning_rate": 2.5377000059429944e-05,
      "loss": 0.116,
      "step": 402500
    },
    {
      "epoch": 2.615219721329046,
      "grad_norm": 0.9047864079475403,
      "learning_rate": 2.5370089601003953e-05,
      "loss": 0.1131,
      "step": 402600
    },
    {
      "epoch": 2.6158693039721976,
      "grad_norm": 1.7982189655303955,
      "learning_rate": 2.5363179142577963e-05,
      "loss": 0.1243,
      "step": 402700
    },
    {
      "epoch": 2.6165188866153497,
      "grad_norm": 1.3834697008132935,
      "learning_rate": 2.535626868415197e-05,
      "loss": 0.1164,
      "step": 402800
    },
    {
      "epoch": 2.6171684692585013,
      "grad_norm": 1.2495863437652588,
      "learning_rate": 2.5349358225725976e-05,
      "loss": 0.1159,
      "step": 402900
    },
    {
      "epoch": 2.6178180519016534,
      "grad_norm": 0.9567090272903442,
      "learning_rate": 2.5342447767299986e-05,
      "loss": 0.1103,
      "step": 403000
    },
    {
      "epoch": 2.618467634544805,
      "grad_norm": 1.2198060750961304,
      "learning_rate": 2.5335537308873996e-05,
      "loss": 0.1098,
      "step": 403100
    },
    {
      "epoch": 2.6191172171879566,
      "grad_norm": 0.5922960638999939,
      "learning_rate": 2.5328626850448006e-05,
      "loss": 0.1189,
      "step": 403200
    },
    {
      "epoch": 2.6197667998311083,
      "grad_norm": 1.1236774921417236,
      "learning_rate": 2.5321716392022016e-05,
      "loss": 0.1071,
      "step": 403300
    },
    {
      "epoch": 2.6204163824742603,
      "grad_norm": 1.1381652355194092,
      "learning_rate": 2.5314805933596025e-05,
      "loss": 0.1112,
      "step": 403400
    },
    {
      "epoch": 2.621065965117412,
      "grad_norm": 0.7762017846107483,
      "learning_rate": 2.5307895475170035e-05,
      "loss": 0.1095,
      "step": 403500
    },
    {
      "epoch": 2.621715547760564,
      "grad_norm": 0.7147111892700195,
      "learning_rate": 2.5300985016744045e-05,
      "loss": 0.1142,
      "step": 403600
    },
    {
      "epoch": 2.6223651304037157,
      "grad_norm": 1.3955025672912598,
      "learning_rate": 2.5294074558318048e-05,
      "loss": 0.1113,
      "step": 403700
    },
    {
      "epoch": 2.6230147130468673,
      "grad_norm": 0.825387179851532,
      "learning_rate": 2.5287164099892058e-05,
      "loss": 0.1134,
      "step": 403800
    },
    {
      "epoch": 2.623664295690019,
      "grad_norm": 2.130380153656006,
      "learning_rate": 2.5280253641466068e-05,
      "loss": 0.1048,
      "step": 403900
    },
    {
      "epoch": 2.624313878333171,
      "grad_norm": 1.5790396928787231,
      "learning_rate": 2.5273343183040078e-05,
      "loss": 0.1128,
      "step": 404000
    },
    {
      "epoch": 2.6249634609763226,
      "grad_norm": 1.1310899257659912,
      "learning_rate": 2.5266432724614088e-05,
      "loss": 0.1065,
      "step": 404100
    },
    {
      "epoch": 2.6256130436194747,
      "grad_norm": 1.377185583114624,
      "learning_rate": 2.5259522266188097e-05,
      "loss": 0.1162,
      "step": 404200
    },
    {
      "epoch": 2.6262626262626263,
      "grad_norm": 0.7040194272994995,
      "learning_rate": 2.5252611807762104e-05,
      "loss": 0.1156,
      "step": 404300
    },
    {
      "epoch": 2.626912208905778,
      "grad_norm": 0.927398145198822,
      "learning_rate": 2.5245701349336114e-05,
      "loss": 0.1075,
      "step": 404400
    },
    {
      "epoch": 2.6275617915489295,
      "grad_norm": 0.9290422201156616,
      "learning_rate": 2.5238790890910124e-05,
      "loss": 0.1121,
      "step": 404500
    },
    {
      "epoch": 2.6282113741920816,
      "grad_norm": 1.3886500597000122,
      "learning_rate": 2.5231880432484133e-05,
      "loss": 0.1076,
      "step": 404600
    },
    {
      "epoch": 2.6288609568352332,
      "grad_norm": 0.9148399233818054,
      "learning_rate": 2.5224969974058137e-05,
      "loss": 0.1153,
      "step": 404700
    },
    {
      "epoch": 2.6295105394783853,
      "grad_norm": 0.9566217064857483,
      "learning_rate": 2.5218059515632146e-05,
      "loss": 0.1083,
      "step": 404800
    },
    {
      "epoch": 2.630160122121537,
      "grad_norm": 0.968126118183136,
      "learning_rate": 2.5211149057206156e-05,
      "loss": 0.1116,
      "step": 404900
    },
    {
      "epoch": 2.6308097047646886,
      "grad_norm": 1.4343928098678589,
      "learning_rate": 2.5204238598780166e-05,
      "loss": 0.111,
      "step": 405000
    },
    {
      "epoch": 2.63145928740784,
      "grad_norm": 1.342793583869934,
      "learning_rate": 2.5197328140354176e-05,
      "loss": 0.1142,
      "step": 405100
    },
    {
      "epoch": 2.6321088700509923,
      "grad_norm": 1.0524115562438965,
      "learning_rate": 2.5190417681928186e-05,
      "loss": 0.1141,
      "step": 405200
    },
    {
      "epoch": 2.632758452694144,
      "grad_norm": 1.4756407737731934,
      "learning_rate": 2.5183507223502196e-05,
      "loss": 0.1127,
      "step": 405300
    },
    {
      "epoch": 2.633408035337296,
      "grad_norm": 1.1411232948303223,
      "learning_rate": 2.5176596765076206e-05,
      "loss": 0.1126,
      "step": 405400
    },
    {
      "epoch": 2.6340576179804476,
      "grad_norm": 1.2991727590560913,
      "learning_rate": 2.5169686306650215e-05,
      "loss": 0.1162,
      "step": 405500
    },
    {
      "epoch": 2.634707200623599,
      "grad_norm": 1.0065556764602661,
      "learning_rate": 2.516277584822422e-05,
      "loss": 0.1072,
      "step": 405600
    },
    {
      "epoch": 2.6353567832667513,
      "grad_norm": 1.1111968755722046,
      "learning_rate": 2.5155865389798228e-05,
      "loss": 0.1116,
      "step": 405700
    },
    {
      "epoch": 2.636006365909903,
      "grad_norm": 0.9143781661987305,
      "learning_rate": 2.5148954931372238e-05,
      "loss": 0.1131,
      "step": 405800
    },
    {
      "epoch": 2.6366559485530545,
      "grad_norm": 1.4970486164093018,
      "learning_rate": 2.5142044472946248e-05,
      "loss": 0.1109,
      "step": 405900
    },
    {
      "epoch": 2.6373055311962066,
      "grad_norm": 0.9024962782859802,
      "learning_rate": 2.5135134014520258e-05,
      "loss": 0.1123,
      "step": 406000
    },
    {
      "epoch": 2.637955113839358,
      "grad_norm": 0.8456099033355713,
      "learning_rate": 2.5128223556094264e-05,
      "loss": 0.1122,
      "step": 406100
    },
    {
      "epoch": 2.63860469648251,
      "grad_norm": 1.9450541734695435,
      "learning_rate": 2.5121313097668274e-05,
      "loss": 0.1122,
      "step": 406200
    },
    {
      "epoch": 2.639254279125662,
      "grad_norm": 1.045675277709961,
      "learning_rate": 2.5114402639242284e-05,
      "loss": 0.1061,
      "step": 406300
    },
    {
      "epoch": 2.6399038617688135,
      "grad_norm": 0.9706648588180542,
      "learning_rate": 2.5107492180816294e-05,
      "loss": 0.1077,
      "step": 406400
    },
    {
      "epoch": 2.640553444411965,
      "grad_norm": 1.4152737855911255,
      "learning_rate": 2.5100581722390297e-05,
      "loss": 0.1096,
      "step": 406500
    },
    {
      "epoch": 2.6412030270551172,
      "grad_norm": 1.538179874420166,
      "learning_rate": 2.5093671263964307e-05,
      "loss": 0.1153,
      "step": 406600
    },
    {
      "epoch": 2.641852609698269,
      "grad_norm": 1.0955381393432617,
      "learning_rate": 2.5086760805538317e-05,
      "loss": 0.1136,
      "step": 406700
    },
    {
      "epoch": 2.6425021923414205,
      "grad_norm": 1.1448036432266235,
      "learning_rate": 2.5079850347112327e-05,
      "loss": 0.1068,
      "step": 406800
    },
    {
      "epoch": 2.6431517749845725,
      "grad_norm": 0.8286683559417725,
      "learning_rate": 2.5072939888686336e-05,
      "loss": 0.115,
      "step": 406900
    },
    {
      "epoch": 2.643801357627724,
      "grad_norm": 2.756753444671631,
      "learning_rate": 2.5066029430260346e-05,
      "loss": 0.1184,
      "step": 407000
    },
    {
      "epoch": 2.6444509402708762,
      "grad_norm": 0.9672265648841858,
      "learning_rate": 2.5059118971834356e-05,
      "loss": 0.1139,
      "step": 407100
    },
    {
      "epoch": 2.645100522914028,
      "grad_norm": 1.4603228569030762,
      "learning_rate": 2.5052208513408366e-05,
      "loss": 0.1075,
      "step": 407200
    },
    {
      "epoch": 2.6457501055571795,
      "grad_norm": 0.9231134057044983,
      "learning_rate": 2.5045298054982376e-05,
      "loss": 0.11,
      "step": 407300
    },
    {
      "epoch": 2.646399688200331,
      "grad_norm": 1.6024874448776245,
      "learning_rate": 2.5038387596556386e-05,
      "loss": 0.1076,
      "step": 407400
    },
    {
      "epoch": 2.647049270843483,
      "grad_norm": 1.203810691833496,
      "learning_rate": 2.503147713813039e-05,
      "loss": 0.1093,
      "step": 407500
    },
    {
      "epoch": 2.647698853486635,
      "grad_norm": 0.9142441749572754,
      "learning_rate": 2.50245666797044e-05,
      "loss": 0.1059,
      "step": 407600
    },
    {
      "epoch": 2.648348436129787,
      "grad_norm": 1.1066192388534546,
      "learning_rate": 2.501765622127841e-05,
      "loss": 0.111,
      "step": 407700
    },
    {
      "epoch": 2.6489980187729385,
      "grad_norm": 0.9179001450538635,
      "learning_rate": 2.5010745762852418e-05,
      "loss": 0.1068,
      "step": 407800
    },
    {
      "epoch": 2.64964760141609,
      "grad_norm": 0.8405540585517883,
      "learning_rate": 2.5003835304426425e-05,
      "loss": 0.1079,
      "step": 407900
    },
    {
      "epoch": 2.6502971840592418,
      "grad_norm": 0.6006361842155457,
      "learning_rate": 2.4996924846000435e-05,
      "loss": 0.1135,
      "step": 408000
    },
    {
      "epoch": 2.650946766702394,
      "grad_norm": 1.258873462677002,
      "learning_rate": 2.4990014387574444e-05,
      "loss": 0.1116,
      "step": 408100
    },
    {
      "epoch": 2.6515963493455454,
      "grad_norm": 0.9454973340034485,
      "learning_rate": 2.498310392914845e-05,
      "loss": 0.1113,
      "step": 408200
    },
    {
      "epoch": 2.6522459319886975,
      "grad_norm": 1.266786813735962,
      "learning_rate": 2.497619347072246e-05,
      "loss": 0.1077,
      "step": 408300
    },
    {
      "epoch": 2.652895514631849,
      "grad_norm": 1.1985394954681396,
      "learning_rate": 2.496928301229647e-05,
      "loss": 0.1123,
      "step": 408400
    },
    {
      "epoch": 2.6535450972750008,
      "grad_norm": 1.2001469135284424,
      "learning_rate": 2.496237255387048e-05,
      "loss": 0.1062,
      "step": 408500
    },
    {
      "epoch": 2.6541946799181524,
      "grad_norm": 1.0399510860443115,
      "learning_rate": 2.495546209544449e-05,
      "loss": 0.1104,
      "step": 408600
    },
    {
      "epoch": 2.6548442625613045,
      "grad_norm": 1.0702030658721924,
      "learning_rate": 2.4948551637018497e-05,
      "loss": 0.1114,
      "step": 408700
    },
    {
      "epoch": 2.655493845204456,
      "grad_norm": 1.0055359601974487,
      "learning_rate": 2.4941641178592507e-05,
      "loss": 0.1142,
      "step": 408800
    },
    {
      "epoch": 2.656143427847608,
      "grad_norm": 0.9112078547477722,
      "learning_rate": 2.4934730720166516e-05,
      "loss": 0.1111,
      "step": 408900
    },
    {
      "epoch": 2.6567930104907598,
      "grad_norm": 1.3859416246414185,
      "learning_rate": 2.4927820261740526e-05,
      "loss": 0.1107,
      "step": 409000
    },
    {
      "epoch": 2.6574425931339114,
      "grad_norm": 0.8809206485748291,
      "learning_rate": 2.4920909803314533e-05,
      "loss": 0.1088,
      "step": 409100
    },
    {
      "epoch": 2.658092175777063,
      "grad_norm": 0.868474543094635,
      "learning_rate": 2.4913999344888543e-05,
      "loss": 0.1076,
      "step": 409200
    },
    {
      "epoch": 2.658741758420215,
      "grad_norm": 1.3306773900985718,
      "learning_rate": 2.4907088886462552e-05,
      "loss": 0.1063,
      "step": 409300
    },
    {
      "epoch": 2.6593913410633667,
      "grad_norm": 1.426824927330017,
      "learning_rate": 2.4900178428036562e-05,
      "loss": 0.1184,
      "step": 409400
    },
    {
      "epoch": 2.660040923706519,
      "grad_norm": 1.1426031589508057,
      "learning_rate": 2.489326796961057e-05,
      "loss": 0.1156,
      "step": 409500
    },
    {
      "epoch": 2.6606905063496704,
      "grad_norm": 1.3053723573684692,
      "learning_rate": 2.488635751118458e-05,
      "loss": 0.1141,
      "step": 409600
    },
    {
      "epoch": 2.661340088992822,
      "grad_norm": 1.422782063484192,
      "learning_rate": 2.4879447052758585e-05,
      "loss": 0.111,
      "step": 409700
    },
    {
      "epoch": 2.6619896716359737,
      "grad_norm": 1.560987949371338,
      "learning_rate": 2.4872536594332595e-05,
      "loss": 0.1167,
      "step": 409800
    },
    {
      "epoch": 2.6626392542791257,
      "grad_norm": 1.0405280590057373,
      "learning_rate": 2.4865626135906605e-05,
      "loss": 0.1195,
      "step": 409900
    },
    {
      "epoch": 2.6632888369222774,
      "grad_norm": 1.742547631263733,
      "learning_rate": 2.4858715677480615e-05,
      "loss": 0.1183,
      "step": 410000
    },
    {
      "epoch": 2.6639384195654294,
      "grad_norm": 1.3155584335327148,
      "learning_rate": 2.485180521905462e-05,
      "loss": 0.1077,
      "step": 410100
    },
    {
      "epoch": 2.664588002208581,
      "grad_norm": 1.4419312477111816,
      "learning_rate": 2.484489476062863e-05,
      "loss": 0.1121,
      "step": 410200
    },
    {
      "epoch": 2.6652375848517327,
      "grad_norm": 1.1549502611160278,
      "learning_rate": 2.483798430220264e-05,
      "loss": 0.1117,
      "step": 410300
    },
    {
      "epoch": 2.6658871674948843,
      "grad_norm": 1.1613051891326904,
      "learning_rate": 2.483107384377665e-05,
      "loss": 0.1106,
      "step": 410400
    },
    {
      "epoch": 2.6665367501380364,
      "grad_norm": 1.4336977005004883,
      "learning_rate": 2.4824163385350657e-05,
      "loss": 0.1147,
      "step": 410500
    },
    {
      "epoch": 2.667186332781188,
      "grad_norm": 1.6426827907562256,
      "learning_rate": 2.4817252926924667e-05,
      "loss": 0.1102,
      "step": 410600
    },
    {
      "epoch": 2.66783591542434,
      "grad_norm": 1.0262922048568726,
      "learning_rate": 2.4810342468498677e-05,
      "loss": 0.1096,
      "step": 410700
    },
    {
      "epoch": 2.6684854980674917,
      "grad_norm": 1.3909560441970825,
      "learning_rate": 2.4803432010072687e-05,
      "loss": 0.109,
      "step": 410800
    },
    {
      "epoch": 2.6691350807106433,
      "grad_norm": 1.1275615692138672,
      "learning_rate": 2.4796521551646697e-05,
      "loss": 0.1104,
      "step": 410900
    },
    {
      "epoch": 2.669784663353795,
      "grad_norm": 1.0689747333526611,
      "learning_rate": 2.4789611093220703e-05,
      "loss": 0.1126,
      "step": 411000
    },
    {
      "epoch": 2.670434245996947,
      "grad_norm": 0.9233922362327576,
      "learning_rate": 2.4782700634794713e-05,
      "loss": 0.1089,
      "step": 411100
    },
    {
      "epoch": 2.6710838286400986,
      "grad_norm": 1.1566845178604126,
      "learning_rate": 2.4775790176368723e-05,
      "loss": 0.1067,
      "step": 411200
    },
    {
      "epoch": 2.6717334112832507,
      "grad_norm": 1.1053160429000854,
      "learning_rate": 2.476887971794273e-05,
      "loss": 0.1072,
      "step": 411300
    },
    {
      "epoch": 2.6723829939264023,
      "grad_norm": 1.1605104207992554,
      "learning_rate": 2.476196925951674e-05,
      "loss": 0.1008,
      "step": 411400
    },
    {
      "epoch": 2.673032576569554,
      "grad_norm": 1.33443284034729,
      "learning_rate": 2.4755058801090746e-05,
      "loss": 0.1069,
      "step": 411500
    },
    {
      "epoch": 2.6736821592127056,
      "grad_norm": 1.4031951427459717,
      "learning_rate": 2.4748148342664755e-05,
      "loss": 0.1093,
      "step": 411600
    },
    {
      "epoch": 2.6743317418558576,
      "grad_norm": 0.7150386571884155,
      "learning_rate": 2.4741237884238765e-05,
      "loss": 0.1092,
      "step": 411700
    },
    {
      "epoch": 2.6749813244990093,
      "grad_norm": 1.2415330410003662,
      "learning_rate": 2.4734327425812775e-05,
      "loss": 0.1011,
      "step": 411800
    },
    {
      "epoch": 2.6756309071421613,
      "grad_norm": 1.199399709701538,
      "learning_rate": 2.4727416967386785e-05,
      "loss": 0.1115,
      "step": 411900
    },
    {
      "epoch": 2.676280489785313,
      "grad_norm": 1.1834986209869385,
      "learning_rate": 2.472050650896079e-05,
      "loss": 0.111,
      "step": 412000
    },
    {
      "epoch": 2.6769300724284646,
      "grad_norm": 1.0404572486877441,
      "learning_rate": 2.47135960505348e-05,
      "loss": 0.1228,
      "step": 412100
    },
    {
      "epoch": 2.677579655071616,
      "grad_norm": 1.557690143585205,
      "learning_rate": 2.470668559210881e-05,
      "loss": 0.1046,
      "step": 412200
    },
    {
      "epoch": 2.6782292377147683,
      "grad_norm": 1.3141485452651978,
      "learning_rate": 2.469977513368282e-05,
      "loss": 0.1127,
      "step": 412300
    },
    {
      "epoch": 2.67887882035792,
      "grad_norm": 1.329162359237671,
      "learning_rate": 2.4692864675256827e-05,
      "loss": 0.1087,
      "step": 412400
    },
    {
      "epoch": 2.679528403001072,
      "grad_norm": 0.9680566191673279,
      "learning_rate": 2.4685954216830837e-05,
      "loss": 0.1087,
      "step": 412500
    },
    {
      "epoch": 2.6801779856442236,
      "grad_norm": 0.8499649167060852,
      "learning_rate": 2.4679043758404847e-05,
      "loss": 0.1055,
      "step": 412600
    },
    {
      "epoch": 2.6808275682873752,
      "grad_norm": 2.512554168701172,
      "learning_rate": 2.4672133299978857e-05,
      "loss": 0.106,
      "step": 412700
    },
    {
      "epoch": 2.6814771509305273,
      "grad_norm": 1.169207215309143,
      "learning_rate": 2.4665222841552867e-05,
      "loss": 0.1055,
      "step": 412800
    },
    {
      "epoch": 2.682126733573679,
      "grad_norm": 1.24014151096344,
      "learning_rate": 2.4658312383126873e-05,
      "loss": 0.1156,
      "step": 412900
    },
    {
      "epoch": 2.6827763162168305,
      "grad_norm": 1.8997634649276733,
      "learning_rate": 2.4651401924700883e-05,
      "loss": 0.1106,
      "step": 413000
    },
    {
      "epoch": 2.6834258988599826,
      "grad_norm": 1.4919817447662354,
      "learning_rate": 2.464449146627489e-05,
      "loss": 0.1086,
      "step": 413100
    },
    {
      "epoch": 2.6840754815031342,
      "grad_norm": 1.3309277296066284,
      "learning_rate": 2.46375810078489e-05,
      "loss": 0.113,
      "step": 413200
    },
    {
      "epoch": 2.684725064146286,
      "grad_norm": 1.612267255783081,
      "learning_rate": 2.463067054942291e-05,
      "loss": 0.1122,
      "step": 413300
    },
    {
      "epoch": 2.685374646789438,
      "grad_norm": 0.9653304219245911,
      "learning_rate": 2.4623760090996916e-05,
      "loss": 0.1071,
      "step": 413400
    },
    {
      "epoch": 2.6860242294325896,
      "grad_norm": 1.1757171154022217,
      "learning_rate": 2.4616849632570926e-05,
      "loss": 0.1053,
      "step": 413500
    },
    {
      "epoch": 2.686673812075741,
      "grad_norm": 1.3249223232269287,
      "learning_rate": 2.4609939174144935e-05,
      "loss": 0.113,
      "step": 413600
    },
    {
      "epoch": 2.6873233947188933,
      "grad_norm": 1.0988446474075317,
      "learning_rate": 2.4603028715718945e-05,
      "loss": 0.1034,
      "step": 413700
    },
    {
      "epoch": 2.687972977362045,
      "grad_norm": 1.1780331134796143,
      "learning_rate": 2.4596118257292952e-05,
      "loss": 0.1089,
      "step": 413800
    },
    {
      "epoch": 2.6886225600051965,
      "grad_norm": 1.7202774286270142,
      "learning_rate": 2.458920779886696e-05,
      "loss": 0.113,
      "step": 413900
    },
    {
      "epoch": 2.6892721426483486,
      "grad_norm": 0.9985059499740601,
      "learning_rate": 2.458229734044097e-05,
      "loss": 0.1113,
      "step": 414000
    },
    {
      "epoch": 2.6899217252915,
      "grad_norm": 0.8524831533432007,
      "learning_rate": 2.457538688201498e-05,
      "loss": 0.1071,
      "step": 414100
    },
    {
      "epoch": 2.690571307934652,
      "grad_norm": 1.3370143175125122,
      "learning_rate": 2.456847642358899e-05,
      "loss": 0.1067,
      "step": 414200
    },
    {
      "epoch": 2.691220890577804,
      "grad_norm": 1.3707424402236938,
      "learning_rate": 2.4561565965162998e-05,
      "loss": 0.1085,
      "step": 414300
    },
    {
      "epoch": 2.6918704732209555,
      "grad_norm": 0.8933839797973633,
      "learning_rate": 2.4554655506737007e-05,
      "loss": 0.1059,
      "step": 414400
    },
    {
      "epoch": 2.692520055864107,
      "grad_norm": 1.3292040824890137,
      "learning_rate": 2.4547745048311017e-05,
      "loss": 0.1056,
      "step": 414500
    },
    {
      "epoch": 2.693169638507259,
      "grad_norm": 1.037210464477539,
      "learning_rate": 2.4540834589885027e-05,
      "loss": 0.1036,
      "step": 414600
    },
    {
      "epoch": 2.693819221150411,
      "grad_norm": 0.876973032951355,
      "learning_rate": 2.4533924131459034e-05,
      "loss": 0.1095,
      "step": 414700
    },
    {
      "epoch": 2.694468803793563,
      "grad_norm": 0.6660336256027222,
      "learning_rate": 2.4527013673033044e-05,
      "loss": 0.1096,
      "step": 414800
    },
    {
      "epoch": 2.6951183864367145,
      "grad_norm": 1.4687254428863525,
      "learning_rate": 2.452010321460705e-05,
      "loss": 0.1092,
      "step": 414900
    },
    {
      "epoch": 2.695767969079866,
      "grad_norm": 1.888148307800293,
      "learning_rate": 2.451319275618106e-05,
      "loss": 0.1168,
      "step": 415000
    },
    {
      "epoch": 2.696417551723018,
      "grad_norm": 0.8447999954223633,
      "learning_rate": 2.450628229775507e-05,
      "loss": 0.1093,
      "step": 415100
    },
    {
      "epoch": 2.69706713436617,
      "grad_norm": 1.1212846040725708,
      "learning_rate": 2.4499371839329076e-05,
      "loss": 0.1146,
      "step": 415200
    },
    {
      "epoch": 2.6977167170093215,
      "grad_norm": 1.091614007949829,
      "learning_rate": 2.4492461380903086e-05,
      "loss": 0.1037,
      "step": 415300
    },
    {
      "epoch": 2.6983662996524735,
      "grad_norm": 1.7360981702804565,
      "learning_rate": 2.4485550922477096e-05,
      "loss": 0.1088,
      "step": 415400
    },
    {
      "epoch": 2.699015882295625,
      "grad_norm": 1.2733325958251953,
      "learning_rate": 2.4478640464051106e-05,
      "loss": 0.112,
      "step": 415500
    },
    {
      "epoch": 2.699665464938777,
      "grad_norm": 0.7300309538841248,
      "learning_rate": 2.4471730005625116e-05,
      "loss": 0.1089,
      "step": 415600
    },
    {
      "epoch": 2.7003150475819284,
      "grad_norm": 1.162940263748169,
      "learning_rate": 2.4464819547199122e-05,
      "loss": 0.1082,
      "step": 415700
    },
    {
      "epoch": 2.7009646302250805,
      "grad_norm": 1.3135113716125488,
      "learning_rate": 2.4457909088773132e-05,
      "loss": 0.1097,
      "step": 415800
    },
    {
      "epoch": 2.701614212868232,
      "grad_norm": 1.147647738456726,
      "learning_rate": 2.4450998630347142e-05,
      "loss": 0.1161,
      "step": 415900
    },
    {
      "epoch": 2.702263795511384,
      "grad_norm": 1.1805521249771118,
      "learning_rate": 2.444408817192115e-05,
      "loss": 0.1219,
      "step": 416000
    },
    {
      "epoch": 2.702913378154536,
      "grad_norm": 1.2778947353363037,
      "learning_rate": 2.443717771349516e-05,
      "loss": 0.1125,
      "step": 416100
    },
    {
      "epoch": 2.7035629607976874,
      "grad_norm": 1.1683590412139893,
      "learning_rate": 2.4430267255069168e-05,
      "loss": 0.1052,
      "step": 416200
    },
    {
      "epoch": 2.704212543440839,
      "grad_norm": 0.8073164820671082,
      "learning_rate": 2.4423356796643178e-05,
      "loss": 0.1039,
      "step": 416300
    },
    {
      "epoch": 2.704862126083991,
      "grad_norm": 1.408050298690796,
      "learning_rate": 2.4416446338217188e-05,
      "loss": 0.1113,
      "step": 416400
    },
    {
      "epoch": 2.7055117087271428,
      "grad_norm": 0.7735266089439392,
      "learning_rate": 2.4409535879791194e-05,
      "loss": 0.1063,
      "step": 416500
    },
    {
      "epoch": 2.706161291370295,
      "grad_norm": 1.7302112579345703,
      "learning_rate": 2.4402625421365204e-05,
      "loss": 0.1124,
      "step": 416600
    },
    {
      "epoch": 2.7068108740134464,
      "grad_norm": 1.0906646251678467,
      "learning_rate": 2.439571496293921e-05,
      "loss": 0.1085,
      "step": 416700
    },
    {
      "epoch": 2.707460456656598,
      "grad_norm": 1.0250861644744873,
      "learning_rate": 2.438880450451322e-05,
      "loss": 0.104,
      "step": 416800
    },
    {
      "epoch": 2.7081100392997497,
      "grad_norm": 0.6231412887573242,
      "learning_rate": 2.438189404608723e-05,
      "loss": 0.1052,
      "step": 416900
    },
    {
      "epoch": 2.7087596219429018,
      "grad_norm": 0.9925655722618103,
      "learning_rate": 2.437498358766124e-05,
      "loss": 0.1138,
      "step": 417000
    },
    {
      "epoch": 2.7094092045860534,
      "grad_norm": 1.130719780921936,
      "learning_rate": 2.4368073129235246e-05,
      "loss": 0.111,
      "step": 417100
    },
    {
      "epoch": 2.7100587872292055,
      "grad_norm": 0.652058482170105,
      "learning_rate": 2.4361162670809256e-05,
      "loss": 0.1096,
      "step": 417200
    },
    {
      "epoch": 2.710708369872357,
      "grad_norm": 1.7143548727035522,
      "learning_rate": 2.4354252212383266e-05,
      "loss": 0.1031,
      "step": 417300
    },
    {
      "epoch": 2.7113579525155087,
      "grad_norm": 1.3745992183685303,
      "learning_rate": 2.4347341753957276e-05,
      "loss": 0.108,
      "step": 417400
    },
    {
      "epoch": 2.7120075351586603,
      "grad_norm": 1.0036677122116089,
      "learning_rate": 2.4340431295531286e-05,
      "loss": 0.0995,
      "step": 417500
    },
    {
      "epoch": 2.7126571178018124,
      "grad_norm": 1.6902134418487549,
      "learning_rate": 2.4333520837105292e-05,
      "loss": 0.1097,
      "step": 417600
    },
    {
      "epoch": 2.713306700444964,
      "grad_norm": 1.180303931236267,
      "learning_rate": 2.4326610378679302e-05,
      "loss": 0.1138,
      "step": 417700
    },
    {
      "epoch": 2.713956283088116,
      "grad_norm": 1.2910822629928589,
      "learning_rate": 2.4319699920253312e-05,
      "loss": 0.1101,
      "step": 417800
    },
    {
      "epoch": 2.7146058657312677,
      "grad_norm": 1.0579571723937988,
      "learning_rate": 2.4312789461827322e-05,
      "loss": 0.1073,
      "step": 417900
    },
    {
      "epoch": 2.7152554483744193,
      "grad_norm": 1.3573979139328003,
      "learning_rate": 2.430587900340133e-05,
      "loss": 0.1103,
      "step": 418000
    },
    {
      "epoch": 2.715905031017571,
      "grad_norm": 1.5745129585266113,
      "learning_rate": 2.4298968544975338e-05,
      "loss": 0.1094,
      "step": 418100
    },
    {
      "epoch": 2.716554613660723,
      "grad_norm": 0.8150279521942139,
      "learning_rate": 2.4292058086549348e-05,
      "loss": 0.1098,
      "step": 418200
    },
    {
      "epoch": 2.7172041963038747,
      "grad_norm": 1.1575520038604736,
      "learning_rate": 2.4285147628123354e-05,
      "loss": 0.1068,
      "step": 418300
    },
    {
      "epoch": 2.7178537789470267,
      "grad_norm": 1.2817116975784302,
      "learning_rate": 2.4278237169697364e-05,
      "loss": 0.111,
      "step": 418400
    },
    {
      "epoch": 2.7185033615901784,
      "grad_norm": 1.254286766052246,
      "learning_rate": 2.427132671127137e-05,
      "loss": 0.1095,
      "step": 418500
    },
    {
      "epoch": 2.71915294423333,
      "grad_norm": 0.8592067956924438,
      "learning_rate": 2.426441625284538e-05,
      "loss": 0.1024,
      "step": 418600
    },
    {
      "epoch": 2.7198025268764816,
      "grad_norm": 0.8249800205230713,
      "learning_rate": 2.425750579441939e-05,
      "loss": 0.1049,
      "step": 418700
    },
    {
      "epoch": 2.7204521095196337,
      "grad_norm": 1.456932783126831,
      "learning_rate": 2.42505953359934e-05,
      "loss": 0.1081,
      "step": 418800
    },
    {
      "epoch": 2.7211016921627853,
      "grad_norm": 1.3305631875991821,
      "learning_rate": 2.424368487756741e-05,
      "loss": 0.1087,
      "step": 418900
    },
    {
      "epoch": 2.7217512748059374,
      "grad_norm": 1.3613158464431763,
      "learning_rate": 2.4236774419141417e-05,
      "loss": 0.1069,
      "step": 419000
    },
    {
      "epoch": 2.722400857449089,
      "grad_norm": 1.0352305173873901,
      "learning_rate": 2.4229863960715426e-05,
      "loss": 0.1137,
      "step": 419100
    },
    {
      "epoch": 2.7230504400922406,
      "grad_norm": 0.8979203701019287,
      "learning_rate": 2.4222953502289436e-05,
      "loss": 0.108,
      "step": 419200
    },
    {
      "epoch": 2.7237000227353922,
      "grad_norm": 1.1387492418289185,
      "learning_rate": 2.4216043043863446e-05,
      "loss": 0.11,
      "step": 419300
    },
    {
      "epoch": 2.7243496053785443,
      "grad_norm": 1.4511287212371826,
      "learning_rate": 2.4209132585437456e-05,
      "loss": 0.1138,
      "step": 419400
    },
    {
      "epoch": 2.724999188021696,
      "grad_norm": 1.0218687057495117,
      "learning_rate": 2.4202222127011463e-05,
      "loss": 0.1106,
      "step": 419500
    },
    {
      "epoch": 2.725648770664848,
      "grad_norm": 1.2939919233322144,
      "learning_rate": 2.4195311668585472e-05,
      "loss": 0.1127,
      "step": 419600
    },
    {
      "epoch": 2.7262983533079996,
      "grad_norm": 1.2995810508728027,
      "learning_rate": 2.4188401210159482e-05,
      "loss": 0.1062,
      "step": 419700
    },
    {
      "epoch": 2.7269479359511513,
      "grad_norm": 1.1384836435317993,
      "learning_rate": 2.4181490751733492e-05,
      "loss": 0.1038,
      "step": 419800
    },
    {
      "epoch": 2.727597518594303,
      "grad_norm": 1.7638447284698486,
      "learning_rate": 2.41745802933075e-05,
      "loss": 0.1089,
      "step": 419900
    },
    {
      "epoch": 2.728247101237455,
      "grad_norm": 1.0770697593688965,
      "learning_rate": 2.416766983488151e-05,
      "loss": 0.1,
      "step": 420000
    },
    {
      "epoch": 2.7288966838806066,
      "grad_norm": 1.2529911994934082,
      "learning_rate": 2.4160759376455515e-05,
      "loss": 0.1078,
      "step": 420100
    },
    {
      "epoch": 2.7295462665237586,
      "grad_norm": 0.978635847568512,
      "learning_rate": 2.4153848918029525e-05,
      "loss": 0.1086,
      "step": 420200
    },
    {
      "epoch": 2.7301958491669103,
      "grad_norm": 0.8285915851593018,
      "learning_rate": 2.4146938459603535e-05,
      "loss": 0.1103,
      "step": 420300
    },
    {
      "epoch": 2.730845431810062,
      "grad_norm": 1.0987168550491333,
      "learning_rate": 2.414002800117754e-05,
      "loss": 0.1098,
      "step": 420400
    },
    {
      "epoch": 2.731495014453214,
      "grad_norm": 1.0012757778167725,
      "learning_rate": 2.413311754275155e-05,
      "loss": 0.114,
      "step": 420500
    },
    {
      "epoch": 2.7321445970963656,
      "grad_norm": 0.6187496781349182,
      "learning_rate": 2.412620708432556e-05,
      "loss": 0.1039,
      "step": 420600
    },
    {
      "epoch": 2.732794179739517,
      "grad_norm": 1.2037400007247925,
      "learning_rate": 2.411929662589957e-05,
      "loss": 0.1059,
      "step": 420700
    },
    {
      "epoch": 2.7334437623826693,
      "grad_norm": 1.3603100776672363,
      "learning_rate": 2.411238616747358e-05,
      "loss": 0.1065,
      "step": 420800
    },
    {
      "epoch": 2.734093345025821,
      "grad_norm": 1.5153523683547974,
      "learning_rate": 2.4105475709047587e-05,
      "loss": 0.1076,
      "step": 420900
    },
    {
      "epoch": 2.7347429276689725,
      "grad_norm": 1.0506830215454102,
      "learning_rate": 2.4098565250621597e-05,
      "loss": 0.1104,
      "step": 421000
    },
    {
      "epoch": 2.7353925103121246,
      "grad_norm": 1.0936765670776367,
      "learning_rate": 2.4091654792195607e-05,
      "loss": 0.1141,
      "step": 421100
    },
    {
      "epoch": 2.7360420929552762,
      "grad_norm": 1.335424780845642,
      "learning_rate": 2.4084744333769616e-05,
      "loss": 0.109,
      "step": 421200
    },
    {
      "epoch": 2.736691675598428,
      "grad_norm": 0.9860469698905945,
      "learning_rate": 2.4077833875343626e-05,
      "loss": 0.1111,
      "step": 421300
    },
    {
      "epoch": 2.73734125824158,
      "grad_norm": 0.8171685338020325,
      "learning_rate": 2.4070923416917633e-05,
      "loss": 0.1089,
      "step": 421400
    },
    {
      "epoch": 2.7379908408847315,
      "grad_norm": 1.9211468696594238,
      "learning_rate": 2.4064012958491643e-05,
      "loss": 0.1117,
      "step": 421500
    },
    {
      "epoch": 2.738640423527883,
      "grad_norm": 0.9871895909309387,
      "learning_rate": 2.4057102500065652e-05,
      "loss": 0.1055,
      "step": 421600
    },
    {
      "epoch": 2.7392900061710352,
      "grad_norm": 1.0721659660339355,
      "learning_rate": 2.405019204163966e-05,
      "loss": 0.1014,
      "step": 421700
    },
    {
      "epoch": 2.739939588814187,
      "grad_norm": 0.9808206558227539,
      "learning_rate": 2.404328158321367e-05,
      "loss": 0.1054,
      "step": 421800
    },
    {
      "epoch": 2.740589171457339,
      "grad_norm": 1.6077284812927246,
      "learning_rate": 2.4036371124787675e-05,
      "loss": 0.1071,
      "step": 421900
    },
    {
      "epoch": 2.7412387541004906,
      "grad_norm": 1.1862276792526245,
      "learning_rate": 2.4029460666361685e-05,
      "loss": 0.1082,
      "step": 422000
    },
    {
      "epoch": 2.741888336743642,
      "grad_norm": 1.613800287246704,
      "learning_rate": 2.4022550207935695e-05,
      "loss": 0.1079,
      "step": 422100
    },
    {
      "epoch": 2.742537919386794,
      "grad_norm": 1.2838693857192993,
      "learning_rate": 2.4015639749509705e-05,
      "loss": 0.105,
      "step": 422200
    },
    {
      "epoch": 2.743187502029946,
      "grad_norm": 1.4505094289779663,
      "learning_rate": 2.400872929108371e-05,
      "loss": 0.1141,
      "step": 422300
    },
    {
      "epoch": 2.7438370846730975,
      "grad_norm": 1.4932141304016113,
      "learning_rate": 2.400181883265772e-05,
      "loss": 0.1067,
      "step": 422400
    },
    {
      "epoch": 2.7444866673162496,
      "grad_norm": 1.0636839866638184,
      "learning_rate": 2.399490837423173e-05,
      "loss": 0.1082,
      "step": 422500
    },
    {
      "epoch": 2.745136249959401,
      "grad_norm": 1.684234380722046,
      "learning_rate": 2.398799791580574e-05,
      "loss": 0.1069,
      "step": 422600
    },
    {
      "epoch": 2.745785832602553,
      "grad_norm": 1.5840202569961548,
      "learning_rate": 2.398108745737975e-05,
      "loss": 0.1096,
      "step": 422700
    },
    {
      "epoch": 2.7464354152457044,
      "grad_norm": 1.4063116312026978,
      "learning_rate": 2.3974176998953757e-05,
      "loss": 0.1085,
      "step": 422800
    },
    {
      "epoch": 2.7470849978888565,
      "grad_norm": 1.2560770511627197,
      "learning_rate": 2.3967266540527767e-05,
      "loss": 0.1093,
      "step": 422900
    },
    {
      "epoch": 2.747734580532008,
      "grad_norm": 1.263056993484497,
      "learning_rate": 2.3960356082101777e-05,
      "loss": 0.1081,
      "step": 423000
    },
    {
      "epoch": 2.74838416317516,
      "grad_norm": 1.150783658027649,
      "learning_rate": 2.3953445623675787e-05,
      "loss": 0.1103,
      "step": 423100
    },
    {
      "epoch": 2.749033745818312,
      "grad_norm": 1.0454607009887695,
      "learning_rate": 2.3946535165249793e-05,
      "loss": 0.1079,
      "step": 423200
    },
    {
      "epoch": 2.7496833284614635,
      "grad_norm": 0.8913592100143433,
      "learning_rate": 2.3939624706823803e-05,
      "loss": 0.1087,
      "step": 423300
    },
    {
      "epoch": 2.750332911104615,
      "grad_norm": 1.019596815109253,
      "learning_rate": 2.3932714248397813e-05,
      "loss": 0.1109,
      "step": 423400
    },
    {
      "epoch": 2.750982493747767,
      "grad_norm": 1.1283299922943115,
      "learning_rate": 2.392580378997182e-05,
      "loss": 0.1071,
      "step": 423500
    },
    {
      "epoch": 2.751632076390919,
      "grad_norm": 0.822270393371582,
      "learning_rate": 2.391889333154583e-05,
      "loss": 0.1085,
      "step": 423600
    },
    {
      "epoch": 2.752281659034071,
      "grad_norm": 1.302294373512268,
      "learning_rate": 2.3911982873119836e-05,
      "loss": 0.1122,
      "step": 423700
    },
    {
      "epoch": 2.7529312416772225,
      "grad_norm": 1.1689692735671997,
      "learning_rate": 2.3905072414693845e-05,
      "loss": 0.1136,
      "step": 423800
    },
    {
      "epoch": 2.753580824320374,
      "grad_norm": 1.4934262037277222,
      "learning_rate": 2.3898161956267855e-05,
      "loss": 0.1035,
      "step": 423900
    },
    {
      "epoch": 2.7542304069635257,
      "grad_norm": 1.3266745805740356,
      "learning_rate": 2.3891251497841865e-05,
      "loss": 0.1128,
      "step": 424000
    },
    {
      "epoch": 2.754879989606678,
      "grad_norm": 0.6485727429389954,
      "learning_rate": 2.3884341039415875e-05,
      "loss": 0.1068,
      "step": 424100
    },
    {
      "epoch": 2.7555295722498294,
      "grad_norm": 1.6934123039245605,
      "learning_rate": 2.387743058098988e-05,
      "loss": 0.1072,
      "step": 424200
    },
    {
      "epoch": 2.7561791548929815,
      "grad_norm": 0.9721608757972717,
      "learning_rate": 2.387052012256389e-05,
      "loss": 0.1075,
      "step": 424300
    },
    {
      "epoch": 2.756828737536133,
      "grad_norm": 1.0604915618896484,
      "learning_rate": 2.38636096641379e-05,
      "loss": 0.1063,
      "step": 424400
    },
    {
      "epoch": 2.7574783201792847,
      "grad_norm": 1.1298481225967407,
      "learning_rate": 2.385669920571191e-05,
      "loss": 0.1055,
      "step": 424500
    },
    {
      "epoch": 2.7581279028224364,
      "grad_norm": 0.9999634623527527,
      "learning_rate": 2.384978874728592e-05,
      "loss": 0.1129,
      "step": 424600
    },
    {
      "epoch": 2.7587774854655884,
      "grad_norm": 1.0860124826431274,
      "learning_rate": 2.3842878288859927e-05,
      "loss": 0.1056,
      "step": 424700
    },
    {
      "epoch": 2.75942706810874,
      "grad_norm": 0.8976784944534302,
      "learning_rate": 2.3835967830433937e-05,
      "loss": 0.1128,
      "step": 424800
    },
    {
      "epoch": 2.760076650751892,
      "grad_norm": 1.3685351610183716,
      "learning_rate": 2.3829057372007947e-05,
      "loss": 0.1126,
      "step": 424900
    },
    {
      "epoch": 2.7607262333950437,
      "grad_norm": 1.360433578491211,
      "learning_rate": 2.3822146913581957e-05,
      "loss": 0.1123,
      "step": 425000
    },
    {
      "epoch": 2.7613758160381954,
      "grad_norm": 0.9455605149269104,
      "learning_rate": 2.3815236455155963e-05,
      "loss": 0.1123,
      "step": 425100
    },
    {
      "epoch": 2.762025398681347,
      "grad_norm": 1.2284972667694092,
      "learning_rate": 2.380832599672997e-05,
      "loss": 0.1055,
      "step": 425200
    },
    {
      "epoch": 2.762674981324499,
      "grad_norm": 1.1287555694580078,
      "learning_rate": 2.380141553830398e-05,
      "loss": 0.1128,
      "step": 425300
    },
    {
      "epoch": 2.7633245639676507,
      "grad_norm": 0.9841275811195374,
      "learning_rate": 2.379450507987799e-05,
      "loss": 0.1103,
      "step": 425400
    },
    {
      "epoch": 2.7639741466108028,
      "grad_norm": 1.4969087839126587,
      "learning_rate": 2.3787594621452e-05,
      "loss": 0.1108,
      "step": 425500
    },
    {
      "epoch": 2.7646237292539544,
      "grad_norm": 0.8386893272399902,
      "learning_rate": 2.3780684163026006e-05,
      "loss": 0.1062,
      "step": 425600
    },
    {
      "epoch": 2.765273311897106,
      "grad_norm": 1.2056357860565186,
      "learning_rate": 2.3773773704600016e-05,
      "loss": 0.105,
      "step": 425700
    },
    {
      "epoch": 2.7659228945402576,
      "grad_norm": 0.7069337964057922,
      "learning_rate": 2.3766863246174026e-05,
      "loss": 0.1108,
      "step": 425800
    },
    {
      "epoch": 2.7665724771834097,
      "grad_norm": 1.0743186473846436,
      "learning_rate": 2.3759952787748035e-05,
      "loss": 0.1019,
      "step": 425900
    },
    {
      "epoch": 2.7672220598265613,
      "grad_norm": 1.5317106246948242,
      "learning_rate": 2.3753042329322045e-05,
      "loss": 0.1112,
      "step": 426000
    },
    {
      "epoch": 2.7678716424697134,
      "grad_norm": 1.3210970163345337,
      "learning_rate": 2.3746131870896052e-05,
      "loss": 0.1117,
      "step": 426100
    },
    {
      "epoch": 2.768521225112865,
      "grad_norm": 1.284847378730774,
      "learning_rate": 2.373922141247006e-05,
      "loss": 0.1175,
      "step": 426200
    },
    {
      "epoch": 2.7691708077560167,
      "grad_norm": 1.1900118589401245,
      "learning_rate": 2.373231095404407e-05,
      "loss": 0.1034,
      "step": 426300
    },
    {
      "epoch": 2.7698203903991683,
      "grad_norm": 0.7795968055725098,
      "learning_rate": 2.372540049561808e-05,
      "loss": 0.1061,
      "step": 426400
    },
    {
      "epoch": 2.7704699730423203,
      "grad_norm": 1.410689115524292,
      "learning_rate": 2.3718490037192088e-05,
      "loss": 0.1076,
      "step": 426500
    },
    {
      "epoch": 2.771119555685472,
      "grad_norm": 1.3231457471847534,
      "learning_rate": 2.3711579578766098e-05,
      "loss": 0.1121,
      "step": 426600
    },
    {
      "epoch": 2.771769138328624,
      "grad_norm": 0.7900450825691223,
      "learning_rate": 2.3704669120340107e-05,
      "loss": 0.1097,
      "step": 426700
    },
    {
      "epoch": 2.7724187209717757,
      "grad_norm": 1.2691141366958618,
      "learning_rate": 2.3697758661914117e-05,
      "loss": 0.1098,
      "step": 426800
    },
    {
      "epoch": 2.7730683036149273,
      "grad_norm": 0.8650400042533875,
      "learning_rate": 2.3690848203488124e-05,
      "loss": 0.1114,
      "step": 426900
    },
    {
      "epoch": 2.773717886258079,
      "grad_norm": 1.2635475397109985,
      "learning_rate": 2.368393774506213e-05,
      "loss": 0.1105,
      "step": 427000
    },
    {
      "epoch": 2.774367468901231,
      "grad_norm": 1.464296579360962,
      "learning_rate": 2.367702728663614e-05,
      "loss": 0.1069,
      "step": 427100
    },
    {
      "epoch": 2.7750170515443826,
      "grad_norm": 0.7983655333518982,
      "learning_rate": 2.367011682821015e-05,
      "loss": 0.1055,
      "step": 427200
    },
    {
      "epoch": 2.7756666341875347,
      "grad_norm": 1.0867596864700317,
      "learning_rate": 2.366320636978416e-05,
      "loss": 0.1032,
      "step": 427300
    },
    {
      "epoch": 2.7763162168306863,
      "grad_norm": 0.7838372588157654,
      "learning_rate": 2.365629591135817e-05,
      "loss": 0.1123,
      "step": 427400
    },
    {
      "epoch": 2.776965799473838,
      "grad_norm": 0.8843907117843628,
      "learning_rate": 2.3649385452932176e-05,
      "loss": 0.1136,
      "step": 427500
    },
    {
      "epoch": 2.77761538211699,
      "grad_norm": 1.3182806968688965,
      "learning_rate": 2.3642474994506186e-05,
      "loss": 0.1084,
      "step": 427600
    },
    {
      "epoch": 2.7782649647601416,
      "grad_norm": 0.7793493270874023,
      "learning_rate": 2.3635564536080196e-05,
      "loss": 0.1143,
      "step": 427700
    },
    {
      "epoch": 2.7789145474032932,
      "grad_norm": 1.1893538236618042,
      "learning_rate": 2.3628654077654206e-05,
      "loss": 0.1132,
      "step": 427800
    },
    {
      "epoch": 2.7795641300464453,
      "grad_norm": 1.4227707386016846,
      "learning_rate": 2.3621743619228212e-05,
      "loss": 0.1098,
      "step": 427900
    },
    {
      "epoch": 2.780213712689597,
      "grad_norm": 0.7155098915100098,
      "learning_rate": 2.3614833160802222e-05,
      "loss": 0.1105,
      "step": 428000
    },
    {
      "epoch": 2.7808632953327486,
      "grad_norm": 0.9280757904052734,
      "learning_rate": 2.3607922702376232e-05,
      "loss": 0.1028,
      "step": 428100
    },
    {
      "epoch": 2.7815128779759006,
      "grad_norm": 1.063680648803711,
      "learning_rate": 2.3601012243950242e-05,
      "loss": 0.1049,
      "step": 428200
    },
    {
      "epoch": 2.7821624606190523,
      "grad_norm": 1.360114336013794,
      "learning_rate": 2.359410178552425e-05,
      "loss": 0.108,
      "step": 428300
    },
    {
      "epoch": 2.782812043262204,
      "grad_norm": 0.817156970500946,
      "learning_rate": 2.3587191327098258e-05,
      "loss": 0.1083,
      "step": 428400
    },
    {
      "epoch": 2.783461625905356,
      "grad_norm": 1.4047297239303589,
      "learning_rate": 2.3580280868672268e-05,
      "loss": 0.108,
      "step": 428500
    },
    {
      "epoch": 2.7841112085485076,
      "grad_norm": 0.866862416267395,
      "learning_rate": 2.3573370410246274e-05,
      "loss": 0.1102,
      "step": 428600
    },
    {
      "epoch": 2.784760791191659,
      "grad_norm": 0.8129038214683533,
      "learning_rate": 2.3566459951820284e-05,
      "loss": 0.1064,
      "step": 428700
    },
    {
      "epoch": 2.7854103738348113,
      "grad_norm": 1.318080186843872,
      "learning_rate": 2.3559549493394294e-05,
      "loss": 0.1038,
      "step": 428800
    },
    {
      "epoch": 2.786059956477963,
      "grad_norm": 1.6420812606811523,
      "learning_rate": 2.35526390349683e-05,
      "loss": 0.1088,
      "step": 428900
    },
    {
      "epoch": 2.7867095391211145,
      "grad_norm": 1.1430678367614746,
      "learning_rate": 2.354572857654231e-05,
      "loss": 0.1069,
      "step": 429000
    },
    {
      "epoch": 2.7873591217642666,
      "grad_norm": 1.4706100225448608,
      "learning_rate": 2.353881811811632e-05,
      "loss": 0.1071,
      "step": 429100
    },
    {
      "epoch": 2.788008704407418,
      "grad_norm": 1.0662623643875122,
      "learning_rate": 2.353190765969033e-05,
      "loss": 0.1104,
      "step": 429200
    },
    {
      "epoch": 2.78865828705057,
      "grad_norm": 1.2469757795333862,
      "learning_rate": 2.352499720126434e-05,
      "loss": 0.1046,
      "step": 429300
    },
    {
      "epoch": 2.789307869693722,
      "grad_norm": 0.8576980233192444,
      "learning_rate": 2.3518086742838346e-05,
      "loss": 0.1022,
      "step": 429400
    },
    {
      "epoch": 2.7899574523368735,
      "grad_norm": 1.2330299615859985,
      "learning_rate": 2.3511176284412356e-05,
      "loss": 0.11,
      "step": 429500
    },
    {
      "epoch": 2.7906070349800256,
      "grad_norm": 1.1126337051391602,
      "learning_rate": 2.3504265825986366e-05,
      "loss": 0.1179,
      "step": 429600
    },
    {
      "epoch": 2.7912566176231772,
      "grad_norm": 0.9135797023773193,
      "learning_rate": 2.3497355367560376e-05,
      "loss": 0.1081,
      "step": 429700
    },
    {
      "epoch": 2.791906200266329,
      "grad_norm": 1.2394517660140991,
      "learning_rate": 2.3490444909134382e-05,
      "loss": 0.1128,
      "step": 429800
    },
    {
      "epoch": 2.7925557829094805,
      "grad_norm": 1.1605522632598877,
      "learning_rate": 2.3483534450708392e-05,
      "loss": 0.107,
      "step": 429900
    },
    {
      "epoch": 2.7932053655526325,
      "grad_norm": 1.2110779285430908,
      "learning_rate": 2.3476623992282402e-05,
      "loss": 0.1118,
      "step": 430000
    },
    {
      "epoch": 2.793854948195784,
      "grad_norm": 0.6907387971878052,
      "learning_rate": 2.3469713533856412e-05,
      "loss": 0.11,
      "step": 430100
    },
    {
      "epoch": 2.7945045308389362,
      "grad_norm": 0.8026404976844788,
      "learning_rate": 2.3462803075430422e-05,
      "loss": 0.1091,
      "step": 430200
    },
    {
      "epoch": 2.795154113482088,
      "grad_norm": 0.9903259873390198,
      "learning_rate": 2.3455892617004428e-05,
      "loss": 0.1109,
      "step": 430300
    },
    {
      "epoch": 2.7958036961252395,
      "grad_norm": 1.279966115951538,
      "learning_rate": 2.3448982158578435e-05,
      "loss": 0.1105,
      "step": 430400
    },
    {
      "epoch": 2.796453278768391,
      "grad_norm": 0.8265889286994934,
      "learning_rate": 2.3442071700152445e-05,
      "loss": 0.1012,
      "step": 430500
    },
    {
      "epoch": 2.797102861411543,
      "grad_norm": 1.0185579061508179,
      "learning_rate": 2.3435161241726454e-05,
      "loss": 0.1087,
      "step": 430600
    },
    {
      "epoch": 2.797752444054695,
      "grad_norm": 0.955312967300415,
      "learning_rate": 2.3428250783300464e-05,
      "loss": 0.1034,
      "step": 430700
    },
    {
      "epoch": 2.798402026697847,
      "grad_norm": 1.0029163360595703,
      "learning_rate": 2.342134032487447e-05,
      "loss": 0.1025,
      "step": 430800
    },
    {
      "epoch": 2.7990516093409985,
      "grad_norm": 0.7941145300865173,
      "learning_rate": 2.341442986644848e-05,
      "loss": 0.1088,
      "step": 430900
    },
    {
      "epoch": 2.79970119198415,
      "grad_norm": 1.2390871047973633,
      "learning_rate": 2.340751940802249e-05,
      "loss": 0.1091,
      "step": 431000
    },
    {
      "epoch": 2.8003507746273018,
      "grad_norm": 0.9798733592033386,
      "learning_rate": 2.34006089495965e-05,
      "loss": 0.1121,
      "step": 431100
    },
    {
      "epoch": 2.801000357270454,
      "grad_norm": 1.0272126197814941,
      "learning_rate": 2.3393698491170507e-05,
      "loss": 0.1124,
      "step": 431200
    },
    {
      "epoch": 2.8016499399136054,
      "grad_norm": 1.6814035177230835,
      "learning_rate": 2.3386788032744517e-05,
      "loss": 0.1101,
      "step": 431300
    },
    {
      "epoch": 2.8022995225567575,
      "grad_norm": 1.6599935293197632,
      "learning_rate": 2.3379877574318526e-05,
      "loss": 0.1055,
      "step": 431400
    },
    {
      "epoch": 2.802949105199909,
      "grad_norm": 1.0894125699996948,
      "learning_rate": 2.3372967115892536e-05,
      "loss": 0.1059,
      "step": 431500
    },
    {
      "epoch": 2.8035986878430608,
      "grad_norm": 1.4334388971328735,
      "learning_rate": 2.3366056657466546e-05,
      "loss": 0.1104,
      "step": 431600
    },
    {
      "epoch": 2.8042482704862124,
      "grad_norm": 1.5394299030303955,
      "learning_rate": 2.3359146199040553e-05,
      "loss": 0.1067,
      "step": 431700
    },
    {
      "epoch": 2.8048978531293645,
      "grad_norm": 1.4702527523040771,
      "learning_rate": 2.3352235740614562e-05,
      "loss": 0.1025,
      "step": 431800
    },
    {
      "epoch": 2.805547435772516,
      "grad_norm": 1.4621162414550781,
      "learning_rate": 2.3345325282188572e-05,
      "loss": 0.1115,
      "step": 431900
    },
    {
      "epoch": 2.806197018415668,
      "grad_norm": 1.4026061296463013,
      "learning_rate": 2.333841482376258e-05,
      "loss": 0.1126,
      "step": 432000
    },
    {
      "epoch": 2.80684660105882,
      "grad_norm": 0.7332238554954529,
      "learning_rate": 2.333150436533659e-05,
      "loss": 0.1012,
      "step": 432100
    },
    {
      "epoch": 2.8074961837019714,
      "grad_norm": 1.4619395732879639,
      "learning_rate": 2.3324593906910595e-05,
      "loss": 0.111,
      "step": 432200
    },
    {
      "epoch": 2.808145766345123,
      "grad_norm": 1.1776598691940308,
      "learning_rate": 2.3317683448484605e-05,
      "loss": 0.11,
      "step": 432300
    },
    {
      "epoch": 2.808795348988275,
      "grad_norm": 1.1237415075302124,
      "learning_rate": 2.3310772990058615e-05,
      "loss": 0.1163,
      "step": 432400
    },
    {
      "epoch": 2.8094449316314267,
      "grad_norm": 0.9416802525520325,
      "learning_rate": 2.3303862531632625e-05,
      "loss": 0.1096,
      "step": 432500
    },
    {
      "epoch": 2.810094514274579,
      "grad_norm": 1.0054197311401367,
      "learning_rate": 2.3296952073206635e-05,
      "loss": 0.1062,
      "step": 432600
    },
    {
      "epoch": 2.8107440969177304,
      "grad_norm": 0.8180671334266663,
      "learning_rate": 2.329004161478064e-05,
      "loss": 0.1093,
      "step": 432700
    },
    {
      "epoch": 2.811393679560882,
      "grad_norm": 1.0163078308105469,
      "learning_rate": 2.328313115635465e-05,
      "loss": 0.1099,
      "step": 432800
    },
    {
      "epoch": 2.8120432622040337,
      "grad_norm": 0.7950790524482727,
      "learning_rate": 2.327622069792866e-05,
      "loss": 0.1083,
      "step": 432900
    },
    {
      "epoch": 2.8126928448471857,
      "grad_norm": 0.9658082723617554,
      "learning_rate": 2.326931023950267e-05,
      "loss": 0.1101,
      "step": 433000
    },
    {
      "epoch": 2.8133424274903374,
      "grad_norm": 1.0183504819869995,
      "learning_rate": 2.3262399781076677e-05,
      "loss": 0.1032,
      "step": 433100
    },
    {
      "epoch": 2.8139920101334894,
      "grad_norm": 1.0645655393600464,
      "learning_rate": 2.3255489322650687e-05,
      "loss": 0.1074,
      "step": 433200
    },
    {
      "epoch": 2.814641592776641,
      "grad_norm": 1.1709874868392944,
      "learning_rate": 2.3248578864224697e-05,
      "loss": 0.1067,
      "step": 433300
    },
    {
      "epoch": 2.8152911754197927,
      "grad_norm": 0.7194546461105347,
      "learning_rate": 2.3241668405798707e-05,
      "loss": 0.1092,
      "step": 433400
    },
    {
      "epoch": 2.8159407580629443,
      "grad_norm": 0.9826641082763672,
      "learning_rate": 2.3234757947372716e-05,
      "loss": 0.1105,
      "step": 433500
    },
    {
      "epoch": 2.8165903407060964,
      "grad_norm": 0.5641548037528992,
      "learning_rate": 2.3227847488946723e-05,
      "loss": 0.1065,
      "step": 433600
    },
    {
      "epoch": 2.817239923349248,
      "grad_norm": 0.9314709305763245,
      "learning_rate": 2.3220937030520733e-05,
      "loss": 0.1047,
      "step": 433700
    },
    {
      "epoch": 2.8178895059924,
      "grad_norm": 1.0645872354507446,
      "learning_rate": 2.321402657209474e-05,
      "loss": 0.0951,
      "step": 433800
    },
    {
      "epoch": 2.8185390886355517,
      "grad_norm": 0.6286923289299011,
      "learning_rate": 2.320711611366875e-05,
      "loss": 0.1112,
      "step": 433900
    },
    {
      "epoch": 2.8191886712787033,
      "grad_norm": 1.2565175294876099,
      "learning_rate": 2.320020565524276e-05,
      "loss": 0.1086,
      "step": 434000
    },
    {
      "epoch": 2.819838253921855,
      "grad_norm": 0.9057528972625732,
      "learning_rate": 2.3193295196816765e-05,
      "loss": 0.1131,
      "step": 434100
    },
    {
      "epoch": 2.820487836565007,
      "grad_norm": 1.049864649772644,
      "learning_rate": 2.3186384738390775e-05,
      "loss": 0.1096,
      "step": 434200
    },
    {
      "epoch": 2.8211374192081586,
      "grad_norm": 1.1197819709777832,
      "learning_rate": 2.3179474279964785e-05,
      "loss": 0.1037,
      "step": 434300
    },
    {
      "epoch": 2.8217870018513107,
      "grad_norm": 1.2812020778656006,
      "learning_rate": 2.3172563821538795e-05,
      "loss": 0.1082,
      "step": 434400
    },
    {
      "epoch": 2.8224365844944623,
      "grad_norm": 0.8308939933776855,
      "learning_rate": 2.31656533631128e-05,
      "loss": 0.1126,
      "step": 434500
    },
    {
      "epoch": 2.823086167137614,
      "grad_norm": 1.4428246021270752,
      "learning_rate": 2.315874290468681e-05,
      "loss": 0.1111,
      "step": 434600
    },
    {
      "epoch": 2.8237357497807656,
      "grad_norm": 1.586865782737732,
      "learning_rate": 2.315183244626082e-05,
      "loss": 0.1098,
      "step": 434700
    },
    {
      "epoch": 2.8243853324239176,
      "grad_norm": 1.31880521774292,
      "learning_rate": 2.314492198783483e-05,
      "loss": 0.1088,
      "step": 434800
    },
    {
      "epoch": 2.8250349150670693,
      "grad_norm": 1.252152919769287,
      "learning_rate": 2.313801152940884e-05,
      "loss": 0.1093,
      "step": 434900
    },
    {
      "epoch": 2.8256844977102213,
      "grad_norm": 1.190596103668213,
      "learning_rate": 2.3131101070982847e-05,
      "loss": 0.1125,
      "step": 435000
    },
    {
      "epoch": 2.826334080353373,
      "grad_norm": 1.0353199243545532,
      "learning_rate": 2.3124190612556857e-05,
      "loss": 0.105,
      "step": 435100
    },
    {
      "epoch": 2.8269836629965246,
      "grad_norm": 0.862710177898407,
      "learning_rate": 2.3117280154130867e-05,
      "loss": 0.1102,
      "step": 435200
    },
    {
      "epoch": 2.8276332456396767,
      "grad_norm": 0.7285146713256836,
      "learning_rate": 2.3110369695704877e-05,
      "loss": 0.1098,
      "step": 435300
    },
    {
      "epoch": 2.8282828282828283,
      "grad_norm": 0.9159889817237854,
      "learning_rate": 2.3103459237278883e-05,
      "loss": 0.11,
      "step": 435400
    },
    {
      "epoch": 2.82893241092598,
      "grad_norm": 1.2993522882461548,
      "learning_rate": 2.3096548778852893e-05,
      "loss": 0.1037,
      "step": 435500
    },
    {
      "epoch": 2.829581993569132,
      "grad_norm": 1.1176782846450806,
      "learning_rate": 2.30896383204269e-05,
      "loss": 0.105,
      "step": 435600
    },
    {
      "epoch": 2.8302315762122836,
      "grad_norm": 1.0488773584365845,
      "learning_rate": 2.308272786200091e-05,
      "loss": 0.1077,
      "step": 435700
    },
    {
      "epoch": 2.8308811588554352,
      "grad_norm": 1.0119143724441528,
      "learning_rate": 2.307581740357492e-05,
      "loss": 0.108,
      "step": 435800
    },
    {
      "epoch": 2.8315307414985873,
      "grad_norm": 1.3444846868515015,
      "learning_rate": 2.3068906945148926e-05,
      "loss": 0.112,
      "step": 435900
    },
    {
      "epoch": 2.832180324141739,
      "grad_norm": 1.0907343626022339,
      "learning_rate": 2.3061996486722936e-05,
      "loss": 0.1034,
      "step": 436000
    },
    {
      "epoch": 2.8328299067848906,
      "grad_norm": 1.0641335248947144,
      "learning_rate": 2.3055086028296945e-05,
      "loss": 0.1088,
      "step": 436100
    },
    {
      "epoch": 2.8334794894280426,
      "grad_norm": 1.0557693243026733,
      "learning_rate": 2.3048175569870955e-05,
      "loss": 0.112,
      "step": 436200
    },
    {
      "epoch": 2.8341290720711942,
      "grad_norm": 0.9583086371421814,
      "learning_rate": 2.3041265111444965e-05,
      "loss": 0.1151,
      "step": 436300
    },
    {
      "epoch": 2.834778654714346,
      "grad_norm": 1.3766992092132568,
      "learning_rate": 2.303435465301897e-05,
      "loss": 0.1066,
      "step": 436400
    },
    {
      "epoch": 2.835428237357498,
      "grad_norm": 0.7173134088516235,
      "learning_rate": 2.302744419459298e-05,
      "loss": 0.1088,
      "step": 436500
    },
    {
      "epoch": 2.8360778200006496,
      "grad_norm": 1.432910442352295,
      "learning_rate": 2.302053373616699e-05,
      "loss": 0.1074,
      "step": 436600
    },
    {
      "epoch": 2.8367274026438016,
      "grad_norm": 0.9856924414634705,
      "learning_rate": 2.3013623277741e-05,
      "loss": 0.1069,
      "step": 436700
    },
    {
      "epoch": 2.8373769852869533,
      "grad_norm": 0.8610895276069641,
      "learning_rate": 2.300671281931501e-05,
      "loss": 0.1024,
      "step": 436800
    },
    {
      "epoch": 2.838026567930105,
      "grad_norm": 1.328249216079712,
      "learning_rate": 2.2999802360889017e-05,
      "loss": 0.1114,
      "step": 436900
    },
    {
      "epoch": 2.8386761505732565,
      "grad_norm": 1.0197174549102783,
      "learning_rate": 2.2992891902463027e-05,
      "loss": 0.1018,
      "step": 437000
    },
    {
      "epoch": 2.8393257332164086,
      "grad_norm": 1.0237975120544434,
      "learning_rate": 2.2985981444037037e-05,
      "loss": 0.1064,
      "step": 437100
    },
    {
      "epoch": 2.83997531585956,
      "grad_norm": 1.0292762517929077,
      "learning_rate": 2.2979070985611044e-05,
      "loss": 0.1086,
      "step": 437200
    },
    {
      "epoch": 2.8406248985027123,
      "grad_norm": 1.1486194133758545,
      "learning_rate": 2.2972160527185054e-05,
      "loss": 0.108,
      "step": 437300
    },
    {
      "epoch": 2.841274481145864,
      "grad_norm": 1.231581449508667,
      "learning_rate": 2.296525006875906e-05,
      "loss": 0.1075,
      "step": 437400
    },
    {
      "epoch": 2.8419240637890155,
      "grad_norm": 0.7824472784996033,
      "learning_rate": 2.295833961033307e-05,
      "loss": 0.1066,
      "step": 437500
    },
    {
      "epoch": 2.842573646432167,
      "grad_norm": 0.9355814456939697,
      "learning_rate": 2.295142915190708e-05,
      "loss": 0.1141,
      "step": 437600
    },
    {
      "epoch": 2.843223229075319,
      "grad_norm": 0.7808446288108826,
      "learning_rate": 2.294451869348109e-05,
      "loss": 0.1021,
      "step": 437700
    },
    {
      "epoch": 2.843872811718471,
      "grad_norm": 0.9831136465072632,
      "learning_rate": 2.2937608235055096e-05,
      "loss": 0.1049,
      "step": 437800
    },
    {
      "epoch": 2.844522394361623,
      "grad_norm": 0.9766436815261841,
      "learning_rate": 2.2930697776629106e-05,
      "loss": 0.1172,
      "step": 437900
    },
    {
      "epoch": 2.8451719770047745,
      "grad_norm": 0.7132859230041504,
      "learning_rate": 2.2923787318203116e-05,
      "loss": 0.1036,
      "step": 438000
    },
    {
      "epoch": 2.845821559647926,
      "grad_norm": 1.0167075395584106,
      "learning_rate": 2.2916876859777126e-05,
      "loss": 0.1127,
      "step": 438100
    },
    {
      "epoch": 2.846471142291078,
      "grad_norm": 0.9080489873886108,
      "learning_rate": 2.2909966401351135e-05,
      "loss": 0.1116,
      "step": 438200
    },
    {
      "epoch": 2.84712072493423,
      "grad_norm": 1.3457609415054321,
      "learning_rate": 2.2903055942925142e-05,
      "loss": 0.1072,
      "step": 438300
    },
    {
      "epoch": 2.8477703075773815,
      "grad_norm": 0.9640311598777771,
      "learning_rate": 2.2896145484499152e-05,
      "loss": 0.1047,
      "step": 438400
    },
    {
      "epoch": 2.8484198902205335,
      "grad_norm": 1.1882750988006592,
      "learning_rate": 2.288923502607316e-05,
      "loss": 0.1078,
      "step": 438500
    },
    {
      "epoch": 2.849069472863685,
      "grad_norm": 1.337322473526001,
      "learning_rate": 2.288232456764717e-05,
      "loss": 0.1082,
      "step": 438600
    },
    {
      "epoch": 2.849719055506837,
      "grad_norm": 0.7892436385154724,
      "learning_rate": 2.287541410922118e-05,
      "loss": 0.1072,
      "step": 438700
    },
    {
      "epoch": 2.8503686381499884,
      "grad_norm": 0.9909924268722534,
      "learning_rate": 2.2868503650795188e-05,
      "loss": 0.1091,
      "step": 438800
    },
    {
      "epoch": 2.8510182207931405,
      "grad_norm": 1.0031627416610718,
      "learning_rate": 2.2861593192369198e-05,
      "loss": 0.115,
      "step": 438900
    },
    {
      "epoch": 2.851667803436292,
      "grad_norm": 1.4648730754852295,
      "learning_rate": 2.2854682733943204e-05,
      "loss": 0.1119,
      "step": 439000
    },
    {
      "epoch": 2.852317386079444,
      "grad_norm": 1.3266453742980957,
      "learning_rate": 2.2847772275517214e-05,
      "loss": 0.1138,
      "step": 439100
    },
    {
      "epoch": 2.852966968722596,
      "grad_norm": 0.9781871438026428,
      "learning_rate": 2.284086181709122e-05,
      "loss": 0.1132,
      "step": 439200
    },
    {
      "epoch": 2.8536165513657474,
      "grad_norm": 1.178913950920105,
      "learning_rate": 2.283395135866523e-05,
      "loss": 0.1073,
      "step": 439300
    },
    {
      "epoch": 2.854266134008899,
      "grad_norm": 1.1840499639511108,
      "learning_rate": 2.282704090023924e-05,
      "loss": 0.1134,
      "step": 439400
    },
    {
      "epoch": 2.854915716652051,
      "grad_norm": 1.0077486038208008,
      "learning_rate": 2.282013044181325e-05,
      "loss": 0.1025,
      "step": 439500
    },
    {
      "epoch": 2.8555652992952028,
      "grad_norm": 1.226149082183838,
      "learning_rate": 2.281321998338726e-05,
      "loss": 0.1047,
      "step": 439600
    },
    {
      "epoch": 2.856214881938355,
      "grad_norm": 1.3891950845718384,
      "learning_rate": 2.2806309524961266e-05,
      "loss": 0.1139,
      "step": 439700
    },
    {
      "epoch": 2.8568644645815064,
      "grad_norm": 1.2756285667419434,
      "learning_rate": 2.2799399066535276e-05,
      "loss": 0.108,
      "step": 439800
    },
    {
      "epoch": 2.857514047224658,
      "grad_norm": 1.009594202041626,
      "learning_rate": 2.2792488608109286e-05,
      "loss": 0.1138,
      "step": 439900
    },
    {
      "epoch": 2.8581636298678097,
      "grad_norm": 1.0862243175506592,
      "learning_rate": 2.2785578149683296e-05,
      "loss": 0.113,
      "step": 440000
    },
    {
      "epoch": 2.8588132125109618,
      "grad_norm": 0.9892469048500061,
      "learning_rate": 2.2778667691257306e-05,
      "loss": 0.1038,
      "step": 440100
    },
    {
      "epoch": 2.8594627951541134,
      "grad_norm": 0.895167350769043,
      "learning_rate": 2.2771757232831312e-05,
      "loss": 0.1117,
      "step": 440200
    },
    {
      "epoch": 2.8601123777972655,
      "grad_norm": 0.8411252498626709,
      "learning_rate": 2.2764846774405322e-05,
      "loss": 0.1092,
      "step": 440300
    },
    {
      "epoch": 2.860761960440417,
      "grad_norm": 1.0300750732421875,
      "learning_rate": 2.2757936315979332e-05,
      "loss": 0.1109,
      "step": 440400
    },
    {
      "epoch": 2.8614115430835687,
      "grad_norm": 1.1704671382904053,
      "learning_rate": 2.275102585755334e-05,
      "loss": 0.1035,
      "step": 440500
    },
    {
      "epoch": 2.8620611257267203,
      "grad_norm": 1.3618366718292236,
      "learning_rate": 2.2744115399127348e-05,
      "loss": 0.1109,
      "step": 440600
    },
    {
      "epoch": 2.8627107083698724,
      "grad_norm": 1.2581263780593872,
      "learning_rate": 2.2737204940701358e-05,
      "loss": 0.1137,
      "step": 440700
    },
    {
      "epoch": 2.863360291013024,
      "grad_norm": 0.9733088612556458,
      "learning_rate": 2.2730294482275364e-05,
      "loss": 0.1061,
      "step": 440800
    },
    {
      "epoch": 2.864009873656176,
      "grad_norm": 0.9149119257926941,
      "learning_rate": 2.2723384023849374e-05,
      "loss": 0.1065,
      "step": 440900
    },
    {
      "epoch": 2.8646594562993277,
      "grad_norm": 0.9053581953048706,
      "learning_rate": 2.2716473565423384e-05,
      "loss": 0.1094,
      "step": 441000
    },
    {
      "epoch": 2.8653090389424793,
      "grad_norm": 1.269835352897644,
      "learning_rate": 2.270956310699739e-05,
      "loss": 0.1056,
      "step": 441100
    },
    {
      "epoch": 2.865958621585631,
      "grad_norm": 1.7117689847946167,
      "learning_rate": 2.27026526485714e-05,
      "loss": 0.1096,
      "step": 441200
    },
    {
      "epoch": 2.866608204228783,
      "grad_norm": 1.0533983707427979,
      "learning_rate": 2.269574219014541e-05,
      "loss": 0.1065,
      "step": 441300
    },
    {
      "epoch": 2.8672577868719347,
      "grad_norm": 1.0900607109069824,
      "learning_rate": 2.268883173171942e-05,
      "loss": 0.1091,
      "step": 441400
    },
    {
      "epoch": 2.8679073695150867,
      "grad_norm": 2.133664846420288,
      "learning_rate": 2.268192127329343e-05,
      "loss": 0.1047,
      "step": 441500
    },
    {
      "epoch": 2.8685569521582384,
      "grad_norm": 1.2212705612182617,
      "learning_rate": 2.2675010814867436e-05,
      "loss": 0.1045,
      "step": 441600
    },
    {
      "epoch": 2.86920653480139,
      "grad_norm": 1.0525240898132324,
      "learning_rate": 2.2668100356441446e-05,
      "loss": 0.104,
      "step": 441700
    },
    {
      "epoch": 2.8698561174445416,
      "grad_norm": 1.2296490669250488,
      "learning_rate": 2.2661189898015456e-05,
      "loss": 0.1022,
      "step": 441800
    },
    {
      "epoch": 2.8705057000876937,
      "grad_norm": 1.1448651552200317,
      "learning_rate": 2.2654279439589466e-05,
      "loss": 0.1162,
      "step": 441900
    },
    {
      "epoch": 2.8711552827308453,
      "grad_norm": 1.1221809387207031,
      "learning_rate": 2.2647368981163476e-05,
      "loss": 0.1078,
      "step": 442000
    },
    {
      "epoch": 2.8718048653739974,
      "grad_norm": 0.8277944922447205,
      "learning_rate": 2.2640458522737482e-05,
      "loss": 0.1049,
      "step": 442100
    },
    {
      "epoch": 2.872454448017149,
      "grad_norm": 0.7869095802307129,
      "learning_rate": 2.2633548064311492e-05,
      "loss": 0.1136,
      "step": 442200
    },
    {
      "epoch": 2.8731040306603006,
      "grad_norm": 0.882204532623291,
      "learning_rate": 2.2626637605885502e-05,
      "loss": 0.1092,
      "step": 442300
    },
    {
      "epoch": 2.8737536133034527,
      "grad_norm": 1.5588487386703491,
      "learning_rate": 2.261972714745951e-05,
      "loss": 0.1114,
      "step": 442400
    },
    {
      "epoch": 2.8744031959466043,
      "grad_norm": 0.9514520764350891,
      "learning_rate": 2.261281668903352e-05,
      "loss": 0.1081,
      "step": 442500
    },
    {
      "epoch": 2.875052778589756,
      "grad_norm": 1.4801210165023804,
      "learning_rate": 2.2605906230607525e-05,
      "loss": 0.1033,
      "step": 442600
    },
    {
      "epoch": 2.875702361232908,
      "grad_norm": 1.081149935722351,
      "learning_rate": 2.2598995772181535e-05,
      "loss": 0.1008,
      "step": 442700
    },
    {
      "epoch": 2.8763519438760596,
      "grad_norm": 1.4393903017044067,
      "learning_rate": 2.2592085313755545e-05,
      "loss": 0.1124,
      "step": 442800
    },
    {
      "epoch": 2.8770015265192113,
      "grad_norm": 1.5008779764175415,
      "learning_rate": 2.2585174855329554e-05,
      "loss": 0.1093,
      "step": 442900
    },
    {
      "epoch": 2.8776511091623633,
      "grad_norm": 1.4190775156021118,
      "learning_rate": 2.257826439690356e-05,
      "loss": 0.1036,
      "step": 443000
    },
    {
      "epoch": 2.878300691805515,
      "grad_norm": 1.0340194702148438,
      "learning_rate": 2.257135393847757e-05,
      "loss": 0.1016,
      "step": 443100
    },
    {
      "epoch": 2.8789502744486666,
      "grad_norm": 1.2790133953094482,
      "learning_rate": 2.256444348005158e-05,
      "loss": 0.1117,
      "step": 443200
    },
    {
      "epoch": 2.8795998570918186,
      "grad_norm": 1.142602801322937,
      "learning_rate": 2.255753302162559e-05,
      "loss": 0.1066,
      "step": 443300
    },
    {
      "epoch": 2.8802494397349703,
      "grad_norm": 0.5997290015220642,
      "learning_rate": 2.25506225631996e-05,
      "loss": 0.112,
      "step": 443400
    },
    {
      "epoch": 2.880899022378122,
      "grad_norm": 1.0481542348861694,
      "learning_rate": 2.2543712104773607e-05,
      "loss": 0.1076,
      "step": 443500
    },
    {
      "epoch": 2.881548605021274,
      "grad_norm": 0.8784300684928894,
      "learning_rate": 2.2536801646347617e-05,
      "loss": 0.108,
      "step": 443600
    },
    {
      "epoch": 2.8821981876644256,
      "grad_norm": 1.327850341796875,
      "learning_rate": 2.2529891187921626e-05,
      "loss": 0.1082,
      "step": 443700
    },
    {
      "epoch": 2.882847770307577,
      "grad_norm": 1.0451834201812744,
      "learning_rate": 2.2522980729495636e-05,
      "loss": 0.1064,
      "step": 443800
    },
    {
      "epoch": 2.8834973529507293,
      "grad_norm": 1.035107135772705,
      "learning_rate": 2.2516070271069643e-05,
      "loss": 0.1141,
      "step": 443900
    },
    {
      "epoch": 2.884146935593881,
      "grad_norm": 0.8086304068565369,
      "learning_rate": 2.2509159812643653e-05,
      "loss": 0.1088,
      "step": 444000
    },
    {
      "epoch": 2.8847965182370325,
      "grad_norm": 0.9703518152236938,
      "learning_rate": 2.2502249354217662e-05,
      "loss": 0.1068,
      "step": 444100
    },
    {
      "epoch": 2.8854461008801846,
      "grad_norm": 1.310709834098816,
      "learning_rate": 2.249533889579167e-05,
      "loss": 0.1077,
      "step": 444200
    },
    {
      "epoch": 2.8860956835233362,
      "grad_norm": 1.6720538139343262,
      "learning_rate": 2.248842843736568e-05,
      "loss": 0.1093,
      "step": 444300
    },
    {
      "epoch": 2.8867452661664883,
      "grad_norm": 1.0806987285614014,
      "learning_rate": 2.2481517978939685e-05,
      "loss": 0.1051,
      "step": 444400
    },
    {
      "epoch": 2.88739484880964,
      "grad_norm": 1.20304536819458,
      "learning_rate": 2.2474607520513695e-05,
      "loss": 0.1043,
      "step": 444500
    },
    {
      "epoch": 2.8880444314527915,
      "grad_norm": 1.136332392692566,
      "learning_rate": 2.2467697062087705e-05,
      "loss": 0.1125,
      "step": 444600
    },
    {
      "epoch": 2.888694014095943,
      "grad_norm": 1.4253355264663696,
      "learning_rate": 2.2460786603661715e-05,
      "loss": 0.1092,
      "step": 444700
    },
    {
      "epoch": 2.8893435967390952,
      "grad_norm": 1.213107943534851,
      "learning_rate": 2.2453876145235725e-05,
      "loss": 0.1055,
      "step": 444800
    },
    {
      "epoch": 2.889993179382247,
      "grad_norm": 1.1037501096725464,
      "learning_rate": 2.244696568680973e-05,
      "loss": 0.1043,
      "step": 444900
    },
    {
      "epoch": 2.890642762025399,
      "grad_norm": 1.556406855583191,
      "learning_rate": 2.244005522838374e-05,
      "loss": 0.1057,
      "step": 445000
    },
    {
      "epoch": 2.8912923446685506,
      "grad_norm": 1.3548669815063477,
      "learning_rate": 2.243314476995775e-05,
      "loss": 0.1059,
      "step": 445100
    },
    {
      "epoch": 2.891941927311702,
      "grad_norm": 1.0286128520965576,
      "learning_rate": 2.242623431153176e-05,
      "loss": 0.1063,
      "step": 445200
    },
    {
      "epoch": 2.892591509954854,
      "grad_norm": 1.1701539754867554,
      "learning_rate": 2.241932385310577e-05,
      "loss": 0.1105,
      "step": 445300
    },
    {
      "epoch": 2.893241092598006,
      "grad_norm": 0.7912535071372986,
      "learning_rate": 2.2412413394679777e-05,
      "loss": 0.1044,
      "step": 445400
    },
    {
      "epoch": 2.8938906752411575,
      "grad_norm": 1.469724416732788,
      "learning_rate": 2.2405502936253787e-05,
      "loss": 0.108,
      "step": 445500
    },
    {
      "epoch": 2.8945402578843096,
      "grad_norm": 0.8285654187202454,
      "learning_rate": 2.2398592477827797e-05,
      "loss": 0.1053,
      "step": 445600
    },
    {
      "epoch": 2.895189840527461,
      "grad_norm": 0.8839974403381348,
      "learning_rate": 2.2391682019401807e-05,
      "loss": 0.1106,
      "step": 445700
    },
    {
      "epoch": 2.895839423170613,
      "grad_norm": 1.310845136642456,
      "learning_rate": 2.2384771560975813e-05,
      "loss": 0.1036,
      "step": 445800
    },
    {
      "epoch": 2.8964890058137645,
      "grad_norm": 1.2503490447998047,
      "learning_rate": 2.2377861102549823e-05,
      "loss": 0.1054,
      "step": 445900
    },
    {
      "epoch": 2.8971385884569165,
      "grad_norm": 0.9992425441741943,
      "learning_rate": 2.237095064412383e-05,
      "loss": 0.0995,
      "step": 446000
    },
    {
      "epoch": 2.897788171100068,
      "grad_norm": 0.9585585594177246,
      "learning_rate": 2.236404018569784e-05,
      "loss": 0.1057,
      "step": 446100
    },
    {
      "epoch": 2.89843775374322,
      "grad_norm": 0.6026273965835571,
      "learning_rate": 2.235712972727185e-05,
      "loss": 0.1079,
      "step": 446200
    },
    {
      "epoch": 2.899087336386372,
      "grad_norm": 1.0943431854248047,
      "learning_rate": 2.2350219268845855e-05,
      "loss": 0.1078,
      "step": 446300
    },
    {
      "epoch": 2.8997369190295235,
      "grad_norm": 1.0653661489486694,
      "learning_rate": 2.2343308810419865e-05,
      "loss": 0.1074,
      "step": 446400
    },
    {
      "epoch": 2.900386501672675,
      "grad_norm": 0.9947758913040161,
      "learning_rate": 2.2336398351993875e-05,
      "loss": 0.1078,
      "step": 446500
    },
    {
      "epoch": 2.901036084315827,
      "grad_norm": 0.9634615778923035,
      "learning_rate": 2.2329487893567885e-05,
      "loss": 0.0992,
      "step": 446600
    },
    {
      "epoch": 2.901685666958979,
      "grad_norm": 1.2125593423843384,
      "learning_rate": 2.2322577435141895e-05,
      "loss": 0.1097,
      "step": 446700
    },
    {
      "epoch": 2.902335249602131,
      "grad_norm": 1.1829315423965454,
      "learning_rate": 2.23156669767159e-05,
      "loss": 0.1152,
      "step": 446800
    },
    {
      "epoch": 2.9029848322452825,
      "grad_norm": 0.9196435213088989,
      "learning_rate": 2.230875651828991e-05,
      "loss": 0.1062,
      "step": 446900
    },
    {
      "epoch": 2.903634414888434,
      "grad_norm": 1.1447895765304565,
      "learning_rate": 2.230184605986392e-05,
      "loss": 0.1027,
      "step": 447000
    },
    {
      "epoch": 2.9042839975315857,
      "grad_norm": 1.2697328329086304,
      "learning_rate": 2.229493560143793e-05,
      "loss": 0.1096,
      "step": 447100
    },
    {
      "epoch": 2.904933580174738,
      "grad_norm": 0.8692299723625183,
      "learning_rate": 2.2288025143011937e-05,
      "loss": 0.1075,
      "step": 447200
    },
    {
      "epoch": 2.9055831628178894,
      "grad_norm": 1.1256321668624878,
      "learning_rate": 2.2281114684585947e-05,
      "loss": 0.1046,
      "step": 447300
    },
    {
      "epoch": 2.9062327454610415,
      "grad_norm": 1.1050928831100464,
      "learning_rate": 2.2274204226159957e-05,
      "loss": 0.113,
      "step": 447400
    },
    {
      "epoch": 2.906882328104193,
      "grad_norm": 1.1849448680877686,
      "learning_rate": 2.2267293767733967e-05,
      "loss": 0.1091,
      "step": 447500
    },
    {
      "epoch": 2.9075319107473447,
      "grad_norm": 1.040733814239502,
      "learning_rate": 2.2260383309307973e-05,
      "loss": 0.1051,
      "step": 447600
    },
    {
      "epoch": 2.9081814933904964,
      "grad_norm": 2.0368752479553223,
      "learning_rate": 2.2253472850881983e-05,
      "loss": 0.1052,
      "step": 447700
    },
    {
      "epoch": 2.9088310760336484,
      "grad_norm": 1.6213037967681885,
      "learning_rate": 2.224656239245599e-05,
      "loss": 0.1047,
      "step": 447800
    },
    {
      "epoch": 2.9094806586768,
      "grad_norm": 1.2823513746261597,
      "learning_rate": 2.223965193403e-05,
      "loss": 0.1043,
      "step": 447900
    },
    {
      "epoch": 2.910130241319952,
      "grad_norm": 1.249722957611084,
      "learning_rate": 2.223274147560401e-05,
      "loss": 0.1067,
      "step": 448000
    },
    {
      "epoch": 2.9107798239631038,
      "grad_norm": 1.0276631116867065,
      "learning_rate": 2.222583101717802e-05,
      "loss": 0.1083,
      "step": 448100
    },
    {
      "epoch": 2.9114294066062554,
      "grad_norm": 0.7818586826324463,
      "learning_rate": 2.2218920558752026e-05,
      "loss": 0.1087,
      "step": 448200
    },
    {
      "epoch": 2.912078989249407,
      "grad_norm": 1.3460967540740967,
      "learning_rate": 2.2212010100326036e-05,
      "loss": 0.1057,
      "step": 448300
    },
    {
      "epoch": 2.912728571892559,
      "grad_norm": 1.242903470993042,
      "learning_rate": 2.2205099641900045e-05,
      "loss": 0.1122,
      "step": 448400
    },
    {
      "epoch": 2.9133781545357107,
      "grad_norm": 1.1363818645477295,
      "learning_rate": 2.2198189183474055e-05,
      "loss": 0.1135,
      "step": 448500
    },
    {
      "epoch": 2.9140277371788628,
      "grad_norm": 0.9943941235542297,
      "learning_rate": 2.2191278725048062e-05,
      "loss": 0.111,
      "step": 448600
    },
    {
      "epoch": 2.9146773198220144,
      "grad_norm": 1.3213634490966797,
      "learning_rate": 2.218436826662207e-05,
      "loss": 0.1129,
      "step": 448700
    },
    {
      "epoch": 2.915326902465166,
      "grad_norm": 0.8625496625900269,
      "learning_rate": 2.217745780819608e-05,
      "loss": 0.1137,
      "step": 448800
    },
    {
      "epoch": 2.9159764851083176,
      "grad_norm": 1.2669365406036377,
      "learning_rate": 2.217054734977009e-05,
      "loss": 0.1013,
      "step": 448900
    },
    {
      "epoch": 2.9166260677514697,
      "grad_norm": 0.812163233757019,
      "learning_rate": 2.21636368913441e-05,
      "loss": 0.109,
      "step": 449000
    },
    {
      "epoch": 2.9172756503946213,
      "grad_norm": 0.6184192299842834,
      "learning_rate": 2.2156726432918108e-05,
      "loss": 0.1032,
      "step": 449100
    },
    {
      "epoch": 2.9179252330377734,
      "grad_norm": 1.0357773303985596,
      "learning_rate": 2.2149815974492117e-05,
      "loss": 0.114,
      "step": 449200
    },
    {
      "epoch": 2.918574815680925,
      "grad_norm": 1.384669303894043,
      "learning_rate": 2.2142905516066127e-05,
      "loss": 0.1068,
      "step": 449300
    },
    {
      "epoch": 2.9192243983240767,
      "grad_norm": 1.4463824033737183,
      "learning_rate": 2.2135995057640134e-05,
      "loss": 0.1032,
      "step": 449400
    },
    {
      "epoch": 2.9198739809672287,
      "grad_norm": 0.7698756456375122,
      "learning_rate": 2.2129084599214144e-05,
      "loss": 0.1068,
      "step": 449500
    },
    {
      "epoch": 2.9205235636103803,
      "grad_norm": 1.4933658838272095,
      "learning_rate": 2.212217414078815e-05,
      "loss": 0.1094,
      "step": 449600
    },
    {
      "epoch": 2.921173146253532,
      "grad_norm": 1.337517499923706,
      "learning_rate": 2.211526368236216e-05,
      "loss": 0.1095,
      "step": 449700
    },
    {
      "epoch": 2.921822728896684,
      "grad_norm": 1.125354290008545,
      "learning_rate": 2.210835322393617e-05,
      "loss": 0.1076,
      "step": 449800
    },
    {
      "epoch": 2.9224723115398357,
      "grad_norm": 1.128671646118164,
      "learning_rate": 2.210144276551018e-05,
      "loss": 0.1145,
      "step": 449900
    },
    {
      "epoch": 2.9231218941829873,
      "grad_norm": 0.7305952906608582,
      "learning_rate": 2.209453230708419e-05,
      "loss": 0.1047,
      "step": 450000
    },
    {
      "epoch": 2.9237714768261394,
      "grad_norm": 1.1760849952697754,
      "learning_rate": 2.2087621848658196e-05,
      "loss": 0.1004,
      "step": 450100
    },
    {
      "epoch": 2.924421059469291,
      "grad_norm": 0.8368438482284546,
      "learning_rate": 2.2080711390232206e-05,
      "loss": 0.105,
      "step": 450200
    },
    {
      "epoch": 2.9250706421124426,
      "grad_norm": 1.158695936203003,
      "learning_rate": 2.2073800931806216e-05,
      "loss": 0.1091,
      "step": 450300
    },
    {
      "epoch": 2.9257202247555947,
      "grad_norm": 1.1912933588027954,
      "learning_rate": 2.2066890473380226e-05,
      "loss": 0.1075,
      "step": 450400
    },
    {
      "epoch": 2.9263698073987463,
      "grad_norm": 0.7778763771057129,
      "learning_rate": 2.2059980014954232e-05,
      "loss": 0.1062,
      "step": 450500
    },
    {
      "epoch": 2.927019390041898,
      "grad_norm": 1.2297441959381104,
      "learning_rate": 2.2053069556528242e-05,
      "loss": 0.0986,
      "step": 450600
    },
    {
      "epoch": 2.92766897268505,
      "grad_norm": 1.5987931489944458,
      "learning_rate": 2.2046159098102252e-05,
      "loss": 0.1086,
      "step": 450700
    },
    {
      "epoch": 2.9283185553282016,
      "grad_norm": 1.2354823350906372,
      "learning_rate": 2.203924863967626e-05,
      "loss": 0.1073,
      "step": 450800
    },
    {
      "epoch": 2.9289681379713532,
      "grad_norm": 1.4826534986495972,
      "learning_rate": 2.203233818125027e-05,
      "loss": 0.1089,
      "step": 450900
    },
    {
      "epoch": 2.9296177206145053,
      "grad_norm": 1.4967398643493652,
      "learning_rate": 2.2025427722824278e-05,
      "loss": 0.1085,
      "step": 451000
    },
    {
      "epoch": 2.930267303257657,
      "grad_norm": 1.2882241010665894,
      "learning_rate": 2.2018517264398288e-05,
      "loss": 0.1115,
      "step": 451100
    },
    {
      "epoch": 2.9309168859008086,
      "grad_norm": 0.8012433648109436,
      "learning_rate": 2.2011606805972294e-05,
      "loss": 0.1109,
      "step": 451200
    },
    {
      "epoch": 2.9315664685439606,
      "grad_norm": 1.541256308555603,
      "learning_rate": 2.2004696347546304e-05,
      "loss": 0.102,
      "step": 451300
    },
    {
      "epoch": 2.9322160511871123,
      "grad_norm": 1.5516340732574463,
      "learning_rate": 2.1997785889120314e-05,
      "loss": 0.1049,
      "step": 451400
    },
    {
      "epoch": 2.9328656338302643,
      "grad_norm": 1.4153043031692505,
      "learning_rate": 2.199087543069432e-05,
      "loss": 0.11,
      "step": 451500
    },
    {
      "epoch": 2.933515216473416,
      "grad_norm": 1.0345494747161865,
      "learning_rate": 2.198396497226833e-05,
      "loss": 0.1027,
      "step": 451600
    },
    {
      "epoch": 2.9341647991165676,
      "grad_norm": 1.4154257774353027,
      "learning_rate": 2.197705451384234e-05,
      "loss": 0.1034,
      "step": 451700
    },
    {
      "epoch": 2.934814381759719,
      "grad_norm": 1.4674878120422363,
      "learning_rate": 2.197014405541635e-05,
      "loss": 0.099,
      "step": 451800
    },
    {
      "epoch": 2.9354639644028713,
      "grad_norm": 1.258116364479065,
      "learning_rate": 2.1963233596990356e-05,
      "loss": 0.1033,
      "step": 451900
    },
    {
      "epoch": 2.936113547046023,
      "grad_norm": 0.8994032740592957,
      "learning_rate": 2.1956323138564366e-05,
      "loss": 0.1036,
      "step": 452000
    },
    {
      "epoch": 2.936763129689175,
      "grad_norm": 1.1982793807983398,
      "learning_rate": 2.1949412680138376e-05,
      "loss": 0.1057,
      "step": 452100
    },
    {
      "epoch": 2.9374127123323266,
      "grad_norm": 0.6583281755447388,
      "learning_rate": 2.1942502221712386e-05,
      "loss": 0.1029,
      "step": 452200
    },
    {
      "epoch": 2.938062294975478,
      "grad_norm": 1.772207260131836,
      "learning_rate": 2.1935591763286396e-05,
      "loss": 0.1046,
      "step": 452300
    },
    {
      "epoch": 2.93871187761863,
      "grad_norm": 0.8330410122871399,
      "learning_rate": 2.1928681304860402e-05,
      "loss": 0.1061,
      "step": 452400
    },
    {
      "epoch": 2.939361460261782,
      "grad_norm": 1.3336126804351807,
      "learning_rate": 2.1921770846434412e-05,
      "loss": 0.1133,
      "step": 452500
    },
    {
      "epoch": 2.9400110429049335,
      "grad_norm": 1.1526585817337036,
      "learning_rate": 2.1914860388008422e-05,
      "loss": 0.1025,
      "step": 452600
    },
    {
      "epoch": 2.9406606255480856,
      "grad_norm": 1.5010459423065186,
      "learning_rate": 2.1907949929582432e-05,
      "loss": 0.1029,
      "step": 452700
    },
    {
      "epoch": 2.9413102081912372,
      "grad_norm": 1.2131366729736328,
      "learning_rate": 2.1901039471156438e-05,
      "loss": 0.1101,
      "step": 452800
    },
    {
      "epoch": 2.941959790834389,
      "grad_norm": 1.114760160446167,
      "learning_rate": 2.1894129012730448e-05,
      "loss": 0.1055,
      "step": 452900
    },
    {
      "epoch": 2.9426093734775405,
      "grad_norm": 0.9134335517883301,
      "learning_rate": 2.1887218554304455e-05,
      "loss": 0.1046,
      "step": 453000
    },
    {
      "epoch": 2.9432589561206925,
      "grad_norm": 1.6747000217437744,
      "learning_rate": 2.1880308095878464e-05,
      "loss": 0.1039,
      "step": 453100
    },
    {
      "epoch": 2.943908538763844,
      "grad_norm": 2.218865394592285,
      "learning_rate": 2.1873397637452474e-05,
      "loss": 0.1035,
      "step": 453200
    },
    {
      "epoch": 2.9445581214069962,
      "grad_norm": 0.9188686609268188,
      "learning_rate": 2.1866487179026484e-05,
      "loss": 0.1076,
      "step": 453300
    },
    {
      "epoch": 2.945207704050148,
      "grad_norm": 1.1365909576416016,
      "learning_rate": 2.185957672060049e-05,
      "loss": 0.1059,
      "step": 453400
    },
    {
      "epoch": 2.9458572866932995,
      "grad_norm": 0.44737696647644043,
      "learning_rate": 2.18526662621745e-05,
      "loss": 0.1031,
      "step": 453500
    },
    {
      "epoch": 2.946506869336451,
      "grad_norm": 0.8847010731697083,
      "learning_rate": 2.184575580374851e-05,
      "loss": 0.1073,
      "step": 453600
    },
    {
      "epoch": 2.947156451979603,
      "grad_norm": 0.6917091608047485,
      "learning_rate": 2.183884534532252e-05,
      "loss": 0.107,
      "step": 453700
    },
    {
      "epoch": 2.947806034622755,
      "grad_norm": 1.2391924858093262,
      "learning_rate": 2.1831934886896527e-05,
      "loss": 0.1074,
      "step": 453800
    },
    {
      "epoch": 2.948455617265907,
      "grad_norm": 0.8050551414489746,
      "learning_rate": 2.1825024428470536e-05,
      "loss": 0.1053,
      "step": 453900
    },
    {
      "epoch": 2.9491051999090585,
      "grad_norm": 0.825713038444519,
      "learning_rate": 2.1818113970044546e-05,
      "loss": 0.1047,
      "step": 454000
    },
    {
      "epoch": 2.94975478255221,
      "grad_norm": 0.9205541610717773,
      "learning_rate": 2.1811203511618556e-05,
      "loss": 0.1087,
      "step": 454100
    },
    {
      "epoch": 2.9504043651953618,
      "grad_norm": 1.4286748170852661,
      "learning_rate": 2.1804293053192566e-05,
      "loss": 0.103,
      "step": 454200
    },
    {
      "epoch": 2.951053947838514,
      "grad_norm": 1.0548243522644043,
      "learning_rate": 2.1797382594766572e-05,
      "loss": 0.1132,
      "step": 454300
    },
    {
      "epoch": 2.9517035304816654,
      "grad_norm": 1.575354814529419,
      "learning_rate": 2.1790472136340582e-05,
      "loss": 0.1066,
      "step": 454400
    },
    {
      "epoch": 2.9523531131248175,
      "grad_norm": 0.8296318650245667,
      "learning_rate": 2.1783561677914592e-05,
      "loss": 0.1039,
      "step": 454500
    },
    {
      "epoch": 2.953002695767969,
      "grad_norm": 1.6880037784576416,
      "learning_rate": 2.17766512194886e-05,
      "loss": 0.1062,
      "step": 454600
    },
    {
      "epoch": 2.9536522784111208,
      "grad_norm": 0.828359842300415,
      "learning_rate": 2.176974076106261e-05,
      "loss": 0.1059,
      "step": 454700
    },
    {
      "epoch": 2.9543018610542724,
      "grad_norm": 1.2039785385131836,
      "learning_rate": 2.1762830302636615e-05,
      "loss": 0.1058,
      "step": 454800
    },
    {
      "epoch": 2.9549514436974245,
      "grad_norm": 0.9368821382522583,
      "learning_rate": 2.1755919844210625e-05,
      "loss": 0.1076,
      "step": 454900
    },
    {
      "epoch": 2.955601026340576,
      "grad_norm": 0.8555988073348999,
      "learning_rate": 2.1749009385784635e-05,
      "loss": 0.1074,
      "step": 455000
    },
    {
      "epoch": 2.956250608983728,
      "grad_norm": 0.9469828009605408,
      "learning_rate": 2.1742098927358645e-05,
      "loss": 0.1035,
      "step": 455100
    },
    {
      "epoch": 2.95690019162688,
      "grad_norm": 0.8814883232116699,
      "learning_rate": 2.173518846893265e-05,
      "loss": 0.1083,
      "step": 455200
    },
    {
      "epoch": 2.9575497742700314,
      "grad_norm": 0.8447550535202026,
      "learning_rate": 2.172827801050666e-05,
      "loss": 0.107,
      "step": 455300
    },
    {
      "epoch": 2.958199356913183,
      "grad_norm": 1.0757004022598267,
      "learning_rate": 2.172136755208067e-05,
      "loss": 0.1,
      "step": 455400
    },
    {
      "epoch": 2.958848939556335,
      "grad_norm": 0.8389934301376343,
      "learning_rate": 2.171445709365468e-05,
      "loss": 0.1033,
      "step": 455500
    },
    {
      "epoch": 2.9594985221994867,
      "grad_norm": 1.4280486106872559,
      "learning_rate": 2.170754663522869e-05,
      "loss": 0.1113,
      "step": 455600
    },
    {
      "epoch": 2.960148104842639,
      "grad_norm": 0.7835720181465149,
      "learning_rate": 2.1700636176802697e-05,
      "loss": 0.1046,
      "step": 455700
    },
    {
      "epoch": 2.9607976874857904,
      "grad_norm": 0.9540650844573975,
      "learning_rate": 2.1693725718376707e-05,
      "loss": 0.1027,
      "step": 455800
    },
    {
      "epoch": 2.961447270128942,
      "grad_norm": 1.1263930797576904,
      "learning_rate": 2.1686815259950717e-05,
      "loss": 0.1024,
      "step": 455900
    },
    {
      "epoch": 2.9620968527720937,
      "grad_norm": 0.6465073823928833,
      "learning_rate": 2.1679904801524726e-05,
      "loss": 0.1072,
      "step": 456000
    },
    {
      "epoch": 2.9627464354152457,
      "grad_norm": 0.84434974193573,
      "learning_rate": 2.1672994343098736e-05,
      "loss": 0.107,
      "step": 456100
    },
    {
      "epoch": 2.9633960180583974,
      "grad_norm": 0.6277087330818176,
      "learning_rate": 2.1666083884672743e-05,
      "loss": 0.1044,
      "step": 456200
    },
    {
      "epoch": 2.9640456007015494,
      "grad_norm": 1.2222577333450317,
      "learning_rate": 2.1659173426246753e-05,
      "loss": 0.1069,
      "step": 456300
    },
    {
      "epoch": 2.964695183344701,
      "grad_norm": 1.6986061334609985,
      "learning_rate": 2.165226296782076e-05,
      "loss": 0.1019,
      "step": 456400
    },
    {
      "epoch": 2.9653447659878527,
      "grad_norm": 1.3519985675811768,
      "learning_rate": 2.164535250939477e-05,
      "loss": 0.1053,
      "step": 456500
    },
    {
      "epoch": 2.9659943486310043,
      "grad_norm": 1.313435435295105,
      "learning_rate": 2.1638442050968775e-05,
      "loss": 0.1048,
      "step": 456600
    },
    {
      "epoch": 2.9666439312741564,
      "grad_norm": 1.3662269115447998,
      "learning_rate": 2.1631531592542785e-05,
      "loss": 0.1028,
      "step": 456700
    },
    {
      "epoch": 2.967293513917308,
      "grad_norm": 1.1382811069488525,
      "learning_rate": 2.1624621134116795e-05,
      "loss": 0.1078,
      "step": 456800
    },
    {
      "epoch": 2.96794309656046,
      "grad_norm": 1.2646385431289673,
      "learning_rate": 2.1617710675690805e-05,
      "loss": 0.1103,
      "step": 456900
    },
    {
      "epoch": 2.9685926792036117,
      "grad_norm": 0.9572320580482483,
      "learning_rate": 2.1610800217264815e-05,
      "loss": 0.1072,
      "step": 457000
    },
    {
      "epoch": 2.9692422618467633,
      "grad_norm": 1.2274643182754517,
      "learning_rate": 2.160388975883882e-05,
      "loss": 0.1057,
      "step": 457100
    },
    {
      "epoch": 2.9698918444899154,
      "grad_norm": 1.0656245946884155,
      "learning_rate": 2.159697930041283e-05,
      "loss": 0.1087,
      "step": 457200
    },
    {
      "epoch": 2.970541427133067,
      "grad_norm": 1.1571589708328247,
      "learning_rate": 2.159006884198684e-05,
      "loss": 0.1044,
      "step": 457300
    },
    {
      "epoch": 2.9711910097762186,
      "grad_norm": 1.3449674844741821,
      "learning_rate": 2.158315838356085e-05,
      "loss": 0.1056,
      "step": 457400
    },
    {
      "epoch": 2.9718405924193707,
      "grad_norm": 1.2810711860656738,
      "learning_rate": 2.157624792513486e-05,
      "loss": 0.108,
      "step": 457500
    },
    {
      "epoch": 2.9724901750625223,
      "grad_norm": 1.358176589012146,
      "learning_rate": 2.1569337466708867e-05,
      "loss": 0.1095,
      "step": 457600
    },
    {
      "epoch": 2.973139757705674,
      "grad_norm": 1.0362547636032104,
      "learning_rate": 2.1562427008282877e-05,
      "loss": 0.1067,
      "step": 457700
    },
    {
      "epoch": 2.973789340348826,
      "grad_norm": 0.8938512802124023,
      "learning_rate": 2.1555516549856887e-05,
      "loss": 0.1079,
      "step": 457800
    },
    {
      "epoch": 2.9744389229919777,
      "grad_norm": 0.7692005038261414,
      "learning_rate": 2.1548606091430897e-05,
      "loss": 0.1035,
      "step": 457900
    },
    {
      "epoch": 2.9750885056351293,
      "grad_norm": 1.5057833194732666,
      "learning_rate": 2.1541695633004903e-05,
      "loss": 0.1082,
      "step": 458000
    },
    {
      "epoch": 2.9757380882782813,
      "grad_norm": 0.8456073999404907,
      "learning_rate": 2.1534785174578913e-05,
      "loss": 0.1047,
      "step": 458100
    },
    {
      "epoch": 2.976387670921433,
      "grad_norm": 1.0208375453948975,
      "learning_rate": 2.152787471615292e-05,
      "loss": 0.1041,
      "step": 458200
    },
    {
      "epoch": 2.9770372535645846,
      "grad_norm": 1.140566349029541,
      "learning_rate": 2.152096425772693e-05,
      "loss": 0.1059,
      "step": 458300
    },
    {
      "epoch": 2.9776868362077367,
      "grad_norm": 1.1318665742874146,
      "learning_rate": 2.151405379930094e-05,
      "loss": 0.1058,
      "step": 458400
    },
    {
      "epoch": 2.9783364188508883,
      "grad_norm": 1.2128180265426636,
      "learning_rate": 2.1507143340874946e-05,
      "loss": 0.102,
      "step": 458500
    },
    {
      "epoch": 2.97898600149404,
      "grad_norm": 1.0121248960494995,
      "learning_rate": 2.1500232882448955e-05,
      "loss": 0.1079,
      "step": 458600
    },
    {
      "epoch": 2.979635584137192,
      "grad_norm": 1.0784099102020264,
      "learning_rate": 2.1493322424022965e-05,
      "loss": 0.101,
      "step": 458700
    },
    {
      "epoch": 2.9802851667803436,
      "grad_norm": 1.2732501029968262,
      "learning_rate": 2.1486411965596975e-05,
      "loss": 0.1038,
      "step": 458800
    },
    {
      "epoch": 2.9809347494234952,
      "grad_norm": 1.07035231590271,
      "learning_rate": 2.1479501507170985e-05,
      "loss": 0.1066,
      "step": 458900
    },
    {
      "epoch": 2.9815843320666473,
      "grad_norm": 0.9261208772659302,
      "learning_rate": 2.147259104874499e-05,
      "loss": 0.114,
      "step": 459000
    },
    {
      "epoch": 2.982233914709799,
      "grad_norm": 1.1546403169631958,
      "learning_rate": 2.1465680590319e-05,
      "loss": 0.1121,
      "step": 459100
    },
    {
      "epoch": 2.982883497352951,
      "grad_norm": 0.665378987789154,
      "learning_rate": 2.145877013189301e-05,
      "loss": 0.1113,
      "step": 459200
    },
    {
      "epoch": 2.9835330799961026,
      "grad_norm": 1.1297454833984375,
      "learning_rate": 2.145185967346702e-05,
      "loss": 0.1088,
      "step": 459300
    },
    {
      "epoch": 2.9841826626392542,
      "grad_norm": 0.9061223864555359,
      "learning_rate": 2.144494921504103e-05,
      "loss": 0.1079,
      "step": 459400
    },
    {
      "epoch": 2.984832245282406,
      "grad_norm": 1.2042779922485352,
      "learning_rate": 2.1438038756615037e-05,
      "loss": 0.1047,
      "step": 459500
    },
    {
      "epoch": 2.985481827925558,
      "grad_norm": 1.0969820022583008,
      "learning_rate": 2.1431128298189047e-05,
      "loss": 0.1044,
      "step": 459600
    },
    {
      "epoch": 2.9861314105687096,
      "grad_norm": 1.5828922986984253,
      "learning_rate": 2.1424217839763057e-05,
      "loss": 0.112,
      "step": 459700
    },
    {
      "epoch": 2.9867809932118616,
      "grad_norm": 1.3164167404174805,
      "learning_rate": 2.1417307381337064e-05,
      "loss": 0.1041,
      "step": 459800
    },
    {
      "epoch": 2.9874305758550133,
      "grad_norm": 1.2829691171646118,
      "learning_rate": 2.1410396922911073e-05,
      "loss": 0.1035,
      "step": 459900
    },
    {
      "epoch": 2.988080158498165,
      "grad_norm": 1.1111828088760376,
      "learning_rate": 2.140348646448508e-05,
      "loss": 0.0972,
      "step": 460000
    },
    {
      "epoch": 2.9887297411413165,
      "grad_norm": 1.435919165611267,
      "learning_rate": 2.139657600605909e-05,
      "loss": 0.1089,
      "step": 460100
    },
    {
      "epoch": 2.9893793237844686,
      "grad_norm": 0.922753095626831,
      "learning_rate": 2.13896655476331e-05,
      "loss": 0.1031,
      "step": 460200
    },
    {
      "epoch": 2.99002890642762,
      "grad_norm": 0.8475651741027832,
      "learning_rate": 2.138275508920711e-05,
      "loss": 0.0996,
      "step": 460300
    },
    {
      "epoch": 2.9906784890707723,
      "grad_norm": 1.0901509523391724,
      "learning_rate": 2.1375844630781116e-05,
      "loss": 0.1064,
      "step": 460400
    },
    {
      "epoch": 2.991328071713924,
      "grad_norm": 0.8365611433982849,
      "learning_rate": 2.1368934172355126e-05,
      "loss": 0.1086,
      "step": 460500
    },
    {
      "epoch": 2.9919776543570755,
      "grad_norm": 0.9753214120864868,
      "learning_rate": 2.1362023713929136e-05,
      "loss": 0.1107,
      "step": 460600
    },
    {
      "epoch": 2.992627237000227,
      "grad_norm": 1.2689111232757568,
      "learning_rate": 2.1355113255503145e-05,
      "loss": 0.112,
      "step": 460700
    },
    {
      "epoch": 2.993276819643379,
      "grad_norm": 0.8406762480735779,
      "learning_rate": 2.1348202797077155e-05,
      "loss": 0.1009,
      "step": 460800
    },
    {
      "epoch": 2.993926402286531,
      "grad_norm": 1.5145078897476196,
      "learning_rate": 2.1341292338651162e-05,
      "loss": 0.1043,
      "step": 460900
    },
    {
      "epoch": 2.994575984929683,
      "grad_norm": 0.8859783411026001,
      "learning_rate": 2.133438188022517e-05,
      "loss": 0.1077,
      "step": 461000
    },
    {
      "epoch": 2.9952255675728345,
      "grad_norm": 0.7613955140113831,
      "learning_rate": 2.132747142179918e-05,
      "loss": 0.1064,
      "step": 461100
    },
    {
      "epoch": 2.995875150215986,
      "grad_norm": 1.2959749698638916,
      "learning_rate": 2.132056096337319e-05,
      "loss": 0.1052,
      "step": 461200
    },
    {
      "epoch": 2.996524732859138,
      "grad_norm": 1.4828715324401855,
      "learning_rate": 2.1313650504947198e-05,
      "loss": 0.1008,
      "step": 461300
    },
    {
      "epoch": 2.99717431550229,
      "grad_norm": 1.1502656936645508,
      "learning_rate": 2.1306740046521208e-05,
      "loss": 0.1066,
      "step": 461400
    },
    {
      "epoch": 2.9978238981454415,
      "grad_norm": 0.9155524969100952,
      "learning_rate": 2.1299829588095217e-05,
      "loss": 0.1098,
      "step": 461500
    },
    {
      "epoch": 2.9984734807885935,
      "grad_norm": 1.1501545906066895,
      "learning_rate": 2.1292919129669224e-05,
      "loss": 0.109,
      "step": 461600
    },
    {
      "epoch": 2.999123063431745,
      "grad_norm": 1.5295461416244507,
      "learning_rate": 2.1286008671243234e-05,
      "loss": 0.1094,
      "step": 461700
    },
    {
      "epoch": 2.999772646074897,
      "grad_norm": 0.8402360081672668,
      "learning_rate": 2.127909821281724e-05,
      "loss": 0.1038,
      "step": 461800
    },
    {
      "epoch": 3.000422228718049,
      "grad_norm": 1.6082793474197388,
      "learning_rate": 2.127218775439125e-05,
      "loss": 0.1043,
      "step": 461900
    },
    {
      "epoch": 3.0010718113612005,
      "grad_norm": 1.2250968217849731,
      "learning_rate": 2.126527729596526e-05,
      "loss": 0.1016,
      "step": 462000
    },
    {
      "epoch": 3.001721394004352,
      "grad_norm": 0.9449012279510498,
      "learning_rate": 2.125836683753927e-05,
      "loss": 0.102,
      "step": 462100
    },
    {
      "epoch": 3.002370976647504,
      "grad_norm": 0.6419175863265991,
      "learning_rate": 2.125145637911328e-05,
      "loss": 0.1059,
      "step": 462200
    },
    {
      "epoch": 3.003020559290656,
      "grad_norm": 1.0437742471694946,
      "learning_rate": 2.1244545920687286e-05,
      "loss": 0.1037,
      "step": 462300
    },
    {
      "epoch": 3.0036701419338074,
      "grad_norm": 0.6135749816894531,
      "learning_rate": 2.1237635462261296e-05,
      "loss": 0.101,
      "step": 462400
    },
    {
      "epoch": 3.0043197245769595,
      "grad_norm": 1.0167101621627808,
      "learning_rate": 2.1230725003835306e-05,
      "loss": 0.1024,
      "step": 462500
    },
    {
      "epoch": 3.004969307220111,
      "grad_norm": 0.8977813720703125,
      "learning_rate": 2.1223814545409316e-05,
      "loss": 0.1083,
      "step": 462600
    },
    {
      "epoch": 3.0056188898632628,
      "grad_norm": 1.0650216341018677,
      "learning_rate": 2.1216904086983326e-05,
      "loss": 0.1061,
      "step": 462700
    },
    {
      "epoch": 3.006268472506415,
      "grad_norm": 1.3761039972305298,
      "learning_rate": 2.1209993628557332e-05,
      "loss": 0.1053,
      "step": 462800
    },
    {
      "epoch": 3.0069180551495664,
      "grad_norm": 1.1890960931777954,
      "learning_rate": 2.1203083170131342e-05,
      "loss": 0.1045,
      "step": 462900
    },
    {
      "epoch": 3.007567637792718,
      "grad_norm": 1.1686121225357056,
      "learning_rate": 2.119617271170535e-05,
      "loss": 0.1032,
      "step": 463000
    },
    {
      "epoch": 3.00821722043587,
      "grad_norm": 0.8646482825279236,
      "learning_rate": 2.118926225327936e-05,
      "loss": 0.1036,
      "step": 463100
    },
    {
      "epoch": 3.0088668030790218,
      "grad_norm": 1.015485405921936,
      "learning_rate": 2.1182351794853368e-05,
      "loss": 0.1003,
      "step": 463200
    },
    {
      "epoch": 3.0095163857221734,
      "grad_norm": 0.8832438588142395,
      "learning_rate": 2.1175441336427378e-05,
      "loss": 0.1095,
      "step": 463300
    },
    {
      "epoch": 3.0101659683653255,
      "grad_norm": 1.5010576248168945,
      "learning_rate": 2.1168530878001384e-05,
      "loss": 0.1095,
      "step": 463400
    },
    {
      "epoch": 3.010815551008477,
      "grad_norm": 1.479380488395691,
      "learning_rate": 2.1161620419575394e-05,
      "loss": 0.1092,
      "step": 463500
    },
    {
      "epoch": 3.0114651336516287,
      "grad_norm": 0.7344821095466614,
      "learning_rate": 2.1154709961149404e-05,
      "loss": 0.1026,
      "step": 463600
    },
    {
      "epoch": 3.012114716294781,
      "grad_norm": 1.3126293420791626,
      "learning_rate": 2.114779950272341e-05,
      "loss": 0.1063,
      "step": 463700
    },
    {
      "epoch": 3.0127642989379324,
      "grad_norm": 0.9293903112411499,
      "learning_rate": 2.114088904429742e-05,
      "loss": 0.1068,
      "step": 463800
    },
    {
      "epoch": 3.013413881581084,
      "grad_norm": 1.377233624458313,
      "learning_rate": 2.113397858587143e-05,
      "loss": 0.1024,
      "step": 463900
    },
    {
      "epoch": 3.014063464224236,
      "grad_norm": 1.0602079629898071,
      "learning_rate": 2.112706812744544e-05,
      "loss": 0.1071,
      "step": 464000
    },
    {
      "epoch": 3.0147130468673877,
      "grad_norm": 1.2255274057388306,
      "learning_rate": 2.112015766901945e-05,
      "loss": 0.1081,
      "step": 464100
    },
    {
      "epoch": 3.0153626295105393,
      "grad_norm": 0.9569676518440247,
      "learning_rate": 2.1113247210593456e-05,
      "loss": 0.1091,
      "step": 464200
    },
    {
      "epoch": 3.0160122121536914,
      "grad_norm": 0.7730587720870972,
      "learning_rate": 2.1106336752167466e-05,
      "loss": 0.1037,
      "step": 464300
    },
    {
      "epoch": 3.016661794796843,
      "grad_norm": 1.0668493509292603,
      "learning_rate": 2.1099426293741476e-05,
      "loss": 0.104,
      "step": 464400
    },
    {
      "epoch": 3.0173113774399947,
      "grad_norm": 1.1306707859039307,
      "learning_rate": 2.1092515835315486e-05,
      "loss": 0.1026,
      "step": 464500
    },
    {
      "epoch": 3.0179609600831467,
      "grad_norm": 1.320291519165039,
      "learning_rate": 2.1085605376889492e-05,
      "loss": 0.1101,
      "step": 464600
    },
    {
      "epoch": 3.0186105427262984,
      "grad_norm": 1.0637762546539307,
      "learning_rate": 2.1078694918463502e-05,
      "loss": 0.103,
      "step": 464700
    },
    {
      "epoch": 3.01926012536945,
      "grad_norm": 1.1710368394851685,
      "learning_rate": 2.1071784460037512e-05,
      "loss": 0.1133,
      "step": 464800
    },
    {
      "epoch": 3.019909708012602,
      "grad_norm": 2.1507701873779297,
      "learning_rate": 2.1064874001611522e-05,
      "loss": 0.1076,
      "step": 464900
    },
    {
      "epoch": 3.0205592906557537,
      "grad_norm": 0.8879004716873169,
      "learning_rate": 2.105796354318553e-05,
      "loss": 0.1097,
      "step": 465000
    },
    {
      "epoch": 3.0212088732989053,
      "grad_norm": 0.9337167143821716,
      "learning_rate": 2.1051053084759538e-05,
      "loss": 0.1022,
      "step": 465100
    },
    {
      "epoch": 3.0218584559420574,
      "grad_norm": 1.6083558797836304,
      "learning_rate": 2.1044142626333545e-05,
      "loss": 0.104,
      "step": 465200
    },
    {
      "epoch": 3.022508038585209,
      "grad_norm": 1.0538595914840698,
      "learning_rate": 2.1037232167907555e-05,
      "loss": 0.1036,
      "step": 465300
    },
    {
      "epoch": 3.0231576212283606,
      "grad_norm": 1.0538671016693115,
      "learning_rate": 2.1030321709481564e-05,
      "loss": 0.1099,
      "step": 465400
    },
    {
      "epoch": 3.0238072038715127,
      "grad_norm": 0.8296588659286499,
      "learning_rate": 2.1023411251055574e-05,
      "loss": 0.1077,
      "step": 465500
    },
    {
      "epoch": 3.0244567865146643,
      "grad_norm": 1.391268253326416,
      "learning_rate": 2.101650079262958e-05,
      "loss": 0.1111,
      "step": 465600
    },
    {
      "epoch": 3.025106369157816,
      "grad_norm": 0.9766851663589478,
      "learning_rate": 2.100959033420359e-05,
      "loss": 0.1063,
      "step": 465700
    },
    {
      "epoch": 3.025755951800968,
      "grad_norm": 0.774461030960083,
      "learning_rate": 2.10026798757776e-05,
      "loss": 0.1029,
      "step": 465800
    },
    {
      "epoch": 3.0264055344441196,
      "grad_norm": 0.9870756268501282,
      "learning_rate": 2.099576941735161e-05,
      "loss": 0.1105,
      "step": 465900
    },
    {
      "epoch": 3.0270551170872713,
      "grad_norm": 0.8050683736801147,
      "learning_rate": 2.098885895892562e-05,
      "loss": 0.1003,
      "step": 466000
    },
    {
      "epoch": 3.0277046997304233,
      "grad_norm": 1.2667521238327026,
      "learning_rate": 2.0981948500499627e-05,
      "loss": 0.1051,
      "step": 466100
    },
    {
      "epoch": 3.028354282373575,
      "grad_norm": 1.2326451539993286,
      "learning_rate": 2.0975038042073636e-05,
      "loss": 0.105,
      "step": 466200
    },
    {
      "epoch": 3.0290038650167266,
      "grad_norm": 1.7926228046417236,
      "learning_rate": 2.0968127583647646e-05,
      "loss": 0.1053,
      "step": 466300
    },
    {
      "epoch": 3.0296534476598787,
      "grad_norm": 1.0100953578948975,
      "learning_rate": 2.0961217125221656e-05,
      "loss": 0.0997,
      "step": 466400
    },
    {
      "epoch": 3.0303030303030303,
      "grad_norm": 1.4579921960830688,
      "learning_rate": 2.0954306666795663e-05,
      "loss": 0.1045,
      "step": 466500
    },
    {
      "epoch": 3.030952612946182,
      "grad_norm": 1.3507986068725586,
      "learning_rate": 2.0947396208369672e-05,
      "loss": 0.1069,
      "step": 466600
    },
    {
      "epoch": 3.031602195589334,
      "grad_norm": 1.4513107538223267,
      "learning_rate": 2.0940485749943682e-05,
      "loss": 0.1087,
      "step": 466700
    },
    {
      "epoch": 3.0322517782324856,
      "grad_norm": 0.9617812037467957,
      "learning_rate": 2.093357529151769e-05,
      "loss": 0.107,
      "step": 466800
    },
    {
      "epoch": 3.032901360875637,
      "grad_norm": 0.7967174649238586,
      "learning_rate": 2.09266648330917e-05,
      "loss": 0.1029,
      "step": 466900
    },
    {
      "epoch": 3.0335509435187893,
      "grad_norm": 0.9220049381256104,
      "learning_rate": 2.0919754374665705e-05,
      "loss": 0.1042,
      "step": 467000
    },
    {
      "epoch": 3.034200526161941,
      "grad_norm": 1.4499708414077759,
      "learning_rate": 2.0912843916239715e-05,
      "loss": 0.1157,
      "step": 467100
    },
    {
      "epoch": 3.0348501088050925,
      "grad_norm": 1.2583963871002197,
      "learning_rate": 2.0905933457813725e-05,
      "loss": 0.1021,
      "step": 467200
    },
    {
      "epoch": 3.0354996914482446,
      "grad_norm": 1.9072293043136597,
      "learning_rate": 2.0899022999387735e-05,
      "loss": 0.1037,
      "step": 467300
    },
    {
      "epoch": 3.0361492740913962,
      "grad_norm": 1.0680360794067383,
      "learning_rate": 2.0892112540961745e-05,
      "loss": 0.1092,
      "step": 467400
    },
    {
      "epoch": 3.036798856734548,
      "grad_norm": 1.6086597442626953,
      "learning_rate": 2.088520208253575e-05,
      "loss": 0.1055,
      "step": 467500
    },
    {
      "epoch": 3.0374484393777,
      "grad_norm": 0.8009001016616821,
      "learning_rate": 2.087829162410976e-05,
      "loss": 0.1044,
      "step": 467600
    },
    {
      "epoch": 3.0380980220208516,
      "grad_norm": 1.0953904390335083,
      "learning_rate": 2.087138116568377e-05,
      "loss": 0.1074,
      "step": 467700
    },
    {
      "epoch": 3.038747604664003,
      "grad_norm": 1.5010789632797241,
      "learning_rate": 2.086447070725778e-05,
      "loss": 0.1119,
      "step": 467800
    },
    {
      "epoch": 3.0393971873071552,
      "grad_norm": 1.2081868648529053,
      "learning_rate": 2.0857560248831787e-05,
      "loss": 0.1106,
      "step": 467900
    },
    {
      "epoch": 3.040046769950307,
      "grad_norm": 1.3591210842132568,
      "learning_rate": 2.0850649790405797e-05,
      "loss": 0.0971,
      "step": 468000
    },
    {
      "epoch": 3.0406963525934585,
      "grad_norm": 1.3534533977508545,
      "learning_rate": 2.0843739331979807e-05,
      "loss": 0.1057,
      "step": 468100
    },
    {
      "epoch": 3.0413459352366106,
      "grad_norm": 0.8178310394287109,
      "learning_rate": 2.0836828873553817e-05,
      "loss": 0.1081,
      "step": 468200
    },
    {
      "epoch": 3.041995517879762,
      "grad_norm": 1.364798903465271,
      "learning_rate": 2.0829918415127826e-05,
      "loss": 0.1067,
      "step": 468300
    },
    {
      "epoch": 3.042645100522914,
      "grad_norm": 1.048951268196106,
      "learning_rate": 2.0823007956701833e-05,
      "loss": 0.1056,
      "step": 468400
    },
    {
      "epoch": 3.043294683166066,
      "grad_norm": 1.6041964292526245,
      "learning_rate": 2.0816097498275843e-05,
      "loss": 0.1053,
      "step": 468500
    },
    {
      "epoch": 3.0439442658092175,
      "grad_norm": 1.0610761642456055,
      "learning_rate": 2.080918703984985e-05,
      "loss": 0.107,
      "step": 468600
    },
    {
      "epoch": 3.0445938484523696,
      "grad_norm": 1.0827990770339966,
      "learning_rate": 2.080227658142386e-05,
      "loss": 0.1046,
      "step": 468700
    },
    {
      "epoch": 3.045243431095521,
      "grad_norm": 1.7904173135757446,
      "learning_rate": 2.079536612299787e-05,
      "loss": 0.1091,
      "step": 468800
    },
    {
      "epoch": 3.045893013738673,
      "grad_norm": 1.0567785501480103,
      "learning_rate": 2.0788455664571875e-05,
      "loss": 0.0949,
      "step": 468900
    },
    {
      "epoch": 3.0465425963818245,
      "grad_norm": 1.1007249355316162,
      "learning_rate": 2.0781545206145885e-05,
      "loss": 0.1095,
      "step": 469000
    },
    {
      "epoch": 3.0471921790249765,
      "grad_norm": 1.0903208255767822,
      "learning_rate": 2.0774634747719895e-05,
      "loss": 0.1076,
      "step": 469100
    },
    {
      "epoch": 3.047841761668128,
      "grad_norm": 1.0388301610946655,
      "learning_rate": 2.0767724289293905e-05,
      "loss": 0.1037,
      "step": 469200
    },
    {
      "epoch": 3.04849134431128,
      "grad_norm": 1.7525134086608887,
      "learning_rate": 2.076081383086791e-05,
      "loss": 0.1108,
      "step": 469300
    },
    {
      "epoch": 3.049140926954432,
      "grad_norm": 1.2493348121643066,
      "learning_rate": 2.075390337244192e-05,
      "loss": 0.1086,
      "step": 469400
    },
    {
      "epoch": 3.0497905095975835,
      "grad_norm": 1.2072300910949707,
      "learning_rate": 2.074699291401593e-05,
      "loss": 0.1065,
      "step": 469500
    },
    {
      "epoch": 3.0504400922407355,
      "grad_norm": 1.36625337600708,
      "learning_rate": 2.074008245558994e-05,
      "loss": 0.107,
      "step": 469600
    },
    {
      "epoch": 3.051089674883887,
      "grad_norm": 1.2796204090118408,
      "learning_rate": 2.073317199716395e-05,
      "loss": 0.1101,
      "step": 469700
    },
    {
      "epoch": 3.051739257527039,
      "grad_norm": 0.9461027383804321,
      "learning_rate": 2.0726261538737957e-05,
      "loss": 0.1074,
      "step": 469800
    },
    {
      "epoch": 3.052388840170191,
      "grad_norm": 0.9909328818321228,
      "learning_rate": 2.0719351080311967e-05,
      "loss": 0.1074,
      "step": 469900
    },
    {
      "epoch": 3.0530384228133425,
      "grad_norm": 0.9691495895385742,
      "learning_rate": 2.0712440621885977e-05,
      "loss": 0.1109,
      "step": 470000
    },
    {
      "epoch": 3.053688005456494,
      "grad_norm": 1.051688313484192,
      "learning_rate": 2.0705530163459987e-05,
      "loss": 0.1081,
      "step": 470100
    },
    {
      "epoch": 3.054337588099646,
      "grad_norm": 0.8337419629096985,
      "learning_rate": 2.0698619705033993e-05,
      "loss": 0.1079,
      "step": 470200
    },
    {
      "epoch": 3.054987170742798,
      "grad_norm": 1.3243135213851929,
      "learning_rate": 2.0691709246608003e-05,
      "loss": 0.1062,
      "step": 470300
    },
    {
      "epoch": 3.0556367533859494,
      "grad_norm": 0.743111789226532,
      "learning_rate": 2.068479878818201e-05,
      "loss": 0.1044,
      "step": 470400
    },
    {
      "epoch": 3.0562863360291015,
      "grad_norm": 1.189510703086853,
      "learning_rate": 2.067788832975602e-05,
      "loss": 0.1041,
      "step": 470500
    },
    {
      "epoch": 3.056935918672253,
      "grad_norm": 1.0047515630722046,
      "learning_rate": 2.067097787133003e-05,
      "loss": 0.0987,
      "step": 470600
    },
    {
      "epoch": 3.0575855013154047,
      "grad_norm": 1.3630887269973755,
      "learning_rate": 2.066406741290404e-05,
      "loss": 0.1081,
      "step": 470700
    },
    {
      "epoch": 3.058235083958557,
      "grad_norm": 1.5533462762832642,
      "learning_rate": 2.0657156954478046e-05,
      "loss": 0.1089,
      "step": 470800
    },
    {
      "epoch": 3.0588846666017084,
      "grad_norm": 1.074666142463684,
      "learning_rate": 2.0650246496052055e-05,
      "loss": 0.1076,
      "step": 470900
    },
    {
      "epoch": 3.05953424924486,
      "grad_norm": 0.9247144460678101,
      "learning_rate": 2.0643336037626065e-05,
      "loss": 0.1072,
      "step": 471000
    },
    {
      "epoch": 3.060183831888012,
      "grad_norm": 0.9101737141609192,
      "learning_rate": 2.0636425579200075e-05,
      "loss": 0.1102,
      "step": 471100
    },
    {
      "epoch": 3.0608334145311638,
      "grad_norm": 1.5075089931488037,
      "learning_rate": 2.062951512077408e-05,
      "loss": 0.1049,
      "step": 471200
    },
    {
      "epoch": 3.0614829971743154,
      "grad_norm": 1.0494475364685059,
      "learning_rate": 2.062260466234809e-05,
      "loss": 0.1014,
      "step": 471300
    },
    {
      "epoch": 3.0621325798174674,
      "grad_norm": 1.241323471069336,
      "learning_rate": 2.06156942039221e-05,
      "loss": 0.1076,
      "step": 471400
    },
    {
      "epoch": 3.062782162460619,
      "grad_norm": 1.2191433906555176,
      "learning_rate": 2.060878374549611e-05,
      "loss": 0.1032,
      "step": 471500
    },
    {
      "epoch": 3.0634317451037707,
      "grad_norm": 1.3865277767181396,
      "learning_rate": 2.060187328707012e-05,
      "loss": 0.1074,
      "step": 471600
    },
    {
      "epoch": 3.0640813277469228,
      "grad_norm": 1.3548731803894043,
      "learning_rate": 2.0594962828644127e-05,
      "loss": 0.1085,
      "step": 471700
    },
    {
      "epoch": 3.0647309103900744,
      "grad_norm": 1.11762535572052,
      "learning_rate": 2.0588052370218137e-05,
      "loss": 0.1037,
      "step": 471800
    },
    {
      "epoch": 3.065380493033226,
      "grad_norm": 0.9712014198303223,
      "learning_rate": 2.0581141911792147e-05,
      "loss": 0.1039,
      "step": 471900
    },
    {
      "epoch": 3.066030075676378,
      "grad_norm": 1.527105689048767,
      "learning_rate": 2.0574231453366154e-05,
      "loss": 0.105,
      "step": 472000
    },
    {
      "epoch": 3.0666796583195297,
      "grad_norm": 0.967963457107544,
      "learning_rate": 2.0567320994940164e-05,
      "loss": 0.1094,
      "step": 472100
    },
    {
      "epoch": 3.0673292409626813,
      "grad_norm": 1.2616444826126099,
      "learning_rate": 2.056041053651417e-05,
      "loss": 0.1048,
      "step": 472200
    },
    {
      "epoch": 3.0679788236058334,
      "grad_norm": 1.6081947088241577,
      "learning_rate": 2.055350007808818e-05,
      "loss": 0.1017,
      "step": 472300
    },
    {
      "epoch": 3.068628406248985,
      "grad_norm": 0.9125831127166748,
      "learning_rate": 2.054658961966219e-05,
      "loss": 0.1093,
      "step": 472400
    },
    {
      "epoch": 3.0692779888921367,
      "grad_norm": 0.9261736869812012,
      "learning_rate": 2.05396791612362e-05,
      "loss": 0.1134,
      "step": 472500
    },
    {
      "epoch": 3.0699275715352887,
      "grad_norm": 1.1678966283798218,
      "learning_rate": 2.0532768702810206e-05,
      "loss": 0.1107,
      "step": 472600
    },
    {
      "epoch": 3.0705771541784403,
      "grad_norm": 1.2424335479736328,
      "learning_rate": 2.0525858244384216e-05,
      "loss": 0.1059,
      "step": 472700
    },
    {
      "epoch": 3.071226736821592,
      "grad_norm": 0.8764585852622986,
      "learning_rate": 2.0518947785958226e-05,
      "loss": 0.1105,
      "step": 472800
    },
    {
      "epoch": 3.071876319464744,
      "grad_norm": 0.9117241501808167,
      "learning_rate": 2.0512037327532236e-05,
      "loss": 0.0998,
      "step": 472900
    },
    {
      "epoch": 3.0725259021078957,
      "grad_norm": 1.0466008186340332,
      "learning_rate": 2.0505126869106245e-05,
      "loss": 0.1023,
      "step": 473000
    },
    {
      "epoch": 3.0731754847510473,
      "grad_norm": 0.9114276766777039,
      "learning_rate": 2.0498216410680252e-05,
      "loss": 0.1043,
      "step": 473100
    },
    {
      "epoch": 3.0738250673941994,
      "grad_norm": 1.0188831090927124,
      "learning_rate": 2.0491305952254262e-05,
      "loss": 0.1033,
      "step": 473200
    },
    {
      "epoch": 3.074474650037351,
      "grad_norm": 2.124366283416748,
      "learning_rate": 2.048439549382827e-05,
      "loss": 0.1061,
      "step": 473300
    },
    {
      "epoch": 3.0751242326805026,
      "grad_norm": 1.2021740674972534,
      "learning_rate": 2.047748503540228e-05,
      "loss": 0.1048,
      "step": 473400
    },
    {
      "epoch": 3.0757738153236547,
      "grad_norm": 1.5058962106704712,
      "learning_rate": 2.047057457697629e-05,
      "loss": 0.1051,
      "step": 473500
    },
    {
      "epoch": 3.0764233979668063,
      "grad_norm": 0.986780047416687,
      "learning_rate": 2.0463664118550298e-05,
      "loss": 0.1003,
      "step": 473600
    },
    {
      "epoch": 3.077072980609958,
      "grad_norm": 1.3643310070037842,
      "learning_rate": 2.0456753660124308e-05,
      "loss": 0.1057,
      "step": 473700
    },
    {
      "epoch": 3.07772256325311,
      "grad_norm": 1.2944042682647705,
      "learning_rate": 2.0449843201698314e-05,
      "loss": 0.0985,
      "step": 473800
    },
    {
      "epoch": 3.0783721458962616,
      "grad_norm": 0.8026634454727173,
      "learning_rate": 2.0442932743272324e-05,
      "loss": 0.1052,
      "step": 473900
    },
    {
      "epoch": 3.0790217285394132,
      "grad_norm": 1.4757304191589355,
      "learning_rate": 2.043602228484633e-05,
      "loss": 0.1031,
      "step": 474000
    },
    {
      "epoch": 3.0796713111825653,
      "grad_norm": 0.9608644247055054,
      "learning_rate": 2.042911182642034e-05,
      "loss": 0.103,
      "step": 474100
    },
    {
      "epoch": 3.080320893825717,
      "grad_norm": 1.2684334516525269,
      "learning_rate": 2.042220136799435e-05,
      "loss": 0.1078,
      "step": 474200
    },
    {
      "epoch": 3.0809704764688686,
      "grad_norm": 1.0804975032806396,
      "learning_rate": 2.041529090956836e-05,
      "loss": 0.1064,
      "step": 474300
    },
    {
      "epoch": 3.0816200591120206,
      "grad_norm": 1.4285900592803955,
      "learning_rate": 2.040838045114237e-05,
      "loss": 0.1029,
      "step": 474400
    },
    {
      "epoch": 3.0822696417551723,
      "grad_norm": 1.038615107536316,
      "learning_rate": 2.0401469992716376e-05,
      "loss": 0.105,
      "step": 474500
    },
    {
      "epoch": 3.082919224398324,
      "grad_norm": 1.096238374710083,
      "learning_rate": 2.0394559534290386e-05,
      "loss": 0.1081,
      "step": 474600
    },
    {
      "epoch": 3.083568807041476,
      "grad_norm": 0.9103721380233765,
      "learning_rate": 2.0387649075864396e-05,
      "loss": 0.105,
      "step": 474700
    },
    {
      "epoch": 3.0842183896846276,
      "grad_norm": 1.1102041006088257,
      "learning_rate": 2.0380738617438406e-05,
      "loss": 0.1075,
      "step": 474800
    },
    {
      "epoch": 3.084867972327779,
      "grad_norm": 0.7604531645774841,
      "learning_rate": 2.0373828159012416e-05,
      "loss": 0.1022,
      "step": 474900
    },
    {
      "epoch": 3.0855175549709313,
      "grad_norm": 1.3025723695755005,
      "learning_rate": 2.0366917700586422e-05,
      "loss": 0.1006,
      "step": 475000
    },
    {
      "epoch": 3.086167137614083,
      "grad_norm": 1.6154394149780273,
      "learning_rate": 2.0360007242160432e-05,
      "loss": 0.105,
      "step": 475100
    },
    {
      "epoch": 3.0868167202572345,
      "grad_norm": 0.9591647386550903,
      "learning_rate": 2.0353096783734442e-05,
      "loss": 0.1051,
      "step": 475200
    },
    {
      "epoch": 3.0874663029003866,
      "grad_norm": 1.2104854583740234,
      "learning_rate": 2.034618632530845e-05,
      "loss": 0.1122,
      "step": 475300
    },
    {
      "epoch": 3.088115885543538,
      "grad_norm": 0.7433031797409058,
      "learning_rate": 2.0339275866882458e-05,
      "loss": 0.1026,
      "step": 475400
    },
    {
      "epoch": 3.08876546818669,
      "grad_norm": 2.0979130268096924,
      "learning_rate": 2.0332365408456468e-05,
      "loss": 0.1025,
      "step": 475500
    },
    {
      "epoch": 3.089415050829842,
      "grad_norm": 0.8803223371505737,
      "learning_rate": 2.0325454950030474e-05,
      "loss": 0.1087,
      "step": 475600
    },
    {
      "epoch": 3.0900646334729935,
      "grad_norm": 0.846430778503418,
      "learning_rate": 2.0318544491604484e-05,
      "loss": 0.107,
      "step": 475700
    },
    {
      "epoch": 3.090714216116145,
      "grad_norm": 1.900098443031311,
      "learning_rate": 2.0311634033178494e-05,
      "loss": 0.1019,
      "step": 475800
    },
    {
      "epoch": 3.0913637987592972,
      "grad_norm": 1.2157999277114868,
      "learning_rate": 2.03047235747525e-05,
      "loss": 0.1056,
      "step": 475900
    },
    {
      "epoch": 3.092013381402449,
      "grad_norm": 1.161733627319336,
      "learning_rate": 2.029781311632651e-05,
      "loss": 0.1011,
      "step": 476000
    },
    {
      "epoch": 3.0926629640456005,
      "grad_norm": 1.433791995048523,
      "learning_rate": 2.029090265790052e-05,
      "loss": 0.0953,
      "step": 476100
    },
    {
      "epoch": 3.0933125466887526,
      "grad_norm": 1.1366546154022217,
      "learning_rate": 2.028399219947453e-05,
      "loss": 0.1077,
      "step": 476200
    },
    {
      "epoch": 3.093962129331904,
      "grad_norm": 0.8629484176635742,
      "learning_rate": 2.027708174104854e-05,
      "loss": 0.1053,
      "step": 476300
    },
    {
      "epoch": 3.0946117119750562,
      "grad_norm": 1.223514437675476,
      "learning_rate": 2.0270171282622546e-05,
      "loss": 0.1065,
      "step": 476400
    },
    {
      "epoch": 3.095261294618208,
      "grad_norm": 1.0602169036865234,
      "learning_rate": 2.0263260824196556e-05,
      "loss": 0.1017,
      "step": 476500
    },
    {
      "epoch": 3.0959108772613595,
      "grad_norm": 1.3594019412994385,
      "learning_rate": 2.0256350365770566e-05,
      "loss": 0.1087,
      "step": 476600
    },
    {
      "epoch": 3.0965604599045116,
      "grad_norm": 1.5413085222244263,
      "learning_rate": 2.0249439907344576e-05,
      "loss": 0.1107,
      "step": 476700
    },
    {
      "epoch": 3.097210042547663,
      "grad_norm": 0.9439612627029419,
      "learning_rate": 2.0242529448918586e-05,
      "loss": 0.1046,
      "step": 476800
    },
    {
      "epoch": 3.097859625190815,
      "grad_norm": 1.3372610807418823,
      "learning_rate": 2.0235618990492592e-05,
      "loss": 0.1055,
      "step": 476900
    },
    {
      "epoch": 3.098509207833967,
      "grad_norm": 1.3957477807998657,
      "learning_rate": 2.0228708532066602e-05,
      "loss": 0.1091,
      "step": 477000
    },
    {
      "epoch": 3.0991587904771185,
      "grad_norm": 1.7973487377166748,
      "learning_rate": 2.0221798073640612e-05,
      "loss": 0.1048,
      "step": 477100
    },
    {
      "epoch": 3.09980837312027,
      "grad_norm": 1.0736675262451172,
      "learning_rate": 2.021488761521462e-05,
      "loss": 0.1038,
      "step": 477200
    },
    {
      "epoch": 3.100457955763422,
      "grad_norm": 0.9520107507705688,
      "learning_rate": 2.020797715678863e-05,
      "loss": 0.1038,
      "step": 477300
    },
    {
      "epoch": 3.101107538406574,
      "grad_norm": 0.975243330001831,
      "learning_rate": 2.0201066698362635e-05,
      "loss": 0.0973,
      "step": 477400
    },
    {
      "epoch": 3.1017571210497255,
      "grad_norm": 0.9746682643890381,
      "learning_rate": 2.0194156239936645e-05,
      "loss": 0.1066,
      "step": 477500
    },
    {
      "epoch": 3.1024067036928775,
      "grad_norm": 1.0932458639144897,
      "learning_rate": 2.0187245781510655e-05,
      "loss": 0.1061,
      "step": 477600
    },
    {
      "epoch": 3.103056286336029,
      "grad_norm": 1.2351242303848267,
      "learning_rate": 2.0180335323084664e-05,
      "loss": 0.1072,
      "step": 477700
    },
    {
      "epoch": 3.1037058689791808,
      "grad_norm": 1.2630579471588135,
      "learning_rate": 2.017342486465867e-05,
      "loss": 0.1102,
      "step": 477800
    },
    {
      "epoch": 3.104355451622333,
      "grad_norm": 1.0669084787368774,
      "learning_rate": 2.016651440623268e-05,
      "loss": 0.1071,
      "step": 477900
    },
    {
      "epoch": 3.1050050342654845,
      "grad_norm": 1.6228598356246948,
      "learning_rate": 2.015960394780669e-05,
      "loss": 0.1028,
      "step": 478000
    },
    {
      "epoch": 3.105654616908636,
      "grad_norm": 1.4099394083023071,
      "learning_rate": 2.01526934893807e-05,
      "loss": 0.1023,
      "step": 478100
    },
    {
      "epoch": 3.106304199551788,
      "grad_norm": 1.050078272819519,
      "learning_rate": 2.014578303095471e-05,
      "loss": 0.1026,
      "step": 478200
    },
    {
      "epoch": 3.10695378219494,
      "grad_norm": 1.1080288887023926,
      "learning_rate": 2.0138872572528717e-05,
      "loss": 0.1006,
      "step": 478300
    },
    {
      "epoch": 3.1076033648380914,
      "grad_norm": 1.2491190433502197,
      "learning_rate": 2.0131962114102727e-05,
      "loss": 0.1072,
      "step": 478400
    },
    {
      "epoch": 3.1082529474812435,
      "grad_norm": 1.1841399669647217,
      "learning_rate": 2.0125051655676736e-05,
      "loss": 0.098,
      "step": 478500
    },
    {
      "epoch": 3.108902530124395,
      "grad_norm": 1.2622581720352173,
      "learning_rate": 2.0118141197250746e-05,
      "loss": 0.1073,
      "step": 478600
    },
    {
      "epoch": 3.1095521127675467,
      "grad_norm": 0.8004284501075745,
      "learning_rate": 2.0111230738824756e-05,
      "loss": 0.1019,
      "step": 478700
    },
    {
      "epoch": 3.110201695410699,
      "grad_norm": 0.9613120555877686,
      "learning_rate": 2.0104320280398763e-05,
      "loss": 0.1067,
      "step": 478800
    },
    {
      "epoch": 3.1108512780538504,
      "grad_norm": 1.229121208190918,
      "learning_rate": 2.0097409821972772e-05,
      "loss": 0.1055,
      "step": 478900
    },
    {
      "epoch": 3.111500860697002,
      "grad_norm": 1.4726556539535522,
      "learning_rate": 2.009049936354678e-05,
      "loss": 0.0995,
      "step": 479000
    },
    {
      "epoch": 3.112150443340154,
      "grad_norm": 0.8196647763252258,
      "learning_rate": 2.008358890512079e-05,
      "loss": 0.1044,
      "step": 479100
    },
    {
      "epoch": 3.1128000259833057,
      "grad_norm": 0.7650165557861328,
      "learning_rate": 2.0076678446694795e-05,
      "loss": 0.1043,
      "step": 479200
    },
    {
      "epoch": 3.1134496086264574,
      "grad_norm": 0.8481762409210205,
      "learning_rate": 2.0069767988268805e-05,
      "loss": 0.1045,
      "step": 479300
    },
    {
      "epoch": 3.1140991912696094,
      "grad_norm": 1.5362375974655151,
      "learning_rate": 2.0062857529842815e-05,
      "loss": 0.1054,
      "step": 479400
    },
    {
      "epoch": 3.114748773912761,
      "grad_norm": 1.1863700151443481,
      "learning_rate": 2.0055947071416825e-05,
      "loss": 0.1047,
      "step": 479500
    },
    {
      "epoch": 3.1153983565559127,
      "grad_norm": 1.0282492637634277,
      "learning_rate": 2.0049036612990835e-05,
      "loss": 0.1042,
      "step": 479600
    },
    {
      "epoch": 3.1160479391990648,
      "grad_norm": 1.1159074306488037,
      "learning_rate": 2.004212615456484e-05,
      "loss": 0.1075,
      "step": 479700
    },
    {
      "epoch": 3.1166975218422164,
      "grad_norm": 1.3226650953292847,
      "learning_rate": 2.003521569613885e-05,
      "loss": 0.1035,
      "step": 479800
    },
    {
      "epoch": 3.117347104485368,
      "grad_norm": 1.287704586982727,
      "learning_rate": 2.002830523771286e-05,
      "loss": 0.1031,
      "step": 479900
    },
    {
      "epoch": 3.11799668712852,
      "grad_norm": 0.7352117896080017,
      "learning_rate": 2.002139477928687e-05,
      "loss": 0.1011,
      "step": 480000
    },
    {
      "epoch": 3.1186462697716717,
      "grad_norm": 1.2649799585342407,
      "learning_rate": 2.001448432086088e-05,
      "loss": 0.1106,
      "step": 480100
    },
    {
      "epoch": 3.1192958524148233,
      "grad_norm": 1.0160032510757446,
      "learning_rate": 2.0007573862434887e-05,
      "loss": 0.107,
      "step": 480200
    },
    {
      "epoch": 3.1199454350579754,
      "grad_norm": 1.178067922592163,
      "learning_rate": 2.0000663404008897e-05,
      "loss": 0.1038,
      "step": 480300
    },
    {
      "epoch": 3.120595017701127,
      "grad_norm": 1.0960001945495605,
      "learning_rate": 1.9993752945582907e-05,
      "loss": 0.1045,
      "step": 480400
    },
    {
      "epoch": 3.1212446003442786,
      "grad_norm": 0.8544372916221619,
      "learning_rate": 1.9986842487156917e-05,
      "loss": 0.1024,
      "step": 480500
    },
    {
      "epoch": 3.1218941829874307,
      "grad_norm": 1.1623198986053467,
      "learning_rate": 1.9979932028730923e-05,
      "loss": 0.1022,
      "step": 480600
    },
    {
      "epoch": 3.1225437656305823,
      "grad_norm": 1.4229415655136108,
      "learning_rate": 1.9973021570304933e-05,
      "loss": 0.1087,
      "step": 480700
    },
    {
      "epoch": 3.123193348273734,
      "grad_norm": 0.9321449398994446,
      "learning_rate": 1.996611111187894e-05,
      "loss": 0.1002,
      "step": 480800
    },
    {
      "epoch": 3.123842930916886,
      "grad_norm": 0.9464974999427795,
      "learning_rate": 1.995920065345295e-05,
      "loss": 0.0976,
      "step": 480900
    },
    {
      "epoch": 3.1244925135600377,
      "grad_norm": 1.0902202129364014,
      "learning_rate": 1.995229019502696e-05,
      "loss": 0.1061,
      "step": 481000
    },
    {
      "epoch": 3.1251420962031893,
      "grad_norm": 0.8197698593139648,
      "learning_rate": 1.9945379736600965e-05,
      "loss": 0.1036,
      "step": 481100
    },
    {
      "epoch": 3.1257916788463413,
      "grad_norm": 0.693719208240509,
      "learning_rate": 1.9938469278174975e-05,
      "loss": 0.1048,
      "step": 481200
    },
    {
      "epoch": 3.126441261489493,
      "grad_norm": 1.0827547311782837,
      "learning_rate": 1.9931558819748985e-05,
      "loss": 0.1102,
      "step": 481300
    },
    {
      "epoch": 3.1270908441326446,
      "grad_norm": 1.068265676498413,
      "learning_rate": 1.9924648361322995e-05,
      "loss": 0.1033,
      "step": 481400
    },
    {
      "epoch": 3.1277404267757967,
      "grad_norm": 1.4501136541366577,
      "learning_rate": 1.9917737902897005e-05,
      "loss": 0.1079,
      "step": 481500
    },
    {
      "epoch": 3.1283900094189483,
      "grad_norm": 1.1824088096618652,
      "learning_rate": 1.991082744447101e-05,
      "loss": 0.1071,
      "step": 481600
    },
    {
      "epoch": 3.1290395920621,
      "grad_norm": 0.9887948632240295,
      "learning_rate": 1.990391698604502e-05,
      "loss": 0.1029,
      "step": 481700
    },
    {
      "epoch": 3.129689174705252,
      "grad_norm": 1.3229689598083496,
      "learning_rate": 1.989700652761903e-05,
      "loss": 0.0994,
      "step": 481800
    },
    {
      "epoch": 3.1303387573484036,
      "grad_norm": 1.223770022392273,
      "learning_rate": 1.989009606919304e-05,
      "loss": 0.1049,
      "step": 481900
    },
    {
      "epoch": 3.1309883399915552,
      "grad_norm": 1.1459978818893433,
      "learning_rate": 1.9883185610767047e-05,
      "loss": 0.1012,
      "step": 482000
    },
    {
      "epoch": 3.1316379226347073,
      "grad_norm": 0.8014498353004456,
      "learning_rate": 1.9876275152341057e-05,
      "loss": 0.1087,
      "step": 482100
    },
    {
      "epoch": 3.132287505277859,
      "grad_norm": 0.9150066375732422,
      "learning_rate": 1.9869364693915067e-05,
      "loss": 0.0999,
      "step": 482200
    },
    {
      "epoch": 3.1329370879210106,
      "grad_norm": 1.1757327318191528,
      "learning_rate": 1.9862454235489077e-05,
      "loss": 0.1046,
      "step": 482300
    },
    {
      "epoch": 3.1335866705641626,
      "grad_norm": 1.2413374185562134,
      "learning_rate": 1.9855543777063083e-05,
      "loss": 0.1096,
      "step": 482400
    },
    {
      "epoch": 3.1342362532073142,
      "grad_norm": 1.0410244464874268,
      "learning_rate": 1.984863331863709e-05,
      "loss": 0.1056,
      "step": 482500
    },
    {
      "epoch": 3.134885835850466,
      "grad_norm": 1.2036749124526978,
      "learning_rate": 1.98417228602111e-05,
      "loss": 0.1064,
      "step": 482600
    },
    {
      "epoch": 3.135535418493618,
      "grad_norm": 1.1102896928787231,
      "learning_rate": 1.983481240178511e-05,
      "loss": 0.1003,
      "step": 482700
    },
    {
      "epoch": 3.1361850011367696,
      "grad_norm": 1.3462518453598022,
      "learning_rate": 1.982790194335912e-05,
      "loss": 0.109,
      "step": 482800
    },
    {
      "epoch": 3.136834583779921,
      "grad_norm": 0.9251030087471008,
      "learning_rate": 1.982099148493313e-05,
      "loss": 0.109,
      "step": 482900
    },
    {
      "epoch": 3.1374841664230733,
      "grad_norm": 1.3609740734100342,
      "learning_rate": 1.9814081026507136e-05,
      "loss": 0.1098,
      "step": 483000
    },
    {
      "epoch": 3.138133749066225,
      "grad_norm": 0.6622353196144104,
      "learning_rate": 1.9807170568081146e-05,
      "loss": 0.1049,
      "step": 483100
    },
    {
      "epoch": 3.1387833317093765,
      "grad_norm": 1.2140082120895386,
      "learning_rate": 1.9800260109655155e-05,
      "loss": 0.1049,
      "step": 483200
    },
    {
      "epoch": 3.1394329143525286,
      "grad_norm": 0.8038175106048584,
      "learning_rate": 1.9793349651229165e-05,
      "loss": 0.1037,
      "step": 483300
    },
    {
      "epoch": 3.14008249699568,
      "grad_norm": 1.2431752681732178,
      "learning_rate": 1.9786439192803175e-05,
      "loss": 0.0952,
      "step": 483400
    },
    {
      "epoch": 3.1407320796388323,
      "grad_norm": 1.4950058460235596,
      "learning_rate": 1.977952873437718e-05,
      "loss": 0.1007,
      "step": 483500
    },
    {
      "epoch": 3.141381662281984,
      "grad_norm": 0.8126383423805237,
      "learning_rate": 1.977261827595119e-05,
      "loss": 0.1094,
      "step": 483600
    },
    {
      "epoch": 3.1420312449251355,
      "grad_norm": 1.2458754777908325,
      "learning_rate": 1.97657078175252e-05,
      "loss": 0.1091,
      "step": 483700
    },
    {
      "epoch": 3.142680827568287,
      "grad_norm": 0.9030396938323975,
      "learning_rate": 1.975879735909921e-05,
      "loss": 0.1048,
      "step": 483800
    },
    {
      "epoch": 3.143330410211439,
      "grad_norm": 0.9135504364967346,
      "learning_rate": 1.9751886900673218e-05,
      "loss": 0.1051,
      "step": 483900
    },
    {
      "epoch": 3.143979992854591,
      "grad_norm": 1.0216132402420044,
      "learning_rate": 1.9744976442247227e-05,
      "loss": 0.1039,
      "step": 484000
    },
    {
      "epoch": 3.144629575497743,
      "grad_norm": 0.48504966497421265,
      "learning_rate": 1.9738065983821234e-05,
      "loss": 0.1008,
      "step": 484100
    },
    {
      "epoch": 3.1452791581408945,
      "grad_norm": 0.8935724496841431,
      "learning_rate": 1.9731155525395244e-05,
      "loss": 0.1042,
      "step": 484200
    },
    {
      "epoch": 3.145928740784046,
      "grad_norm": 1.2674591541290283,
      "learning_rate": 1.9724245066969254e-05,
      "loss": 0.1072,
      "step": 484300
    },
    {
      "epoch": 3.1465783234271982,
      "grad_norm": 1.404130220413208,
      "learning_rate": 1.971733460854326e-05,
      "loss": 0.0999,
      "step": 484400
    },
    {
      "epoch": 3.14722790607035,
      "grad_norm": 1.347108006477356,
      "learning_rate": 1.971042415011727e-05,
      "loss": 0.1087,
      "step": 484500
    },
    {
      "epoch": 3.1478774887135015,
      "grad_norm": 0.8678277134895325,
      "learning_rate": 1.970351369169128e-05,
      "loss": 0.1035,
      "step": 484600
    },
    {
      "epoch": 3.1485270713566536,
      "grad_norm": 1.488468050956726,
      "learning_rate": 1.969660323326529e-05,
      "loss": 0.1003,
      "step": 484700
    },
    {
      "epoch": 3.149176653999805,
      "grad_norm": 0.8439023494720459,
      "learning_rate": 1.96896927748393e-05,
      "loss": 0.1035,
      "step": 484800
    },
    {
      "epoch": 3.149826236642957,
      "grad_norm": 0.8051586151123047,
      "learning_rate": 1.9682782316413306e-05,
      "loss": 0.1025,
      "step": 484900
    },
    {
      "epoch": 3.150475819286109,
      "grad_norm": 1.2639602422714233,
      "learning_rate": 1.9675871857987316e-05,
      "loss": 0.1008,
      "step": 485000
    },
    {
      "epoch": 3.1511254019292605,
      "grad_norm": 1.3991628885269165,
      "learning_rate": 1.9668961399561326e-05,
      "loss": 0.1024,
      "step": 485100
    },
    {
      "epoch": 3.151774984572412,
      "grad_norm": 0.6188873648643494,
      "learning_rate": 1.9662050941135336e-05,
      "loss": 0.0989,
      "step": 485200
    },
    {
      "epoch": 3.152424567215564,
      "grad_norm": 0.938193142414093,
      "learning_rate": 1.9655140482709342e-05,
      "loss": 0.1132,
      "step": 485300
    },
    {
      "epoch": 3.153074149858716,
      "grad_norm": 1.0176541805267334,
      "learning_rate": 1.9648230024283352e-05,
      "loss": 0.1049,
      "step": 485400
    },
    {
      "epoch": 3.1537237325018674,
      "grad_norm": 1.0222887992858887,
      "learning_rate": 1.964131956585736e-05,
      "loss": 0.105,
      "step": 485500
    },
    {
      "epoch": 3.1543733151450195,
      "grad_norm": 1.4591701030731201,
      "learning_rate": 1.963440910743137e-05,
      "loss": 0.1065,
      "step": 485600
    },
    {
      "epoch": 3.155022897788171,
      "grad_norm": 0.9615864157676697,
      "learning_rate": 1.962749864900538e-05,
      "loss": 0.0954,
      "step": 485700
    },
    {
      "epoch": 3.1556724804313228,
      "grad_norm": 0.7487448453903198,
      "learning_rate": 1.9620588190579388e-05,
      "loss": 0.1056,
      "step": 485800
    },
    {
      "epoch": 3.156322063074475,
      "grad_norm": 1.0693385601043701,
      "learning_rate": 1.9613677732153394e-05,
      "loss": 0.1031,
      "step": 485900
    },
    {
      "epoch": 3.1569716457176265,
      "grad_norm": 1.1655508279800415,
      "learning_rate": 1.9606767273727404e-05,
      "loss": 0.108,
      "step": 486000
    },
    {
      "epoch": 3.157621228360778,
      "grad_norm": 1.3701175451278687,
      "learning_rate": 1.9599856815301414e-05,
      "loss": 0.1035,
      "step": 486100
    },
    {
      "epoch": 3.15827081100393,
      "grad_norm": 1.2939854860305786,
      "learning_rate": 1.9592946356875424e-05,
      "loss": 0.1078,
      "step": 486200
    },
    {
      "epoch": 3.1589203936470818,
      "grad_norm": 1.3598793745040894,
      "learning_rate": 1.958603589844943e-05,
      "loss": 0.1038,
      "step": 486300
    },
    {
      "epoch": 3.1595699762902334,
      "grad_norm": 1.351258397102356,
      "learning_rate": 1.957912544002344e-05,
      "loss": 0.1034,
      "step": 486400
    },
    {
      "epoch": 3.1602195589333855,
      "grad_norm": 1.1279228925704956,
      "learning_rate": 1.957221498159745e-05,
      "loss": 0.1055,
      "step": 486500
    },
    {
      "epoch": 3.160869141576537,
      "grad_norm": 1.1061086654663086,
      "learning_rate": 1.956530452317146e-05,
      "loss": 0.1027,
      "step": 486600
    },
    {
      "epoch": 3.1615187242196887,
      "grad_norm": 1.610927700996399,
      "learning_rate": 1.9558394064745466e-05,
      "loss": 0.0994,
      "step": 486700
    },
    {
      "epoch": 3.162168306862841,
      "grad_norm": 0.9773193001747131,
      "learning_rate": 1.9551483606319476e-05,
      "loss": 0.1038,
      "step": 486800
    },
    {
      "epoch": 3.1628178895059924,
      "grad_norm": 1.3193012475967407,
      "learning_rate": 1.9544573147893486e-05,
      "loss": 0.1088,
      "step": 486900
    },
    {
      "epoch": 3.163467472149144,
      "grad_norm": 1.5321292877197266,
      "learning_rate": 1.9537662689467496e-05,
      "loss": 0.1013,
      "step": 487000
    },
    {
      "epoch": 3.164117054792296,
      "grad_norm": 0.9929538369178772,
      "learning_rate": 1.9530752231041506e-05,
      "loss": 0.108,
      "step": 487100
    },
    {
      "epoch": 3.1647666374354477,
      "grad_norm": 1.3236221075057983,
      "learning_rate": 1.9523841772615512e-05,
      "loss": 0.0967,
      "step": 487200
    },
    {
      "epoch": 3.1654162200785994,
      "grad_norm": 1.4418483972549438,
      "learning_rate": 1.9516931314189522e-05,
      "loss": 0.1061,
      "step": 487300
    },
    {
      "epoch": 3.1660658027217514,
      "grad_norm": 0.7250114679336548,
      "learning_rate": 1.9510020855763532e-05,
      "loss": 0.1041,
      "step": 487400
    },
    {
      "epoch": 3.166715385364903,
      "grad_norm": 1.1978797912597656,
      "learning_rate": 1.950311039733754e-05,
      "loss": 0.1075,
      "step": 487500
    },
    {
      "epoch": 3.1673649680080547,
      "grad_norm": 0.9337471723556519,
      "learning_rate": 1.9496199938911548e-05,
      "loss": 0.1085,
      "step": 487600
    },
    {
      "epoch": 3.1680145506512067,
      "grad_norm": 0.9934155344963074,
      "learning_rate": 1.9489289480485555e-05,
      "loss": 0.1058,
      "step": 487700
    },
    {
      "epoch": 3.1686641332943584,
      "grad_norm": 0.8319616913795471,
      "learning_rate": 1.9482379022059565e-05,
      "loss": 0.107,
      "step": 487800
    },
    {
      "epoch": 3.16931371593751,
      "grad_norm": 0.6021291017532349,
      "learning_rate": 1.9475468563633574e-05,
      "loss": 0.1063,
      "step": 487900
    },
    {
      "epoch": 3.169963298580662,
      "grad_norm": 0.8271430134773254,
      "learning_rate": 1.9468558105207584e-05,
      "loss": 0.0987,
      "step": 488000
    },
    {
      "epoch": 3.1706128812238137,
      "grad_norm": 0.9991257190704346,
      "learning_rate": 1.9461647646781594e-05,
      "loss": 0.1015,
      "step": 488100
    },
    {
      "epoch": 3.1712624638669653,
      "grad_norm": 1.0711737871170044,
      "learning_rate": 1.94547371883556e-05,
      "loss": 0.1017,
      "step": 488200
    },
    {
      "epoch": 3.1719120465101174,
      "grad_norm": 1.2990635633468628,
      "learning_rate": 1.944782672992961e-05,
      "loss": 0.1118,
      "step": 488300
    },
    {
      "epoch": 3.172561629153269,
      "grad_norm": 0.9628391265869141,
      "learning_rate": 1.944091627150362e-05,
      "loss": 0.1062,
      "step": 488400
    },
    {
      "epoch": 3.1732112117964206,
      "grad_norm": 1.2132633924484253,
      "learning_rate": 1.943400581307763e-05,
      "loss": 0.1052,
      "step": 488500
    },
    {
      "epoch": 3.1738607944395727,
      "grad_norm": 2.0456700325012207,
      "learning_rate": 1.9427095354651637e-05,
      "loss": 0.102,
      "step": 488600
    },
    {
      "epoch": 3.1745103770827243,
      "grad_norm": 1.194671630859375,
      "learning_rate": 1.9420184896225646e-05,
      "loss": 0.1012,
      "step": 488700
    },
    {
      "epoch": 3.175159959725876,
      "grad_norm": 1.1327563524246216,
      "learning_rate": 1.9413274437799656e-05,
      "loss": 0.1064,
      "step": 488800
    },
    {
      "epoch": 3.175809542369028,
      "grad_norm": 1.1551131010055542,
      "learning_rate": 1.9406363979373666e-05,
      "loss": 0.1049,
      "step": 488900
    },
    {
      "epoch": 3.1764591250121796,
      "grad_norm": 1.0866812467575073,
      "learning_rate": 1.9399453520947676e-05,
      "loss": 0.1049,
      "step": 489000
    },
    {
      "epoch": 3.1771087076553313,
      "grad_norm": 1.4760711193084717,
      "learning_rate": 1.9392543062521682e-05,
      "loss": 0.1002,
      "step": 489100
    },
    {
      "epoch": 3.1777582902984833,
      "grad_norm": 1.0290756225585938,
      "learning_rate": 1.9385632604095692e-05,
      "loss": 0.0993,
      "step": 489200
    },
    {
      "epoch": 3.178407872941635,
      "grad_norm": 0.8179758787155151,
      "learning_rate": 1.93787221456697e-05,
      "loss": 0.1041,
      "step": 489300
    },
    {
      "epoch": 3.1790574555847866,
      "grad_norm": 0.9523749351501465,
      "learning_rate": 1.937181168724371e-05,
      "loss": 0.1048,
      "step": 489400
    },
    {
      "epoch": 3.1797070382279387,
      "grad_norm": 1.7040992975234985,
      "learning_rate": 1.936490122881772e-05,
      "loss": 0.1021,
      "step": 489500
    },
    {
      "epoch": 3.1803566208710903,
      "grad_norm": 1.3705646991729736,
      "learning_rate": 1.9357990770391725e-05,
      "loss": 0.1042,
      "step": 489600
    },
    {
      "epoch": 3.181006203514242,
      "grad_norm": 1.1696105003356934,
      "learning_rate": 1.9351080311965735e-05,
      "loss": 0.1057,
      "step": 489700
    },
    {
      "epoch": 3.181655786157394,
      "grad_norm": 1.0929040908813477,
      "learning_rate": 1.9344169853539745e-05,
      "loss": 0.1068,
      "step": 489800
    },
    {
      "epoch": 3.1823053688005456,
      "grad_norm": 1.0979822874069214,
      "learning_rate": 1.9337259395113755e-05,
      "loss": 0.1056,
      "step": 489900
    },
    {
      "epoch": 3.1829549514436972,
      "grad_norm": 1.1731330156326294,
      "learning_rate": 1.933034893668776e-05,
      "loss": 0.1036,
      "step": 490000
    },
    {
      "epoch": 3.1836045340868493,
      "grad_norm": 0.9994438886642456,
      "learning_rate": 1.932343847826177e-05,
      "loss": 0.1031,
      "step": 490100
    },
    {
      "epoch": 3.184254116730001,
      "grad_norm": 1.0701435804367065,
      "learning_rate": 1.931652801983578e-05,
      "loss": 0.1072,
      "step": 490200
    },
    {
      "epoch": 3.1849036993731525,
      "grad_norm": 0.9919571876525879,
      "learning_rate": 1.930961756140979e-05,
      "loss": 0.1017,
      "step": 490300
    },
    {
      "epoch": 3.1855532820163046,
      "grad_norm": 0.9495324492454529,
      "learning_rate": 1.93027071029838e-05,
      "loss": 0.1049,
      "step": 490400
    },
    {
      "epoch": 3.1862028646594562,
      "grad_norm": 0.8619044423103333,
      "learning_rate": 1.9295796644557807e-05,
      "loss": 0.1011,
      "step": 490500
    },
    {
      "epoch": 3.1868524473026083,
      "grad_norm": 1.4986684322357178,
      "learning_rate": 1.9288886186131817e-05,
      "loss": 0.105,
      "step": 490600
    },
    {
      "epoch": 3.18750202994576,
      "grad_norm": 0.9239009618759155,
      "learning_rate": 1.9281975727705827e-05,
      "loss": 0.1056,
      "step": 490700
    },
    {
      "epoch": 3.1881516125889116,
      "grad_norm": 0.6879318952560425,
      "learning_rate": 1.9275065269279836e-05,
      "loss": 0.1024,
      "step": 490800
    },
    {
      "epoch": 3.188801195232063,
      "grad_norm": 0.8947542309761047,
      "learning_rate": 1.9268154810853843e-05,
      "loss": 0.0956,
      "step": 490900
    },
    {
      "epoch": 3.1894507778752152,
      "grad_norm": 1.527552604675293,
      "learning_rate": 1.9261244352427853e-05,
      "loss": 0.1165,
      "step": 491000
    },
    {
      "epoch": 3.190100360518367,
      "grad_norm": 1.2630066871643066,
      "learning_rate": 1.925433389400186e-05,
      "loss": 0.1015,
      "step": 491100
    },
    {
      "epoch": 3.190749943161519,
      "grad_norm": 1.0079128742218018,
      "learning_rate": 1.924742343557587e-05,
      "loss": 0.1005,
      "step": 491200
    },
    {
      "epoch": 3.1913995258046706,
      "grad_norm": 0.9948533177375793,
      "learning_rate": 1.924051297714988e-05,
      "loss": 0.1049,
      "step": 491300
    },
    {
      "epoch": 3.192049108447822,
      "grad_norm": 1.425046682357788,
      "learning_rate": 1.923360251872389e-05,
      "loss": 0.1064,
      "step": 491400
    },
    {
      "epoch": 3.192698691090974,
      "grad_norm": 0.8220000863075256,
      "learning_rate": 1.9226692060297895e-05,
      "loss": 0.1096,
      "step": 491500
    },
    {
      "epoch": 3.193348273734126,
      "grad_norm": 0.46137842535972595,
      "learning_rate": 1.9219781601871905e-05,
      "loss": 0.1009,
      "step": 491600
    },
    {
      "epoch": 3.1939978563772775,
      "grad_norm": 0.8982817530632019,
      "learning_rate": 1.9212871143445915e-05,
      "loss": 0.103,
      "step": 491700
    },
    {
      "epoch": 3.1946474390204296,
      "grad_norm": 1.073182463645935,
      "learning_rate": 1.9205960685019925e-05,
      "loss": 0.1024,
      "step": 491800
    },
    {
      "epoch": 3.195297021663581,
      "grad_norm": 1.2506194114685059,
      "learning_rate": 1.919905022659393e-05,
      "loss": 0.1008,
      "step": 491900
    },
    {
      "epoch": 3.195946604306733,
      "grad_norm": 1.0293769836425781,
      "learning_rate": 1.919213976816794e-05,
      "loss": 0.1002,
      "step": 492000
    },
    {
      "epoch": 3.196596186949885,
      "grad_norm": 1.2753406763076782,
      "learning_rate": 1.918522930974195e-05,
      "loss": 0.1066,
      "step": 492100
    },
    {
      "epoch": 3.1972457695930365,
      "grad_norm": 0.5894432663917542,
      "learning_rate": 1.917831885131596e-05,
      "loss": 0.1026,
      "step": 492200
    },
    {
      "epoch": 3.197895352236188,
      "grad_norm": 1.0897979736328125,
      "learning_rate": 1.917140839288997e-05,
      "loss": 0.1091,
      "step": 492300
    },
    {
      "epoch": 3.19854493487934,
      "grad_norm": 1.248512625694275,
      "learning_rate": 1.9164497934463977e-05,
      "loss": 0.1004,
      "step": 492400
    },
    {
      "epoch": 3.199194517522492,
      "grad_norm": 0.7198584079742432,
      "learning_rate": 1.9157587476037987e-05,
      "loss": 0.0987,
      "step": 492500
    },
    {
      "epoch": 3.1998441001656435,
      "grad_norm": 1.0209989547729492,
      "learning_rate": 1.9150677017611997e-05,
      "loss": 0.1018,
      "step": 492600
    },
    {
      "epoch": 3.2004936828087955,
      "grad_norm": 1.5410327911376953,
      "learning_rate": 1.9143766559186003e-05,
      "loss": 0.1058,
      "step": 492700
    },
    {
      "epoch": 3.201143265451947,
      "grad_norm": 1.3028498888015747,
      "learning_rate": 1.9136856100760013e-05,
      "loss": 0.105,
      "step": 492800
    },
    {
      "epoch": 3.201792848095099,
      "grad_norm": 0.8352516889572144,
      "learning_rate": 1.912994564233402e-05,
      "loss": 0.1087,
      "step": 492900
    },
    {
      "epoch": 3.202442430738251,
      "grad_norm": 1.2193912267684937,
      "learning_rate": 1.912303518390803e-05,
      "loss": 0.1031,
      "step": 493000
    },
    {
      "epoch": 3.2030920133814025,
      "grad_norm": 1.1519006490707397,
      "learning_rate": 1.911612472548204e-05,
      "loss": 0.105,
      "step": 493100
    },
    {
      "epoch": 3.203741596024554,
      "grad_norm": 1.2843767404556274,
      "learning_rate": 1.910921426705605e-05,
      "loss": 0.099,
      "step": 493200
    },
    {
      "epoch": 3.204391178667706,
      "grad_norm": 1.035975456237793,
      "learning_rate": 1.9102303808630056e-05,
      "loss": 0.1081,
      "step": 493300
    },
    {
      "epoch": 3.205040761310858,
      "grad_norm": 0.8800087571144104,
      "learning_rate": 1.9095393350204065e-05,
      "loss": 0.1019,
      "step": 493400
    },
    {
      "epoch": 3.2056903439540094,
      "grad_norm": 0.895958662033081,
      "learning_rate": 1.9088482891778075e-05,
      "loss": 0.1091,
      "step": 493500
    },
    {
      "epoch": 3.2063399265971615,
      "grad_norm": 1.0736422538757324,
      "learning_rate": 1.9081572433352085e-05,
      "loss": 0.0992,
      "step": 493600
    },
    {
      "epoch": 3.206989509240313,
      "grad_norm": 1.3045096397399902,
      "learning_rate": 1.9074661974926095e-05,
      "loss": 0.1038,
      "step": 493700
    },
    {
      "epoch": 3.2076390918834647,
      "grad_norm": 1.6776373386383057,
      "learning_rate": 1.90677515165001e-05,
      "loss": 0.1027,
      "step": 493800
    },
    {
      "epoch": 3.208288674526617,
      "grad_norm": 0.7115437388420105,
      "learning_rate": 1.906084105807411e-05,
      "loss": 0.1065,
      "step": 493900
    },
    {
      "epoch": 3.2089382571697684,
      "grad_norm": 1.0586570501327515,
      "learning_rate": 1.905393059964812e-05,
      "loss": 0.103,
      "step": 494000
    },
    {
      "epoch": 3.20958783981292,
      "grad_norm": 1.269413948059082,
      "learning_rate": 1.904702014122213e-05,
      "loss": 0.105,
      "step": 494100
    },
    {
      "epoch": 3.210237422456072,
      "grad_norm": 1.2783565521240234,
      "learning_rate": 1.904010968279614e-05,
      "loss": 0.1064,
      "step": 494200
    },
    {
      "epoch": 3.2108870050992238,
      "grad_norm": 1.1308581829071045,
      "learning_rate": 1.9033199224370147e-05,
      "loss": 0.1031,
      "step": 494300
    },
    {
      "epoch": 3.2115365877423754,
      "grad_norm": 1.1356837749481201,
      "learning_rate": 1.9026288765944157e-05,
      "loss": 0.1047,
      "step": 494400
    },
    {
      "epoch": 3.2121861703855275,
      "grad_norm": 0.6995229721069336,
      "learning_rate": 1.9019378307518164e-05,
      "loss": 0.0995,
      "step": 494500
    },
    {
      "epoch": 3.212835753028679,
      "grad_norm": 0.9777916073799133,
      "learning_rate": 1.9012467849092174e-05,
      "loss": 0.0987,
      "step": 494600
    },
    {
      "epoch": 3.2134853356718307,
      "grad_norm": 0.8560736775398254,
      "learning_rate": 1.900555739066618e-05,
      "loss": 0.0994,
      "step": 494700
    },
    {
      "epoch": 3.2141349183149828,
      "grad_norm": 0.7422285079956055,
      "learning_rate": 1.899864693224019e-05,
      "loss": 0.1059,
      "step": 494800
    },
    {
      "epoch": 3.2147845009581344,
      "grad_norm": 1.5293573141098022,
      "learning_rate": 1.89917364738142e-05,
      "loss": 0.1044,
      "step": 494900
    },
    {
      "epoch": 3.215434083601286,
      "grad_norm": 0.609062910079956,
      "learning_rate": 1.898482601538821e-05,
      "loss": 0.1056,
      "step": 495000
    },
    {
      "epoch": 3.216083666244438,
      "grad_norm": 1.2861253023147583,
      "learning_rate": 1.897791555696222e-05,
      "loss": 0.1024,
      "step": 495100
    },
    {
      "epoch": 3.2167332488875897,
      "grad_norm": 1.3480204343795776,
      "learning_rate": 1.8971005098536226e-05,
      "loss": 0.1135,
      "step": 495200
    },
    {
      "epoch": 3.2173828315307413,
      "grad_norm": 1.4962966442108154,
      "learning_rate": 1.8964094640110236e-05,
      "loss": 0.103,
      "step": 495300
    },
    {
      "epoch": 3.2180324141738934,
      "grad_norm": 1.1165034770965576,
      "learning_rate": 1.8957184181684246e-05,
      "loss": 0.1155,
      "step": 495400
    },
    {
      "epoch": 3.218681996817045,
      "grad_norm": 0.9242002964019775,
      "learning_rate": 1.8950273723258255e-05,
      "loss": 0.106,
      "step": 495500
    },
    {
      "epoch": 3.2193315794601967,
      "grad_norm": 1.0744482278823853,
      "learning_rate": 1.8943363264832265e-05,
      "loss": 0.1039,
      "step": 495600
    },
    {
      "epoch": 3.2199811621033487,
      "grad_norm": 0.96114182472229,
      "learning_rate": 1.8936452806406272e-05,
      "loss": 0.104,
      "step": 495700
    },
    {
      "epoch": 3.2206307447465004,
      "grad_norm": 0.9097071886062622,
      "learning_rate": 1.892954234798028e-05,
      "loss": 0.1023,
      "step": 495800
    },
    {
      "epoch": 3.221280327389652,
      "grad_norm": 0.8625569939613342,
      "learning_rate": 1.892263188955429e-05,
      "loss": 0.1034,
      "step": 495900
    },
    {
      "epoch": 3.221929910032804,
      "grad_norm": 0.6316384673118591,
      "learning_rate": 1.89157214311283e-05,
      "loss": 0.1029,
      "step": 496000
    },
    {
      "epoch": 3.2225794926759557,
      "grad_norm": 1.3564257621765137,
      "learning_rate": 1.8908810972702308e-05,
      "loss": 0.1005,
      "step": 496100
    },
    {
      "epoch": 3.2232290753191073,
      "grad_norm": 1.0233997106552124,
      "learning_rate": 1.8901900514276318e-05,
      "loss": 0.0974,
      "step": 496200
    },
    {
      "epoch": 3.2238786579622594,
      "grad_norm": 1.1465425491333008,
      "learning_rate": 1.8894990055850324e-05,
      "loss": 0.0999,
      "step": 496300
    },
    {
      "epoch": 3.224528240605411,
      "grad_norm": 0.7519225478172302,
      "learning_rate": 1.8888079597424334e-05,
      "loss": 0.0984,
      "step": 496400
    },
    {
      "epoch": 3.2251778232485626,
      "grad_norm": 0.9698954820632935,
      "learning_rate": 1.8881169138998344e-05,
      "loss": 0.1068,
      "step": 496500
    },
    {
      "epoch": 3.2258274058917147,
      "grad_norm": 1.4934473037719727,
      "learning_rate": 1.887425868057235e-05,
      "loss": 0.1079,
      "step": 496600
    },
    {
      "epoch": 3.2264769885348663,
      "grad_norm": 1.1688518524169922,
      "learning_rate": 1.886734822214636e-05,
      "loss": 0.1047,
      "step": 496700
    },
    {
      "epoch": 3.227126571178018,
      "grad_norm": 0.6849613785743713,
      "learning_rate": 1.886043776372037e-05,
      "loss": 0.1094,
      "step": 496800
    },
    {
      "epoch": 3.22777615382117,
      "grad_norm": 0.8066734075546265,
      "learning_rate": 1.885352730529438e-05,
      "loss": 0.1015,
      "step": 496900
    },
    {
      "epoch": 3.2284257364643216,
      "grad_norm": 0.9698902368545532,
      "learning_rate": 1.884661684686839e-05,
      "loss": 0.1026,
      "step": 497000
    },
    {
      "epoch": 3.2290753191074733,
      "grad_norm": 1.1807807683944702,
      "learning_rate": 1.8839706388442396e-05,
      "loss": 0.105,
      "step": 497100
    },
    {
      "epoch": 3.2297249017506253,
      "grad_norm": 1.017350673675537,
      "learning_rate": 1.8832795930016406e-05,
      "loss": 0.1012,
      "step": 497200
    },
    {
      "epoch": 3.230374484393777,
      "grad_norm": 1.082702398300171,
      "learning_rate": 1.8825885471590416e-05,
      "loss": 0.1039,
      "step": 497300
    },
    {
      "epoch": 3.2310240670369286,
      "grad_norm": 1.2855931520462036,
      "learning_rate": 1.8818975013164426e-05,
      "loss": 0.1078,
      "step": 497400
    },
    {
      "epoch": 3.2316736496800806,
      "grad_norm": 1.1338669061660767,
      "learning_rate": 1.8812064554738436e-05,
      "loss": 0.0942,
      "step": 497500
    },
    {
      "epoch": 3.2323232323232323,
      "grad_norm": 1.1499388217926025,
      "learning_rate": 1.8805154096312442e-05,
      "loss": 0.103,
      "step": 497600
    },
    {
      "epoch": 3.2329728149663843,
      "grad_norm": 1.3842343091964722,
      "learning_rate": 1.8798243637886452e-05,
      "loss": 0.1043,
      "step": 497700
    },
    {
      "epoch": 3.233622397609536,
      "grad_norm": 0.9177685976028442,
      "learning_rate": 1.879133317946046e-05,
      "loss": 0.0977,
      "step": 497800
    },
    {
      "epoch": 3.2342719802526876,
      "grad_norm": 1.440958857536316,
      "learning_rate": 1.8784422721034468e-05,
      "loss": 0.1041,
      "step": 497900
    },
    {
      "epoch": 3.234921562895839,
      "grad_norm": 0.9264931082725525,
      "learning_rate": 1.8777512262608478e-05,
      "loss": 0.105,
      "step": 498000
    },
    {
      "epoch": 3.2355711455389913,
      "grad_norm": 1.6046960353851318,
      "learning_rate": 1.8770601804182484e-05,
      "loss": 0.105,
      "step": 498100
    },
    {
      "epoch": 3.236220728182143,
      "grad_norm": 0.5789558291435242,
      "learning_rate": 1.8763691345756494e-05,
      "loss": 0.1053,
      "step": 498200
    },
    {
      "epoch": 3.236870310825295,
      "grad_norm": 0.8563741445541382,
      "learning_rate": 1.8756780887330504e-05,
      "loss": 0.1034,
      "step": 498300
    },
    {
      "epoch": 3.2375198934684466,
      "grad_norm": 1.3110802173614502,
      "learning_rate": 1.8749870428904514e-05,
      "loss": 0.1043,
      "step": 498400
    },
    {
      "epoch": 3.238169476111598,
      "grad_norm": 0.8685094118118286,
      "learning_rate": 1.874295997047852e-05,
      "loss": 0.1083,
      "step": 498500
    },
    {
      "epoch": 3.23881905875475,
      "grad_norm": 1.3900084495544434,
      "learning_rate": 1.873604951205253e-05,
      "loss": 0.101,
      "step": 498600
    },
    {
      "epoch": 3.239468641397902,
      "grad_norm": 1.4321860074996948,
      "learning_rate": 1.872913905362654e-05,
      "loss": 0.1114,
      "step": 498700
    },
    {
      "epoch": 3.2401182240410535,
      "grad_norm": 1.1008270978927612,
      "learning_rate": 1.872222859520055e-05,
      "loss": 0.0997,
      "step": 498800
    },
    {
      "epoch": 3.2407678066842056,
      "grad_norm": 1.4238063097000122,
      "learning_rate": 1.871531813677456e-05,
      "loss": 0.1059,
      "step": 498900
    },
    {
      "epoch": 3.2414173893273572,
      "grad_norm": 0.712205708026886,
      "learning_rate": 1.8708407678348566e-05,
      "loss": 0.103,
      "step": 499000
    },
    {
      "epoch": 3.242066971970509,
      "grad_norm": 1.0812287330627441,
      "learning_rate": 1.8701497219922576e-05,
      "loss": 0.0995,
      "step": 499100
    },
    {
      "epoch": 3.242716554613661,
      "grad_norm": 1.333957314491272,
      "learning_rate": 1.8694586761496586e-05,
      "loss": 0.0989,
      "step": 499200
    },
    {
      "epoch": 3.2433661372568126,
      "grad_norm": 1.56980299949646,
      "learning_rate": 1.8687676303070596e-05,
      "loss": 0.1006,
      "step": 499300
    },
    {
      "epoch": 3.244015719899964,
      "grad_norm": 1.0458346605300903,
      "learning_rate": 1.8680765844644602e-05,
      "loss": 0.1033,
      "step": 499400
    },
    {
      "epoch": 3.2446653025431162,
      "grad_norm": 1.4807162284851074,
      "learning_rate": 1.8673855386218612e-05,
      "loss": 0.1094,
      "step": 499500
    },
    {
      "epoch": 3.245314885186268,
      "grad_norm": 1.8289200067520142,
      "learning_rate": 1.8666944927792622e-05,
      "loss": 0.1123,
      "step": 499600
    },
    {
      "epoch": 3.2459644678294195,
      "grad_norm": 0.7702560424804688,
      "learning_rate": 1.866003446936663e-05,
      "loss": 0.0992,
      "step": 499700
    },
    {
      "epoch": 3.2466140504725716,
      "grad_norm": 1.1967105865478516,
      "learning_rate": 1.865312401094064e-05,
      "loss": 0.1039,
      "step": 499800
    },
    {
      "epoch": 3.247263633115723,
      "grad_norm": 0.7481545209884644,
      "learning_rate": 1.8646213552514645e-05,
      "loss": 0.1056,
      "step": 499900
    },
    {
      "epoch": 3.247913215758875,
      "grad_norm": 1.390191912651062,
      "learning_rate": 1.8639303094088655e-05,
      "loss": 0.1014,
      "step": 500000
    },
    {
      "epoch": 3.248562798402027,
      "grad_norm": 1.0837905406951904,
      "learning_rate": 1.8632392635662665e-05,
      "loss": 0.1005,
      "step": 500100
    },
    {
      "epoch": 3.2492123810451785,
      "grad_norm": 0.7581714987754822,
      "learning_rate": 1.8625482177236674e-05,
      "loss": 0.1018,
      "step": 500200
    },
    {
      "epoch": 3.24986196368833,
      "grad_norm": 1.0059361457824707,
      "learning_rate": 1.8618571718810684e-05,
      "loss": 0.1068,
      "step": 500300
    },
    {
      "epoch": 3.250511546331482,
      "grad_norm": 1.1836042404174805,
      "learning_rate": 1.861166126038469e-05,
      "loss": 0.1042,
      "step": 500400
    },
    {
      "epoch": 3.251161128974634,
      "grad_norm": 1.2855273485183716,
      "learning_rate": 1.86047508019587e-05,
      "loss": 0.1049,
      "step": 500500
    },
    {
      "epoch": 3.2518107116177855,
      "grad_norm": 1.3197193145751953,
      "learning_rate": 1.859784034353271e-05,
      "loss": 0.103,
      "step": 500600
    },
    {
      "epoch": 3.2524602942609375,
      "grad_norm": 0.7595090270042419,
      "learning_rate": 1.859092988510672e-05,
      "loss": 0.1044,
      "step": 500700
    },
    {
      "epoch": 3.253109876904089,
      "grad_norm": 1.020421028137207,
      "learning_rate": 1.858401942668073e-05,
      "loss": 0.1007,
      "step": 500800
    },
    {
      "epoch": 3.2537594595472408,
      "grad_norm": 1.6265240907669067,
      "learning_rate": 1.8577108968254737e-05,
      "loss": 0.1066,
      "step": 500900
    },
    {
      "epoch": 3.254409042190393,
      "grad_norm": 1.494426965713501,
      "learning_rate": 1.8570198509828746e-05,
      "loss": 0.106,
      "step": 501000
    },
    {
      "epoch": 3.2550586248335445,
      "grad_norm": 0.9357027411460876,
      "learning_rate": 1.8563288051402756e-05,
      "loss": 0.1036,
      "step": 501100
    },
    {
      "epoch": 3.255708207476696,
      "grad_norm": 1.5839142799377441,
      "learning_rate": 1.8556377592976766e-05,
      "loss": 0.1089,
      "step": 501200
    },
    {
      "epoch": 3.256357790119848,
      "grad_norm": 1.3196812868118286,
      "learning_rate": 1.8549467134550773e-05,
      "loss": 0.1094,
      "step": 501300
    },
    {
      "epoch": 3.257007372763,
      "grad_norm": 0.9132360816001892,
      "learning_rate": 1.8542556676124782e-05,
      "loss": 0.1015,
      "step": 501400
    },
    {
      "epoch": 3.2576569554061514,
      "grad_norm": 0.8681399822235107,
      "learning_rate": 1.853564621769879e-05,
      "loss": 0.0979,
      "step": 501500
    },
    {
      "epoch": 3.2583065380493035,
      "grad_norm": 1.2538169622421265,
      "learning_rate": 1.85287357592728e-05,
      "loss": 0.0988,
      "step": 501600
    },
    {
      "epoch": 3.258956120692455,
      "grad_norm": 1.0340056419372559,
      "learning_rate": 1.852182530084681e-05,
      "loss": 0.1031,
      "step": 501700
    },
    {
      "epoch": 3.2596057033356067,
      "grad_norm": 1.033581256866455,
      "learning_rate": 1.8514914842420815e-05,
      "loss": 0.105,
      "step": 501800
    },
    {
      "epoch": 3.260255285978759,
      "grad_norm": 1.0608121156692505,
      "learning_rate": 1.8508004383994825e-05,
      "loss": 0.1058,
      "step": 501900
    },
    {
      "epoch": 3.2609048686219104,
      "grad_norm": 1.823663592338562,
      "learning_rate": 1.8501093925568835e-05,
      "loss": 0.0975,
      "step": 502000
    },
    {
      "epoch": 3.261554451265062,
      "grad_norm": 1.1462241411209106,
      "learning_rate": 1.8494183467142845e-05,
      "loss": 0.105,
      "step": 502100
    },
    {
      "epoch": 3.262204033908214,
      "grad_norm": 1.349879264831543,
      "learning_rate": 1.8487273008716854e-05,
      "loss": 0.0987,
      "step": 502200
    },
    {
      "epoch": 3.2628536165513657,
      "grad_norm": 1.0397652387619019,
      "learning_rate": 1.848036255029086e-05,
      "loss": 0.1033,
      "step": 502300
    },
    {
      "epoch": 3.2635031991945174,
      "grad_norm": 1.399317979812622,
      "learning_rate": 1.847345209186487e-05,
      "loss": 0.1054,
      "step": 502400
    },
    {
      "epoch": 3.2641527818376694,
      "grad_norm": 0.8362553715705872,
      "learning_rate": 1.846654163343888e-05,
      "loss": 0.102,
      "step": 502500
    },
    {
      "epoch": 3.264802364480821,
      "grad_norm": 0.8131645321846008,
      "learning_rate": 1.845963117501289e-05,
      "loss": 0.0979,
      "step": 502600
    },
    {
      "epoch": 3.2654519471239727,
      "grad_norm": 0.8862969875335693,
      "learning_rate": 1.8452720716586897e-05,
      "loss": 0.096,
      "step": 502700
    },
    {
      "epoch": 3.2661015297671248,
      "grad_norm": 1.3281872272491455,
      "learning_rate": 1.8445810258160907e-05,
      "loss": 0.1011,
      "step": 502800
    },
    {
      "epoch": 3.2667511124102764,
      "grad_norm": 1.0974215269088745,
      "learning_rate": 1.8438899799734917e-05,
      "loss": 0.1023,
      "step": 502900
    },
    {
      "epoch": 3.267400695053428,
      "grad_norm": 1.3088412284851074,
      "learning_rate": 1.8431989341308927e-05,
      "loss": 0.1035,
      "step": 503000
    },
    {
      "epoch": 3.26805027769658,
      "grad_norm": 1.2810496091842651,
      "learning_rate": 1.8425078882882933e-05,
      "loss": 0.1049,
      "step": 503100
    },
    {
      "epoch": 3.2686998603397317,
      "grad_norm": 0.8504379987716675,
      "learning_rate": 1.8418168424456943e-05,
      "loss": 0.1003,
      "step": 503200
    },
    {
      "epoch": 3.2693494429828833,
      "grad_norm": 1.5869349241256714,
      "learning_rate": 1.841125796603095e-05,
      "loss": 0.103,
      "step": 503300
    },
    {
      "epoch": 3.2699990256260354,
      "grad_norm": 1.236119270324707,
      "learning_rate": 1.840434750760496e-05,
      "loss": 0.1072,
      "step": 503400
    },
    {
      "epoch": 3.270648608269187,
      "grad_norm": 1.1234314441680908,
      "learning_rate": 1.839743704917897e-05,
      "loss": 0.1138,
      "step": 503500
    },
    {
      "epoch": 3.2712981909123386,
      "grad_norm": 1.2245995998382568,
      "learning_rate": 1.839052659075298e-05,
      "loss": 0.1068,
      "step": 503600
    },
    {
      "epoch": 3.2719477735554907,
      "grad_norm": 1.1433875560760498,
      "learning_rate": 1.8383616132326985e-05,
      "loss": 0.1028,
      "step": 503700
    },
    {
      "epoch": 3.2725973561986423,
      "grad_norm": 0.9400755763053894,
      "learning_rate": 1.8376705673900995e-05,
      "loss": 0.1012,
      "step": 503800
    },
    {
      "epoch": 3.273246938841794,
      "grad_norm": 1.2035373449325562,
      "learning_rate": 1.8369795215475005e-05,
      "loss": 0.0951,
      "step": 503900
    },
    {
      "epoch": 3.273896521484946,
      "grad_norm": 1.38358473777771,
      "learning_rate": 1.8362884757049015e-05,
      "loss": 0.1029,
      "step": 504000
    },
    {
      "epoch": 3.2745461041280977,
      "grad_norm": 1.0170977115631104,
      "learning_rate": 1.8355974298623025e-05,
      "loss": 0.1039,
      "step": 504100
    },
    {
      "epoch": 3.2751956867712493,
      "grad_norm": 1.2477209568023682,
      "learning_rate": 1.834906384019703e-05,
      "loss": 0.1027,
      "step": 504200
    },
    {
      "epoch": 3.2758452694144014,
      "grad_norm": 1.487831473350525,
      "learning_rate": 1.834215338177104e-05,
      "loss": 0.1087,
      "step": 504300
    },
    {
      "epoch": 3.276494852057553,
      "grad_norm": 1.7442078590393066,
      "learning_rate": 1.833524292334505e-05,
      "loss": 0.0957,
      "step": 504400
    },
    {
      "epoch": 3.2771444347007046,
      "grad_norm": 1.1450618505477905,
      "learning_rate": 1.832833246491906e-05,
      "loss": 0.1026,
      "step": 504500
    },
    {
      "epoch": 3.2777940173438567,
      "grad_norm": 1.4648098945617676,
      "learning_rate": 1.8321422006493067e-05,
      "loss": 0.098,
      "step": 504600
    },
    {
      "epoch": 3.2784435999870083,
      "grad_norm": 1.1949779987335205,
      "learning_rate": 1.8314511548067077e-05,
      "loss": 0.1025,
      "step": 504700
    },
    {
      "epoch": 3.2790931826301604,
      "grad_norm": 1.1752619743347168,
      "learning_rate": 1.8307601089641087e-05,
      "loss": 0.1123,
      "step": 504800
    },
    {
      "epoch": 3.279742765273312,
      "grad_norm": 0.7663495540618896,
      "learning_rate": 1.8300690631215093e-05,
      "loss": 0.1008,
      "step": 504900
    },
    {
      "epoch": 3.2803923479164636,
      "grad_norm": 0.8480333685874939,
      "learning_rate": 1.8293780172789103e-05,
      "loss": 0.1018,
      "step": 505000
    },
    {
      "epoch": 3.2810419305596152,
      "grad_norm": 0.6406063437461853,
      "learning_rate": 1.828686971436311e-05,
      "loss": 0.095,
      "step": 505100
    },
    {
      "epoch": 3.2816915132027673,
      "grad_norm": 1.4098496437072754,
      "learning_rate": 1.827995925593712e-05,
      "loss": 0.1048,
      "step": 505200
    },
    {
      "epoch": 3.282341095845919,
      "grad_norm": 1.2006696462631226,
      "learning_rate": 1.827304879751113e-05,
      "loss": 0.103,
      "step": 505300
    },
    {
      "epoch": 3.282990678489071,
      "grad_norm": 0.8855336308479309,
      "learning_rate": 1.826613833908514e-05,
      "loss": 0.1068,
      "step": 505400
    },
    {
      "epoch": 3.2836402611322226,
      "grad_norm": 1.0224248170852661,
      "learning_rate": 1.825922788065915e-05,
      "loss": 0.1027,
      "step": 505500
    },
    {
      "epoch": 3.2842898437753743,
      "grad_norm": 0.9018846750259399,
      "learning_rate": 1.8252317422233156e-05,
      "loss": 0.1036,
      "step": 505600
    },
    {
      "epoch": 3.284939426418526,
      "grad_norm": 1.0910428762435913,
      "learning_rate": 1.8245406963807165e-05,
      "loss": 0.0976,
      "step": 505700
    },
    {
      "epoch": 3.285589009061678,
      "grad_norm": 1.3871517181396484,
      "learning_rate": 1.8238496505381175e-05,
      "loss": 0.1038,
      "step": 505800
    },
    {
      "epoch": 3.2862385917048296,
      "grad_norm": 0.6329545974731445,
      "learning_rate": 1.8231586046955185e-05,
      "loss": 0.0959,
      "step": 505900
    },
    {
      "epoch": 3.2868881743479816,
      "grad_norm": 1.0683404207229614,
      "learning_rate": 1.822467558852919e-05,
      "loss": 0.1037,
      "step": 506000
    },
    {
      "epoch": 3.2875377569911333,
      "grad_norm": 0.7755811810493469,
      "learning_rate": 1.82177651301032e-05,
      "loss": 0.1053,
      "step": 506100
    },
    {
      "epoch": 3.288187339634285,
      "grad_norm": 0.9927212595939636,
      "learning_rate": 1.821085467167721e-05,
      "loss": 0.0999,
      "step": 506200
    },
    {
      "epoch": 3.2888369222774365,
      "grad_norm": 1.3197418451309204,
      "learning_rate": 1.820394421325122e-05,
      "loss": 0.0918,
      "step": 506300
    },
    {
      "epoch": 3.2894865049205886,
      "grad_norm": 1.4250537157058716,
      "learning_rate": 1.819703375482523e-05,
      "loss": 0.1096,
      "step": 506400
    },
    {
      "epoch": 3.29013608756374,
      "grad_norm": 0.7581362724304199,
      "learning_rate": 1.8190123296399237e-05,
      "loss": 0.1064,
      "step": 506500
    },
    {
      "epoch": 3.2907856702068923,
      "grad_norm": 1.0190188884735107,
      "learning_rate": 1.8183212837973247e-05,
      "loss": 0.1003,
      "step": 506600
    },
    {
      "epoch": 3.291435252850044,
      "grad_norm": 1.10628342628479,
      "learning_rate": 1.8176302379547254e-05,
      "loss": 0.1064,
      "step": 506700
    },
    {
      "epoch": 3.2920848354931955,
      "grad_norm": 1.196498990058899,
      "learning_rate": 1.8169391921121264e-05,
      "loss": 0.1017,
      "step": 506800
    },
    {
      "epoch": 3.292734418136347,
      "grad_norm": 1.2321045398712158,
      "learning_rate": 1.8162481462695273e-05,
      "loss": 0.1025,
      "step": 506900
    },
    {
      "epoch": 3.293384000779499,
      "grad_norm": 1.2211339473724365,
      "learning_rate": 1.815557100426928e-05,
      "loss": 0.1003,
      "step": 507000
    },
    {
      "epoch": 3.294033583422651,
      "grad_norm": 0.741868257522583,
      "learning_rate": 1.814866054584329e-05,
      "loss": 0.0978,
      "step": 507100
    },
    {
      "epoch": 3.294683166065803,
      "grad_norm": 0.9403312802314758,
      "learning_rate": 1.81417500874173e-05,
      "loss": 0.1025,
      "step": 507200
    },
    {
      "epoch": 3.2953327487089545,
      "grad_norm": 1.37791109085083,
      "learning_rate": 1.813483962899131e-05,
      "loss": 0.1039,
      "step": 507300
    },
    {
      "epoch": 3.295982331352106,
      "grad_norm": 1.2076222896575928,
      "learning_rate": 1.8127929170565316e-05,
      "loss": 0.1063,
      "step": 507400
    },
    {
      "epoch": 3.2966319139952582,
      "grad_norm": 1.2116715908050537,
      "learning_rate": 1.8121018712139326e-05,
      "loss": 0.0999,
      "step": 507500
    },
    {
      "epoch": 3.29728149663841,
      "grad_norm": 1.2046276330947876,
      "learning_rate": 1.8114108253713336e-05,
      "loss": 0.1065,
      "step": 507600
    },
    {
      "epoch": 3.2979310792815615,
      "grad_norm": 0.8321117758750916,
      "learning_rate": 1.8107197795287346e-05,
      "loss": 0.1003,
      "step": 507700
    },
    {
      "epoch": 3.2985806619247136,
      "grad_norm": 1.2229787111282349,
      "learning_rate": 1.8100287336861355e-05,
      "loss": 0.1089,
      "step": 507800
    },
    {
      "epoch": 3.299230244567865,
      "grad_norm": 1.048634648323059,
      "learning_rate": 1.8093376878435362e-05,
      "loss": 0.1001,
      "step": 507900
    },
    {
      "epoch": 3.299879827211017,
      "grad_norm": 1.9758350849151611,
      "learning_rate": 1.8086466420009372e-05,
      "loss": 0.1023,
      "step": 508000
    },
    {
      "epoch": 3.300529409854169,
      "grad_norm": 1.2054708003997803,
      "learning_rate": 1.807955596158338e-05,
      "loss": 0.1115,
      "step": 508100
    },
    {
      "epoch": 3.3011789924973205,
      "grad_norm": 1.2685067653656006,
      "learning_rate": 1.807264550315739e-05,
      "loss": 0.1059,
      "step": 508200
    },
    {
      "epoch": 3.301828575140472,
      "grad_norm": 1.5958256721496582,
      "learning_rate": 1.8065735044731398e-05,
      "loss": 0.1018,
      "step": 508300
    },
    {
      "epoch": 3.302478157783624,
      "grad_norm": 1.9545742273330688,
      "learning_rate": 1.8058824586305408e-05,
      "loss": 0.1035,
      "step": 508400
    },
    {
      "epoch": 3.303127740426776,
      "grad_norm": 0.8624845147132874,
      "learning_rate": 1.8051914127879414e-05,
      "loss": 0.0961,
      "step": 508500
    },
    {
      "epoch": 3.3037773230699274,
      "grad_norm": 1.3706459999084473,
      "learning_rate": 1.8045003669453424e-05,
      "loss": 0.1092,
      "step": 508600
    },
    {
      "epoch": 3.3044269057130795,
      "grad_norm": 1.1546423435211182,
      "learning_rate": 1.8038093211027434e-05,
      "loss": 0.1051,
      "step": 508700
    },
    {
      "epoch": 3.305076488356231,
      "grad_norm": 1.1475893259048462,
      "learning_rate": 1.8031182752601444e-05,
      "loss": 0.0988,
      "step": 508800
    },
    {
      "epoch": 3.3057260709993828,
      "grad_norm": 1.0326272249221802,
      "learning_rate": 1.802427229417545e-05,
      "loss": 0.1076,
      "step": 508900
    },
    {
      "epoch": 3.306375653642535,
      "grad_norm": 1.4992895126342773,
      "learning_rate": 1.801736183574946e-05,
      "loss": 0.1012,
      "step": 509000
    },
    {
      "epoch": 3.3070252362856865,
      "grad_norm": 0.846865713596344,
      "learning_rate": 1.801045137732347e-05,
      "loss": 0.1032,
      "step": 509100
    },
    {
      "epoch": 3.307674818928838,
      "grad_norm": 0.8836446404457092,
      "learning_rate": 1.800354091889748e-05,
      "loss": 0.0984,
      "step": 509200
    },
    {
      "epoch": 3.30832440157199,
      "grad_norm": 1.0460715293884277,
      "learning_rate": 1.7996630460471486e-05,
      "loss": 0.1075,
      "step": 509300
    },
    {
      "epoch": 3.3089739842151418,
      "grad_norm": 1.2993499040603638,
      "learning_rate": 1.7989720002045496e-05,
      "loss": 0.1066,
      "step": 509400
    },
    {
      "epoch": 3.3096235668582934,
      "grad_norm": 0.9243285655975342,
      "learning_rate": 1.7982809543619506e-05,
      "loss": 0.1019,
      "step": 509500
    },
    {
      "epoch": 3.3102731495014455,
      "grad_norm": 1.0684278011322021,
      "learning_rate": 1.7975899085193516e-05,
      "loss": 0.1021,
      "step": 509600
    },
    {
      "epoch": 3.310922732144597,
      "grad_norm": 0.931053102016449,
      "learning_rate": 1.7968988626767526e-05,
      "loss": 0.1079,
      "step": 509700
    },
    {
      "epoch": 3.3115723147877487,
      "grad_norm": 0.7980329394340515,
      "learning_rate": 1.7962078168341532e-05,
      "loss": 0.1019,
      "step": 509800
    },
    {
      "epoch": 3.312221897430901,
      "grad_norm": 1.0187323093414307,
      "learning_rate": 1.7955167709915542e-05,
      "loss": 0.1065,
      "step": 509900
    },
    {
      "epoch": 3.3128714800740524,
      "grad_norm": 1.1196730136871338,
      "learning_rate": 1.7948257251489552e-05,
      "loss": 0.0993,
      "step": 510000
    },
    {
      "epoch": 3.313521062717204,
      "grad_norm": 1.0454570055007935,
      "learning_rate": 1.7941346793063558e-05,
      "loss": 0.1068,
      "step": 510100
    },
    {
      "epoch": 3.314170645360356,
      "grad_norm": 1.1468713283538818,
      "learning_rate": 1.7934436334637568e-05,
      "loss": 0.1029,
      "step": 510200
    },
    {
      "epoch": 3.3148202280035077,
      "grad_norm": 1.2744581699371338,
      "learning_rate": 1.7927525876211575e-05,
      "loss": 0.0988,
      "step": 510300
    },
    {
      "epoch": 3.3154698106466594,
      "grad_norm": 0.7829214334487915,
      "learning_rate": 1.7920615417785584e-05,
      "loss": 0.104,
      "step": 510400
    },
    {
      "epoch": 3.3161193932898114,
      "grad_norm": 1.5846242904663086,
      "learning_rate": 1.7913704959359594e-05,
      "loss": 0.1043,
      "step": 510500
    },
    {
      "epoch": 3.316768975932963,
      "grad_norm": 0.8348664045333862,
      "learning_rate": 1.7906794500933604e-05,
      "loss": 0.1039,
      "step": 510600
    },
    {
      "epoch": 3.3174185585761147,
      "grad_norm": 1.2029845714569092,
      "learning_rate": 1.789988404250761e-05,
      "loss": 0.104,
      "step": 510700
    },
    {
      "epoch": 3.3180681412192667,
      "grad_norm": 1.1500850915908813,
      "learning_rate": 1.789297358408162e-05,
      "loss": 0.1066,
      "step": 510800
    },
    {
      "epoch": 3.3187177238624184,
      "grad_norm": 1.2480061054229736,
      "learning_rate": 1.788606312565563e-05,
      "loss": 0.1086,
      "step": 510900
    },
    {
      "epoch": 3.31936730650557,
      "grad_norm": 0.7790704369544983,
      "learning_rate": 1.787915266722964e-05,
      "loss": 0.1052,
      "step": 511000
    },
    {
      "epoch": 3.320016889148722,
      "grad_norm": 0.9811421632766724,
      "learning_rate": 1.787224220880365e-05,
      "loss": 0.1005,
      "step": 511100
    },
    {
      "epoch": 3.3206664717918737,
      "grad_norm": 0.8683648705482483,
      "learning_rate": 1.7865331750377656e-05,
      "loss": 0.1041,
      "step": 511200
    },
    {
      "epoch": 3.3213160544350253,
      "grad_norm": 0.9404497146606445,
      "learning_rate": 1.7858421291951666e-05,
      "loss": 0.1069,
      "step": 511300
    },
    {
      "epoch": 3.3219656370781774,
      "grad_norm": 1.184593677520752,
      "learning_rate": 1.7851510833525676e-05,
      "loss": 0.1037,
      "step": 511400
    },
    {
      "epoch": 3.322615219721329,
      "grad_norm": 1.3829580545425415,
      "learning_rate": 1.7844600375099686e-05,
      "loss": 0.1024,
      "step": 511500
    },
    {
      "epoch": 3.3232648023644806,
      "grad_norm": 1.5196362733840942,
      "learning_rate": 1.7837689916673696e-05,
      "loss": 0.1011,
      "step": 511600
    },
    {
      "epoch": 3.3239143850076327,
      "grad_norm": 1.185123324394226,
      "learning_rate": 1.7830779458247702e-05,
      "loss": 0.0995,
      "step": 511700
    },
    {
      "epoch": 3.3245639676507843,
      "grad_norm": 1.3744652271270752,
      "learning_rate": 1.7823868999821712e-05,
      "loss": 0.1048,
      "step": 511800
    },
    {
      "epoch": 3.3252135502939364,
      "grad_norm": 0.7927117347717285,
      "learning_rate": 1.781695854139572e-05,
      "loss": 0.0962,
      "step": 511900
    },
    {
      "epoch": 3.325863132937088,
      "grad_norm": 0.9651883244514465,
      "learning_rate": 1.781004808296973e-05,
      "loss": 0.1039,
      "step": 512000
    },
    {
      "epoch": 3.3265127155802396,
      "grad_norm": 1.0061161518096924,
      "learning_rate": 1.7803137624543735e-05,
      "loss": 0.1028,
      "step": 512100
    },
    {
      "epoch": 3.3271622982233913,
      "grad_norm": 1.4411028623580933,
      "learning_rate": 1.7796227166117745e-05,
      "loss": 0.1048,
      "step": 512200
    },
    {
      "epoch": 3.3278118808665433,
      "grad_norm": 0.782372772693634,
      "learning_rate": 1.7789316707691755e-05,
      "loss": 0.0992,
      "step": 512300
    },
    {
      "epoch": 3.328461463509695,
      "grad_norm": 1.0800402164459229,
      "learning_rate": 1.7782406249265765e-05,
      "loss": 0.1027,
      "step": 512400
    },
    {
      "epoch": 3.329111046152847,
      "grad_norm": 0.6929815411567688,
      "learning_rate": 1.7775495790839774e-05,
      "loss": 0.1008,
      "step": 512500
    },
    {
      "epoch": 3.3297606287959987,
      "grad_norm": 1.1254159212112427,
      "learning_rate": 1.776858533241378e-05,
      "loss": 0.1038,
      "step": 512600
    },
    {
      "epoch": 3.3304102114391503,
      "grad_norm": 0.874295711517334,
      "learning_rate": 1.776167487398779e-05,
      "loss": 0.1014,
      "step": 512700
    },
    {
      "epoch": 3.331059794082302,
      "grad_norm": 0.7598658204078674,
      "learning_rate": 1.77547644155618e-05,
      "loss": 0.103,
      "step": 512800
    },
    {
      "epoch": 3.331709376725454,
      "grad_norm": 1.2214152812957764,
      "learning_rate": 1.774785395713581e-05,
      "loss": 0.1104,
      "step": 512900
    },
    {
      "epoch": 3.3323589593686056,
      "grad_norm": 1.9357000589370728,
      "learning_rate": 1.774094349870982e-05,
      "loss": 0.1012,
      "step": 513000
    },
    {
      "epoch": 3.3330085420117577,
      "grad_norm": 1.2878477573394775,
      "learning_rate": 1.7734033040283827e-05,
      "loss": 0.1032,
      "step": 513100
    },
    {
      "epoch": 3.3336581246549093,
      "grad_norm": 1.707899570465088,
      "learning_rate": 1.7727122581857837e-05,
      "loss": 0.0964,
      "step": 513200
    },
    {
      "epoch": 3.334307707298061,
      "grad_norm": 1.2483421564102173,
      "learning_rate": 1.7720212123431846e-05,
      "loss": 0.1032,
      "step": 513300
    },
    {
      "epoch": 3.3349572899412125,
      "grad_norm": 0.955795168876648,
      "learning_rate": 1.7713301665005856e-05,
      "loss": 0.0966,
      "step": 513400
    },
    {
      "epoch": 3.3356068725843646,
      "grad_norm": 1.0795339345932007,
      "learning_rate": 1.7706391206579863e-05,
      "loss": 0.0943,
      "step": 513500
    },
    {
      "epoch": 3.3362564552275162,
      "grad_norm": 0.5549031496047974,
      "learning_rate": 1.7699480748153873e-05,
      "loss": 0.1009,
      "step": 513600
    },
    {
      "epoch": 3.3369060378706683,
      "grad_norm": 0.8621622920036316,
      "learning_rate": 1.769257028972788e-05,
      "loss": 0.0967,
      "step": 513700
    },
    {
      "epoch": 3.33755562051382,
      "grad_norm": 1.2924549579620361,
      "learning_rate": 1.768565983130189e-05,
      "loss": 0.1017,
      "step": 513800
    },
    {
      "epoch": 3.3382052031569716,
      "grad_norm": 0.48925209045410156,
      "learning_rate": 1.76787493728759e-05,
      "loss": 0.101,
      "step": 513900
    },
    {
      "epoch": 3.338854785800123,
      "grad_norm": 1.1978302001953125,
      "learning_rate": 1.7671838914449905e-05,
      "loss": 0.103,
      "step": 514000
    },
    {
      "epoch": 3.3395043684432753,
      "grad_norm": 0.6230157613754272,
      "learning_rate": 1.7664928456023915e-05,
      "loss": 0.0937,
      "step": 514100
    },
    {
      "epoch": 3.340153951086427,
      "grad_norm": 1.8341443538665771,
      "learning_rate": 1.7658017997597925e-05,
      "loss": 0.1055,
      "step": 514200
    },
    {
      "epoch": 3.340803533729579,
      "grad_norm": 0.988614559173584,
      "learning_rate": 1.7651107539171935e-05,
      "loss": 0.1034,
      "step": 514300
    },
    {
      "epoch": 3.3414531163727306,
      "grad_norm": 1.106264352798462,
      "learning_rate": 1.7644197080745945e-05,
      "loss": 0.0994,
      "step": 514400
    },
    {
      "epoch": 3.342102699015882,
      "grad_norm": 1.1140506267547607,
      "learning_rate": 1.763728662231995e-05,
      "loss": 0.1004,
      "step": 514500
    },
    {
      "epoch": 3.3427522816590343,
      "grad_norm": 0.8846196532249451,
      "learning_rate": 1.763037616389396e-05,
      "loss": 0.1003,
      "step": 514600
    },
    {
      "epoch": 3.343401864302186,
      "grad_norm": 1.3809432983398438,
      "learning_rate": 1.762346570546797e-05,
      "loss": 0.1021,
      "step": 514700
    },
    {
      "epoch": 3.3440514469453375,
      "grad_norm": 1.5564020872116089,
      "learning_rate": 1.761655524704198e-05,
      "loss": 0.0994,
      "step": 514800
    },
    {
      "epoch": 3.3447010295884896,
      "grad_norm": 1.2680186033248901,
      "learning_rate": 1.760964478861599e-05,
      "loss": 0.1028,
      "step": 514900
    },
    {
      "epoch": 3.345350612231641,
      "grad_norm": 0.9551310539245605,
      "learning_rate": 1.7602734330189997e-05,
      "loss": 0.0991,
      "step": 515000
    },
    {
      "epoch": 3.346000194874793,
      "grad_norm": 1.5735540390014648,
      "learning_rate": 1.7595823871764007e-05,
      "loss": 0.1072,
      "step": 515100
    },
    {
      "epoch": 3.346649777517945,
      "grad_norm": 0.8729056119918823,
      "learning_rate": 1.7588913413338017e-05,
      "loss": 0.1045,
      "step": 515200
    },
    {
      "epoch": 3.3472993601610965,
      "grad_norm": 1.0481384992599487,
      "learning_rate": 1.7582002954912023e-05,
      "loss": 0.1045,
      "step": 515300
    },
    {
      "epoch": 3.347948942804248,
      "grad_norm": 1.5113457441329956,
      "learning_rate": 1.7575092496486033e-05,
      "loss": 0.1035,
      "step": 515400
    },
    {
      "epoch": 3.3485985254474,
      "grad_norm": 1.3626608848571777,
      "learning_rate": 1.756818203806004e-05,
      "loss": 0.0979,
      "step": 515500
    },
    {
      "epoch": 3.349248108090552,
      "grad_norm": 1.1200734376907349,
      "learning_rate": 1.756127157963405e-05,
      "loss": 0.1004,
      "step": 515600
    },
    {
      "epoch": 3.3498976907337035,
      "grad_norm": 1.3365793228149414,
      "learning_rate": 1.755436112120806e-05,
      "loss": 0.1036,
      "step": 515700
    },
    {
      "epoch": 3.3505472733768555,
      "grad_norm": 1.274014949798584,
      "learning_rate": 1.754745066278207e-05,
      "loss": 0.1082,
      "step": 515800
    },
    {
      "epoch": 3.351196856020007,
      "grad_norm": 1.258172869682312,
      "learning_rate": 1.7540540204356075e-05,
      "loss": 0.1072,
      "step": 515900
    },
    {
      "epoch": 3.351846438663159,
      "grad_norm": 0.9130789041519165,
      "learning_rate": 1.7533629745930085e-05,
      "loss": 0.1022,
      "step": 516000
    },
    {
      "epoch": 3.352496021306311,
      "grad_norm": 0.8623112440109253,
      "learning_rate": 1.7526719287504095e-05,
      "loss": 0.1019,
      "step": 516100
    },
    {
      "epoch": 3.3531456039494625,
      "grad_norm": 1.1503633260726929,
      "learning_rate": 1.7519808829078105e-05,
      "loss": 0.1072,
      "step": 516200
    },
    {
      "epoch": 3.353795186592614,
      "grad_norm": 1.4202624559402466,
      "learning_rate": 1.7512898370652115e-05,
      "loss": 0.1025,
      "step": 516300
    },
    {
      "epoch": 3.354444769235766,
      "grad_norm": 0.7284249663352966,
      "learning_rate": 1.750598791222612e-05,
      "loss": 0.0992,
      "step": 516400
    },
    {
      "epoch": 3.355094351878918,
      "grad_norm": 0.850899875164032,
      "learning_rate": 1.749907745380013e-05,
      "loss": 0.1001,
      "step": 516500
    },
    {
      "epoch": 3.3557439345220694,
      "grad_norm": 1.2292165756225586,
      "learning_rate": 1.749216699537414e-05,
      "loss": 0.1013,
      "step": 516600
    },
    {
      "epoch": 3.3563935171652215,
      "grad_norm": 1.1397279500961304,
      "learning_rate": 1.748525653694815e-05,
      "loss": 0.1041,
      "step": 516700
    },
    {
      "epoch": 3.357043099808373,
      "grad_norm": 1.4368548393249512,
      "learning_rate": 1.747834607852216e-05,
      "loss": 0.0982,
      "step": 516800
    },
    {
      "epoch": 3.3576926824515247,
      "grad_norm": 0.9535197019577026,
      "learning_rate": 1.7471435620096167e-05,
      "loss": 0.1021,
      "step": 516900
    },
    {
      "epoch": 3.358342265094677,
      "grad_norm": 0.864048182964325,
      "learning_rate": 1.7464525161670177e-05,
      "loss": 0.0999,
      "step": 517000
    },
    {
      "epoch": 3.3589918477378284,
      "grad_norm": 1.2450926303863525,
      "learning_rate": 1.7457614703244184e-05,
      "loss": 0.1027,
      "step": 517100
    },
    {
      "epoch": 3.35964143038098,
      "grad_norm": 1.368212103843689,
      "learning_rate": 1.7450704244818193e-05,
      "loss": 0.1009,
      "step": 517200
    },
    {
      "epoch": 3.360291013024132,
      "grad_norm": 1.2684309482574463,
      "learning_rate": 1.74437937863922e-05,
      "loss": 0.1007,
      "step": 517300
    },
    {
      "epoch": 3.3609405956672838,
      "grad_norm": 1.414908528327942,
      "learning_rate": 1.743688332796621e-05,
      "loss": 0.1062,
      "step": 517400
    },
    {
      "epoch": 3.3615901783104354,
      "grad_norm": 1.3137743473052979,
      "learning_rate": 1.742997286954022e-05,
      "loss": 0.1031,
      "step": 517500
    },
    {
      "epoch": 3.3622397609535875,
      "grad_norm": 1.234867811203003,
      "learning_rate": 1.742306241111423e-05,
      "loss": 0.0992,
      "step": 517600
    },
    {
      "epoch": 3.362889343596739,
      "grad_norm": 0.7456042170524597,
      "learning_rate": 1.741615195268824e-05,
      "loss": 0.1006,
      "step": 517700
    },
    {
      "epoch": 3.3635389262398907,
      "grad_norm": 1.6197503805160522,
      "learning_rate": 1.7409241494262246e-05,
      "loss": 0.1027,
      "step": 517800
    },
    {
      "epoch": 3.3641885088830428,
      "grad_norm": 0.5390793681144714,
      "learning_rate": 1.7402331035836256e-05,
      "loss": 0.1045,
      "step": 517900
    },
    {
      "epoch": 3.3648380915261944,
      "grad_norm": 0.865822434425354,
      "learning_rate": 1.7395420577410265e-05,
      "loss": 0.1013,
      "step": 518000
    },
    {
      "epoch": 3.365487674169346,
      "grad_norm": 0.9109897613525391,
      "learning_rate": 1.7388510118984275e-05,
      "loss": 0.1046,
      "step": 518100
    },
    {
      "epoch": 3.366137256812498,
      "grad_norm": 1.044692873954773,
      "learning_rate": 1.7381599660558285e-05,
      "loss": 0.0981,
      "step": 518200
    },
    {
      "epoch": 3.3667868394556497,
      "grad_norm": 0.5397356152534485,
      "learning_rate": 1.737468920213229e-05,
      "loss": 0.0993,
      "step": 518300
    },
    {
      "epoch": 3.3674364220988013,
      "grad_norm": 1.069451093673706,
      "learning_rate": 1.73677787437063e-05,
      "loss": 0.1034,
      "step": 518400
    },
    {
      "epoch": 3.3680860047419534,
      "grad_norm": 1.2170848846435547,
      "learning_rate": 1.736086828528031e-05,
      "loss": 0.1,
      "step": 518500
    },
    {
      "epoch": 3.368735587385105,
      "grad_norm": 1.0113639831542969,
      "learning_rate": 1.735395782685432e-05,
      "loss": 0.1087,
      "step": 518600
    },
    {
      "epoch": 3.3693851700282567,
      "grad_norm": 1.2234362363815308,
      "learning_rate": 1.7347047368428328e-05,
      "loss": 0.1022,
      "step": 518700
    },
    {
      "epoch": 3.3700347526714087,
      "grad_norm": 1.0276490449905396,
      "learning_rate": 1.7340136910002337e-05,
      "loss": 0.0988,
      "step": 518800
    },
    {
      "epoch": 3.3706843353145604,
      "grad_norm": 1.199408769607544,
      "learning_rate": 1.7333226451576344e-05,
      "loss": 0.1018,
      "step": 518900
    },
    {
      "epoch": 3.371333917957712,
      "grad_norm": 0.9936274886131287,
      "learning_rate": 1.7326315993150354e-05,
      "loss": 0.0986,
      "step": 519000
    },
    {
      "epoch": 3.371983500600864,
      "grad_norm": 1.1669164896011353,
      "learning_rate": 1.7319405534724364e-05,
      "loss": 0.1008,
      "step": 519100
    },
    {
      "epoch": 3.3726330832440157,
      "grad_norm": 0.8180949687957764,
      "learning_rate": 1.731249507629837e-05,
      "loss": 0.1045,
      "step": 519200
    },
    {
      "epoch": 3.3732826658871673,
      "grad_norm": 1.3337303400039673,
      "learning_rate": 1.730558461787238e-05,
      "loss": 0.1012,
      "step": 519300
    },
    {
      "epoch": 3.3739322485303194,
      "grad_norm": 1.3944758176803589,
      "learning_rate": 1.729867415944639e-05,
      "loss": 0.1063,
      "step": 519400
    },
    {
      "epoch": 3.374581831173471,
      "grad_norm": 2.3514981269836426,
      "learning_rate": 1.72917637010204e-05,
      "loss": 0.1027,
      "step": 519500
    },
    {
      "epoch": 3.375231413816623,
      "grad_norm": 1.0379012823104858,
      "learning_rate": 1.728485324259441e-05,
      "loss": 0.0955,
      "step": 519600
    },
    {
      "epoch": 3.3758809964597747,
      "grad_norm": 0.9858607053756714,
      "learning_rate": 1.7277942784168416e-05,
      "loss": 0.094,
      "step": 519700
    },
    {
      "epoch": 3.3765305791029263,
      "grad_norm": 1.369720458984375,
      "learning_rate": 1.7271032325742426e-05,
      "loss": 0.0993,
      "step": 519800
    },
    {
      "epoch": 3.377180161746078,
      "grad_norm": 0.5190497040748596,
      "learning_rate": 1.7264121867316436e-05,
      "loss": 0.1006,
      "step": 519900
    },
    {
      "epoch": 3.37782974438923,
      "grad_norm": 1.2462776899337769,
      "learning_rate": 1.7257211408890446e-05,
      "loss": 0.1007,
      "step": 520000
    },
    {
      "epoch": 3.3784793270323816,
      "grad_norm": 0.8187239766120911,
      "learning_rate": 1.7250300950464452e-05,
      "loss": 0.0985,
      "step": 520100
    },
    {
      "epoch": 3.3791289096755337,
      "grad_norm": 1.7044180631637573,
      "learning_rate": 1.7243390492038462e-05,
      "loss": 0.103,
      "step": 520200
    },
    {
      "epoch": 3.3797784923186853,
      "grad_norm": 1.1267640590667725,
      "learning_rate": 1.723648003361247e-05,
      "loss": 0.0973,
      "step": 520300
    },
    {
      "epoch": 3.380428074961837,
      "grad_norm": 1.0972691774368286,
      "learning_rate": 1.722956957518648e-05,
      "loss": 0.1025,
      "step": 520400
    },
    {
      "epoch": 3.3810776576049886,
      "grad_norm": 1.6038403511047363,
      "learning_rate": 1.7222659116760488e-05,
      "loss": 0.1042,
      "step": 520500
    },
    {
      "epoch": 3.3817272402481406,
      "grad_norm": 1.2554047107696533,
      "learning_rate": 1.7215748658334498e-05,
      "loss": 0.0998,
      "step": 520600
    },
    {
      "epoch": 3.3823768228912923,
      "grad_norm": 0.7627405524253845,
      "learning_rate": 1.7208838199908504e-05,
      "loss": 0.0973,
      "step": 520700
    },
    {
      "epoch": 3.3830264055344443,
      "grad_norm": 1.4392975568771362,
      "learning_rate": 1.7201927741482514e-05,
      "loss": 0.1052,
      "step": 520800
    },
    {
      "epoch": 3.383675988177596,
      "grad_norm": 1.1073769330978394,
      "learning_rate": 1.7195017283056524e-05,
      "loss": 0.102,
      "step": 520900
    },
    {
      "epoch": 3.3843255708207476,
      "grad_norm": 1.0943095684051514,
      "learning_rate": 1.7188106824630534e-05,
      "loss": 0.1012,
      "step": 521000
    },
    {
      "epoch": 3.384975153463899,
      "grad_norm": 1.4234439134597778,
      "learning_rate": 1.718119636620454e-05,
      "loss": 0.1019,
      "step": 521100
    },
    {
      "epoch": 3.3856247361070513,
      "grad_norm": 1.0384819507598877,
      "learning_rate": 1.717428590777855e-05,
      "loss": 0.103,
      "step": 521200
    },
    {
      "epoch": 3.386274318750203,
      "grad_norm": 0.9317142963409424,
      "learning_rate": 1.716737544935256e-05,
      "loss": 0.1043,
      "step": 521300
    },
    {
      "epoch": 3.386923901393355,
      "grad_norm": 1.1634984016418457,
      "learning_rate": 1.716046499092657e-05,
      "loss": 0.1019,
      "step": 521400
    },
    {
      "epoch": 3.3875734840365066,
      "grad_norm": 1.2610949277877808,
      "learning_rate": 1.715355453250058e-05,
      "loss": 0.101,
      "step": 521500
    },
    {
      "epoch": 3.3882230666796582,
      "grad_norm": 2.2863900661468506,
      "learning_rate": 1.7146644074074586e-05,
      "loss": 0.1029,
      "step": 521600
    },
    {
      "epoch": 3.38887264932281,
      "grad_norm": 1.2084934711456299,
      "learning_rate": 1.7139733615648596e-05,
      "loss": 0.1044,
      "step": 521700
    },
    {
      "epoch": 3.389522231965962,
      "grad_norm": 1.4774956703186035,
      "learning_rate": 1.7132823157222606e-05,
      "loss": 0.1041,
      "step": 521800
    },
    {
      "epoch": 3.3901718146091135,
      "grad_norm": 1.3791626691818237,
      "learning_rate": 1.7125912698796616e-05,
      "loss": 0.0969,
      "step": 521900
    },
    {
      "epoch": 3.3908213972522656,
      "grad_norm": 0.9142745137214661,
      "learning_rate": 1.7119002240370622e-05,
      "loss": 0.0944,
      "step": 522000
    },
    {
      "epoch": 3.3914709798954172,
      "grad_norm": 1.3770188093185425,
      "learning_rate": 1.7112091781944632e-05,
      "loss": 0.1041,
      "step": 522100
    },
    {
      "epoch": 3.392120562538569,
      "grad_norm": 1.395790934562683,
      "learning_rate": 1.7105181323518642e-05,
      "loss": 0.1013,
      "step": 522200
    },
    {
      "epoch": 3.392770145181721,
      "grad_norm": 1.6789551973342896,
      "learning_rate": 1.709827086509265e-05,
      "loss": 0.1044,
      "step": 522300
    },
    {
      "epoch": 3.3934197278248726,
      "grad_norm": 1.1488382816314697,
      "learning_rate": 1.7091360406666658e-05,
      "loss": 0.1058,
      "step": 522400
    },
    {
      "epoch": 3.394069310468024,
      "grad_norm": 0.8956081867218018,
      "learning_rate": 1.7084449948240665e-05,
      "loss": 0.0979,
      "step": 522500
    },
    {
      "epoch": 3.3947188931111762,
      "grad_norm": 1.2087503671646118,
      "learning_rate": 1.7077539489814675e-05,
      "loss": 0.1007,
      "step": 522600
    },
    {
      "epoch": 3.395368475754328,
      "grad_norm": 1.2535759210586548,
      "learning_rate": 1.7070629031388684e-05,
      "loss": 0.0979,
      "step": 522700
    },
    {
      "epoch": 3.3960180583974795,
      "grad_norm": 1.0445270538330078,
      "learning_rate": 1.7063718572962694e-05,
      "loss": 0.1042,
      "step": 522800
    },
    {
      "epoch": 3.3966676410406316,
      "grad_norm": 0.8625117540359497,
      "learning_rate": 1.7056808114536704e-05,
      "loss": 0.0988,
      "step": 522900
    },
    {
      "epoch": 3.397317223683783,
      "grad_norm": 0.8838788270950317,
      "learning_rate": 1.704989765611071e-05,
      "loss": 0.0991,
      "step": 523000
    },
    {
      "epoch": 3.397966806326935,
      "grad_norm": 0.8587587475776672,
      "learning_rate": 1.704298719768472e-05,
      "loss": 0.1037,
      "step": 523100
    },
    {
      "epoch": 3.398616388970087,
      "grad_norm": 1.5264742374420166,
      "learning_rate": 1.703607673925873e-05,
      "loss": 0.1047,
      "step": 523200
    },
    {
      "epoch": 3.3992659716132385,
      "grad_norm": 0.8619580268859863,
      "learning_rate": 1.702916628083274e-05,
      "loss": 0.1049,
      "step": 523300
    },
    {
      "epoch": 3.39991555425639,
      "grad_norm": 1.1826270818710327,
      "learning_rate": 1.7022255822406747e-05,
      "loss": 0.1039,
      "step": 523400
    },
    {
      "epoch": 3.400565136899542,
      "grad_norm": 1.0619410276412964,
      "learning_rate": 1.7015345363980756e-05,
      "loss": 0.0991,
      "step": 523500
    },
    {
      "epoch": 3.401214719542694,
      "grad_norm": 1.4291013479232788,
      "learning_rate": 1.7008434905554766e-05,
      "loss": 0.1021,
      "step": 523600
    },
    {
      "epoch": 3.4018643021858455,
      "grad_norm": 0.9903867244720459,
      "learning_rate": 1.7001524447128776e-05,
      "loss": 0.0971,
      "step": 523700
    },
    {
      "epoch": 3.4025138848289975,
      "grad_norm": 1.0112454891204834,
      "learning_rate": 1.6994613988702786e-05,
      "loss": 0.0983,
      "step": 523800
    },
    {
      "epoch": 3.403163467472149,
      "grad_norm": 1.2136366367340088,
      "learning_rate": 1.6987703530276792e-05,
      "loss": 0.1037,
      "step": 523900
    },
    {
      "epoch": 3.4038130501153008,
      "grad_norm": 1.0657471418380737,
      "learning_rate": 1.6980793071850802e-05,
      "loss": 0.1006,
      "step": 524000
    },
    {
      "epoch": 3.404462632758453,
      "grad_norm": 0.8657837510108948,
      "learning_rate": 1.697388261342481e-05,
      "loss": 0.1003,
      "step": 524100
    },
    {
      "epoch": 3.4051122154016045,
      "grad_norm": 0.8436123728752136,
      "learning_rate": 1.696697215499882e-05,
      "loss": 0.1,
      "step": 524200
    },
    {
      "epoch": 3.405761798044756,
      "grad_norm": 1.4819755554199219,
      "learning_rate": 1.696006169657283e-05,
      "loss": 0.106,
      "step": 524300
    },
    {
      "epoch": 3.406411380687908,
      "grad_norm": 1.3151878118515015,
      "learning_rate": 1.6953151238146835e-05,
      "loss": 0.1008,
      "step": 524400
    },
    {
      "epoch": 3.40706096333106,
      "grad_norm": 1.3196215629577637,
      "learning_rate": 1.6946240779720845e-05,
      "loss": 0.1028,
      "step": 524500
    },
    {
      "epoch": 3.4077105459742114,
      "grad_norm": 1.220463752746582,
      "learning_rate": 1.6939330321294855e-05,
      "loss": 0.1055,
      "step": 524600
    },
    {
      "epoch": 3.4083601286173635,
      "grad_norm": 1.4377202987670898,
      "learning_rate": 1.6932419862868865e-05,
      "loss": 0.107,
      "step": 524700
    },
    {
      "epoch": 3.409009711260515,
      "grad_norm": 1.3499830961227417,
      "learning_rate": 1.692550940444287e-05,
      "loss": 0.0983,
      "step": 524800
    },
    {
      "epoch": 3.4096592939036667,
      "grad_norm": 0.9915606379508972,
      "learning_rate": 1.691859894601688e-05,
      "loss": 0.1043,
      "step": 524900
    },
    {
      "epoch": 3.410308876546819,
      "grad_norm": 1.6376874446868896,
      "learning_rate": 1.691168848759089e-05,
      "loss": 0.1052,
      "step": 525000
    },
    {
      "epoch": 3.4109584591899704,
      "grad_norm": 0.9556039571762085,
      "learning_rate": 1.69047780291649e-05,
      "loss": 0.1014,
      "step": 525100
    },
    {
      "epoch": 3.411608041833122,
      "grad_norm": 0.9324881434440613,
      "learning_rate": 1.689786757073891e-05,
      "loss": 0.101,
      "step": 525200
    },
    {
      "epoch": 3.412257624476274,
      "grad_norm": 1.7914341688156128,
      "learning_rate": 1.6890957112312917e-05,
      "loss": 0.099,
      "step": 525300
    },
    {
      "epoch": 3.4129072071194257,
      "grad_norm": 0.9693344831466675,
      "learning_rate": 1.6884046653886927e-05,
      "loss": 0.1024,
      "step": 525400
    },
    {
      "epoch": 3.4135567897625774,
      "grad_norm": 0.9742556214332581,
      "learning_rate": 1.6877136195460937e-05,
      "loss": 0.1034,
      "step": 525500
    },
    {
      "epoch": 3.4142063724057294,
      "grad_norm": 0.6023299694061279,
      "learning_rate": 1.6870225737034946e-05,
      "loss": 0.0992,
      "step": 525600
    },
    {
      "epoch": 3.414855955048881,
      "grad_norm": 0.9209145903587341,
      "learning_rate": 1.6863315278608953e-05,
      "loss": 0.1032,
      "step": 525700
    },
    {
      "epoch": 3.4155055376920327,
      "grad_norm": 1.3694889545440674,
      "learning_rate": 1.6856404820182963e-05,
      "loss": 0.1033,
      "step": 525800
    },
    {
      "epoch": 3.4161551203351848,
      "grad_norm": 1.1798648834228516,
      "learning_rate": 1.684949436175697e-05,
      "loss": 0.0976,
      "step": 525900
    },
    {
      "epoch": 3.4168047029783364,
      "grad_norm": 0.878797709941864,
      "learning_rate": 1.684258390333098e-05,
      "loss": 0.1017,
      "step": 526000
    },
    {
      "epoch": 3.417454285621488,
      "grad_norm": 1.3252718448638916,
      "learning_rate": 1.683567344490499e-05,
      "loss": 0.1038,
      "step": 526100
    },
    {
      "epoch": 3.41810386826464,
      "grad_norm": 1.0914720296859741,
      "learning_rate": 1.6828762986479e-05,
      "loss": 0.0992,
      "step": 526200
    },
    {
      "epoch": 3.4187534509077917,
      "grad_norm": 0.7157183289527893,
      "learning_rate": 1.6821852528053005e-05,
      "loss": 0.0942,
      "step": 526300
    },
    {
      "epoch": 3.4194030335509433,
      "grad_norm": 1.9662781953811646,
      "learning_rate": 1.6814942069627015e-05,
      "loss": 0.104,
      "step": 526400
    },
    {
      "epoch": 3.4200526161940954,
      "grad_norm": 1.393123745918274,
      "learning_rate": 1.6808031611201025e-05,
      "loss": 0.1045,
      "step": 526500
    },
    {
      "epoch": 3.420702198837247,
      "grad_norm": 0.7759559750556946,
      "learning_rate": 1.6801121152775035e-05,
      "loss": 0.1047,
      "step": 526600
    },
    {
      "epoch": 3.421351781480399,
      "grad_norm": 1.012600302696228,
      "learning_rate": 1.679421069434904e-05,
      "loss": 0.1015,
      "step": 526700
    },
    {
      "epoch": 3.4220013641235507,
      "grad_norm": 1.247477650642395,
      "learning_rate": 1.678730023592305e-05,
      "loss": 0.1016,
      "step": 526800
    },
    {
      "epoch": 3.4226509467667023,
      "grad_norm": 1.0130679607391357,
      "learning_rate": 1.678038977749706e-05,
      "loss": 0.1032,
      "step": 526900
    },
    {
      "epoch": 3.423300529409854,
      "grad_norm": 1.2359727621078491,
      "learning_rate": 1.677347931907107e-05,
      "loss": 0.1038,
      "step": 527000
    },
    {
      "epoch": 3.423950112053006,
      "grad_norm": 1.2340484857559204,
      "learning_rate": 1.676656886064508e-05,
      "loss": 0.1027,
      "step": 527100
    },
    {
      "epoch": 3.4245996946961577,
      "grad_norm": 1.1577752828598022,
      "learning_rate": 1.6759658402219087e-05,
      "loss": 0.0984,
      "step": 527200
    },
    {
      "epoch": 3.4252492773393097,
      "grad_norm": 1.5031373500823975,
      "learning_rate": 1.6752747943793097e-05,
      "loss": 0.1014,
      "step": 527300
    },
    {
      "epoch": 3.4258988599824614,
      "grad_norm": 1.4385064840316772,
      "learning_rate": 1.6745837485367107e-05,
      "loss": 0.1021,
      "step": 527400
    },
    {
      "epoch": 3.426548442625613,
      "grad_norm": 1.0833295583724976,
      "learning_rate": 1.6738927026941113e-05,
      "loss": 0.111,
      "step": 527500
    },
    {
      "epoch": 3.4271980252687646,
      "grad_norm": 1.0834102630615234,
      "learning_rate": 1.6732016568515123e-05,
      "loss": 0.1006,
      "step": 527600
    },
    {
      "epoch": 3.4278476079119167,
      "grad_norm": 0.8600823879241943,
      "learning_rate": 1.672510611008913e-05,
      "loss": 0.0974,
      "step": 527700
    },
    {
      "epoch": 3.4284971905550683,
      "grad_norm": 0.767189621925354,
      "learning_rate": 1.671819565166314e-05,
      "loss": 0.0993,
      "step": 527800
    },
    {
      "epoch": 3.4291467731982204,
      "grad_norm": 1.1808077096939087,
      "learning_rate": 1.671128519323715e-05,
      "loss": 0.1016,
      "step": 527900
    },
    {
      "epoch": 3.429796355841372,
      "grad_norm": 0.7613866329193115,
      "learning_rate": 1.670437473481116e-05,
      "loss": 0.0991,
      "step": 528000
    },
    {
      "epoch": 3.4304459384845236,
      "grad_norm": 1.2241697311401367,
      "learning_rate": 1.6697464276385166e-05,
      "loss": 0.1018,
      "step": 528100
    },
    {
      "epoch": 3.4310955211276752,
      "grad_norm": 0.8211606740951538,
      "learning_rate": 1.6690553817959175e-05,
      "loss": 0.1024,
      "step": 528200
    },
    {
      "epoch": 3.4317451037708273,
      "grad_norm": 1.336144208908081,
      "learning_rate": 1.6683643359533185e-05,
      "loss": 0.1045,
      "step": 528300
    },
    {
      "epoch": 3.432394686413979,
      "grad_norm": 1.103284478187561,
      "learning_rate": 1.6676732901107195e-05,
      "loss": 0.1006,
      "step": 528400
    },
    {
      "epoch": 3.433044269057131,
      "grad_norm": 1.2016470432281494,
      "learning_rate": 1.6669822442681205e-05,
      "loss": 0.0959,
      "step": 528500
    },
    {
      "epoch": 3.4336938517002826,
      "grad_norm": 1.1566998958587646,
      "learning_rate": 1.666291198425521e-05,
      "loss": 0.0974,
      "step": 528600
    },
    {
      "epoch": 3.4343434343434343,
      "grad_norm": 1.2877066135406494,
      "learning_rate": 1.665600152582922e-05,
      "loss": 0.0994,
      "step": 528700
    },
    {
      "epoch": 3.434993016986586,
      "grad_norm": 1.3305407762527466,
      "learning_rate": 1.664909106740323e-05,
      "loss": 0.0991,
      "step": 528800
    },
    {
      "epoch": 3.435642599629738,
      "grad_norm": 0.6726522445678711,
      "learning_rate": 1.664218060897724e-05,
      "loss": 0.1032,
      "step": 528900
    },
    {
      "epoch": 3.4362921822728896,
      "grad_norm": 1.549695372581482,
      "learning_rate": 1.663527015055125e-05,
      "loss": 0.1,
      "step": 529000
    },
    {
      "epoch": 3.4369417649160416,
      "grad_norm": 1.3012036085128784,
      "learning_rate": 1.6628359692125257e-05,
      "loss": 0.1059,
      "step": 529100
    },
    {
      "epoch": 3.4375913475591933,
      "grad_norm": 1.4375954866409302,
      "learning_rate": 1.6621449233699267e-05,
      "loss": 0.0989,
      "step": 529200
    },
    {
      "epoch": 3.438240930202345,
      "grad_norm": 0.9659761786460876,
      "learning_rate": 1.6614538775273274e-05,
      "loss": 0.1026,
      "step": 529300
    },
    {
      "epoch": 3.438890512845497,
      "grad_norm": 1.1452580690383911,
      "learning_rate": 1.6607628316847284e-05,
      "loss": 0.1006,
      "step": 529400
    },
    {
      "epoch": 3.4395400954886486,
      "grad_norm": 1.2666404247283936,
      "learning_rate": 1.6600717858421293e-05,
      "loss": 0.1041,
      "step": 529500
    },
    {
      "epoch": 3.4401896781318,
      "grad_norm": 1.387291431427002,
      "learning_rate": 1.65938073999953e-05,
      "loss": 0.0987,
      "step": 529600
    },
    {
      "epoch": 3.4408392607749523,
      "grad_norm": 1.216119408607483,
      "learning_rate": 1.658689694156931e-05,
      "loss": 0.1043,
      "step": 529700
    },
    {
      "epoch": 3.441488843418104,
      "grad_norm": 0.9592553377151489,
      "learning_rate": 1.657998648314332e-05,
      "loss": 0.1057,
      "step": 529800
    },
    {
      "epoch": 3.4421384260612555,
      "grad_norm": 0.6531388163566589,
      "learning_rate": 1.657307602471733e-05,
      "loss": 0.1031,
      "step": 529900
    },
    {
      "epoch": 3.4427880087044076,
      "grad_norm": 0.7918151021003723,
      "learning_rate": 1.6566165566291336e-05,
      "loss": 0.1084,
      "step": 530000
    },
    {
      "epoch": 3.4434375913475592,
      "grad_norm": 1.432471752166748,
      "learning_rate": 1.6559255107865346e-05,
      "loss": 0.1031,
      "step": 530100
    },
    {
      "epoch": 3.444087173990711,
      "grad_norm": 1.2130496501922607,
      "learning_rate": 1.6552344649439356e-05,
      "loss": 0.1018,
      "step": 530200
    },
    {
      "epoch": 3.444736756633863,
      "grad_norm": 0.757168710231781,
      "learning_rate": 1.6545434191013365e-05,
      "loss": 0.1043,
      "step": 530300
    },
    {
      "epoch": 3.4453863392770145,
      "grad_norm": 1.0037131309509277,
      "learning_rate": 1.6538523732587375e-05,
      "loss": 0.1038,
      "step": 530400
    },
    {
      "epoch": 3.446035921920166,
      "grad_norm": 1.0508469343185425,
      "learning_rate": 1.6531613274161382e-05,
      "loss": 0.0983,
      "step": 530500
    },
    {
      "epoch": 3.4466855045633182,
      "grad_norm": 1.260176658630371,
      "learning_rate": 1.652470281573539e-05,
      "loss": 0.1015,
      "step": 530600
    },
    {
      "epoch": 3.44733508720647,
      "grad_norm": 0.8957430124282837,
      "learning_rate": 1.65177923573094e-05,
      "loss": 0.0979,
      "step": 530700
    },
    {
      "epoch": 3.4479846698496215,
      "grad_norm": 1.2385313510894775,
      "learning_rate": 1.651088189888341e-05,
      "loss": 0.0998,
      "step": 530800
    },
    {
      "epoch": 3.4486342524927736,
      "grad_norm": 1.3380353450775146,
      "learning_rate": 1.6503971440457418e-05,
      "loss": 0.0917,
      "step": 530900
    },
    {
      "epoch": 3.449283835135925,
      "grad_norm": 0.8683223128318787,
      "learning_rate": 1.6497060982031428e-05,
      "loss": 0.1006,
      "step": 531000
    },
    {
      "epoch": 3.449933417779077,
      "grad_norm": 0.7457877993583679,
      "learning_rate": 1.6490150523605434e-05,
      "loss": 0.1014,
      "step": 531100
    },
    {
      "epoch": 3.450583000422229,
      "grad_norm": 1.278510570526123,
      "learning_rate": 1.6483240065179444e-05,
      "loss": 0.0975,
      "step": 531200
    },
    {
      "epoch": 3.4512325830653805,
      "grad_norm": 0.8332785964012146,
      "learning_rate": 1.6476329606753454e-05,
      "loss": 0.0999,
      "step": 531300
    },
    {
      "epoch": 3.451882165708532,
      "grad_norm": 0.7843794226646423,
      "learning_rate": 1.646941914832746e-05,
      "loss": 0.1065,
      "step": 531400
    },
    {
      "epoch": 3.452531748351684,
      "grad_norm": 1.2191216945648193,
      "learning_rate": 1.646250868990147e-05,
      "loss": 0.0975,
      "step": 531500
    },
    {
      "epoch": 3.453181330994836,
      "grad_norm": 1.5596439838409424,
      "learning_rate": 1.645559823147548e-05,
      "loss": 0.1045,
      "step": 531600
    },
    {
      "epoch": 3.4538309136379874,
      "grad_norm": 0.8263027667999268,
      "learning_rate": 1.644868777304949e-05,
      "loss": 0.1041,
      "step": 531700
    },
    {
      "epoch": 3.4544804962811395,
      "grad_norm": 1.303113579750061,
      "learning_rate": 1.64417773146235e-05,
      "loss": 0.1037,
      "step": 531800
    },
    {
      "epoch": 3.455130078924291,
      "grad_norm": 1.2376289367675781,
      "learning_rate": 1.6434866856197506e-05,
      "loss": 0.1023,
      "step": 531900
    },
    {
      "epoch": 3.4557796615674428,
      "grad_norm": 1.2030210494995117,
      "learning_rate": 1.6427956397771516e-05,
      "loss": 0.1014,
      "step": 532000
    },
    {
      "epoch": 3.456429244210595,
      "grad_norm": 0.7842666506767273,
      "learning_rate": 1.6421045939345526e-05,
      "loss": 0.1028,
      "step": 532100
    },
    {
      "epoch": 3.4570788268537465,
      "grad_norm": 0.856270968914032,
      "learning_rate": 1.6414135480919536e-05,
      "loss": 0.0975,
      "step": 532200
    },
    {
      "epoch": 3.457728409496898,
      "grad_norm": 0.649395227432251,
      "learning_rate": 1.6407225022493545e-05,
      "loss": 0.1015,
      "step": 532300
    },
    {
      "epoch": 3.45837799214005,
      "grad_norm": 1.309533953666687,
      "learning_rate": 1.6400314564067552e-05,
      "loss": 0.1042,
      "step": 532400
    },
    {
      "epoch": 3.4590275747832018,
      "grad_norm": 1.5731699466705322,
      "learning_rate": 1.6393404105641562e-05,
      "loss": 0.0995,
      "step": 532500
    },
    {
      "epoch": 3.4596771574263534,
      "grad_norm": 0.6712274551391602,
      "learning_rate": 1.638649364721557e-05,
      "loss": 0.1005,
      "step": 532600
    },
    {
      "epoch": 3.4603267400695055,
      "grad_norm": 1.1592215299606323,
      "learning_rate": 1.6379583188789578e-05,
      "loss": 0.1007,
      "step": 532700
    },
    {
      "epoch": 3.460976322712657,
      "grad_norm": 0.9343886375427246,
      "learning_rate": 1.6372672730363588e-05,
      "loss": 0.1022,
      "step": 532800
    },
    {
      "epoch": 3.4616259053558087,
      "grad_norm": 1.1741124391555786,
      "learning_rate": 1.6365762271937594e-05,
      "loss": 0.1071,
      "step": 532900
    },
    {
      "epoch": 3.462275487998961,
      "grad_norm": 0.710686206817627,
      "learning_rate": 1.6358851813511604e-05,
      "loss": 0.1045,
      "step": 533000
    },
    {
      "epoch": 3.4629250706421124,
      "grad_norm": 1.4426743984222412,
      "learning_rate": 1.6351941355085614e-05,
      "loss": 0.1039,
      "step": 533100
    },
    {
      "epoch": 3.463574653285264,
      "grad_norm": 1.4558475017547607,
      "learning_rate": 1.6345030896659624e-05,
      "loss": 0.1001,
      "step": 533200
    },
    {
      "epoch": 3.464224235928416,
      "grad_norm": 1.8241863250732422,
      "learning_rate": 1.633812043823363e-05,
      "loss": 0.1004,
      "step": 533300
    },
    {
      "epoch": 3.4648738185715677,
      "grad_norm": 1.2193496227264404,
      "learning_rate": 1.633120997980764e-05,
      "loss": 0.1024,
      "step": 533400
    },
    {
      "epoch": 3.4655234012147194,
      "grad_norm": 1.37050199508667,
      "learning_rate": 1.632429952138165e-05,
      "loss": 0.1,
      "step": 533500
    },
    {
      "epoch": 3.4661729838578714,
      "grad_norm": 0.962379515171051,
      "learning_rate": 1.631738906295566e-05,
      "loss": 0.1011,
      "step": 533600
    },
    {
      "epoch": 3.466822566501023,
      "grad_norm": 1.5619451999664307,
      "learning_rate": 1.631047860452967e-05,
      "loss": 0.097,
      "step": 533700
    },
    {
      "epoch": 3.4674721491441747,
      "grad_norm": 1.246712565422058,
      "learning_rate": 1.6303568146103676e-05,
      "loss": 0.1035,
      "step": 533800
    },
    {
      "epoch": 3.4681217317873267,
      "grad_norm": 1.1331031322479248,
      "learning_rate": 1.6296657687677686e-05,
      "loss": 0.1011,
      "step": 533900
    },
    {
      "epoch": 3.4687713144304784,
      "grad_norm": 1.6217215061187744,
      "learning_rate": 1.6289747229251696e-05,
      "loss": 0.101,
      "step": 534000
    },
    {
      "epoch": 3.46942089707363,
      "grad_norm": 0.9240666031837463,
      "learning_rate": 1.6282836770825706e-05,
      "loss": 0.1051,
      "step": 534100
    },
    {
      "epoch": 3.470070479716782,
      "grad_norm": 1.8533400297164917,
      "learning_rate": 1.6275926312399716e-05,
      "loss": 0.1033,
      "step": 534200
    },
    {
      "epoch": 3.4707200623599337,
      "grad_norm": 0.7848975658416748,
      "learning_rate": 1.6269015853973722e-05,
      "loss": 0.1049,
      "step": 534300
    },
    {
      "epoch": 3.4713696450030858,
      "grad_norm": 1.0103281736373901,
      "learning_rate": 1.6262105395547732e-05,
      "loss": 0.1001,
      "step": 534400
    },
    {
      "epoch": 3.4720192276462374,
      "grad_norm": 0.8283418416976929,
      "learning_rate": 1.625519493712174e-05,
      "loss": 0.1033,
      "step": 534500
    },
    {
      "epoch": 3.472668810289389,
      "grad_norm": 0.6706140637397766,
      "learning_rate": 1.624828447869575e-05,
      "loss": 0.1026,
      "step": 534600
    },
    {
      "epoch": 3.4733183929325406,
      "grad_norm": 1.690118670463562,
      "learning_rate": 1.6241374020269755e-05,
      "loss": 0.1014,
      "step": 534700
    },
    {
      "epoch": 3.4739679755756927,
      "grad_norm": 0.9058604836463928,
      "learning_rate": 1.6234463561843765e-05,
      "loss": 0.0997,
      "step": 534800
    },
    {
      "epoch": 3.4746175582188443,
      "grad_norm": 0.881240725517273,
      "learning_rate": 1.6227553103417775e-05,
      "loss": 0.1057,
      "step": 534900
    },
    {
      "epoch": 3.4752671408619964,
      "grad_norm": 1.3809423446655273,
      "learning_rate": 1.6220642644991784e-05,
      "loss": 0.108,
      "step": 535000
    },
    {
      "epoch": 3.475916723505148,
      "grad_norm": 1.124686598777771,
      "learning_rate": 1.6213732186565794e-05,
      "loss": 0.0966,
      "step": 535100
    },
    {
      "epoch": 3.4765663061482996,
      "grad_norm": 0.7355191111564636,
      "learning_rate": 1.62068217281398e-05,
      "loss": 0.1028,
      "step": 535200
    },
    {
      "epoch": 3.4772158887914513,
      "grad_norm": 1.6121865510940552,
      "learning_rate": 1.619991126971381e-05,
      "loss": 0.1124,
      "step": 535300
    },
    {
      "epoch": 3.4778654714346033,
      "grad_norm": 1.3667006492614746,
      "learning_rate": 1.619300081128782e-05,
      "loss": 0.1058,
      "step": 535400
    },
    {
      "epoch": 3.478515054077755,
      "grad_norm": 1.1090458631515503,
      "learning_rate": 1.618609035286183e-05,
      "loss": 0.1094,
      "step": 535500
    },
    {
      "epoch": 3.479164636720907,
      "grad_norm": 1.6985671520233154,
      "learning_rate": 1.617917989443584e-05,
      "loss": 0.1063,
      "step": 535600
    },
    {
      "epoch": 3.4798142193640587,
      "grad_norm": 1.0959124565124512,
      "learning_rate": 1.6172269436009847e-05,
      "loss": 0.0995,
      "step": 535700
    },
    {
      "epoch": 3.4804638020072103,
      "grad_norm": 0.750318706035614,
      "learning_rate": 1.6165358977583856e-05,
      "loss": 0.0976,
      "step": 535800
    },
    {
      "epoch": 3.481113384650362,
      "grad_norm": 0.7729012370109558,
      "learning_rate": 1.6158448519157866e-05,
      "loss": 0.1024,
      "step": 535900
    },
    {
      "epoch": 3.481762967293514,
      "grad_norm": 1.418958306312561,
      "learning_rate": 1.6151538060731876e-05,
      "loss": 0.0974,
      "step": 536000
    },
    {
      "epoch": 3.4824125499366656,
      "grad_norm": 0.954826295375824,
      "learning_rate": 1.6144627602305883e-05,
      "loss": 0.1005,
      "step": 536100
    },
    {
      "epoch": 3.4830621325798177,
      "grad_norm": 1.1395703554153442,
      "learning_rate": 1.6137717143879892e-05,
      "loss": 0.0996,
      "step": 536200
    },
    {
      "epoch": 3.4837117152229693,
      "grad_norm": 0.8794723749160767,
      "learning_rate": 1.61308066854539e-05,
      "loss": 0.1007,
      "step": 536300
    },
    {
      "epoch": 3.484361297866121,
      "grad_norm": 0.7511826753616333,
      "learning_rate": 1.612389622702791e-05,
      "loss": 0.0998,
      "step": 536400
    },
    {
      "epoch": 3.4850108805092725,
      "grad_norm": 0.8555325269699097,
      "learning_rate": 1.611698576860192e-05,
      "loss": 0.1065,
      "step": 536500
    },
    {
      "epoch": 3.4856604631524246,
      "grad_norm": 1.4541600942611694,
      "learning_rate": 1.6110075310175925e-05,
      "loss": 0.1016,
      "step": 536600
    },
    {
      "epoch": 3.4863100457955762,
      "grad_norm": 1.1028892993927002,
      "learning_rate": 1.6103164851749935e-05,
      "loss": 0.1017,
      "step": 536700
    },
    {
      "epoch": 3.4869596284387283,
      "grad_norm": 0.9811379909515381,
      "learning_rate": 1.6096254393323945e-05,
      "loss": 0.1009,
      "step": 536800
    },
    {
      "epoch": 3.48760921108188,
      "grad_norm": 0.8842518925666809,
      "learning_rate": 1.6089343934897955e-05,
      "loss": 0.1025,
      "step": 536900
    },
    {
      "epoch": 3.4882587937250316,
      "grad_norm": 1.4499281644821167,
      "learning_rate": 1.6082433476471964e-05,
      "loss": 0.1062,
      "step": 537000
    },
    {
      "epoch": 3.4889083763681836,
      "grad_norm": 0.7219815254211426,
      "learning_rate": 1.607552301804597e-05,
      "loss": 0.1016,
      "step": 537100
    },
    {
      "epoch": 3.4895579590113353,
      "grad_norm": 1.2576602697372437,
      "learning_rate": 1.606861255961998e-05,
      "loss": 0.101,
      "step": 537200
    },
    {
      "epoch": 3.490207541654487,
      "grad_norm": 0.790998101234436,
      "learning_rate": 1.606170210119399e-05,
      "loss": 0.1008,
      "step": 537300
    },
    {
      "epoch": 3.490857124297639,
      "grad_norm": 0.9576283693313599,
      "learning_rate": 1.6054791642768e-05,
      "loss": 0.1031,
      "step": 537400
    },
    {
      "epoch": 3.4915067069407906,
      "grad_norm": 1.1720389127731323,
      "learning_rate": 1.6047881184342007e-05,
      "loss": 0.1046,
      "step": 537500
    },
    {
      "epoch": 3.492156289583942,
      "grad_norm": 1.0453190803527832,
      "learning_rate": 1.6040970725916017e-05,
      "loss": 0.1013,
      "step": 537600
    },
    {
      "epoch": 3.4928058722270943,
      "grad_norm": 1.1790066957473755,
      "learning_rate": 1.6034060267490027e-05,
      "loss": 0.0997,
      "step": 537700
    },
    {
      "epoch": 3.493455454870246,
      "grad_norm": 0.9784988164901733,
      "learning_rate": 1.6027149809064037e-05,
      "loss": 0.1022,
      "step": 537800
    },
    {
      "epoch": 3.4941050375133975,
      "grad_norm": 1.111716628074646,
      "learning_rate": 1.6020239350638043e-05,
      "loss": 0.1042,
      "step": 537900
    },
    {
      "epoch": 3.4947546201565496,
      "grad_norm": 0.7862347960472107,
      "learning_rate": 1.601332889221205e-05,
      "loss": 0.1068,
      "step": 538000
    },
    {
      "epoch": 3.495404202799701,
      "grad_norm": 1.0032925605773926,
      "learning_rate": 1.600641843378606e-05,
      "loss": 0.102,
      "step": 538100
    },
    {
      "epoch": 3.496053785442853,
      "grad_norm": 0.7815048694610596,
      "learning_rate": 1.599950797536007e-05,
      "loss": 0.1019,
      "step": 538200
    },
    {
      "epoch": 3.496703368086005,
      "grad_norm": 1.4253544807434082,
      "learning_rate": 1.599259751693408e-05,
      "loss": 0.1052,
      "step": 538300
    },
    {
      "epoch": 3.4973529507291565,
      "grad_norm": 1.2063604593276978,
      "learning_rate": 1.598568705850809e-05,
      "loss": 0.0999,
      "step": 538400
    },
    {
      "epoch": 3.498002533372308,
      "grad_norm": 0.9720965027809143,
      "learning_rate": 1.5978776600082095e-05,
      "loss": 0.1071,
      "step": 538500
    },
    {
      "epoch": 3.4986521160154602,
      "grad_norm": 1.4980027675628662,
      "learning_rate": 1.5971866141656105e-05,
      "loss": 0.1038,
      "step": 538600
    },
    {
      "epoch": 3.499301698658612,
      "grad_norm": 1.336431860923767,
      "learning_rate": 1.5964955683230115e-05,
      "loss": 0.0989,
      "step": 538700
    },
    {
      "epoch": 3.4999512813017635,
      "grad_norm": 1.1456129550933838,
      "learning_rate": 1.5958045224804125e-05,
      "loss": 0.0959,
      "step": 538800
    },
    {
      "epoch": 3.5006008639449155,
      "grad_norm": 0.6144376397132874,
      "learning_rate": 1.5951134766378135e-05,
      "loss": 0.0994,
      "step": 538900
    },
    {
      "epoch": 3.501250446588067,
      "grad_norm": 1.135502815246582,
      "learning_rate": 1.594422430795214e-05,
      "loss": 0.0956,
      "step": 539000
    },
    {
      "epoch": 3.501900029231219,
      "grad_norm": 1.023627519607544,
      "learning_rate": 1.593731384952615e-05,
      "loss": 0.0954,
      "step": 539100
    },
    {
      "epoch": 3.502549611874371,
      "grad_norm": 0.9886214137077332,
      "learning_rate": 1.593040339110016e-05,
      "loss": 0.099,
      "step": 539200
    },
    {
      "epoch": 3.5031991945175225,
      "grad_norm": 1.1559734344482422,
      "learning_rate": 1.592349293267417e-05,
      "loss": 0.1042,
      "step": 539300
    },
    {
      "epoch": 3.503848777160674,
      "grad_norm": 0.8978856205940247,
      "learning_rate": 1.5916582474248177e-05,
      "loss": 0.1014,
      "step": 539400
    },
    {
      "epoch": 3.504498359803826,
      "grad_norm": 0.9032735824584961,
      "learning_rate": 1.5909672015822187e-05,
      "loss": 0.0995,
      "step": 539500
    },
    {
      "epoch": 3.505147942446978,
      "grad_norm": 1.3374508619308472,
      "learning_rate": 1.5902761557396194e-05,
      "loss": 0.1041,
      "step": 539600
    },
    {
      "epoch": 3.5057975250901294,
      "grad_norm": 1.148756980895996,
      "learning_rate": 1.5895851098970203e-05,
      "loss": 0.0956,
      "step": 539700
    },
    {
      "epoch": 3.5064471077332815,
      "grad_norm": 1.575438141822815,
      "learning_rate": 1.5888940640544213e-05,
      "loss": 0.0957,
      "step": 539800
    },
    {
      "epoch": 3.507096690376433,
      "grad_norm": 0.6985929608345032,
      "learning_rate": 1.588203018211822e-05,
      "loss": 0.0959,
      "step": 539900
    },
    {
      "epoch": 3.5077462730195847,
      "grad_norm": 0.9320010542869568,
      "learning_rate": 1.587511972369223e-05,
      "loss": 0.1061,
      "step": 540000
    },
    {
      "epoch": 3.508395855662737,
      "grad_norm": 1.1202280521392822,
      "learning_rate": 1.586820926526624e-05,
      "loss": 0.1008,
      "step": 540100
    },
    {
      "epoch": 3.5090454383058884,
      "grad_norm": 1.182376503944397,
      "learning_rate": 1.586129880684025e-05,
      "loss": 0.1033,
      "step": 540200
    },
    {
      "epoch": 3.5096950209490405,
      "grad_norm": 1.0725113153457642,
      "learning_rate": 1.585438834841426e-05,
      "loss": 0.1026,
      "step": 540300
    },
    {
      "epoch": 3.510344603592192,
      "grad_norm": 1.5815211534500122,
      "learning_rate": 1.5847477889988266e-05,
      "loss": 0.0982,
      "step": 540400
    },
    {
      "epoch": 3.5109941862353438,
      "grad_norm": 1.3928916454315186,
      "learning_rate": 1.5840567431562275e-05,
      "loss": 0.1012,
      "step": 540500
    },
    {
      "epoch": 3.5116437688784954,
      "grad_norm": 1.044740080833435,
      "learning_rate": 1.5833656973136285e-05,
      "loss": 0.1014,
      "step": 540600
    },
    {
      "epoch": 3.5122933515216475,
      "grad_norm": 1.5413200855255127,
      "learning_rate": 1.5826746514710295e-05,
      "loss": 0.097,
      "step": 540700
    },
    {
      "epoch": 3.512942934164799,
      "grad_norm": 1.496535301208496,
      "learning_rate": 1.58198360562843e-05,
      "loss": 0.1038,
      "step": 540800
    },
    {
      "epoch": 3.513592516807951,
      "grad_norm": 1.169703722000122,
      "learning_rate": 1.581292559785831e-05,
      "loss": 0.1036,
      "step": 540900
    },
    {
      "epoch": 3.5142420994511028,
      "grad_norm": 1.1874099969863892,
      "learning_rate": 1.580601513943232e-05,
      "loss": 0.0981,
      "step": 541000
    },
    {
      "epoch": 3.5148916820942544,
      "grad_norm": 1.3246304988861084,
      "learning_rate": 1.579910468100633e-05,
      "loss": 0.0987,
      "step": 541100
    },
    {
      "epoch": 3.515541264737406,
      "grad_norm": 0.8655884861946106,
      "learning_rate": 1.579219422258034e-05,
      "loss": 0.099,
      "step": 541200
    },
    {
      "epoch": 3.516190847380558,
      "grad_norm": 1.4273390769958496,
      "learning_rate": 1.5785283764154347e-05,
      "loss": 0.105,
      "step": 541300
    },
    {
      "epoch": 3.5168404300237097,
      "grad_norm": 0.9620642066001892,
      "learning_rate": 1.5778373305728354e-05,
      "loss": 0.0983,
      "step": 541400
    },
    {
      "epoch": 3.517490012666862,
      "grad_norm": 1.4938440322875977,
      "learning_rate": 1.5771462847302364e-05,
      "loss": 0.1028,
      "step": 541500
    },
    {
      "epoch": 3.5181395953100134,
      "grad_norm": 1.1512353420257568,
      "learning_rate": 1.5764552388876374e-05,
      "loss": 0.0986,
      "step": 541600
    },
    {
      "epoch": 3.518789177953165,
      "grad_norm": 0.8133350014686584,
      "learning_rate": 1.5757641930450383e-05,
      "loss": 0.101,
      "step": 541700
    },
    {
      "epoch": 3.5194387605963167,
      "grad_norm": 1.2138725519180298,
      "learning_rate": 1.575073147202439e-05,
      "loss": 0.1017,
      "step": 541800
    },
    {
      "epoch": 3.5200883432394687,
      "grad_norm": 0.7756360769271851,
      "learning_rate": 1.57438210135984e-05,
      "loss": 0.0962,
      "step": 541900
    },
    {
      "epoch": 3.5207379258826204,
      "grad_norm": 0.9554539322853088,
      "learning_rate": 1.573691055517241e-05,
      "loss": 0.1014,
      "step": 542000
    },
    {
      "epoch": 3.5213875085257724,
      "grad_norm": 1.036436676979065,
      "learning_rate": 1.573000009674642e-05,
      "loss": 0.1052,
      "step": 542100
    },
    {
      "epoch": 3.522037091168924,
      "grad_norm": 0.8399176597595215,
      "learning_rate": 1.572308963832043e-05,
      "loss": 0.1095,
      "step": 542200
    },
    {
      "epoch": 3.5226866738120757,
      "grad_norm": 1.0121183395385742,
      "learning_rate": 1.5716179179894436e-05,
      "loss": 0.103,
      "step": 542300
    },
    {
      "epoch": 3.5233362564552273,
      "grad_norm": 1.629915475845337,
      "learning_rate": 1.5709268721468446e-05,
      "loss": 0.0986,
      "step": 542400
    },
    {
      "epoch": 3.5239858390983794,
      "grad_norm": 1.3784940242767334,
      "learning_rate": 1.5702358263042456e-05,
      "loss": 0.097,
      "step": 542500
    },
    {
      "epoch": 3.524635421741531,
      "grad_norm": 1.311832308769226,
      "learning_rate": 1.5695447804616465e-05,
      "loss": 0.0982,
      "step": 542600
    },
    {
      "epoch": 3.525285004384683,
      "grad_norm": 1.325479507446289,
      "learning_rate": 1.5688537346190472e-05,
      "loss": 0.0938,
      "step": 542700
    },
    {
      "epoch": 3.5259345870278347,
      "grad_norm": 1.060835599899292,
      "learning_rate": 1.568162688776448e-05,
      "loss": 0.1004,
      "step": 542800
    },
    {
      "epoch": 3.5265841696709863,
      "grad_norm": 1.225793480873108,
      "learning_rate": 1.567471642933849e-05,
      "loss": 0.0946,
      "step": 542900
    },
    {
      "epoch": 3.527233752314138,
      "grad_norm": 1.23404061794281,
      "learning_rate": 1.5667805970912498e-05,
      "loss": 0.1084,
      "step": 543000
    },
    {
      "epoch": 3.52788333495729,
      "grad_norm": 1.9101290702819824,
      "learning_rate": 1.5660895512486508e-05,
      "loss": 0.0991,
      "step": 543100
    },
    {
      "epoch": 3.5285329176004416,
      "grad_norm": 1.577194333076477,
      "learning_rate": 1.5653985054060514e-05,
      "loss": 0.0971,
      "step": 543200
    },
    {
      "epoch": 3.5291825002435937,
      "grad_norm": 0.8820879459381104,
      "learning_rate": 1.5647074595634524e-05,
      "loss": 0.0998,
      "step": 543300
    },
    {
      "epoch": 3.5298320828867453,
      "grad_norm": 0.801599383354187,
      "learning_rate": 1.5640164137208534e-05,
      "loss": 0.1052,
      "step": 543400
    },
    {
      "epoch": 3.530481665529897,
      "grad_norm": 1.1549029350280762,
      "learning_rate": 1.5633253678782544e-05,
      "loss": 0.1035,
      "step": 543500
    },
    {
      "epoch": 3.5311312481730486,
      "grad_norm": 1.5779191255569458,
      "learning_rate": 1.5626343220356554e-05,
      "loss": 0.1004,
      "step": 543600
    },
    {
      "epoch": 3.5317808308162006,
      "grad_norm": 1.4082887172698975,
      "learning_rate": 1.561943276193056e-05,
      "loss": 0.0991,
      "step": 543700
    },
    {
      "epoch": 3.5324304134593523,
      "grad_norm": 1.0874567031860352,
      "learning_rate": 1.561252230350457e-05,
      "loss": 0.0999,
      "step": 543800
    },
    {
      "epoch": 3.5330799961025043,
      "grad_norm": 1.1304203271865845,
      "learning_rate": 1.560561184507858e-05,
      "loss": 0.1055,
      "step": 543900
    },
    {
      "epoch": 3.533729578745656,
      "grad_norm": 1.363961935043335,
      "learning_rate": 1.559870138665259e-05,
      "loss": 0.0989,
      "step": 544000
    },
    {
      "epoch": 3.5343791613888076,
      "grad_norm": 0.9517326951026917,
      "learning_rate": 1.5591790928226596e-05,
      "loss": 0.097,
      "step": 544100
    },
    {
      "epoch": 3.535028744031959,
      "grad_norm": 0.8938445448875427,
      "learning_rate": 1.5584880469800606e-05,
      "loss": 0.1017,
      "step": 544200
    },
    {
      "epoch": 3.5356783266751113,
      "grad_norm": 1.4016103744506836,
      "learning_rate": 1.5577970011374616e-05,
      "loss": 0.1006,
      "step": 544300
    },
    {
      "epoch": 3.536327909318263,
      "grad_norm": 0.7424646615982056,
      "learning_rate": 1.5571059552948626e-05,
      "loss": 0.0977,
      "step": 544400
    },
    {
      "epoch": 3.536977491961415,
      "grad_norm": 1.0885345935821533,
      "learning_rate": 1.5564149094522636e-05,
      "loss": 0.0979,
      "step": 544500
    },
    {
      "epoch": 3.5376270746045666,
      "grad_norm": 0.9648560285568237,
      "learning_rate": 1.5557238636096642e-05,
      "loss": 0.0953,
      "step": 544600
    },
    {
      "epoch": 3.5382766572477182,
      "grad_norm": 0.9873165488243103,
      "learning_rate": 1.5550328177670652e-05,
      "loss": 0.1082,
      "step": 544700
    },
    {
      "epoch": 3.53892623989087,
      "grad_norm": 0.9491264224052429,
      "learning_rate": 1.554341771924466e-05,
      "loss": 0.1017,
      "step": 544800
    },
    {
      "epoch": 3.539575822534022,
      "grad_norm": 1.580703616142273,
      "learning_rate": 1.5536507260818668e-05,
      "loss": 0.1022,
      "step": 544900
    },
    {
      "epoch": 3.5402254051771735,
      "grad_norm": 1.1685649156570435,
      "learning_rate": 1.5529596802392678e-05,
      "loss": 0.1049,
      "step": 545000
    },
    {
      "epoch": 3.5408749878203256,
      "grad_norm": 1.1129405498504639,
      "learning_rate": 1.5522686343966685e-05,
      "loss": 0.1038,
      "step": 545100
    },
    {
      "epoch": 3.5415245704634772,
      "grad_norm": 0.8459944128990173,
      "learning_rate": 1.5515775885540694e-05,
      "loss": 0.0975,
      "step": 545200
    },
    {
      "epoch": 3.542174153106629,
      "grad_norm": 1.0172066688537598,
      "learning_rate": 1.5508865427114704e-05,
      "loss": 0.095,
      "step": 545300
    },
    {
      "epoch": 3.5428237357497805,
      "grad_norm": 1.0973228216171265,
      "learning_rate": 1.5501954968688714e-05,
      "loss": 0.0955,
      "step": 545400
    },
    {
      "epoch": 3.5434733183929326,
      "grad_norm": 1.5685770511627197,
      "learning_rate": 1.549504451026272e-05,
      "loss": 0.0984,
      "step": 545500
    },
    {
      "epoch": 3.544122901036084,
      "grad_norm": 1.4178134202957153,
      "learning_rate": 1.548813405183673e-05,
      "loss": 0.104,
      "step": 545600
    },
    {
      "epoch": 3.5447724836792363,
      "grad_norm": 1.645690679550171,
      "learning_rate": 1.548122359341074e-05,
      "loss": 0.0996,
      "step": 545700
    },
    {
      "epoch": 3.545422066322388,
      "grad_norm": 1.093756079673767,
      "learning_rate": 1.547431313498475e-05,
      "loss": 0.099,
      "step": 545800
    },
    {
      "epoch": 3.5460716489655395,
      "grad_norm": 1.0682885646820068,
      "learning_rate": 1.546740267655876e-05,
      "loss": 0.1065,
      "step": 545900
    },
    {
      "epoch": 3.5467212316086916,
      "grad_norm": 1.0754104852676392,
      "learning_rate": 1.5460492218132766e-05,
      "loss": 0.1015,
      "step": 546000
    },
    {
      "epoch": 3.547370814251843,
      "grad_norm": 1.1483906507492065,
      "learning_rate": 1.5453581759706776e-05,
      "loss": 0.1006,
      "step": 546100
    },
    {
      "epoch": 3.548020396894995,
      "grad_norm": 0.7540350556373596,
      "learning_rate": 1.5446671301280786e-05,
      "loss": 0.0996,
      "step": 546200
    },
    {
      "epoch": 3.548669979538147,
      "grad_norm": 1.1073784828186035,
      "learning_rate": 1.5439760842854796e-05,
      "loss": 0.1006,
      "step": 546300
    },
    {
      "epoch": 3.5493195621812985,
      "grad_norm": 0.8562795519828796,
      "learning_rate": 1.5432850384428802e-05,
      "loss": 0.102,
      "step": 546400
    },
    {
      "epoch": 3.54996914482445,
      "grad_norm": 1.5082950592041016,
      "learning_rate": 1.5425939926002812e-05,
      "loss": 0.1014,
      "step": 546500
    },
    {
      "epoch": 3.550618727467602,
      "grad_norm": 0.956438422203064,
      "learning_rate": 1.541902946757682e-05,
      "loss": 0.1031,
      "step": 546600
    },
    {
      "epoch": 3.551268310110754,
      "grad_norm": 0.5075227618217468,
      "learning_rate": 1.541211900915083e-05,
      "loss": 0.1046,
      "step": 546700
    },
    {
      "epoch": 3.5519178927539055,
      "grad_norm": 1.078040361404419,
      "learning_rate": 1.540520855072484e-05,
      "loss": 0.1008,
      "step": 546800
    },
    {
      "epoch": 3.5525674753970575,
      "grad_norm": 0.9581997990608215,
      "learning_rate": 1.539829809229885e-05,
      "loss": 0.0999,
      "step": 546900
    },
    {
      "epoch": 3.553217058040209,
      "grad_norm": 1.1324113607406616,
      "learning_rate": 1.5391387633872855e-05,
      "loss": 0.1005,
      "step": 547000
    },
    {
      "epoch": 3.553866640683361,
      "grad_norm": 0.9769859910011292,
      "learning_rate": 1.5384477175446865e-05,
      "loss": 0.0954,
      "step": 547100
    },
    {
      "epoch": 3.554516223326513,
      "grad_norm": 1.3695684671401978,
      "learning_rate": 1.5377566717020875e-05,
      "loss": 0.1024,
      "step": 547200
    },
    {
      "epoch": 3.5551658059696645,
      "grad_norm": 1.0963813066482544,
      "learning_rate": 1.5370656258594884e-05,
      "loss": 0.0991,
      "step": 547300
    },
    {
      "epoch": 3.5558153886128165,
      "grad_norm": 0.9066751003265381,
      "learning_rate": 1.536374580016889e-05,
      "loss": 0.1003,
      "step": 547400
    },
    {
      "epoch": 3.556464971255968,
      "grad_norm": 0.5437199473381042,
      "learning_rate": 1.53568353417429e-05,
      "loss": 0.1008,
      "step": 547500
    },
    {
      "epoch": 3.55711455389912,
      "grad_norm": 1.3482922315597534,
      "learning_rate": 1.534992488331691e-05,
      "loss": 0.1029,
      "step": 547600
    },
    {
      "epoch": 3.5577641365422714,
      "grad_norm": 0.8079104423522949,
      "learning_rate": 1.534301442489092e-05,
      "loss": 0.1044,
      "step": 547700
    },
    {
      "epoch": 3.5584137191854235,
      "grad_norm": 1.2538658380508423,
      "learning_rate": 1.533610396646493e-05,
      "loss": 0.1055,
      "step": 547800
    },
    {
      "epoch": 3.559063301828575,
      "grad_norm": 1.3367811441421509,
      "learning_rate": 1.5329193508038937e-05,
      "loss": 0.0994,
      "step": 547900
    },
    {
      "epoch": 3.559712884471727,
      "grad_norm": 0.9139057397842407,
      "learning_rate": 1.5322283049612947e-05,
      "loss": 0.099,
      "step": 548000
    },
    {
      "epoch": 3.560362467114879,
      "grad_norm": 1.1450669765472412,
      "learning_rate": 1.5315372591186956e-05,
      "loss": 0.1064,
      "step": 548100
    },
    {
      "epoch": 3.5610120497580304,
      "grad_norm": 0.8516538143157959,
      "learning_rate": 1.5308462132760963e-05,
      "loss": 0.1033,
      "step": 548200
    },
    {
      "epoch": 3.561661632401182,
      "grad_norm": 1.3953123092651367,
      "learning_rate": 1.5301551674334973e-05,
      "loss": 0.1003,
      "step": 548300
    },
    {
      "epoch": 3.562311215044334,
      "grad_norm": 1.2663848400115967,
      "learning_rate": 1.529464121590898e-05,
      "loss": 0.1016,
      "step": 548400
    },
    {
      "epoch": 3.5629607976874857,
      "grad_norm": 0.7521559596061707,
      "learning_rate": 1.528773075748299e-05,
      "loss": 0.1012,
      "step": 548500
    },
    {
      "epoch": 3.563610380330638,
      "grad_norm": 1.015949010848999,
      "learning_rate": 1.5280820299057e-05,
      "loss": 0.0956,
      "step": 548600
    },
    {
      "epoch": 3.5642599629737894,
      "grad_norm": 1.6897152662277222,
      "learning_rate": 1.527390984063101e-05,
      "loss": 0.1032,
      "step": 548700
    },
    {
      "epoch": 3.564909545616941,
      "grad_norm": 1.1568547487258911,
      "learning_rate": 1.5266999382205015e-05,
      "loss": 0.1046,
      "step": 548800
    },
    {
      "epoch": 3.5655591282600927,
      "grad_norm": 1.2833682298660278,
      "learning_rate": 1.5260088923779025e-05,
      "loss": 0.1071,
      "step": 548900
    },
    {
      "epoch": 3.5662087109032448,
      "grad_norm": 1.2895489931106567,
      "learning_rate": 1.5253178465353035e-05,
      "loss": 0.0998,
      "step": 549000
    },
    {
      "epoch": 3.5668582935463964,
      "grad_norm": 0.731144368648529,
      "learning_rate": 1.5246268006927045e-05,
      "loss": 0.1035,
      "step": 549100
    },
    {
      "epoch": 3.5675078761895485,
      "grad_norm": 0.8877061009407043,
      "learning_rate": 1.5239357548501055e-05,
      "loss": 0.103,
      "step": 549200
    },
    {
      "epoch": 3.5681574588327,
      "grad_norm": 1.0935237407684326,
      "learning_rate": 1.5232447090075061e-05,
      "loss": 0.104,
      "step": 549300
    },
    {
      "epoch": 3.5688070414758517,
      "grad_norm": 1.5183333158493042,
      "learning_rate": 1.5225536631649071e-05,
      "loss": 0.0964,
      "step": 549400
    },
    {
      "epoch": 3.5694566241190033,
      "grad_norm": 1.4134474992752075,
      "learning_rate": 1.521862617322308e-05,
      "loss": 0.1014,
      "step": 549500
    },
    {
      "epoch": 3.5701062067621554,
      "grad_norm": 1.3102912902832031,
      "learning_rate": 1.5211715714797089e-05,
      "loss": 0.0997,
      "step": 549600
    },
    {
      "epoch": 3.570755789405307,
      "grad_norm": 0.9033132791519165,
      "learning_rate": 1.5204805256371099e-05,
      "loss": 0.1004,
      "step": 549700
    },
    {
      "epoch": 3.571405372048459,
      "grad_norm": 1.0615376234054565,
      "learning_rate": 1.5197894797945105e-05,
      "loss": 0.0999,
      "step": 549800
    },
    {
      "epoch": 3.5720549546916107,
      "grad_norm": 1.2702897787094116,
      "learning_rate": 1.5190984339519115e-05,
      "loss": 0.1016,
      "step": 549900
    },
    {
      "epoch": 3.5727045373347623,
      "grad_norm": 1.040932059288025,
      "learning_rate": 1.5184073881093125e-05,
      "loss": 0.0979,
      "step": 550000
    },
    {
      "epoch": 3.573354119977914,
      "grad_norm": 1.0138156414031982,
      "learning_rate": 1.5177163422667135e-05,
      "loss": 0.1017,
      "step": 550100
    },
    {
      "epoch": 3.574003702621066,
      "grad_norm": 0.9642082452774048,
      "learning_rate": 1.5170252964241141e-05,
      "loss": 0.0949,
      "step": 550200
    },
    {
      "epoch": 3.5746532852642177,
      "grad_norm": 0.8394267559051514,
      "learning_rate": 1.5163342505815151e-05,
      "loss": 0.1027,
      "step": 550300
    },
    {
      "epoch": 3.5753028679073697,
      "grad_norm": 1.4542171955108643,
      "learning_rate": 1.5156432047389161e-05,
      "loss": 0.1008,
      "step": 550400
    },
    {
      "epoch": 3.5759524505505214,
      "grad_norm": 1.3305351734161377,
      "learning_rate": 1.5149521588963169e-05,
      "loss": 0.0978,
      "step": 550500
    },
    {
      "epoch": 3.576602033193673,
      "grad_norm": 0.7532041072845459,
      "learning_rate": 1.5142611130537179e-05,
      "loss": 0.0985,
      "step": 550600
    },
    {
      "epoch": 3.5772516158368246,
      "grad_norm": 1.0046948194503784,
      "learning_rate": 1.5135700672111185e-05,
      "loss": 0.1021,
      "step": 550700
    },
    {
      "epoch": 3.5779011984799767,
      "grad_norm": 0.537617027759552,
      "learning_rate": 1.5128790213685195e-05,
      "loss": 0.0975,
      "step": 550800
    },
    {
      "epoch": 3.5785507811231283,
      "grad_norm": 1.2022875547409058,
      "learning_rate": 1.5121879755259205e-05,
      "loss": 0.1003,
      "step": 550900
    },
    {
      "epoch": 3.5792003637662804,
      "grad_norm": 1.1038129329681396,
      "learning_rate": 1.5114969296833215e-05,
      "loss": 0.1017,
      "step": 551000
    },
    {
      "epoch": 3.579849946409432,
      "grad_norm": 0.8697724342346191,
      "learning_rate": 1.5108058838407225e-05,
      "loss": 0.1001,
      "step": 551100
    },
    {
      "epoch": 3.5804995290525836,
      "grad_norm": 1.0247118473052979,
      "learning_rate": 1.5101148379981231e-05,
      "loss": 0.1006,
      "step": 551200
    },
    {
      "epoch": 3.5811491116957352,
      "grad_norm": 1.535144329071045,
      "learning_rate": 1.5094237921555241e-05,
      "loss": 0.0961,
      "step": 551300
    },
    {
      "epoch": 3.5817986943388873,
      "grad_norm": 1.637168049812317,
      "learning_rate": 1.508732746312925e-05,
      "loss": 0.0978,
      "step": 551400
    },
    {
      "epoch": 3.582448276982039,
      "grad_norm": 1.018046259880066,
      "learning_rate": 1.508041700470326e-05,
      "loss": 0.0963,
      "step": 551500
    },
    {
      "epoch": 3.583097859625191,
      "grad_norm": 0.9925100803375244,
      "learning_rate": 1.5073506546277269e-05,
      "loss": 0.1056,
      "step": 551600
    },
    {
      "epoch": 3.5837474422683426,
      "grad_norm": 1.582265853881836,
      "learning_rate": 1.5066596087851276e-05,
      "loss": 0.1004,
      "step": 551700
    },
    {
      "epoch": 3.5843970249114943,
      "grad_norm": 1.143790364265442,
      "learning_rate": 1.5059685629425285e-05,
      "loss": 0.0973,
      "step": 551800
    },
    {
      "epoch": 3.585046607554646,
      "grad_norm": 1.204514741897583,
      "learning_rate": 1.5052775170999295e-05,
      "loss": 0.0957,
      "step": 551900
    },
    {
      "epoch": 3.585696190197798,
      "grad_norm": 1.5004068613052368,
      "learning_rate": 1.5045864712573305e-05,
      "loss": 0.1049,
      "step": 552000
    },
    {
      "epoch": 3.5863457728409496,
      "grad_norm": 1.7074356079101562,
      "learning_rate": 1.5038954254147312e-05,
      "loss": 0.1015,
      "step": 552100
    },
    {
      "epoch": 3.5869953554841016,
      "grad_norm": 1.2672561407089233,
      "learning_rate": 1.5032043795721321e-05,
      "loss": 0.1025,
      "step": 552200
    },
    {
      "epoch": 3.5876449381272533,
      "grad_norm": 1.0337578058242798,
      "learning_rate": 1.502513333729533e-05,
      "loss": 0.096,
      "step": 552300
    },
    {
      "epoch": 3.588294520770405,
      "grad_norm": 1.0077608823776245,
      "learning_rate": 1.501822287886934e-05,
      "loss": 0.0978,
      "step": 552400
    },
    {
      "epoch": 3.5889441034135565,
      "grad_norm": 0.8966584801673889,
      "learning_rate": 1.501131242044335e-05,
      "loss": 0.0981,
      "step": 552500
    },
    {
      "epoch": 3.5895936860567086,
      "grad_norm": 1.0224894285202026,
      "learning_rate": 1.5004401962017356e-05,
      "loss": 0.1014,
      "step": 552600
    },
    {
      "epoch": 3.59024326869986,
      "grad_norm": 1.1400787830352783,
      "learning_rate": 1.4997491503591366e-05,
      "loss": 0.1002,
      "step": 552700
    },
    {
      "epoch": 3.5908928513430123,
      "grad_norm": 1.3037400245666504,
      "learning_rate": 1.4990581045165375e-05,
      "loss": 0.102,
      "step": 552800
    },
    {
      "epoch": 3.591542433986164,
      "grad_norm": 0.821183979511261,
      "learning_rate": 1.4983670586739385e-05,
      "loss": 0.1048,
      "step": 552900
    },
    {
      "epoch": 3.5921920166293155,
      "grad_norm": 1.5332220792770386,
      "learning_rate": 1.4976760128313393e-05,
      "loss": 0.1016,
      "step": 553000
    },
    {
      "epoch": 3.5928415992724676,
      "grad_norm": 1.113499641418457,
      "learning_rate": 1.4969849669887402e-05,
      "loss": 0.1009,
      "step": 553100
    },
    {
      "epoch": 3.5934911819156192,
      "grad_norm": 1.5094246864318848,
      "learning_rate": 1.496293921146141e-05,
      "loss": 0.1057,
      "step": 553200
    },
    {
      "epoch": 3.594140764558771,
      "grad_norm": 1.085386037826538,
      "learning_rate": 1.495602875303542e-05,
      "loss": 0.1,
      "step": 553300
    },
    {
      "epoch": 3.594790347201923,
      "grad_norm": 0.7248737215995789,
      "learning_rate": 1.494911829460943e-05,
      "loss": 0.0964,
      "step": 553400
    },
    {
      "epoch": 3.5954399298450745,
      "grad_norm": 1.010495901107788,
      "learning_rate": 1.4942207836183436e-05,
      "loss": 0.0983,
      "step": 553500
    },
    {
      "epoch": 3.596089512488226,
      "grad_norm": 1.4981346130371094,
      "learning_rate": 1.4935297377757446e-05,
      "loss": 0.1043,
      "step": 553600
    },
    {
      "epoch": 3.5967390951313782,
      "grad_norm": 0.926727831363678,
      "learning_rate": 1.4928386919331456e-05,
      "loss": 0.0976,
      "step": 553700
    },
    {
      "epoch": 3.59738867777453,
      "grad_norm": 0.785515308380127,
      "learning_rate": 1.4921476460905465e-05,
      "loss": 0.103,
      "step": 553800
    },
    {
      "epoch": 3.5980382604176815,
      "grad_norm": 1.7156850099563599,
      "learning_rate": 1.4914566002479474e-05,
      "loss": 0.1032,
      "step": 553900
    },
    {
      "epoch": 3.5986878430608336,
      "grad_norm": 1.8404511213302612,
      "learning_rate": 1.4907655544053482e-05,
      "loss": 0.0998,
      "step": 554000
    },
    {
      "epoch": 3.599337425703985,
      "grad_norm": 1.4919366836547852,
      "learning_rate": 1.490074508562749e-05,
      "loss": 0.1063,
      "step": 554100
    },
    {
      "epoch": 3.599987008347137,
      "grad_norm": 1.639877438545227,
      "learning_rate": 1.48938346272015e-05,
      "loss": 0.1041,
      "step": 554200
    },
    {
      "epoch": 3.600636590990289,
      "grad_norm": 1.1963738203048706,
      "learning_rate": 1.488692416877551e-05,
      "loss": 0.1,
      "step": 554300
    },
    {
      "epoch": 3.6012861736334405,
      "grad_norm": 1.1989421844482422,
      "learning_rate": 1.488001371034952e-05,
      "loss": 0.0999,
      "step": 554400
    },
    {
      "epoch": 3.601935756276592,
      "grad_norm": 0.9579912424087524,
      "learning_rate": 1.4873103251923526e-05,
      "loss": 0.103,
      "step": 554500
    },
    {
      "epoch": 3.602585338919744,
      "grad_norm": 1.2436379194259644,
      "learning_rate": 1.4866192793497536e-05,
      "loss": 0.0991,
      "step": 554600
    },
    {
      "epoch": 3.603234921562896,
      "grad_norm": 0.9475583434104919,
      "learning_rate": 1.4859282335071546e-05,
      "loss": 0.1036,
      "step": 554700
    },
    {
      "epoch": 3.6038845042060474,
      "grad_norm": 0.9557821750640869,
      "learning_rate": 1.4852371876645554e-05,
      "loss": 0.1002,
      "step": 554800
    },
    {
      "epoch": 3.6045340868491995,
      "grad_norm": 1.0783910751342773,
      "learning_rate": 1.4845461418219564e-05,
      "loss": 0.097,
      "step": 554900
    },
    {
      "epoch": 3.605183669492351,
      "grad_norm": 1.0664985179901123,
      "learning_rate": 1.483855095979357e-05,
      "loss": 0.1042,
      "step": 555000
    },
    {
      "epoch": 3.605833252135503,
      "grad_norm": 1.3162972927093506,
      "learning_rate": 1.483164050136758e-05,
      "loss": 0.098,
      "step": 555100
    },
    {
      "epoch": 3.606482834778655,
      "grad_norm": 0.9774771928787231,
      "learning_rate": 1.482473004294159e-05,
      "loss": 0.1041,
      "step": 555200
    },
    {
      "epoch": 3.6071324174218065,
      "grad_norm": 0.6292826533317566,
      "learning_rate": 1.48178195845156e-05,
      "loss": 0.101,
      "step": 555300
    },
    {
      "epoch": 3.607782000064958,
      "grad_norm": 1.732118010520935,
      "learning_rate": 1.4810909126089606e-05,
      "loss": 0.1,
      "step": 555400
    },
    {
      "epoch": 3.60843158270811,
      "grad_norm": 1.3726776838302612,
      "learning_rate": 1.4803998667663616e-05,
      "loss": 0.0939,
      "step": 555500
    },
    {
      "epoch": 3.6090811653512618,
      "grad_norm": 1.2517036199569702,
      "learning_rate": 1.4797088209237626e-05,
      "loss": 0.0998,
      "step": 555600
    },
    {
      "epoch": 3.609730747994414,
      "grad_norm": 1.5314470529556274,
      "learning_rate": 1.4790177750811634e-05,
      "loss": 0.099,
      "step": 555700
    },
    {
      "epoch": 3.6103803306375655,
      "grad_norm": 1.3429992198944092,
      "learning_rate": 1.4783267292385644e-05,
      "loss": 0.1033,
      "step": 555800
    },
    {
      "epoch": 3.611029913280717,
      "grad_norm": 1.4460465908050537,
      "learning_rate": 1.477635683395965e-05,
      "loss": 0.1092,
      "step": 555900
    },
    {
      "epoch": 3.6116794959238687,
      "grad_norm": 1.6049820184707642,
      "learning_rate": 1.476944637553366e-05,
      "loss": 0.1058,
      "step": 556000
    },
    {
      "epoch": 3.612329078567021,
      "grad_norm": 1.5497009754180908,
      "learning_rate": 1.476253591710767e-05,
      "loss": 0.104,
      "step": 556100
    },
    {
      "epoch": 3.6129786612101724,
      "grad_norm": 1.2822171449661255,
      "learning_rate": 1.475562545868168e-05,
      "loss": 0.1029,
      "step": 556200
    },
    {
      "epoch": 3.6136282438533245,
      "grad_norm": 1.0591516494750977,
      "learning_rate": 1.474871500025569e-05,
      "loss": 0.1049,
      "step": 556300
    },
    {
      "epoch": 3.614277826496476,
      "grad_norm": 0.7852630019187927,
      "learning_rate": 1.4741804541829696e-05,
      "loss": 0.0975,
      "step": 556400
    },
    {
      "epoch": 3.6149274091396277,
      "grad_norm": 1.1081664562225342,
      "learning_rate": 1.4734894083403706e-05,
      "loss": 0.101,
      "step": 556500
    },
    {
      "epoch": 3.6155769917827794,
      "grad_norm": 1.4542442560195923,
      "learning_rate": 1.4727983624977714e-05,
      "loss": 0.1042,
      "step": 556600
    },
    {
      "epoch": 3.6162265744259314,
      "grad_norm": 1.0518782138824463,
      "learning_rate": 1.4721073166551724e-05,
      "loss": 0.0966,
      "step": 556700
    },
    {
      "epoch": 3.616876157069083,
      "grad_norm": 0.9154441356658936,
      "learning_rate": 1.471416270812573e-05,
      "loss": 0.0983,
      "step": 556800
    },
    {
      "epoch": 3.617525739712235,
      "grad_norm": 1.1184040307998657,
      "learning_rate": 1.470725224969974e-05,
      "loss": 0.0999,
      "step": 556900
    },
    {
      "epoch": 3.6181753223553867,
      "grad_norm": 0.7127594947814941,
      "learning_rate": 1.470034179127375e-05,
      "loss": 0.1079,
      "step": 557000
    },
    {
      "epoch": 3.6188249049985384,
      "grad_norm": 1.217154622077942,
      "learning_rate": 1.469343133284776e-05,
      "loss": 0.1093,
      "step": 557100
    },
    {
      "epoch": 3.61947448764169,
      "grad_norm": 1.4275437593460083,
      "learning_rate": 1.468652087442177e-05,
      "loss": 0.0984,
      "step": 557200
    },
    {
      "epoch": 3.620124070284842,
      "grad_norm": 0.5069926977157593,
      "learning_rate": 1.4679610415995776e-05,
      "loss": 0.0937,
      "step": 557300
    },
    {
      "epoch": 3.6207736529279937,
      "grad_norm": 0.9747323393821716,
      "learning_rate": 1.4672699957569786e-05,
      "loss": 0.1044,
      "step": 557400
    },
    {
      "epoch": 3.6214232355711458,
      "grad_norm": 1.2388660907745361,
      "learning_rate": 1.4665789499143794e-05,
      "loss": 0.1003,
      "step": 557500
    },
    {
      "epoch": 3.6220728182142974,
      "grad_norm": 0.8625158667564392,
      "learning_rate": 1.4658879040717804e-05,
      "loss": 0.1027,
      "step": 557600
    },
    {
      "epoch": 3.622722400857449,
      "grad_norm": 0.3919748067855835,
      "learning_rate": 1.4651968582291814e-05,
      "loss": 0.0996,
      "step": 557700
    },
    {
      "epoch": 3.6233719835006006,
      "grad_norm": 1.2453274726867676,
      "learning_rate": 1.464505812386582e-05,
      "loss": 0.0976,
      "step": 557800
    },
    {
      "epoch": 3.6240215661437527,
      "grad_norm": 1.262955904006958,
      "learning_rate": 1.463814766543983e-05,
      "loss": 0.0975,
      "step": 557900
    },
    {
      "epoch": 3.6246711487869043,
      "grad_norm": 1.0868209600448608,
      "learning_rate": 1.463123720701384e-05,
      "loss": 0.0981,
      "step": 558000
    },
    {
      "epoch": 3.6253207314300564,
      "grad_norm": 1.0075430870056152,
      "learning_rate": 1.462432674858785e-05,
      "loss": 0.1035,
      "step": 558100
    },
    {
      "epoch": 3.625970314073208,
      "grad_norm": 0.8138526678085327,
      "learning_rate": 1.4617416290161857e-05,
      "loss": 0.0955,
      "step": 558200
    },
    {
      "epoch": 3.6266198967163596,
      "grad_norm": 0.9513419270515442,
      "learning_rate": 1.4610505831735866e-05,
      "loss": 0.0949,
      "step": 558300
    },
    {
      "epoch": 3.6272694793595113,
      "grad_norm": 1.0838871002197266,
      "learning_rate": 1.4603595373309875e-05,
      "loss": 0.1083,
      "step": 558400
    },
    {
      "epoch": 3.6279190620026633,
      "grad_norm": 1.2053900957107544,
      "learning_rate": 1.4596684914883884e-05,
      "loss": 0.1026,
      "step": 558500
    },
    {
      "epoch": 3.628568644645815,
      "grad_norm": 0.9547865390777588,
      "learning_rate": 1.4589774456457894e-05,
      "loss": 0.094,
      "step": 558600
    },
    {
      "epoch": 3.629218227288967,
      "grad_norm": 1.1291568279266357,
      "learning_rate": 1.45828639980319e-05,
      "loss": 0.0989,
      "step": 558700
    },
    {
      "epoch": 3.6298678099321187,
      "grad_norm": 1.199098825454712,
      "learning_rate": 1.457595353960591e-05,
      "loss": 0.0982,
      "step": 558800
    },
    {
      "epoch": 3.6305173925752703,
      "grad_norm": 1.2374519109725952,
      "learning_rate": 1.456904308117992e-05,
      "loss": 0.0973,
      "step": 558900
    },
    {
      "epoch": 3.631166975218422,
      "grad_norm": 1.0466139316558838,
      "learning_rate": 1.456213262275393e-05,
      "loss": 0.1083,
      "step": 559000
    },
    {
      "epoch": 3.631816557861574,
      "grad_norm": 1.0119706392288208,
      "learning_rate": 1.4555222164327938e-05,
      "loss": 0.1005,
      "step": 559100
    },
    {
      "epoch": 3.6324661405047256,
      "grad_norm": 1.8870441913604736,
      "learning_rate": 1.4548311705901947e-05,
      "loss": 0.1034,
      "step": 559200
    },
    {
      "epoch": 3.6331157231478777,
      "grad_norm": 1.0464495420455933,
      "learning_rate": 1.4541401247475955e-05,
      "loss": 0.0965,
      "step": 559300
    },
    {
      "epoch": 3.6337653057910293,
      "grad_norm": 1.1762237548828125,
      "learning_rate": 1.4534490789049965e-05,
      "loss": 0.1013,
      "step": 559400
    },
    {
      "epoch": 3.634414888434181,
      "grad_norm": 0.8751921653747559,
      "learning_rate": 1.4527580330623974e-05,
      "loss": 0.1059,
      "step": 559500
    },
    {
      "epoch": 3.6350644710773325,
      "grad_norm": 0.7664720416069031,
      "learning_rate": 1.4520669872197984e-05,
      "loss": 0.1025,
      "step": 559600
    },
    {
      "epoch": 3.6357140537204846,
      "grad_norm": 0.7836875319480896,
      "learning_rate": 1.451375941377199e-05,
      "loss": 0.1,
      "step": 559700
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 0.7612467408180237,
      "learning_rate": 1.4506848955346e-05,
      "loss": 0.1034,
      "step": 559800
    },
    {
      "epoch": 3.6370132190067883,
      "grad_norm": 1.076805591583252,
      "learning_rate": 1.449993849692001e-05,
      "loss": 0.1025,
      "step": 559900
    },
    {
      "epoch": 3.63766280164994,
      "grad_norm": 1.1238350868225098,
      "learning_rate": 1.4493028038494019e-05,
      "loss": 0.1034,
      "step": 560000
    },
    {
      "epoch": 3.6383123842930916,
      "grad_norm": 0.9168123006820679,
      "learning_rate": 1.4486117580068027e-05,
      "loss": 0.0989,
      "step": 560100
    },
    {
      "epoch": 3.638961966936243,
      "grad_norm": 0.8936764001846313,
      "learning_rate": 1.4479207121642035e-05,
      "loss": 0.0975,
      "step": 560200
    },
    {
      "epoch": 3.6396115495793953,
      "grad_norm": 1.4209351539611816,
      "learning_rate": 1.4472296663216045e-05,
      "loss": 0.1015,
      "step": 560300
    },
    {
      "epoch": 3.640261132222547,
      "grad_norm": 1.2158100605010986,
      "learning_rate": 1.4465386204790055e-05,
      "loss": 0.1053,
      "step": 560400
    },
    {
      "epoch": 3.640910714865699,
      "grad_norm": 1.0981054306030273,
      "learning_rate": 1.4458475746364065e-05,
      "loss": 0.0967,
      "step": 560500
    },
    {
      "epoch": 3.6415602975088506,
      "grad_norm": 1.4747484922409058,
      "learning_rate": 1.4451565287938071e-05,
      "loss": 0.1,
      "step": 560600
    },
    {
      "epoch": 3.642209880152002,
      "grad_norm": 1.2004883289337158,
      "learning_rate": 1.4444654829512081e-05,
      "loss": 0.1021,
      "step": 560700
    },
    {
      "epoch": 3.6428594627951543,
      "grad_norm": 1.438974142074585,
      "learning_rate": 1.443774437108609e-05,
      "loss": 0.0991,
      "step": 560800
    },
    {
      "epoch": 3.643509045438306,
      "grad_norm": 0.8311817646026611,
      "learning_rate": 1.4430833912660099e-05,
      "loss": 0.1003,
      "step": 560900
    },
    {
      "epoch": 3.6441586280814575,
      "grad_norm": 1.0169260501861572,
      "learning_rate": 1.4423923454234109e-05,
      "loss": 0.0994,
      "step": 561000
    },
    {
      "epoch": 3.6448082107246096,
      "grad_norm": 1.1814533472061157,
      "learning_rate": 1.4417012995808115e-05,
      "loss": 0.0993,
      "step": 561100
    },
    {
      "epoch": 3.645457793367761,
      "grad_norm": 1.0258811712265015,
      "learning_rate": 1.4410102537382125e-05,
      "loss": 0.098,
      "step": 561200
    },
    {
      "epoch": 3.646107376010913,
      "grad_norm": 1.227081060409546,
      "learning_rate": 1.4403192078956135e-05,
      "loss": 0.096,
      "step": 561300
    },
    {
      "epoch": 3.646756958654065,
      "grad_norm": 1.4593957662582397,
      "learning_rate": 1.4396281620530145e-05,
      "loss": 0.1084,
      "step": 561400
    },
    {
      "epoch": 3.6474065412972165,
      "grad_norm": 1.00774347782135,
      "learning_rate": 1.4389371162104151e-05,
      "loss": 0.098,
      "step": 561500
    },
    {
      "epoch": 3.648056123940368,
      "grad_norm": 0.8570981621742249,
      "learning_rate": 1.4382460703678161e-05,
      "loss": 0.0974,
      "step": 561600
    },
    {
      "epoch": 3.6487057065835202,
      "grad_norm": 1.4255801439285278,
      "learning_rate": 1.4375550245252171e-05,
      "loss": 0.0949,
      "step": 561700
    },
    {
      "epoch": 3.649355289226672,
      "grad_norm": 0.97745680809021,
      "learning_rate": 1.4368639786826179e-05,
      "loss": 0.1034,
      "step": 561800
    },
    {
      "epoch": 3.6500048718698235,
      "grad_norm": 0.9956642389297485,
      "learning_rate": 1.4361729328400189e-05,
      "loss": 0.0971,
      "step": 561900
    },
    {
      "epoch": 3.6506544545129755,
      "grad_norm": 1.3040988445281982,
      "learning_rate": 1.4354818869974195e-05,
      "loss": 0.1037,
      "step": 562000
    },
    {
      "epoch": 3.651304037156127,
      "grad_norm": 1.7955865859985352,
      "learning_rate": 1.4347908411548205e-05,
      "loss": 0.0927,
      "step": 562100
    },
    {
      "epoch": 3.6519536197992792,
      "grad_norm": 1.2208726406097412,
      "learning_rate": 1.4340997953122215e-05,
      "loss": 0.1014,
      "step": 562200
    },
    {
      "epoch": 3.652603202442431,
      "grad_norm": 0.9725949168205261,
      "learning_rate": 1.4334087494696225e-05,
      "loss": 0.0968,
      "step": 562300
    },
    {
      "epoch": 3.6532527850855825,
      "grad_norm": 0.7465123534202576,
      "learning_rate": 1.4327177036270235e-05,
      "loss": 0.0988,
      "step": 562400
    },
    {
      "epoch": 3.653902367728734,
      "grad_norm": 1.419250249862671,
      "learning_rate": 1.4320266577844241e-05,
      "loss": 0.1017,
      "step": 562500
    },
    {
      "epoch": 3.654551950371886,
      "grad_norm": 0.7964399456977844,
      "learning_rate": 1.4313356119418251e-05,
      "loss": 0.1002,
      "step": 562600
    },
    {
      "epoch": 3.655201533015038,
      "grad_norm": 1.1760174036026,
      "learning_rate": 1.430644566099226e-05,
      "loss": 0.1015,
      "step": 562700
    },
    {
      "epoch": 3.65585111565819,
      "grad_norm": 1.2200433015823364,
      "learning_rate": 1.4299535202566269e-05,
      "loss": 0.097,
      "step": 562800
    },
    {
      "epoch": 3.6565006983013415,
      "grad_norm": 1.1252720355987549,
      "learning_rate": 1.4292624744140276e-05,
      "loss": 0.0944,
      "step": 562900
    },
    {
      "epoch": 3.657150280944493,
      "grad_norm": 0.7813279032707214,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 0.0994,
      "step": 563000
    },
    {
      "epoch": 3.6577998635876448,
      "grad_norm": 1.0019726753234863,
      "learning_rate": 1.4278803827288295e-05,
      "loss": 0.0981,
      "step": 563100
    },
    {
      "epoch": 3.658449446230797,
      "grad_norm": 1.6697889566421509,
      "learning_rate": 1.4271893368862305e-05,
      "loss": 0.1,
      "step": 563200
    },
    {
      "epoch": 3.6590990288739484,
      "grad_norm": 1.2699668407440186,
      "learning_rate": 1.4264982910436315e-05,
      "loss": 0.0987,
      "step": 563300
    },
    {
      "epoch": 3.6597486115171005,
      "grad_norm": 1.7459216117858887,
      "learning_rate": 1.4258072452010321e-05,
      "loss": 0.1034,
      "step": 563400
    },
    {
      "epoch": 3.660398194160252,
      "grad_norm": 1.1956862211227417,
      "learning_rate": 1.4251161993584331e-05,
      "loss": 0.101,
      "step": 563500
    },
    {
      "epoch": 3.6610477768034038,
      "grad_norm": 1.1658475399017334,
      "learning_rate": 1.424425153515834e-05,
      "loss": 0.1043,
      "step": 563600
    },
    {
      "epoch": 3.6616973594465554,
      "grad_norm": 1.6630887985229492,
      "learning_rate": 1.423734107673235e-05,
      "loss": 0.0954,
      "step": 563700
    },
    {
      "epoch": 3.6623469420897075,
      "grad_norm": 1.0047295093536377,
      "learning_rate": 1.423043061830636e-05,
      "loss": 0.102,
      "step": 563800
    },
    {
      "epoch": 3.662996524732859,
      "grad_norm": 0.7420085668563843,
      "learning_rate": 1.4223520159880366e-05,
      "loss": 0.1048,
      "step": 563900
    },
    {
      "epoch": 3.663646107376011,
      "grad_norm": 1.084883451461792,
      "learning_rate": 1.4216609701454375e-05,
      "loss": 0.0979,
      "step": 564000
    },
    {
      "epoch": 3.6642956900191628,
      "grad_norm": 1.5380773544311523,
      "learning_rate": 1.4209699243028385e-05,
      "loss": 0.0973,
      "step": 564100
    },
    {
      "epoch": 3.6649452726623144,
      "grad_norm": 1.3264650106430054,
      "learning_rate": 1.4202788784602395e-05,
      "loss": 0.0969,
      "step": 564200
    },
    {
      "epoch": 3.665594855305466,
      "grad_norm": 0.9399426579475403,
      "learning_rate": 1.4195878326176403e-05,
      "loss": 0.0971,
      "step": 564300
    },
    {
      "epoch": 3.666244437948618,
      "grad_norm": 1.4301551580429077,
      "learning_rate": 1.418896786775041e-05,
      "loss": 0.1058,
      "step": 564400
    },
    {
      "epoch": 3.6668940205917697,
      "grad_norm": 1.2279162406921387,
      "learning_rate": 1.418205740932442e-05,
      "loss": 0.0972,
      "step": 564500
    },
    {
      "epoch": 3.667543603234922,
      "grad_norm": 0.9634023308753967,
      "learning_rate": 1.417514695089843e-05,
      "loss": 0.0962,
      "step": 564600
    },
    {
      "epoch": 3.6681931858780734,
      "grad_norm": 1.4605958461761475,
      "learning_rate": 1.416823649247244e-05,
      "loss": 0.1004,
      "step": 564700
    },
    {
      "epoch": 3.668842768521225,
      "grad_norm": 1.167486548423767,
      "learning_rate": 1.4161326034046446e-05,
      "loss": 0.0962,
      "step": 564800
    },
    {
      "epoch": 3.6694923511643767,
      "grad_norm": 0.8677736520767212,
      "learning_rate": 1.4154415575620456e-05,
      "loss": 0.0982,
      "step": 564900
    },
    {
      "epoch": 3.6701419338075287,
      "grad_norm": 1.0143399238586426,
      "learning_rate": 1.4147505117194466e-05,
      "loss": 0.0974,
      "step": 565000
    },
    {
      "epoch": 3.6707915164506804,
      "grad_norm": 1.1822229623794556,
      "learning_rate": 1.4140594658768475e-05,
      "loss": 0.0946,
      "step": 565100
    },
    {
      "epoch": 3.6714410990938324,
      "grad_norm": 1.3990994691848755,
      "learning_rate": 1.4133684200342484e-05,
      "loss": 0.1013,
      "step": 565200
    },
    {
      "epoch": 3.672090681736984,
      "grad_norm": 1.2305645942687988,
      "learning_rate": 1.412677374191649e-05,
      "loss": 0.1058,
      "step": 565300
    },
    {
      "epoch": 3.6727402643801357,
      "grad_norm": 1.1044573783874512,
      "learning_rate": 1.41198632834905e-05,
      "loss": 0.0975,
      "step": 565400
    },
    {
      "epoch": 3.6733898470232873,
      "grad_norm": 1.4680558443069458,
      "learning_rate": 1.411295282506451e-05,
      "loss": 0.1038,
      "step": 565500
    },
    {
      "epoch": 3.6740394296664394,
      "grad_norm": 0.8019592761993408,
      "learning_rate": 1.410604236663852e-05,
      "loss": 0.0999,
      "step": 565600
    },
    {
      "epoch": 3.674689012309591,
      "grad_norm": 1.2493470907211304,
      "learning_rate": 1.409913190821253e-05,
      "loss": 0.0995,
      "step": 565700
    },
    {
      "epoch": 3.675338594952743,
      "grad_norm": 1.708416223526001,
      "learning_rate": 1.4092221449786536e-05,
      "loss": 0.1022,
      "step": 565800
    },
    {
      "epoch": 3.6759881775958947,
      "grad_norm": 1.0339951515197754,
      "learning_rate": 1.4085310991360546e-05,
      "loss": 0.0978,
      "step": 565900
    },
    {
      "epoch": 3.6766377602390463,
      "grad_norm": 1.1163582801818848,
      "learning_rate": 1.4078400532934556e-05,
      "loss": 0.0997,
      "step": 566000
    },
    {
      "epoch": 3.677287342882198,
      "grad_norm": 1.1792032718658447,
      "learning_rate": 1.4071490074508564e-05,
      "loss": 0.1065,
      "step": 566100
    },
    {
      "epoch": 3.67793692552535,
      "grad_norm": 1.3280904293060303,
      "learning_rate": 1.406457961608257e-05,
      "loss": 0.1022,
      "step": 566200
    },
    {
      "epoch": 3.6785865081685016,
      "grad_norm": 0.8922976851463318,
      "learning_rate": 1.405766915765658e-05,
      "loss": 0.1006,
      "step": 566300
    },
    {
      "epoch": 3.6792360908116537,
      "grad_norm": 1.521602988243103,
      "learning_rate": 1.405075869923059e-05,
      "loss": 0.1013,
      "step": 566400
    },
    {
      "epoch": 3.6798856734548053,
      "grad_norm": 1.3986281156539917,
      "learning_rate": 1.40438482408046e-05,
      "loss": 0.103,
      "step": 566500
    },
    {
      "epoch": 3.680535256097957,
      "grad_norm": 0.8700078725814819,
      "learning_rate": 1.403693778237861e-05,
      "loss": 0.0936,
      "step": 566600
    },
    {
      "epoch": 3.6811848387411086,
      "grad_norm": 0.9018975496292114,
      "learning_rate": 1.4030027323952616e-05,
      "loss": 0.0941,
      "step": 566700
    },
    {
      "epoch": 3.6818344213842606,
      "grad_norm": 0.832414984703064,
      "learning_rate": 1.4023116865526626e-05,
      "loss": 0.0955,
      "step": 566800
    },
    {
      "epoch": 3.6824840040274123,
      "grad_norm": 1.2954638004302979,
      "learning_rate": 1.4016206407100636e-05,
      "loss": 0.0993,
      "step": 566900
    },
    {
      "epoch": 3.6831335866705643,
      "grad_norm": 1.100764513015747,
      "learning_rate": 1.4009295948674644e-05,
      "loss": 0.0965,
      "step": 567000
    },
    {
      "epoch": 3.683783169313716,
      "grad_norm": 1.5800546407699585,
      "learning_rate": 1.4002385490248654e-05,
      "loss": 0.101,
      "step": 567100
    },
    {
      "epoch": 3.6844327519568676,
      "grad_norm": 1.585868239402771,
      "learning_rate": 1.399547503182266e-05,
      "loss": 0.0992,
      "step": 567200
    },
    {
      "epoch": 3.685082334600019,
      "grad_norm": 1.0662087202072144,
      "learning_rate": 1.398856457339667e-05,
      "loss": 0.1054,
      "step": 567300
    },
    {
      "epoch": 3.6857319172431713,
      "grad_norm": 1.4095797538757324,
      "learning_rate": 1.398165411497068e-05,
      "loss": 0.0977,
      "step": 567400
    },
    {
      "epoch": 3.686381499886323,
      "grad_norm": 1.0543478727340698,
      "learning_rate": 1.397474365654469e-05,
      "loss": 0.0963,
      "step": 567500
    },
    {
      "epoch": 3.687031082529475,
      "grad_norm": 1.2248774766921997,
      "learning_rate": 1.39678331981187e-05,
      "loss": 0.0958,
      "step": 567600
    },
    {
      "epoch": 3.6876806651726266,
      "grad_norm": 1.2274686098098755,
      "learning_rate": 1.3960922739692706e-05,
      "loss": 0.1028,
      "step": 567700
    },
    {
      "epoch": 3.6883302478157782,
      "grad_norm": 1.0744377374649048,
      "learning_rate": 1.3954012281266714e-05,
      "loss": 0.1011,
      "step": 567800
    },
    {
      "epoch": 3.6889798304589303,
      "grad_norm": 1.0991809368133545,
      "learning_rate": 1.3947101822840724e-05,
      "loss": 0.0999,
      "step": 567900
    },
    {
      "epoch": 3.689629413102082,
      "grad_norm": 0.8911697864532471,
      "learning_rate": 1.3940191364414734e-05,
      "loss": 0.0981,
      "step": 568000
    },
    {
      "epoch": 3.6902789957452335,
      "grad_norm": 1.018352746963501,
      "learning_rate": 1.393328090598874e-05,
      "loss": 0.0969,
      "step": 568100
    },
    {
      "epoch": 3.6909285783883856,
      "grad_norm": 1.1214438676834106,
      "learning_rate": 1.392637044756275e-05,
      "loss": 0.0985,
      "step": 568200
    },
    {
      "epoch": 3.6915781610315372,
      "grad_norm": 0.8724915981292725,
      "learning_rate": 1.391945998913676e-05,
      "loss": 0.1005,
      "step": 568300
    },
    {
      "epoch": 3.692227743674689,
      "grad_norm": 1.0648059844970703,
      "learning_rate": 1.391254953071077e-05,
      "loss": 0.0991,
      "step": 568400
    },
    {
      "epoch": 3.692877326317841,
      "grad_norm": 1.2517905235290527,
      "learning_rate": 1.390563907228478e-05,
      "loss": 0.1026,
      "step": 568500
    },
    {
      "epoch": 3.6935269089609926,
      "grad_norm": 1.2237756252288818,
      "learning_rate": 1.3898728613858786e-05,
      "loss": 0.0975,
      "step": 568600
    },
    {
      "epoch": 3.694176491604144,
      "grad_norm": 0.9543136954307556,
      "learning_rate": 1.3891818155432794e-05,
      "loss": 0.0989,
      "step": 568700
    },
    {
      "epoch": 3.6948260742472963,
      "grad_norm": 0.938103973865509,
      "learning_rate": 1.3884907697006804e-05,
      "loss": 0.1004,
      "step": 568800
    },
    {
      "epoch": 3.695475656890448,
      "grad_norm": 1.152913212776184,
      "learning_rate": 1.3877997238580814e-05,
      "loss": 0.098,
      "step": 568900
    },
    {
      "epoch": 3.6961252395335995,
      "grad_norm": 1.0814704895019531,
      "learning_rate": 1.3871086780154824e-05,
      "loss": 0.0963,
      "step": 569000
    },
    {
      "epoch": 3.6967748221767516,
      "grad_norm": 0.6583914160728455,
      "learning_rate": 1.386417632172883e-05,
      "loss": 0.1011,
      "step": 569100
    },
    {
      "epoch": 3.697424404819903,
      "grad_norm": 0.8097166419029236,
      "learning_rate": 1.385726586330284e-05,
      "loss": 0.0962,
      "step": 569200
    },
    {
      "epoch": 3.698073987463055,
      "grad_norm": 1.116877794265747,
      "learning_rate": 1.385035540487685e-05,
      "loss": 0.0977,
      "step": 569300
    },
    {
      "epoch": 3.698723570106207,
      "grad_norm": 1.6526741981506348,
      "learning_rate": 1.384344494645086e-05,
      "loss": 0.1049,
      "step": 569400
    },
    {
      "epoch": 3.6993731527493585,
      "grad_norm": 0.8687570691108704,
      "learning_rate": 1.3836534488024867e-05,
      "loss": 0.0951,
      "step": 569500
    },
    {
      "epoch": 3.70002273539251,
      "grad_norm": 1.1243689060211182,
      "learning_rate": 1.3829624029598875e-05,
      "loss": 0.0973,
      "step": 569600
    },
    {
      "epoch": 3.700672318035662,
      "grad_norm": 1.0089033842086792,
      "learning_rate": 1.3822713571172885e-05,
      "loss": 0.0943,
      "step": 569700
    },
    {
      "epoch": 3.701321900678814,
      "grad_norm": 0.9469247460365295,
      "learning_rate": 1.3815803112746894e-05,
      "loss": 0.1008,
      "step": 569800
    },
    {
      "epoch": 3.701971483321966,
      "grad_norm": 1.2784292697906494,
      "learning_rate": 1.3808892654320904e-05,
      "loss": 0.0955,
      "step": 569900
    },
    {
      "epoch": 3.7026210659651175,
      "grad_norm": 1.1104962825775146,
      "learning_rate": 1.380198219589491e-05,
      "loss": 0.1001,
      "step": 570000
    },
    {
      "epoch": 3.703270648608269,
      "grad_norm": 1.340511679649353,
      "learning_rate": 1.379507173746892e-05,
      "loss": 0.1031,
      "step": 570100
    },
    {
      "epoch": 3.703920231251421,
      "grad_norm": 0.8181905150413513,
      "learning_rate": 1.378816127904293e-05,
      "loss": 0.1003,
      "step": 570200
    },
    {
      "epoch": 3.704569813894573,
      "grad_norm": 1.3334987163543701,
      "learning_rate": 1.3781250820616939e-05,
      "loss": 0.1001,
      "step": 570300
    },
    {
      "epoch": 3.7052193965377245,
      "grad_norm": 0.783470630645752,
      "learning_rate": 1.3774340362190948e-05,
      "loss": 0.1047,
      "step": 570400
    },
    {
      "epoch": 3.7058689791808765,
      "grad_norm": 1.2716513872146606,
      "learning_rate": 1.3767429903764955e-05,
      "loss": 0.1001,
      "step": 570500
    },
    {
      "epoch": 3.706518561824028,
      "grad_norm": 0.773636519908905,
      "learning_rate": 1.3760519445338965e-05,
      "loss": 0.0919,
      "step": 570600
    },
    {
      "epoch": 3.70716814446718,
      "grad_norm": 1.0773701667785645,
      "learning_rate": 1.3753608986912975e-05,
      "loss": 0.1005,
      "step": 570700
    },
    {
      "epoch": 3.7078177271103314,
      "grad_norm": 1.3698737621307373,
      "learning_rate": 1.3746698528486984e-05,
      "loss": 0.1056,
      "step": 570800
    },
    {
      "epoch": 3.7084673097534835,
      "grad_norm": 0.9460174441337585,
      "learning_rate": 1.3739788070060991e-05,
      "loss": 0.1023,
      "step": 570900
    },
    {
      "epoch": 3.709116892396635,
      "grad_norm": 0.8031571507453918,
      "learning_rate": 1.3732877611635e-05,
      "loss": 0.0969,
      "step": 571000
    },
    {
      "epoch": 3.709766475039787,
      "grad_norm": 2.04355788230896,
      "learning_rate": 1.372596715320901e-05,
      "loss": 0.1015,
      "step": 571100
    },
    {
      "epoch": 3.710416057682939,
      "grad_norm": 1.2169301509857178,
      "learning_rate": 1.3719056694783019e-05,
      "loss": 0.0949,
      "step": 571200
    },
    {
      "epoch": 3.7110656403260904,
      "grad_norm": 1.2566583156585693,
      "learning_rate": 1.3712146236357029e-05,
      "loss": 0.0968,
      "step": 571300
    },
    {
      "epoch": 3.711715222969242,
      "grad_norm": 0.7243016362190247,
      "learning_rate": 1.3705235777931035e-05,
      "loss": 0.0974,
      "step": 571400
    },
    {
      "epoch": 3.712364805612394,
      "grad_norm": 0.788058876991272,
      "learning_rate": 1.3698325319505045e-05,
      "loss": 0.0959,
      "step": 571500
    },
    {
      "epoch": 3.7130143882555458,
      "grad_norm": 0.8469870686531067,
      "learning_rate": 1.3691414861079055e-05,
      "loss": 0.1009,
      "step": 571600
    },
    {
      "epoch": 3.713663970898698,
      "grad_norm": 1.0381057262420654,
      "learning_rate": 1.3684504402653065e-05,
      "loss": 0.102,
      "step": 571700
    },
    {
      "epoch": 3.7143135535418494,
      "grad_norm": 1.1999253034591675,
      "learning_rate": 1.3677593944227074e-05,
      "loss": 0.1034,
      "step": 571800
    },
    {
      "epoch": 3.714963136185001,
      "grad_norm": 1.1343573331832886,
      "learning_rate": 1.3670683485801081e-05,
      "loss": 0.1013,
      "step": 571900
    },
    {
      "epoch": 3.7156127188281527,
      "grad_norm": 1.0549380779266357,
      "learning_rate": 1.366377302737509e-05,
      "loss": 0.0996,
      "step": 572000
    },
    {
      "epoch": 3.7162623014713048,
      "grad_norm": 1.4186291694641113,
      "learning_rate": 1.3656862568949099e-05,
      "loss": 0.0991,
      "step": 572100
    },
    {
      "epoch": 3.7169118841144564,
      "grad_norm": 0.8790825009346008,
      "learning_rate": 1.3649952110523109e-05,
      "loss": 0.093,
      "step": 572200
    },
    {
      "epoch": 3.7175614667576085,
      "grad_norm": 1.0252137184143066,
      "learning_rate": 1.3643041652097119e-05,
      "loss": 0.0999,
      "step": 572300
    },
    {
      "epoch": 3.71821104940076,
      "grad_norm": 0.8484604358673096,
      "learning_rate": 1.3636131193671125e-05,
      "loss": 0.0993,
      "step": 572400
    },
    {
      "epoch": 3.7188606320439117,
      "grad_norm": 1.0765610933303833,
      "learning_rate": 1.3629220735245135e-05,
      "loss": 0.097,
      "step": 572500
    },
    {
      "epoch": 3.7195102146870633,
      "grad_norm": 1.1894621849060059,
      "learning_rate": 1.3622310276819145e-05,
      "loss": 0.1031,
      "step": 572600
    },
    {
      "epoch": 3.7201597973302154,
      "grad_norm": 1.0706336498260498,
      "learning_rate": 1.3615399818393155e-05,
      "loss": 0.0967,
      "step": 572700
    },
    {
      "epoch": 3.720809379973367,
      "grad_norm": 0.819274365901947,
      "learning_rate": 1.3608489359967161e-05,
      "loss": 0.0981,
      "step": 572800
    },
    {
      "epoch": 3.721458962616519,
      "grad_norm": 0.9120255708694458,
      "learning_rate": 1.3601578901541171e-05,
      "loss": 0.1027,
      "step": 572900
    },
    {
      "epoch": 3.7221085452596707,
      "grad_norm": 1.120831847190857,
      "learning_rate": 1.3594668443115179e-05,
      "loss": 0.1002,
      "step": 573000
    },
    {
      "epoch": 3.7227581279028223,
      "grad_norm": 1.5789685249328613,
      "learning_rate": 1.3587757984689189e-05,
      "loss": 0.0939,
      "step": 573100
    },
    {
      "epoch": 3.723407710545974,
      "grad_norm": 0.9472358226776123,
      "learning_rate": 1.3580847526263199e-05,
      "loss": 0.1002,
      "step": 573200
    },
    {
      "epoch": 3.724057293189126,
      "grad_norm": 0.9032015204429626,
      "learning_rate": 1.3573937067837205e-05,
      "loss": 0.1021,
      "step": 573300
    },
    {
      "epoch": 3.7247068758322777,
      "grad_norm": 0.8386006951332092,
      "learning_rate": 1.3567026609411215e-05,
      "loss": 0.1024,
      "step": 573400
    },
    {
      "epoch": 3.7253564584754297,
      "grad_norm": 0.8806219100952148,
      "learning_rate": 1.3560116150985225e-05,
      "loss": 0.0983,
      "step": 573500
    },
    {
      "epoch": 3.7260060411185814,
      "grad_norm": 0.8409964442253113,
      "learning_rate": 1.3553205692559235e-05,
      "loss": 0.0951,
      "step": 573600
    },
    {
      "epoch": 3.726655623761733,
      "grad_norm": 1.3086541891098022,
      "learning_rate": 1.3546295234133243e-05,
      "loss": 0.0984,
      "step": 573700
    },
    {
      "epoch": 3.7273052064048846,
      "grad_norm": 0.9355259537696838,
      "learning_rate": 1.3539384775707251e-05,
      "loss": 0.097,
      "step": 573800
    },
    {
      "epoch": 3.7279547890480367,
      "grad_norm": 1.160017728805542,
      "learning_rate": 1.353247431728126e-05,
      "loss": 0.0963,
      "step": 573900
    },
    {
      "epoch": 3.7286043716911883,
      "grad_norm": 1.0628756284713745,
      "learning_rate": 1.352556385885527e-05,
      "loss": 0.0977,
      "step": 574000
    },
    {
      "epoch": 3.7292539543343404,
      "grad_norm": 1.056198239326477,
      "learning_rate": 1.3518653400429279e-05,
      "loss": 0.0972,
      "step": 574100
    },
    {
      "epoch": 3.729903536977492,
      "grad_norm": 1.0312144756317139,
      "learning_rate": 1.3511742942003286e-05,
      "loss": 0.0981,
      "step": 574200
    },
    {
      "epoch": 3.7305531196206436,
      "grad_norm": 0.9059447646141052,
      "learning_rate": 1.3504832483577295e-05,
      "loss": 0.1,
      "step": 574300
    },
    {
      "epoch": 3.7312027022637952,
      "grad_norm": 1.4136719703674316,
      "learning_rate": 1.3497922025151305e-05,
      "loss": 0.0981,
      "step": 574400
    },
    {
      "epoch": 3.7318522849069473,
      "grad_norm": 1.3169289827346802,
      "learning_rate": 1.3491011566725315e-05,
      "loss": 0.0995,
      "step": 574500
    },
    {
      "epoch": 3.732501867550099,
      "grad_norm": 1.0114213228225708,
      "learning_rate": 1.3484101108299323e-05,
      "loss": 0.0968,
      "step": 574600
    },
    {
      "epoch": 3.733151450193251,
      "grad_norm": 1.3925076723098755,
      "learning_rate": 1.3477190649873331e-05,
      "loss": 0.0998,
      "step": 574700
    },
    {
      "epoch": 3.7338010328364026,
      "grad_norm": 1.7722654342651367,
      "learning_rate": 1.347028019144734e-05,
      "loss": 0.1018,
      "step": 574800
    },
    {
      "epoch": 3.7344506154795543,
      "grad_norm": 1.3833266496658325,
      "learning_rate": 1.346336973302135e-05,
      "loss": 0.103,
      "step": 574900
    },
    {
      "epoch": 3.7351001981227063,
      "grad_norm": 1.2844009399414062,
      "learning_rate": 1.345645927459536e-05,
      "loss": 0.0957,
      "step": 575000
    },
    {
      "epoch": 3.735749780765858,
      "grad_norm": 1.0051952600479126,
      "learning_rate": 1.3449548816169369e-05,
      "loss": 0.1024,
      "step": 575100
    },
    {
      "epoch": 3.7363993634090096,
      "grad_norm": 1.1493762731552124,
      "learning_rate": 1.3442638357743376e-05,
      "loss": 0.1042,
      "step": 575200
    },
    {
      "epoch": 3.7370489460521616,
      "grad_norm": 1.1103217601776123,
      "learning_rate": 1.3435727899317385e-05,
      "loss": 0.0923,
      "step": 575300
    },
    {
      "epoch": 3.7376985286953133,
      "grad_norm": 1.436703085899353,
      "learning_rate": 1.3428817440891395e-05,
      "loss": 0.0969,
      "step": 575400
    },
    {
      "epoch": 3.738348111338465,
      "grad_norm": 1.088883399963379,
      "learning_rate": 1.3421906982465403e-05,
      "loss": 0.0992,
      "step": 575500
    },
    {
      "epoch": 3.738997693981617,
      "grad_norm": 1.0503408908843994,
      "learning_rate": 1.3414996524039412e-05,
      "loss": 0.1009,
      "step": 575600
    },
    {
      "epoch": 3.7396472766247686,
      "grad_norm": 1.1068105697631836,
      "learning_rate": 1.340808606561342e-05,
      "loss": 0.0974,
      "step": 575700
    },
    {
      "epoch": 3.74029685926792,
      "grad_norm": 1.174451470375061,
      "learning_rate": 1.340117560718743e-05,
      "loss": 0.1061,
      "step": 575800
    },
    {
      "epoch": 3.7409464419110723,
      "grad_norm": 1.000054121017456,
      "learning_rate": 1.339426514876144e-05,
      "loss": 0.0941,
      "step": 575900
    },
    {
      "epoch": 3.741596024554224,
      "grad_norm": 1.190144658088684,
      "learning_rate": 1.338735469033545e-05,
      "loss": 0.0952,
      "step": 576000
    },
    {
      "epoch": 3.7422456071973755,
      "grad_norm": 1.5794212818145752,
      "learning_rate": 1.3380444231909456e-05,
      "loss": 0.1022,
      "step": 576100
    },
    {
      "epoch": 3.7428951898405276,
      "grad_norm": 0.545025646686554,
      "learning_rate": 1.3373533773483466e-05,
      "loss": 0.0973,
      "step": 576200
    },
    {
      "epoch": 3.7435447724836792,
      "grad_norm": 1.1707881689071655,
      "learning_rate": 1.3366623315057475e-05,
      "loss": 0.0943,
      "step": 576300
    },
    {
      "epoch": 3.744194355126831,
      "grad_norm": 1.606999397277832,
      "learning_rate": 1.3359712856631484e-05,
      "loss": 0.0931,
      "step": 576400
    },
    {
      "epoch": 3.744843937769983,
      "grad_norm": 1.0628176927566528,
      "learning_rate": 1.3352802398205493e-05,
      "loss": 0.096,
      "step": 576500
    },
    {
      "epoch": 3.7454935204131345,
      "grad_norm": 0.7240606546401978,
      "learning_rate": 1.33458919397795e-05,
      "loss": 0.1034,
      "step": 576600
    },
    {
      "epoch": 3.746143103056286,
      "grad_norm": 1.3598304986953735,
      "learning_rate": 1.333898148135351e-05,
      "loss": 0.1019,
      "step": 576700
    },
    {
      "epoch": 3.7467926856994382,
      "grad_norm": 1.1167514324188232,
      "learning_rate": 1.333207102292752e-05,
      "loss": 0.0998,
      "step": 576800
    },
    {
      "epoch": 3.74744226834259,
      "grad_norm": 1.0836371183395386,
      "learning_rate": 1.332516056450153e-05,
      "loss": 0.0967,
      "step": 576900
    },
    {
      "epoch": 3.748091850985742,
      "grad_norm": 0.9266911745071411,
      "learning_rate": 1.331825010607554e-05,
      "loss": 0.0964,
      "step": 577000
    },
    {
      "epoch": 3.7487414336288936,
      "grad_norm": 1.0435996055603027,
      "learning_rate": 1.3311339647649546e-05,
      "loss": 0.1024,
      "step": 577100
    },
    {
      "epoch": 3.749391016272045,
      "grad_norm": 1.4321727752685547,
      "learning_rate": 1.3304429189223556e-05,
      "loss": 0.1041,
      "step": 577200
    },
    {
      "epoch": 3.750040598915197,
      "grad_norm": 1.0799123048782349,
      "learning_rate": 1.3297518730797564e-05,
      "loss": 0.1018,
      "step": 577300
    },
    {
      "epoch": 3.750690181558349,
      "grad_norm": 0.9133935570716858,
      "learning_rate": 1.3290608272371574e-05,
      "loss": 0.0966,
      "step": 577400
    },
    {
      "epoch": 3.7513397642015005,
      "grad_norm": 1.4192875623703003,
      "learning_rate": 1.328369781394558e-05,
      "loss": 0.1015,
      "step": 577500
    },
    {
      "epoch": 3.7519893468446526,
      "grad_norm": 1.03414785861969,
      "learning_rate": 1.327678735551959e-05,
      "loss": 0.0974,
      "step": 577600
    },
    {
      "epoch": 3.752638929487804,
      "grad_norm": 1.3100680112838745,
      "learning_rate": 1.32698768970936e-05,
      "loss": 0.0991,
      "step": 577700
    },
    {
      "epoch": 3.753288512130956,
      "grad_norm": 1.2158432006835938,
      "learning_rate": 1.326296643866761e-05,
      "loss": 0.0933,
      "step": 577800
    },
    {
      "epoch": 3.7539380947741074,
      "grad_norm": 0.6416269540786743,
      "learning_rate": 1.325605598024162e-05,
      "loss": 0.0986,
      "step": 577900
    },
    {
      "epoch": 3.7545876774172595,
      "grad_norm": 1.1451205015182495,
      "learning_rate": 1.3249145521815626e-05,
      "loss": 0.1017,
      "step": 578000
    },
    {
      "epoch": 3.755237260060411,
      "grad_norm": 0.963507354259491,
      "learning_rate": 1.3242235063389636e-05,
      "loss": 0.1012,
      "step": 578100
    },
    {
      "epoch": 3.755886842703563,
      "grad_norm": 0.986873984336853,
      "learning_rate": 1.3235324604963644e-05,
      "loss": 0.0918,
      "step": 578200
    },
    {
      "epoch": 3.756536425346715,
      "grad_norm": 1.134506106376648,
      "learning_rate": 1.3228414146537654e-05,
      "loss": 0.1028,
      "step": 578300
    },
    {
      "epoch": 3.7571860079898665,
      "grad_norm": 1.3583729267120361,
      "learning_rate": 1.3221503688111664e-05,
      "loss": 0.0972,
      "step": 578400
    },
    {
      "epoch": 3.757835590633018,
      "grad_norm": 1.1103427410125732,
      "learning_rate": 1.321459322968567e-05,
      "loss": 0.1013,
      "step": 578500
    },
    {
      "epoch": 3.75848517327617,
      "grad_norm": 1.1643368005752563,
      "learning_rate": 1.320768277125968e-05,
      "loss": 0.0956,
      "step": 578600
    },
    {
      "epoch": 3.759134755919322,
      "grad_norm": 1.3655778169631958,
      "learning_rate": 1.320077231283369e-05,
      "loss": 0.0976,
      "step": 578700
    },
    {
      "epoch": 3.759784338562474,
      "grad_norm": 1.239018440246582,
      "learning_rate": 1.31938618544077e-05,
      "loss": 0.0943,
      "step": 578800
    },
    {
      "epoch": 3.7604339212056255,
      "grad_norm": 0.6045534610748291,
      "learning_rate": 1.3186951395981706e-05,
      "loss": 0.0985,
      "step": 578900
    },
    {
      "epoch": 3.761083503848777,
      "grad_norm": 1.7006663084030151,
      "learning_rate": 1.3180040937555716e-05,
      "loss": 0.1017,
      "step": 579000
    },
    {
      "epoch": 3.7617330864919287,
      "grad_norm": 1.2542909383773804,
      "learning_rate": 1.3173130479129724e-05,
      "loss": 0.0953,
      "step": 579100
    },
    {
      "epoch": 3.762382669135081,
      "grad_norm": 1.3978551626205444,
      "learning_rate": 1.3166220020703734e-05,
      "loss": 0.1005,
      "step": 579200
    },
    {
      "epoch": 3.7630322517782324,
      "grad_norm": 0.8627331852912903,
      "learning_rate": 1.3159309562277744e-05,
      "loss": 0.0927,
      "step": 579300
    },
    {
      "epoch": 3.7636818344213845,
      "grad_norm": 1.0185048580169678,
      "learning_rate": 1.315239910385175e-05,
      "loss": 0.1036,
      "step": 579400
    },
    {
      "epoch": 3.764331417064536,
      "grad_norm": 0.9668338298797607,
      "learning_rate": 1.314548864542576e-05,
      "loss": 0.0977,
      "step": 579500
    },
    {
      "epoch": 3.7649809997076877,
      "grad_norm": 1.0743672847747803,
      "learning_rate": 1.313857818699977e-05,
      "loss": 0.1001,
      "step": 579600
    },
    {
      "epoch": 3.7656305823508394,
      "grad_norm": 0.8851837515830994,
      "learning_rate": 1.313166772857378e-05,
      "loss": 0.0985,
      "step": 579700
    },
    {
      "epoch": 3.7662801649939914,
      "grad_norm": 1.7413321733474731,
      "learning_rate": 1.3124757270147788e-05,
      "loss": 0.1023,
      "step": 579800
    },
    {
      "epoch": 3.766929747637143,
      "grad_norm": 0.7557327747344971,
      "learning_rate": 1.3117846811721796e-05,
      "loss": 0.0985,
      "step": 579900
    },
    {
      "epoch": 3.767579330280295,
      "grad_norm": 1.4194914102554321,
      "learning_rate": 1.3110936353295804e-05,
      "loss": 0.1039,
      "step": 580000
    },
    {
      "epoch": 3.7682289129234467,
      "grad_norm": 1.281551480293274,
      "learning_rate": 1.3104025894869814e-05,
      "loss": 0.099,
      "step": 580100
    },
    {
      "epoch": 3.7688784955665984,
      "grad_norm": 0.9397505521774292,
      "learning_rate": 1.3097115436443824e-05,
      "loss": 0.1001,
      "step": 580200
    },
    {
      "epoch": 3.76952807820975,
      "grad_norm": 1.6224825382232666,
      "learning_rate": 1.3090204978017834e-05,
      "loss": 0.1021,
      "step": 580300
    },
    {
      "epoch": 3.770177660852902,
      "grad_norm": 1.4289630651474,
      "learning_rate": 1.308329451959184e-05,
      "loss": 0.1009,
      "step": 580400
    },
    {
      "epoch": 3.7708272434960537,
      "grad_norm": 1.0634338855743408,
      "learning_rate": 1.307638406116585e-05,
      "loss": 0.0967,
      "step": 580500
    },
    {
      "epoch": 3.7714768261392058,
      "grad_norm": 1.1926995515823364,
      "learning_rate": 1.306947360273986e-05,
      "loss": 0.1012,
      "step": 580600
    },
    {
      "epoch": 3.7721264087823574,
      "grad_norm": 1.3125251531600952,
      "learning_rate": 1.3062563144313868e-05,
      "loss": 0.1066,
      "step": 580700
    },
    {
      "epoch": 3.772775991425509,
      "grad_norm": 1.2887556552886963,
      "learning_rate": 1.3055652685887876e-05,
      "loss": 0.0943,
      "step": 580800
    },
    {
      "epoch": 3.7734255740686606,
      "grad_norm": 0.5123831629753113,
      "learning_rate": 1.3048742227461885e-05,
      "loss": 0.1021,
      "step": 580900
    },
    {
      "epoch": 3.7740751567118127,
      "grad_norm": 0.8645439147949219,
      "learning_rate": 1.3041831769035894e-05,
      "loss": 0.0992,
      "step": 581000
    },
    {
      "epoch": 3.7747247393549643,
      "grad_norm": 1.3318274021148682,
      "learning_rate": 1.3034921310609904e-05,
      "loss": 0.1007,
      "step": 581100
    },
    {
      "epoch": 3.7753743219981164,
      "grad_norm": 1.2961727380752563,
      "learning_rate": 1.3028010852183914e-05,
      "loss": 0.1023,
      "step": 581200
    },
    {
      "epoch": 3.776023904641268,
      "grad_norm": 0.7717939019203186,
      "learning_rate": 1.302110039375792e-05,
      "loss": 0.0923,
      "step": 581300
    },
    {
      "epoch": 3.7766734872844197,
      "grad_norm": 0.9561722278594971,
      "learning_rate": 1.301418993533193e-05,
      "loss": 0.0983,
      "step": 581400
    },
    {
      "epoch": 3.7773230699275713,
      "grad_norm": 1.1877031326293945,
      "learning_rate": 1.300727947690594e-05,
      "loss": 0.1077,
      "step": 581500
    },
    {
      "epoch": 3.7779726525707233,
      "grad_norm": 0.9306022524833679,
      "learning_rate": 1.3000369018479948e-05,
      "loss": 0.0986,
      "step": 581600
    },
    {
      "epoch": 3.778622235213875,
      "grad_norm": 1.0706145763397217,
      "learning_rate": 1.2993458560053958e-05,
      "loss": 0.1005,
      "step": 581700
    },
    {
      "epoch": 3.779271817857027,
      "grad_norm": 0.9969736933708191,
      "learning_rate": 1.2986548101627965e-05,
      "loss": 0.1001,
      "step": 581800
    },
    {
      "epoch": 3.7799214005001787,
      "grad_norm": 0.9194704294204712,
      "learning_rate": 1.2979637643201975e-05,
      "loss": 0.0993,
      "step": 581900
    },
    {
      "epoch": 3.7805709831433303,
      "grad_norm": 0.8144766092300415,
      "learning_rate": 1.2972727184775985e-05,
      "loss": 0.0997,
      "step": 582000
    },
    {
      "epoch": 3.781220565786482,
      "grad_norm": 1.3237656354904175,
      "learning_rate": 1.2965816726349994e-05,
      "loss": 0.0956,
      "step": 582100
    },
    {
      "epoch": 3.781870148429634,
      "grad_norm": 1.2959541082382202,
      "learning_rate": 1.2958906267924e-05,
      "loss": 0.0965,
      "step": 582200
    },
    {
      "epoch": 3.7825197310727856,
      "grad_norm": 0.7656496167182922,
      "learning_rate": 1.295199580949801e-05,
      "loss": 0.0986,
      "step": 582300
    },
    {
      "epoch": 3.7831693137159377,
      "grad_norm": 0.8877753615379333,
      "learning_rate": 1.294508535107202e-05,
      "loss": 0.1011,
      "step": 582400
    },
    {
      "epoch": 3.7838188963590893,
      "grad_norm": 0.9919639825820923,
      "learning_rate": 1.2938174892646029e-05,
      "loss": 0.0994,
      "step": 582500
    },
    {
      "epoch": 3.784468479002241,
      "grad_norm": 1.2374966144561768,
      "learning_rate": 1.2931264434220039e-05,
      "loss": 0.096,
      "step": 582600
    },
    {
      "epoch": 3.785118061645393,
      "grad_norm": 1.2003237009048462,
      "learning_rate": 1.2924353975794045e-05,
      "loss": 0.108,
      "step": 582700
    },
    {
      "epoch": 3.7857676442885446,
      "grad_norm": 0.7941702604293823,
      "learning_rate": 1.2917443517368055e-05,
      "loss": 0.0953,
      "step": 582800
    },
    {
      "epoch": 3.7864172269316962,
      "grad_norm": 1.2327346801757812,
      "learning_rate": 1.2910533058942065e-05,
      "loss": 0.104,
      "step": 582900
    },
    {
      "epoch": 3.7870668095748483,
      "grad_norm": 1.3073620796203613,
      "learning_rate": 1.2903622600516075e-05,
      "loss": 0.0987,
      "step": 583000
    },
    {
      "epoch": 3.787716392218,
      "grad_norm": 1.235998511314392,
      "learning_rate": 1.2896712142090084e-05,
      "loss": 0.0995,
      "step": 583100
    },
    {
      "epoch": 3.7883659748611516,
      "grad_norm": 1.7913976907730103,
      "learning_rate": 1.2889801683664091e-05,
      "loss": 0.0955,
      "step": 583200
    },
    {
      "epoch": 3.7890155575043036,
      "grad_norm": 1.3360679149627686,
      "learning_rate": 1.28828912252381e-05,
      "loss": 0.0971,
      "step": 583300
    },
    {
      "epoch": 3.7896651401474553,
      "grad_norm": 1.7090092897415161,
      "learning_rate": 1.2875980766812109e-05,
      "loss": 0.0992,
      "step": 583400
    },
    {
      "epoch": 3.790314722790607,
      "grad_norm": 1.0614250898361206,
      "learning_rate": 1.2869070308386119e-05,
      "loss": 0.1047,
      "step": 583500
    },
    {
      "epoch": 3.790964305433759,
      "grad_norm": 1.5307985544204712,
      "learning_rate": 1.2862159849960125e-05,
      "loss": 0.1011,
      "step": 583600
    },
    {
      "epoch": 3.7916138880769106,
      "grad_norm": 1.2894225120544434,
      "learning_rate": 1.2855249391534135e-05,
      "loss": 0.1043,
      "step": 583700
    },
    {
      "epoch": 3.792263470720062,
      "grad_norm": 1.0138591527938843,
      "learning_rate": 1.2848338933108145e-05,
      "loss": 0.0919,
      "step": 583800
    },
    {
      "epoch": 3.7929130533632143,
      "grad_norm": 0.7541349530220032,
      "learning_rate": 1.2841428474682155e-05,
      "loss": 0.1011,
      "step": 583900
    },
    {
      "epoch": 3.793562636006366,
      "grad_norm": 1.302550196647644,
      "learning_rate": 1.2834518016256165e-05,
      "loss": 0.1048,
      "step": 584000
    },
    {
      "epoch": 3.7942122186495175,
      "grad_norm": 1.2942410707473755,
      "learning_rate": 1.2827607557830171e-05,
      "loss": 0.1005,
      "step": 584100
    },
    {
      "epoch": 3.7948618012926696,
      "grad_norm": 1.1374878883361816,
      "learning_rate": 1.2820697099404181e-05,
      "loss": 0.1017,
      "step": 584200
    },
    {
      "epoch": 3.795511383935821,
      "grad_norm": 1.0799778699874878,
      "learning_rate": 1.2813786640978189e-05,
      "loss": 0.0932,
      "step": 584300
    },
    {
      "epoch": 3.796160966578973,
      "grad_norm": 0.9469234943389893,
      "learning_rate": 1.2806876182552199e-05,
      "loss": 0.1023,
      "step": 584400
    },
    {
      "epoch": 3.796810549222125,
      "grad_norm": 0.9664324522018433,
      "learning_rate": 1.2799965724126209e-05,
      "loss": 0.0989,
      "step": 584500
    },
    {
      "epoch": 3.7974601318652765,
      "grad_norm": 1.3394312858581543,
      "learning_rate": 1.2793055265700215e-05,
      "loss": 0.0984,
      "step": 584600
    },
    {
      "epoch": 3.7981097145084286,
      "grad_norm": 1.2640267610549927,
      "learning_rate": 1.2786144807274225e-05,
      "loss": 0.1054,
      "step": 584700
    },
    {
      "epoch": 3.7987592971515802,
      "grad_norm": 0.9184856414794922,
      "learning_rate": 1.2779234348848235e-05,
      "loss": 0.0957,
      "step": 584800
    },
    {
      "epoch": 3.799408879794732,
      "grad_norm": 1.2485653162002563,
      "learning_rate": 1.2772323890422245e-05,
      "loss": 0.0985,
      "step": 584900
    },
    {
      "epoch": 3.8000584624378835,
      "grad_norm": 0.9218000769615173,
      "learning_rate": 1.2765413431996253e-05,
      "loss": 0.0989,
      "step": 585000
    },
    {
      "epoch": 3.8007080450810355,
      "grad_norm": 0.8885660171508789,
      "learning_rate": 1.2758502973570261e-05,
      "loss": 0.1051,
      "step": 585100
    },
    {
      "epoch": 3.801357627724187,
      "grad_norm": 1.1679766178131104,
      "learning_rate": 1.275159251514427e-05,
      "loss": 0.0973,
      "step": 585200
    },
    {
      "epoch": 3.8020072103673392,
      "grad_norm": 0.9233914613723755,
      "learning_rate": 1.2744682056718279e-05,
      "loss": 0.0949,
      "step": 585300
    },
    {
      "epoch": 3.802656793010491,
      "grad_norm": 1.075735092163086,
      "learning_rate": 1.2737771598292289e-05,
      "loss": 0.101,
      "step": 585400
    },
    {
      "epoch": 3.8033063756536425,
      "grad_norm": 1.2275865077972412,
      "learning_rate": 1.2730861139866295e-05,
      "loss": 0.0977,
      "step": 585500
    },
    {
      "epoch": 3.803955958296794,
      "grad_norm": 0.6720139980316162,
      "learning_rate": 1.2723950681440305e-05,
      "loss": 0.1034,
      "step": 585600
    },
    {
      "epoch": 3.804605540939946,
      "grad_norm": 0.9021754860877991,
      "learning_rate": 1.2717040223014315e-05,
      "loss": 0.0994,
      "step": 585700
    },
    {
      "epoch": 3.805255123583098,
      "grad_norm": 1.1889525651931763,
      "learning_rate": 1.2710129764588325e-05,
      "loss": 0.1007,
      "step": 585800
    },
    {
      "epoch": 3.80590470622625,
      "grad_norm": 0.8948673605918884,
      "learning_rate": 1.2703219306162333e-05,
      "loss": 0.0946,
      "step": 585900
    },
    {
      "epoch": 3.8065542888694015,
      "grad_norm": 0.7731301188468933,
      "learning_rate": 1.2696308847736341e-05,
      "loss": 0.0978,
      "step": 586000
    },
    {
      "epoch": 3.807203871512553,
      "grad_norm": 1.0002189874649048,
      "learning_rate": 1.268939838931035e-05,
      "loss": 0.1047,
      "step": 586100
    },
    {
      "epoch": 3.8078534541557048,
      "grad_norm": 0.6457329392433167,
      "learning_rate": 1.268248793088436e-05,
      "loss": 0.0932,
      "step": 586200
    },
    {
      "epoch": 3.808503036798857,
      "grad_norm": 1.334875226020813,
      "learning_rate": 1.267557747245837e-05,
      "loss": 0.0994,
      "step": 586300
    },
    {
      "epoch": 3.8091526194420084,
      "grad_norm": 1.4234412908554077,
      "learning_rate": 1.2668667014032379e-05,
      "loss": 0.104,
      "step": 586400
    },
    {
      "epoch": 3.8098022020851605,
      "grad_norm": 1.5637205839157104,
      "learning_rate": 1.2661756555606385e-05,
      "loss": 0.1023,
      "step": 586500
    },
    {
      "epoch": 3.810451784728312,
      "grad_norm": 1.1377195119857788,
      "learning_rate": 1.2654846097180395e-05,
      "loss": 0.101,
      "step": 586600
    },
    {
      "epoch": 3.8111013673714638,
      "grad_norm": 1.9002125263214111,
      "learning_rate": 1.2647935638754405e-05,
      "loss": 0.097,
      "step": 586700
    },
    {
      "epoch": 3.8117509500146154,
      "grad_norm": 1.262855887413025,
      "learning_rate": 1.2641025180328413e-05,
      "loss": 0.0964,
      "step": 586800
    },
    {
      "epoch": 3.8124005326577675,
      "grad_norm": 1.5796927213668823,
      "learning_rate": 1.2634114721902422e-05,
      "loss": 0.1069,
      "step": 586900
    },
    {
      "epoch": 3.813050115300919,
      "grad_norm": 1.0705136060714722,
      "learning_rate": 1.262720426347643e-05,
      "loss": 0.0981,
      "step": 587000
    },
    {
      "epoch": 3.813699697944071,
      "grad_norm": 1.492616057395935,
      "learning_rate": 1.262029380505044e-05,
      "loss": 0.0999,
      "step": 587100
    },
    {
      "epoch": 3.814349280587223,
      "grad_norm": 0.780026912689209,
      "learning_rate": 1.261338334662445e-05,
      "loss": 0.0967,
      "step": 587200
    },
    {
      "epoch": 3.8149988632303744,
      "grad_norm": 1.3047327995300293,
      "learning_rate": 1.260647288819846e-05,
      "loss": 0.1051,
      "step": 587300
    },
    {
      "epoch": 3.815648445873526,
      "grad_norm": 1.2543822526931763,
      "learning_rate": 1.2599562429772466e-05,
      "loss": 0.098,
      "step": 587400
    },
    {
      "epoch": 3.816298028516678,
      "grad_norm": 0.8253544569015503,
      "learning_rate": 1.2592651971346476e-05,
      "loss": 0.1048,
      "step": 587500
    },
    {
      "epoch": 3.8169476111598297,
      "grad_norm": 0.9919533133506775,
      "learning_rate": 1.2585741512920485e-05,
      "loss": 0.1006,
      "step": 587600
    },
    {
      "epoch": 3.817597193802982,
      "grad_norm": 1.191200613975525,
      "learning_rate": 1.2578831054494494e-05,
      "loss": 0.1002,
      "step": 587700
    },
    {
      "epoch": 3.8182467764461334,
      "grad_norm": 1.6010363101959229,
      "learning_rate": 1.2571920596068503e-05,
      "loss": 0.099,
      "step": 587800
    },
    {
      "epoch": 3.818896359089285,
      "grad_norm": 1.3306487798690796,
      "learning_rate": 1.256501013764251e-05,
      "loss": 0.1007,
      "step": 587900
    },
    {
      "epoch": 3.8195459417324367,
      "grad_norm": 1.144972801208496,
      "learning_rate": 1.255809967921652e-05,
      "loss": 0.1054,
      "step": 588000
    },
    {
      "epoch": 3.8201955243755887,
      "grad_norm": 1.4654818773269653,
      "learning_rate": 1.255118922079053e-05,
      "loss": 0.099,
      "step": 588100
    },
    {
      "epoch": 3.8208451070187404,
      "grad_norm": 0.6422821283340454,
      "learning_rate": 1.254427876236454e-05,
      "loss": 0.0945,
      "step": 588200
    },
    {
      "epoch": 3.8214946896618924,
      "grad_norm": 0.9368006587028503,
      "learning_rate": 1.2537368303938546e-05,
      "loss": 0.1007,
      "step": 588300
    },
    {
      "epoch": 3.822144272305044,
      "grad_norm": 1.4728516340255737,
      "learning_rate": 1.2530457845512556e-05,
      "loss": 0.1008,
      "step": 588400
    },
    {
      "epoch": 3.8227938549481957,
      "grad_norm": 0.9031088352203369,
      "learning_rate": 1.2523547387086566e-05,
      "loss": 0.0946,
      "step": 588500
    },
    {
      "epoch": 3.8234434375913473,
      "grad_norm": 1.1972633600234985,
      "learning_rate": 1.2516636928660574e-05,
      "loss": 0.1008,
      "step": 588600
    },
    {
      "epoch": 3.8240930202344994,
      "grad_norm": 1.1642646789550781,
      "learning_rate": 1.2509726470234584e-05,
      "loss": 0.1017,
      "step": 588700
    },
    {
      "epoch": 3.824742602877651,
      "grad_norm": 0.4173514246940613,
      "learning_rate": 1.250281601180859e-05,
      "loss": 0.1,
      "step": 588800
    },
    {
      "epoch": 3.825392185520803,
      "grad_norm": 1.7494521141052246,
      "learning_rate": 1.24959055533826e-05,
      "loss": 0.0992,
      "step": 588900
    },
    {
      "epoch": 3.8260417681639547,
      "grad_norm": 0.7005010843276978,
      "learning_rate": 1.248899509495661e-05,
      "loss": 0.0994,
      "step": 589000
    },
    {
      "epoch": 3.8266913508071063,
      "grad_norm": 1.3199188709259033,
      "learning_rate": 1.248208463653062e-05,
      "loss": 0.1021,
      "step": 589100
    },
    {
      "epoch": 3.827340933450258,
      "grad_norm": 1.0987392663955688,
      "learning_rate": 1.2475174178104628e-05,
      "loss": 0.1001,
      "step": 589200
    },
    {
      "epoch": 3.82799051609341,
      "grad_norm": 0.9102765917778015,
      "learning_rate": 1.2468263719678638e-05,
      "loss": 0.1018,
      "step": 589300
    },
    {
      "epoch": 3.8286400987365616,
      "grad_norm": 1.3784438371658325,
      "learning_rate": 1.2461353261252646e-05,
      "loss": 0.0941,
      "step": 589400
    },
    {
      "epoch": 3.8292896813797137,
      "grad_norm": 1.1226539611816406,
      "learning_rate": 1.2454442802826654e-05,
      "loss": 0.0991,
      "step": 589500
    },
    {
      "epoch": 3.8299392640228653,
      "grad_norm": 1.0792512893676758,
      "learning_rate": 1.2447532344400662e-05,
      "loss": 0.1036,
      "step": 589600
    },
    {
      "epoch": 3.830588846666017,
      "grad_norm": 1.1722147464752197,
      "learning_rate": 1.2440621885974672e-05,
      "loss": 0.1002,
      "step": 589700
    },
    {
      "epoch": 3.831238429309169,
      "grad_norm": 0.9085662364959717,
      "learning_rate": 1.2433711427548682e-05,
      "loss": 0.0975,
      "step": 589800
    },
    {
      "epoch": 3.8318880119523206,
      "grad_norm": 0.6405224800109863,
      "learning_rate": 1.242680096912269e-05,
      "loss": 0.0969,
      "step": 589900
    },
    {
      "epoch": 3.8325375945954723,
      "grad_norm": 1.2459229230880737,
      "learning_rate": 1.24198905106967e-05,
      "loss": 0.0985,
      "step": 590000
    },
    {
      "epoch": 3.8331871772386243,
      "grad_norm": 1.2959518432617188,
      "learning_rate": 1.2412980052270708e-05,
      "loss": 0.1007,
      "step": 590100
    },
    {
      "epoch": 3.833836759881776,
      "grad_norm": 1.4011635780334473,
      "learning_rate": 1.2406069593844718e-05,
      "loss": 0.0988,
      "step": 590200
    },
    {
      "epoch": 3.8344863425249276,
      "grad_norm": 1.5674049854278564,
      "learning_rate": 1.2399159135418726e-05,
      "loss": 0.0958,
      "step": 590300
    },
    {
      "epoch": 3.8351359251680797,
      "grad_norm": 1.103153109550476,
      "learning_rate": 1.2392248676992734e-05,
      "loss": 0.0968,
      "step": 590400
    },
    {
      "epoch": 3.8357855078112313,
      "grad_norm": 0.9713135957717896,
      "learning_rate": 1.2385338218566744e-05,
      "loss": 0.0989,
      "step": 590500
    },
    {
      "epoch": 3.836435090454383,
      "grad_norm": 1.0760284662246704,
      "learning_rate": 1.2378427760140752e-05,
      "loss": 0.107,
      "step": 590600
    },
    {
      "epoch": 3.837084673097535,
      "grad_norm": 0.7515577673912048,
      "learning_rate": 1.2371517301714762e-05,
      "loss": 0.1005,
      "step": 590700
    },
    {
      "epoch": 3.8377342557406866,
      "grad_norm": 1.1500965356826782,
      "learning_rate": 1.236460684328877e-05,
      "loss": 0.0989,
      "step": 590800
    },
    {
      "epoch": 3.8383838383838382,
      "grad_norm": 1.1395032405853271,
      "learning_rate": 1.235769638486278e-05,
      "loss": 0.1013,
      "step": 590900
    },
    {
      "epoch": 3.8390334210269903,
      "grad_norm": 1.3921235799789429,
      "learning_rate": 1.235078592643679e-05,
      "loss": 0.1025,
      "step": 591000
    },
    {
      "epoch": 3.839683003670142,
      "grad_norm": 1.7148008346557617,
      "learning_rate": 1.2343875468010798e-05,
      "loss": 0.095,
      "step": 591100
    },
    {
      "epoch": 3.8403325863132936,
      "grad_norm": 1.694403886795044,
      "learning_rate": 1.2336965009584806e-05,
      "loss": 0.0925,
      "step": 591200
    },
    {
      "epoch": 3.8409821689564456,
      "grad_norm": 1.1825672388076782,
      "learning_rate": 1.2330054551158814e-05,
      "loss": 0.0873,
      "step": 591300
    },
    {
      "epoch": 3.8416317515995972,
      "grad_norm": 1.310075044631958,
      "learning_rate": 1.2323144092732824e-05,
      "loss": 0.0995,
      "step": 591400
    },
    {
      "epoch": 3.842281334242749,
      "grad_norm": 1.4672538042068481,
      "learning_rate": 1.2316233634306832e-05,
      "loss": 0.1011,
      "step": 591500
    },
    {
      "epoch": 3.842930916885901,
      "grad_norm": 1.0158370733261108,
      "learning_rate": 1.2309323175880842e-05,
      "loss": 0.1001,
      "step": 591600
    },
    {
      "epoch": 3.8435804995290526,
      "grad_norm": 0.8487604260444641,
      "learning_rate": 1.2302412717454852e-05,
      "loss": 0.0954,
      "step": 591700
    },
    {
      "epoch": 3.8442300821722046,
      "grad_norm": 1.159131407737732,
      "learning_rate": 1.229550225902886e-05,
      "loss": 0.1022,
      "step": 591800
    },
    {
      "epoch": 3.8448796648153563,
      "grad_norm": 1.2432572841644287,
      "learning_rate": 1.228859180060287e-05,
      "loss": 0.0985,
      "step": 591900
    },
    {
      "epoch": 3.845529247458508,
      "grad_norm": 0.6631463170051575,
      "learning_rate": 1.2281681342176878e-05,
      "loss": 0.0946,
      "step": 592000
    },
    {
      "epoch": 3.8461788301016595,
      "grad_norm": 1.1994529962539673,
      "learning_rate": 1.2274770883750886e-05,
      "loss": 0.0965,
      "step": 592100
    },
    {
      "epoch": 3.8468284127448116,
      "grad_norm": 0.6632589101791382,
      "learning_rate": 1.2267860425324895e-05,
      "loss": 0.0983,
      "step": 592200
    },
    {
      "epoch": 3.847477995387963,
      "grad_norm": 1.0964744091033936,
      "learning_rate": 1.2260949966898904e-05,
      "loss": 0.0965,
      "step": 592300
    },
    {
      "epoch": 3.8481275780311153,
      "grad_norm": 1.204968810081482,
      "learning_rate": 1.2254039508472914e-05,
      "loss": 0.0978,
      "step": 592400
    },
    {
      "epoch": 3.848777160674267,
      "grad_norm": 0.7142303586006165,
      "learning_rate": 1.2247129050046922e-05,
      "loss": 0.102,
      "step": 592500
    },
    {
      "epoch": 3.8494267433174185,
      "grad_norm": 1.0056712627410889,
      "learning_rate": 1.2240218591620932e-05,
      "loss": 0.0977,
      "step": 592600
    },
    {
      "epoch": 3.85007632596057,
      "grad_norm": 0.6380731463432312,
      "learning_rate": 1.223330813319494e-05,
      "loss": 0.1028,
      "step": 592700
    },
    {
      "epoch": 3.850725908603722,
      "grad_norm": 0.8631770610809326,
      "learning_rate": 1.222639767476895e-05,
      "loss": 0.0963,
      "step": 592800
    },
    {
      "epoch": 3.851375491246874,
      "grad_norm": 1.3285212516784668,
      "learning_rate": 1.2219487216342958e-05,
      "loss": 0.1012,
      "step": 592900
    },
    {
      "epoch": 3.852025073890026,
      "grad_norm": 1.0815659761428833,
      "learning_rate": 1.2212576757916967e-05,
      "loss": 0.1049,
      "step": 593000
    },
    {
      "epoch": 3.8526746565331775,
      "grad_norm": 1.30824875831604,
      "learning_rate": 1.2205666299490976e-05,
      "loss": 0.0938,
      "step": 593100
    },
    {
      "epoch": 3.853324239176329,
      "grad_norm": 1.3179657459259033,
      "learning_rate": 1.2198755841064985e-05,
      "loss": 0.0962,
      "step": 593200
    },
    {
      "epoch": 3.853973821819481,
      "grad_norm": 1.3126229047775269,
      "learning_rate": 1.2191845382638994e-05,
      "loss": 0.0959,
      "step": 593300
    },
    {
      "epoch": 3.854623404462633,
      "grad_norm": 1.393112063407898,
      "learning_rate": 1.2184934924213003e-05,
      "loss": 0.1013,
      "step": 593400
    },
    {
      "epoch": 3.8552729871057845,
      "grad_norm": 1.2419111728668213,
      "learning_rate": 1.2178024465787012e-05,
      "loss": 0.0998,
      "step": 593500
    },
    {
      "epoch": 3.8559225697489365,
      "grad_norm": 0.7114100456237793,
      "learning_rate": 1.217111400736102e-05,
      "loss": 0.0956,
      "step": 593600
    },
    {
      "epoch": 3.856572152392088,
      "grad_norm": 1.4008067846298218,
      "learning_rate": 1.216420354893503e-05,
      "loss": 0.095,
      "step": 593700
    },
    {
      "epoch": 3.85722173503524,
      "grad_norm": 1.349226713180542,
      "learning_rate": 1.2157293090509039e-05,
      "loss": 0.0969,
      "step": 593800
    },
    {
      "epoch": 3.8578713176783914,
      "grad_norm": 1.2334712743759155,
      "learning_rate": 1.2150382632083047e-05,
      "loss": 0.0966,
      "step": 593900
    },
    {
      "epoch": 3.8585209003215435,
      "grad_norm": 0.8743274211883545,
      "learning_rate": 1.2143472173657057e-05,
      "loss": 0.1024,
      "step": 594000
    },
    {
      "epoch": 3.859170482964695,
      "grad_norm": 1.4169025421142578,
      "learning_rate": 1.2136561715231065e-05,
      "loss": 0.0958,
      "step": 594100
    },
    {
      "epoch": 3.859820065607847,
      "grad_norm": 1.0577236413955688,
      "learning_rate": 1.2129651256805075e-05,
      "loss": 0.1023,
      "step": 594200
    },
    {
      "epoch": 3.860469648250999,
      "grad_norm": 0.8719182014465332,
      "learning_rate": 1.2122740798379083e-05,
      "loss": 0.0964,
      "step": 594300
    },
    {
      "epoch": 3.8611192308941504,
      "grad_norm": 1.2744171619415283,
      "learning_rate": 1.2115830339953093e-05,
      "loss": 0.0981,
      "step": 594400
    },
    {
      "epoch": 3.861768813537302,
      "grad_norm": 1.2567083835601807,
      "learning_rate": 1.2108919881527102e-05,
      "loss": 0.1013,
      "step": 594500
    },
    {
      "epoch": 3.862418396180454,
      "grad_norm": 1.070098638534546,
      "learning_rate": 1.210200942310111e-05,
      "loss": 0.1021,
      "step": 594600
    },
    {
      "epoch": 3.8630679788236058,
      "grad_norm": 0.9792217016220093,
      "learning_rate": 1.2095098964675119e-05,
      "loss": 0.0956,
      "step": 594700
    },
    {
      "epoch": 3.863717561466758,
      "grad_norm": 0.924237847328186,
      "learning_rate": 1.2088188506249127e-05,
      "loss": 0.1038,
      "step": 594800
    },
    {
      "epoch": 3.8643671441099094,
      "grad_norm": 1.1298565864562988,
      "learning_rate": 1.2081278047823137e-05,
      "loss": 0.1067,
      "step": 594900
    },
    {
      "epoch": 3.865016726753061,
      "grad_norm": 1.2678016424179077,
      "learning_rate": 1.2074367589397147e-05,
      "loss": 0.1005,
      "step": 595000
    },
    {
      "epoch": 3.8656663093962127,
      "grad_norm": 1.697977066040039,
      "learning_rate": 1.2067457130971155e-05,
      "loss": 0.1008,
      "step": 595100
    },
    {
      "epoch": 3.8663158920393648,
      "grad_norm": 1.3214174509048462,
      "learning_rate": 1.2060546672545165e-05,
      "loss": 0.0951,
      "step": 595200
    },
    {
      "epoch": 3.8669654746825164,
      "grad_norm": 0.7313656806945801,
      "learning_rate": 1.2053636214119173e-05,
      "loss": 0.1029,
      "step": 595300
    },
    {
      "epoch": 3.8676150573256685,
      "grad_norm": 1.1907848119735718,
      "learning_rate": 1.2046725755693183e-05,
      "loss": 0.0998,
      "step": 595400
    },
    {
      "epoch": 3.86826463996882,
      "grad_norm": 1.1371619701385498,
      "learning_rate": 1.203981529726719e-05,
      "loss": 0.094,
      "step": 595500
    },
    {
      "epoch": 3.8689142226119717,
      "grad_norm": 0.748053789138794,
      "learning_rate": 1.2032904838841199e-05,
      "loss": 0.1059,
      "step": 595600
    },
    {
      "epoch": 3.8695638052551233,
      "grad_norm": 1.20235013961792,
      "learning_rate": 1.2025994380415209e-05,
      "loss": 0.0994,
      "step": 595700
    },
    {
      "epoch": 3.8702133878982754,
      "grad_norm": 1.156395673751831,
      "learning_rate": 1.2019083921989217e-05,
      "loss": 0.1059,
      "step": 595800
    },
    {
      "epoch": 3.870862970541427,
      "grad_norm": 1.1104220151901245,
      "learning_rate": 1.2012173463563227e-05,
      "loss": 0.0968,
      "step": 595900
    },
    {
      "epoch": 3.871512553184579,
      "grad_norm": 1.5344475507736206,
      "learning_rate": 1.2005263005137235e-05,
      "loss": 0.1018,
      "step": 596000
    },
    {
      "epoch": 3.8721621358277307,
      "grad_norm": 1.2717714309692383,
      "learning_rate": 1.1998352546711245e-05,
      "loss": 0.0926,
      "step": 596100
    },
    {
      "epoch": 3.8728117184708823,
      "grad_norm": 1.385138750076294,
      "learning_rate": 1.1991442088285253e-05,
      "loss": 0.0993,
      "step": 596200
    },
    {
      "epoch": 3.873461301114034,
      "grad_norm": 1.23721182346344,
      "learning_rate": 1.1984531629859263e-05,
      "loss": 0.095,
      "step": 596300
    },
    {
      "epoch": 3.874110883757186,
      "grad_norm": 1.1532024145126343,
      "learning_rate": 1.1977621171433271e-05,
      "loss": 0.1029,
      "step": 596400
    },
    {
      "epoch": 3.8747604664003377,
      "grad_norm": 1.20291268825531,
      "learning_rate": 1.197071071300728e-05,
      "loss": 0.0963,
      "step": 596500
    },
    {
      "epoch": 3.8754100490434897,
      "grad_norm": 0.9991037845611572,
      "learning_rate": 1.1963800254581289e-05,
      "loss": 0.1013,
      "step": 596600
    },
    {
      "epoch": 3.8760596316866414,
      "grad_norm": 0.9072287082672119,
      "learning_rate": 1.1956889796155297e-05,
      "loss": 0.0997,
      "step": 596700
    },
    {
      "epoch": 3.876709214329793,
      "grad_norm": 1.5867183208465576,
      "learning_rate": 1.1949979337729307e-05,
      "loss": 0.094,
      "step": 596800
    },
    {
      "epoch": 3.8773587969729446,
      "grad_norm": 1.0666755437850952,
      "learning_rate": 1.1943068879303315e-05,
      "loss": 0.1029,
      "step": 596900
    },
    {
      "epoch": 3.8780083796160967,
      "grad_norm": 0.5922068953514099,
      "learning_rate": 1.1936158420877325e-05,
      "loss": 0.098,
      "step": 597000
    },
    {
      "epoch": 3.8786579622592483,
      "grad_norm": 0.8606041669845581,
      "learning_rate": 1.1929247962451335e-05,
      "loss": 0.101,
      "step": 597100
    },
    {
      "epoch": 3.8793075449024004,
      "grad_norm": 0.7802014946937561,
      "learning_rate": 1.1922337504025343e-05,
      "loss": 0.0943,
      "step": 597200
    },
    {
      "epoch": 3.879957127545552,
      "grad_norm": 0.8181782364845276,
      "learning_rate": 1.1915427045599351e-05,
      "loss": 0.1013,
      "step": 597300
    },
    {
      "epoch": 3.8806067101887036,
      "grad_norm": 0.800849199295044,
      "learning_rate": 1.190851658717336e-05,
      "loss": 0.1018,
      "step": 597400
    },
    {
      "epoch": 3.8812562928318557,
      "grad_norm": 1.0775134563446045,
      "learning_rate": 1.190160612874737e-05,
      "loss": 0.0957,
      "step": 597500
    },
    {
      "epoch": 3.8819058754750073,
      "grad_norm": 0.886560320854187,
      "learning_rate": 1.1894695670321377e-05,
      "loss": 0.102,
      "step": 597600
    },
    {
      "epoch": 3.882555458118159,
      "grad_norm": 0.9823454022407532,
      "learning_rate": 1.1887785211895387e-05,
      "loss": 0.1028,
      "step": 597700
    },
    {
      "epoch": 3.883205040761311,
      "grad_norm": 1.2971121072769165,
      "learning_rate": 1.1880874753469397e-05,
      "loss": 0.0952,
      "step": 597800
    },
    {
      "epoch": 3.8838546234044626,
      "grad_norm": 1.4348597526550293,
      "learning_rate": 1.1873964295043405e-05,
      "loss": 0.0964,
      "step": 597900
    },
    {
      "epoch": 3.8845042060476143,
      "grad_norm": 1.353580117225647,
      "learning_rate": 1.1867053836617415e-05,
      "loss": 0.097,
      "step": 598000
    },
    {
      "epoch": 3.8851537886907663,
      "grad_norm": 1.5111640691757202,
      "learning_rate": 1.1860143378191423e-05,
      "loss": 0.1003,
      "step": 598100
    },
    {
      "epoch": 3.885803371333918,
      "grad_norm": 1.1347227096557617,
      "learning_rate": 1.1853232919765431e-05,
      "loss": 0.0989,
      "step": 598200
    },
    {
      "epoch": 3.8864529539770696,
      "grad_norm": 1.502506971359253,
      "learning_rate": 1.184632246133944e-05,
      "loss": 0.1013,
      "step": 598300
    },
    {
      "epoch": 3.8871025366202216,
      "grad_norm": 2.036283016204834,
      "learning_rate": 1.183941200291345e-05,
      "loss": 0.1019,
      "step": 598400
    },
    {
      "epoch": 3.8877521192633733,
      "grad_norm": 1.3683364391326904,
      "learning_rate": 1.183250154448746e-05,
      "loss": 0.0961,
      "step": 598500
    },
    {
      "epoch": 3.888401701906525,
      "grad_norm": 1.1167203187942505,
      "learning_rate": 1.1825591086061467e-05,
      "loss": 0.0977,
      "step": 598600
    },
    {
      "epoch": 3.889051284549677,
      "grad_norm": 1.1445839405059814,
      "learning_rate": 1.1818680627635477e-05,
      "loss": 0.0997,
      "step": 598700
    },
    {
      "epoch": 3.8897008671928286,
      "grad_norm": 1.329512119293213,
      "learning_rate": 1.1811770169209485e-05,
      "loss": 0.0985,
      "step": 598800
    },
    {
      "epoch": 3.89035044983598,
      "grad_norm": 0.9289699792861938,
      "learning_rate": 1.1804859710783495e-05,
      "loss": 0.0956,
      "step": 598900
    },
    {
      "epoch": 3.8910000324791323,
      "grad_norm": 1.1928974390029907,
      "learning_rate": 1.1797949252357503e-05,
      "loss": 0.0997,
      "step": 599000
    },
    {
      "epoch": 3.891649615122284,
      "grad_norm": 1.2822462320327759,
      "learning_rate": 1.1791038793931512e-05,
      "loss": 0.1011,
      "step": 599100
    },
    {
      "epoch": 3.8922991977654355,
      "grad_norm": 1.144181251525879,
      "learning_rate": 1.1784128335505521e-05,
      "loss": 0.102,
      "step": 599200
    },
    {
      "epoch": 3.8929487804085876,
      "grad_norm": 0.7786346673965454,
      "learning_rate": 1.177721787707953e-05,
      "loss": 0.1045,
      "step": 599300
    },
    {
      "epoch": 3.8935983630517392,
      "grad_norm": 1.1587446928024292,
      "learning_rate": 1.177030741865354e-05,
      "loss": 0.0911,
      "step": 599400
    },
    {
      "epoch": 3.8942479456948913,
      "grad_norm": 1.4628183841705322,
      "learning_rate": 1.1763396960227548e-05,
      "loss": 0.1006,
      "step": 599500
    },
    {
      "epoch": 3.894897528338043,
      "grad_norm": 1.1932549476623535,
      "learning_rate": 1.1756486501801558e-05,
      "loss": 0.1012,
      "step": 599600
    },
    {
      "epoch": 3.8955471109811945,
      "grad_norm": 1.3427940607070923,
      "learning_rate": 1.1749576043375567e-05,
      "loss": 0.0967,
      "step": 599700
    },
    {
      "epoch": 3.896196693624346,
      "grad_norm": 1.2441328763961792,
      "learning_rate": 1.1742665584949576e-05,
      "loss": 0.1035,
      "step": 599800
    },
    {
      "epoch": 3.8968462762674982,
      "grad_norm": 1.209110140800476,
      "learning_rate": 1.1735755126523584e-05,
      "loss": 0.0986,
      "step": 599900
    },
    {
      "epoch": 3.89749585891065,
      "grad_norm": 0.8989081382751465,
      "learning_rate": 1.1728844668097592e-05,
      "loss": 0.0942,
      "step": 600000
    },
    {
      "epoch": 3.898145441553802,
      "grad_norm": 1.2403913736343384,
      "learning_rate": 1.1721934209671602e-05,
      "loss": 0.103,
      "step": 600100
    },
    {
      "epoch": 3.8987950241969536,
      "grad_norm": 0.3742898106575012,
      "learning_rate": 1.171502375124561e-05,
      "loss": 0.0968,
      "step": 600200
    },
    {
      "epoch": 3.899444606840105,
      "grad_norm": 1.2110247611999512,
      "learning_rate": 1.170811329281962e-05,
      "loss": 0.1002,
      "step": 600300
    },
    {
      "epoch": 3.900094189483257,
      "grad_norm": 1.6228876113891602,
      "learning_rate": 1.170120283439363e-05,
      "loss": 0.0937,
      "step": 600400
    },
    {
      "epoch": 3.900743772126409,
      "grad_norm": 1.1982375383377075,
      "learning_rate": 1.1694292375967638e-05,
      "loss": 0.0962,
      "step": 600500
    },
    {
      "epoch": 3.9013933547695605,
      "grad_norm": 1.077902913093567,
      "learning_rate": 1.1687381917541648e-05,
      "loss": 0.1011,
      "step": 600600
    },
    {
      "epoch": 3.9020429374127126,
      "grad_norm": 0.5930477380752563,
      "learning_rate": 1.1680471459115656e-05,
      "loss": 0.094,
      "step": 600700
    },
    {
      "epoch": 3.902692520055864,
      "grad_norm": 1.0141353607177734,
      "learning_rate": 1.1673561000689664e-05,
      "loss": 0.1027,
      "step": 600800
    },
    {
      "epoch": 3.903342102699016,
      "grad_norm": 1.0576642751693726,
      "learning_rate": 1.1666650542263672e-05,
      "loss": 0.0984,
      "step": 600900
    },
    {
      "epoch": 3.9039916853421675,
      "grad_norm": 0.7848188281059265,
      "learning_rate": 1.1659740083837682e-05,
      "loss": 0.097,
      "step": 601000
    },
    {
      "epoch": 3.9046412679853195,
      "grad_norm": 0.9015927910804749,
      "learning_rate": 1.1652829625411692e-05,
      "loss": 0.0958,
      "step": 601100
    },
    {
      "epoch": 3.905290850628471,
      "grad_norm": 1.0978333950042725,
      "learning_rate": 1.16459191669857e-05,
      "loss": 0.1026,
      "step": 601200
    },
    {
      "epoch": 3.905940433271623,
      "grad_norm": 0.6115937829017639,
      "learning_rate": 1.163900870855971e-05,
      "loss": 0.1037,
      "step": 601300
    },
    {
      "epoch": 3.906590015914775,
      "grad_norm": 1.032979130744934,
      "learning_rate": 1.1632098250133718e-05,
      "loss": 0.0992,
      "step": 601400
    },
    {
      "epoch": 3.9072395985579265,
      "grad_norm": 0.8621972799301147,
      "learning_rate": 1.1625187791707728e-05,
      "loss": 0.0999,
      "step": 601500
    },
    {
      "epoch": 3.907889181201078,
      "grad_norm": 1.0325517654418945,
      "learning_rate": 1.1618277333281736e-05,
      "loss": 0.101,
      "step": 601600
    },
    {
      "epoch": 3.90853876384423,
      "grad_norm": 1.5359688997268677,
      "learning_rate": 1.1611366874855744e-05,
      "loss": 0.1005,
      "step": 601700
    },
    {
      "epoch": 3.909188346487382,
      "grad_norm": 1.1130073070526123,
      "learning_rate": 1.1604456416429754e-05,
      "loss": 0.102,
      "step": 601800
    },
    {
      "epoch": 3.909837929130534,
      "grad_norm": 0.7188476920127869,
      "learning_rate": 1.1597545958003762e-05,
      "loss": 0.0964,
      "step": 601900
    },
    {
      "epoch": 3.9104875117736855,
      "grad_norm": 1.3916428089141846,
      "learning_rate": 1.1590635499577772e-05,
      "loss": 0.0967,
      "step": 602000
    },
    {
      "epoch": 3.911137094416837,
      "grad_norm": 1.2013871669769287,
      "learning_rate": 1.158372504115178e-05,
      "loss": 0.1021,
      "step": 602100
    },
    {
      "epoch": 3.9117866770599887,
      "grad_norm": 0.7623358964920044,
      "learning_rate": 1.157681458272579e-05,
      "loss": 0.0974,
      "step": 602200
    },
    {
      "epoch": 3.912436259703141,
      "grad_norm": 1.205648422241211,
      "learning_rate": 1.1569904124299798e-05,
      "loss": 0.1043,
      "step": 602300
    },
    {
      "epoch": 3.9130858423462924,
      "grad_norm": 0.6484636068344116,
      "learning_rate": 1.1562993665873808e-05,
      "loss": 0.1021,
      "step": 602400
    },
    {
      "epoch": 3.9137354249894445,
      "grad_norm": 0.9580948948860168,
      "learning_rate": 1.1556083207447816e-05,
      "loss": 0.103,
      "step": 602500
    },
    {
      "epoch": 3.914385007632596,
      "grad_norm": 0.7762117385864258,
      "learning_rate": 1.1549172749021824e-05,
      "loss": 0.1002,
      "step": 602600
    },
    {
      "epoch": 3.9150345902757477,
      "grad_norm": 0.8958033323287964,
      "learning_rate": 1.1542262290595834e-05,
      "loss": 0.0926,
      "step": 602700
    },
    {
      "epoch": 3.9156841729188994,
      "grad_norm": 1.4014209508895874,
      "learning_rate": 1.1535351832169842e-05,
      "loss": 0.0897,
      "step": 602800
    },
    {
      "epoch": 3.9163337555620514,
      "grad_norm": 1.5338863134384155,
      "learning_rate": 1.1528441373743852e-05,
      "loss": 0.0944,
      "step": 602900
    },
    {
      "epoch": 3.916983338205203,
      "grad_norm": 1.2643481492996216,
      "learning_rate": 1.152153091531786e-05,
      "loss": 0.1016,
      "step": 603000
    },
    {
      "epoch": 3.917632920848355,
      "grad_norm": 1.0993062257766724,
      "learning_rate": 1.151462045689187e-05,
      "loss": 0.0993,
      "step": 603100
    },
    {
      "epoch": 3.9182825034915068,
      "grad_norm": 0.5301689505577087,
      "learning_rate": 1.150770999846588e-05,
      "loss": 0.0912,
      "step": 603200
    },
    {
      "epoch": 3.9189320861346584,
      "grad_norm": 0.9636676907539368,
      "learning_rate": 1.1500799540039888e-05,
      "loss": 0.0989,
      "step": 603300
    },
    {
      "epoch": 3.91958166877781,
      "grad_norm": 1.3669706583023071,
      "learning_rate": 1.1493889081613896e-05,
      "loss": 0.1048,
      "step": 603400
    },
    {
      "epoch": 3.920231251420962,
      "grad_norm": 0.812288224697113,
      "learning_rate": 1.1486978623187904e-05,
      "loss": 0.0984,
      "step": 603500
    },
    {
      "epoch": 3.9208808340641137,
      "grad_norm": 0.8192429542541504,
      "learning_rate": 1.1480068164761914e-05,
      "loss": 0.1041,
      "step": 603600
    },
    {
      "epoch": 3.9215304167072658,
      "grad_norm": 1.3047170639038086,
      "learning_rate": 1.1473157706335924e-05,
      "loss": 0.0944,
      "step": 603700
    },
    {
      "epoch": 3.9221799993504174,
      "grad_norm": 1.692195177078247,
      "learning_rate": 1.1466247247909932e-05,
      "loss": 0.0994,
      "step": 603800
    },
    {
      "epoch": 3.922829581993569,
      "grad_norm": 0.6374741792678833,
      "learning_rate": 1.1459336789483942e-05,
      "loss": 0.1001,
      "step": 603900
    },
    {
      "epoch": 3.9234791646367206,
      "grad_norm": 0.9814809560775757,
      "learning_rate": 1.145242633105795e-05,
      "loss": 0.1004,
      "step": 604000
    },
    {
      "epoch": 3.9241287472798727,
      "grad_norm": 1.41485595703125,
      "learning_rate": 1.144551587263196e-05,
      "loss": 0.1023,
      "step": 604100
    },
    {
      "epoch": 3.9247783299230243,
      "grad_norm": 1.3903560638427734,
      "learning_rate": 1.1438605414205968e-05,
      "loss": 0.0983,
      "step": 604200
    },
    {
      "epoch": 3.9254279125661764,
      "grad_norm": 1.3669712543487549,
      "learning_rate": 1.1431694955779977e-05,
      "loss": 0.1012,
      "step": 604300
    },
    {
      "epoch": 3.926077495209328,
      "grad_norm": 1.0186223983764648,
      "learning_rate": 1.1424784497353986e-05,
      "loss": 0.0954,
      "step": 604400
    },
    {
      "epoch": 3.9267270778524797,
      "grad_norm": 1.2439513206481934,
      "learning_rate": 1.1417874038927995e-05,
      "loss": 0.1023,
      "step": 604500
    },
    {
      "epoch": 3.9273766604956317,
      "grad_norm": 1.4419032335281372,
      "learning_rate": 1.1410963580502004e-05,
      "loss": 0.1006,
      "step": 604600
    },
    {
      "epoch": 3.9280262431387833,
      "grad_norm": 1.599291443824768,
      "learning_rate": 1.1404053122076013e-05,
      "loss": 0.0962,
      "step": 604700
    },
    {
      "epoch": 3.928675825781935,
      "grad_norm": 0.9805713891983032,
      "learning_rate": 1.1397142663650022e-05,
      "loss": 0.0966,
      "step": 604800
    },
    {
      "epoch": 3.929325408425087,
      "grad_norm": 1.2257964611053467,
      "learning_rate": 1.139023220522403e-05,
      "loss": 0.1051,
      "step": 604900
    },
    {
      "epoch": 3.9299749910682387,
      "grad_norm": 1.411962628364563,
      "learning_rate": 1.138332174679804e-05,
      "loss": 0.0963,
      "step": 605000
    },
    {
      "epoch": 3.9306245737113903,
      "grad_norm": 1.0636953115463257,
      "learning_rate": 1.1376411288372049e-05,
      "loss": 0.0921,
      "step": 605100
    },
    {
      "epoch": 3.9312741563545424,
      "grad_norm": 1.0713059902191162,
      "learning_rate": 1.1369500829946057e-05,
      "loss": 0.1018,
      "step": 605200
    },
    {
      "epoch": 3.931923738997694,
      "grad_norm": 1.3740018606185913,
      "learning_rate": 1.1362590371520067e-05,
      "loss": 0.099,
      "step": 605300
    },
    {
      "epoch": 3.9325733216408456,
      "grad_norm": 1.0349135398864746,
      "learning_rate": 1.1355679913094075e-05,
      "loss": 0.1005,
      "step": 605400
    },
    {
      "epoch": 3.9332229042839977,
      "grad_norm": 1.5863984823226929,
      "learning_rate": 1.1348769454668085e-05,
      "loss": 0.0952,
      "step": 605500
    },
    {
      "epoch": 3.9338724869271493,
      "grad_norm": 0.7267616391181946,
      "learning_rate": 1.1341858996242093e-05,
      "loss": 0.1016,
      "step": 605600
    },
    {
      "epoch": 3.934522069570301,
      "grad_norm": 0.8523587584495544,
      "learning_rate": 1.1334948537816103e-05,
      "loss": 0.0876,
      "step": 605700
    },
    {
      "epoch": 3.935171652213453,
      "grad_norm": 1.2658114433288574,
      "learning_rate": 1.1328038079390112e-05,
      "loss": 0.0943,
      "step": 605800
    },
    {
      "epoch": 3.9358212348566046,
      "grad_norm": 1.0116931200027466,
      "learning_rate": 1.132112762096412e-05,
      "loss": 0.0978,
      "step": 605900
    },
    {
      "epoch": 3.9364708174997562,
      "grad_norm": 1.3264031410217285,
      "learning_rate": 1.1314217162538129e-05,
      "loss": 0.0972,
      "step": 606000
    },
    {
      "epoch": 3.9371204001429083,
      "grad_norm": 1.1087579727172852,
      "learning_rate": 1.1307306704112137e-05,
      "loss": 0.0984,
      "step": 606100
    },
    {
      "epoch": 3.93776998278606,
      "grad_norm": 0.9820888638496399,
      "learning_rate": 1.1300396245686147e-05,
      "loss": 0.0998,
      "step": 606200
    },
    {
      "epoch": 3.9384195654292116,
      "grad_norm": 0.9956678152084351,
      "learning_rate": 1.1293485787260155e-05,
      "loss": 0.0982,
      "step": 606300
    },
    {
      "epoch": 3.9390691480723636,
      "grad_norm": 0.9467127323150635,
      "learning_rate": 1.1286575328834165e-05,
      "loss": 0.1013,
      "step": 606400
    },
    {
      "epoch": 3.9397187307155153,
      "grad_norm": 0.9608361124992371,
      "learning_rate": 1.1279664870408175e-05,
      "loss": 0.0999,
      "step": 606500
    },
    {
      "epoch": 3.9403683133586673,
      "grad_norm": 0.7176440954208374,
      "learning_rate": 1.1272754411982183e-05,
      "loss": 0.0916,
      "step": 606600
    },
    {
      "epoch": 3.941017896001819,
      "grad_norm": 0.8347087502479553,
      "learning_rate": 1.1265843953556193e-05,
      "loss": 0.0998,
      "step": 606700
    },
    {
      "epoch": 3.9416674786449706,
      "grad_norm": 1.1147269010543823,
      "learning_rate": 1.12589334951302e-05,
      "loss": 0.0979,
      "step": 606800
    },
    {
      "epoch": 3.942317061288122,
      "grad_norm": 1.7642649412155151,
      "learning_rate": 1.1252023036704209e-05,
      "loss": 0.0941,
      "step": 606900
    },
    {
      "epoch": 3.9429666439312743,
      "grad_norm": 0.9553717970848083,
      "learning_rate": 1.1245112578278217e-05,
      "loss": 0.1017,
      "step": 607000
    },
    {
      "epoch": 3.943616226574426,
      "grad_norm": 1.073904275894165,
      "learning_rate": 1.1238202119852227e-05,
      "loss": 0.0966,
      "step": 607100
    },
    {
      "epoch": 3.944265809217578,
      "grad_norm": 1.0923373699188232,
      "learning_rate": 1.1231291661426237e-05,
      "loss": 0.106,
      "step": 607200
    },
    {
      "epoch": 3.9449153918607296,
      "grad_norm": 0.9219040870666504,
      "learning_rate": 1.1224381203000245e-05,
      "loss": 0.1,
      "step": 607300
    },
    {
      "epoch": 3.945564974503881,
      "grad_norm": 1.1015220880508423,
      "learning_rate": 1.1217470744574255e-05,
      "loss": 0.1007,
      "step": 607400
    },
    {
      "epoch": 3.946214557147033,
      "grad_norm": 1.0588843822479248,
      "learning_rate": 1.1210560286148263e-05,
      "loss": 0.1028,
      "step": 607500
    },
    {
      "epoch": 3.946864139790185,
      "grad_norm": 1.0498480796813965,
      "learning_rate": 1.1203649827722273e-05,
      "loss": 0.097,
      "step": 607600
    },
    {
      "epoch": 3.9475137224333365,
      "grad_norm": 1.1031441688537598,
      "learning_rate": 1.1196739369296281e-05,
      "loss": 0.104,
      "step": 607700
    },
    {
      "epoch": 3.9481633050764886,
      "grad_norm": 0.903130292892456,
      "learning_rate": 1.1189828910870289e-05,
      "loss": 0.1005,
      "step": 607800
    },
    {
      "epoch": 3.9488128877196402,
      "grad_norm": 1.300142526626587,
      "learning_rate": 1.1182918452444299e-05,
      "loss": 0.0936,
      "step": 607900
    },
    {
      "epoch": 3.949462470362792,
      "grad_norm": 0.9500275254249573,
      "learning_rate": 1.1176007994018307e-05,
      "loss": 0.1038,
      "step": 608000
    },
    {
      "epoch": 3.9501120530059435,
      "grad_norm": 1.6654365062713623,
      "learning_rate": 1.1169097535592317e-05,
      "loss": 0.1088,
      "step": 608100
    },
    {
      "epoch": 3.9507616356490955,
      "grad_norm": 1.3359955549240112,
      "learning_rate": 1.1162187077166325e-05,
      "loss": 0.0972,
      "step": 608200
    },
    {
      "epoch": 3.951411218292247,
      "grad_norm": 1.1872847080230713,
      "learning_rate": 1.1155276618740335e-05,
      "loss": 0.0955,
      "step": 608300
    },
    {
      "epoch": 3.9520608009353992,
      "grad_norm": 0.9371142387390137,
      "learning_rate": 1.1148366160314345e-05,
      "loss": 0.1031,
      "step": 608400
    },
    {
      "epoch": 3.952710383578551,
      "grad_norm": 0.935475766658783,
      "learning_rate": 1.1141455701888353e-05,
      "loss": 0.0956,
      "step": 608500
    },
    {
      "epoch": 3.9533599662217025,
      "grad_norm": 1.1123186349868774,
      "learning_rate": 1.1134545243462361e-05,
      "loss": 0.1051,
      "step": 608600
    },
    {
      "epoch": 3.954009548864854,
      "grad_norm": 1.1632540225982666,
      "learning_rate": 1.112763478503637e-05,
      "loss": 0.0976,
      "step": 608700
    },
    {
      "epoch": 3.954659131508006,
      "grad_norm": 0.5209869742393494,
      "learning_rate": 1.112072432661038e-05,
      "loss": 0.1079,
      "step": 608800
    },
    {
      "epoch": 3.955308714151158,
      "grad_norm": 1.0427887439727783,
      "learning_rate": 1.1113813868184387e-05,
      "loss": 0.1059,
      "step": 608900
    },
    {
      "epoch": 3.95595829679431,
      "grad_norm": 1.1064541339874268,
      "learning_rate": 1.1106903409758397e-05,
      "loss": 0.1024,
      "step": 609000
    },
    {
      "epoch": 3.9566078794374615,
      "grad_norm": 1.0791996717453003,
      "learning_rate": 1.1099992951332407e-05,
      "loss": 0.0957,
      "step": 609100
    },
    {
      "epoch": 3.957257462080613,
      "grad_norm": 1.6499204635620117,
      "learning_rate": 1.1093082492906415e-05,
      "loss": 0.1013,
      "step": 609200
    },
    {
      "epoch": 3.9579070447237648,
      "grad_norm": 1.0032591819763184,
      "learning_rate": 1.1086172034480425e-05,
      "loss": 0.0976,
      "step": 609300
    },
    {
      "epoch": 3.958556627366917,
      "grad_norm": 1.3193475008010864,
      "learning_rate": 1.1079261576054433e-05,
      "loss": 0.0972,
      "step": 609400
    },
    {
      "epoch": 3.9592062100100684,
      "grad_norm": 1.0791735649108887,
      "learning_rate": 1.1072351117628441e-05,
      "loss": 0.094,
      "step": 609500
    },
    {
      "epoch": 3.9598557926532205,
      "grad_norm": 1.2653623819351196,
      "learning_rate": 1.106544065920245e-05,
      "loss": 0.1005,
      "step": 609600
    },
    {
      "epoch": 3.960505375296372,
      "grad_norm": 1.452205777168274,
      "learning_rate": 1.105853020077646e-05,
      "loss": 0.104,
      "step": 609700
    },
    {
      "epoch": 3.9611549579395238,
      "grad_norm": 0.8904374837875366,
      "learning_rate": 1.105161974235047e-05,
      "loss": 0.0972,
      "step": 609800
    },
    {
      "epoch": 3.9618045405826754,
      "grad_norm": 1.0010464191436768,
      "learning_rate": 1.1044709283924477e-05,
      "loss": 0.1011,
      "step": 609900
    },
    {
      "epoch": 3.9624541232258275,
      "grad_norm": 1.2260929346084595,
      "learning_rate": 1.1037798825498487e-05,
      "loss": 0.099,
      "step": 610000
    },
    {
      "epoch": 3.963103705868979,
      "grad_norm": 1.2258238792419434,
      "learning_rate": 1.1030888367072495e-05,
      "loss": 0.1024,
      "step": 610100
    },
    {
      "epoch": 3.963753288512131,
      "grad_norm": 0.49983206391334534,
      "learning_rate": 1.1023977908646505e-05,
      "loss": 0.104,
      "step": 610200
    },
    {
      "epoch": 3.964402871155283,
      "grad_norm": 1.1580439805984497,
      "learning_rate": 1.1017067450220513e-05,
      "loss": 0.0981,
      "step": 610300
    },
    {
      "epoch": 3.9650524537984344,
      "grad_norm": 0.8066468834877014,
      "learning_rate": 1.1010156991794522e-05,
      "loss": 0.1003,
      "step": 610400
    },
    {
      "epoch": 3.965702036441586,
      "grad_norm": 1.03423273563385,
      "learning_rate": 1.1003246533368531e-05,
      "loss": 0.0971,
      "step": 610500
    },
    {
      "epoch": 3.966351619084738,
      "grad_norm": 0.9373117089271545,
      "learning_rate": 1.099633607494254e-05,
      "loss": 0.0961,
      "step": 610600
    },
    {
      "epoch": 3.9670012017278897,
      "grad_norm": 1.2263035774230957,
      "learning_rate": 1.098942561651655e-05,
      "loss": 0.0998,
      "step": 610700
    },
    {
      "epoch": 3.967650784371042,
      "grad_norm": 1.1223807334899902,
      "learning_rate": 1.0982515158090558e-05,
      "loss": 0.0956,
      "step": 610800
    },
    {
      "epoch": 3.9683003670141934,
      "grad_norm": 0.7023486495018005,
      "learning_rate": 1.0975604699664567e-05,
      "loss": 0.0993,
      "step": 610900
    },
    {
      "epoch": 3.968949949657345,
      "grad_norm": 1.1548398733139038,
      "learning_rate": 1.0968694241238576e-05,
      "loss": 0.0958,
      "step": 611000
    },
    {
      "epoch": 3.9695995323004967,
      "grad_norm": 0.9884727597236633,
      "learning_rate": 1.0961783782812585e-05,
      "loss": 0.0973,
      "step": 611100
    },
    {
      "epoch": 3.9702491149436487,
      "grad_norm": 1.3467868566513062,
      "learning_rate": 1.0954873324386594e-05,
      "loss": 0.0969,
      "step": 611200
    },
    {
      "epoch": 3.9708986975868004,
      "grad_norm": 1.363481879234314,
      "learning_rate": 1.0947962865960602e-05,
      "loss": 0.0987,
      "step": 611300
    },
    {
      "epoch": 3.9715482802299524,
      "grad_norm": 1.624127745628357,
      "learning_rate": 1.0941052407534612e-05,
      "loss": 0.0981,
      "step": 611400
    },
    {
      "epoch": 3.972197862873104,
      "grad_norm": 1.972903847694397,
      "learning_rate": 1.093414194910862e-05,
      "loss": 0.0972,
      "step": 611500
    },
    {
      "epoch": 3.9728474455162557,
      "grad_norm": 0.6421907544136047,
      "learning_rate": 1.092723149068263e-05,
      "loss": 0.09,
      "step": 611600
    },
    {
      "epoch": 3.9734970281594073,
      "grad_norm": 0.7383310198783875,
      "learning_rate": 1.092032103225664e-05,
      "loss": 0.0966,
      "step": 611700
    },
    {
      "epoch": 3.9741466108025594,
      "grad_norm": 1.3468403816223145,
      "learning_rate": 1.0913410573830648e-05,
      "loss": 0.1009,
      "step": 611800
    },
    {
      "epoch": 3.974796193445711,
      "grad_norm": 0.9922327399253845,
      "learning_rate": 1.0906500115404657e-05,
      "loss": 0.096,
      "step": 611900
    },
    {
      "epoch": 3.975445776088863,
      "grad_norm": 0.890623152256012,
      "learning_rate": 1.0899589656978666e-05,
      "loss": 0.0981,
      "step": 612000
    },
    {
      "epoch": 3.9760953587320147,
      "grad_norm": 0.889124870300293,
      "learning_rate": 1.0892679198552674e-05,
      "loss": 0.0985,
      "step": 612100
    },
    {
      "epoch": 3.9767449413751663,
      "grad_norm": 1.0077991485595703,
      "learning_rate": 1.0885768740126682e-05,
      "loss": 0.098,
      "step": 612200
    },
    {
      "epoch": 3.9773945240183184,
      "grad_norm": 0.8964030146598816,
      "learning_rate": 1.0878858281700692e-05,
      "loss": 0.0967,
      "step": 612300
    },
    {
      "epoch": 3.97804410666147,
      "grad_norm": 1.695063591003418,
      "learning_rate": 1.0871947823274702e-05,
      "loss": 0.1032,
      "step": 612400
    },
    {
      "epoch": 3.9786936893046216,
      "grad_norm": 0.9654815793037415,
      "learning_rate": 1.086503736484871e-05,
      "loss": 0.1,
      "step": 612500
    },
    {
      "epoch": 3.9793432719477737,
      "grad_norm": 0.7140845656394958,
      "learning_rate": 1.085812690642272e-05,
      "loss": 0.1,
      "step": 612600
    },
    {
      "epoch": 3.9799928545909253,
      "grad_norm": 1.17595374584198,
      "learning_rate": 1.0851216447996728e-05,
      "loss": 0.0951,
      "step": 612700
    },
    {
      "epoch": 3.980642437234077,
      "grad_norm": 1.148390769958496,
      "learning_rate": 1.0844305989570738e-05,
      "loss": 0.0948,
      "step": 612800
    },
    {
      "epoch": 3.981292019877229,
      "grad_norm": 1.2179912328720093,
      "learning_rate": 1.0837395531144746e-05,
      "loss": 0.0928,
      "step": 612900
    },
    {
      "epoch": 3.9819416025203807,
      "grad_norm": 0.8713592886924744,
      "learning_rate": 1.0830485072718754e-05,
      "loss": 0.0961,
      "step": 613000
    },
    {
      "epoch": 3.9825911851635323,
      "grad_norm": 1.1869288682937622,
      "learning_rate": 1.0823574614292764e-05,
      "loss": 0.1011,
      "step": 613100
    },
    {
      "epoch": 3.9832407678066843,
      "grad_norm": 1.0147449970245361,
      "learning_rate": 1.0816664155866772e-05,
      "loss": 0.0941,
      "step": 613200
    },
    {
      "epoch": 3.983890350449836,
      "grad_norm": 0.7274714708328247,
      "learning_rate": 1.0809753697440782e-05,
      "loss": 0.0958,
      "step": 613300
    },
    {
      "epoch": 3.9845399330929876,
      "grad_norm": 1.299812912940979,
      "learning_rate": 1.080284323901479e-05,
      "loss": 0.0911,
      "step": 613400
    },
    {
      "epoch": 3.9851895157361397,
      "grad_norm": 1.1564078330993652,
      "learning_rate": 1.07959327805888e-05,
      "loss": 0.0927,
      "step": 613500
    },
    {
      "epoch": 3.9858390983792913,
      "grad_norm": 0.6371182799339294,
      "learning_rate": 1.0789022322162808e-05,
      "loss": 0.0984,
      "step": 613600
    },
    {
      "epoch": 3.986488681022443,
      "grad_norm": 0.7170802354812622,
      "learning_rate": 1.0782111863736818e-05,
      "loss": 0.1036,
      "step": 613700
    },
    {
      "epoch": 3.987138263665595,
      "grad_norm": 1.1191275119781494,
      "learning_rate": 1.0775201405310826e-05,
      "loss": 0.0981,
      "step": 613800
    },
    {
      "epoch": 3.9877878463087466,
      "grad_norm": 0.39774569869041443,
      "learning_rate": 1.0768290946884834e-05,
      "loss": 0.0952,
      "step": 613900
    },
    {
      "epoch": 3.9884374289518982,
      "grad_norm": 1.3449931144714355,
      "learning_rate": 1.0761380488458844e-05,
      "loss": 0.1051,
      "step": 614000
    },
    {
      "epoch": 3.9890870115950503,
      "grad_norm": 0.8282745480537415,
      "learning_rate": 1.0754470030032852e-05,
      "loss": 0.0951,
      "step": 614100
    },
    {
      "epoch": 3.989736594238202,
      "grad_norm": 1.1878448724746704,
      "learning_rate": 1.0747559571606862e-05,
      "loss": 0.0969,
      "step": 614200
    },
    {
      "epoch": 3.990386176881354,
      "grad_norm": 0.9936156868934631,
      "learning_rate": 1.074064911318087e-05,
      "loss": 0.0985,
      "step": 614300
    },
    {
      "epoch": 3.9910357595245056,
      "grad_norm": 0.8734509944915771,
      "learning_rate": 1.073373865475488e-05,
      "loss": 0.0979,
      "step": 614400
    },
    {
      "epoch": 3.9916853421676572,
      "grad_norm": 1.0013782978057861,
      "learning_rate": 1.072682819632889e-05,
      "loss": 0.1042,
      "step": 614500
    },
    {
      "epoch": 3.992334924810809,
      "grad_norm": 1.0046045780181885,
      "learning_rate": 1.0719917737902898e-05,
      "loss": 0.097,
      "step": 614600
    },
    {
      "epoch": 3.992984507453961,
      "grad_norm": 1.3020135164260864,
      "learning_rate": 1.0713007279476906e-05,
      "loss": 0.0996,
      "step": 614700
    },
    {
      "epoch": 3.9936340900971126,
      "grad_norm": 0.984402596950531,
      "learning_rate": 1.0706096821050914e-05,
      "loss": 0.0936,
      "step": 614800
    },
    {
      "epoch": 3.9942836727402646,
      "grad_norm": 1.0755174160003662,
      "learning_rate": 1.0699186362624924e-05,
      "loss": 0.0946,
      "step": 614900
    },
    {
      "epoch": 3.9949332553834163,
      "grad_norm": 1.4379932880401611,
      "learning_rate": 1.0692275904198932e-05,
      "loss": 0.0989,
      "step": 615000
    },
    {
      "epoch": 3.995582838026568,
      "grad_norm": 0.9005497097969055,
      "learning_rate": 1.0685365445772942e-05,
      "loss": 0.1006,
      "step": 615100
    },
    {
      "epoch": 3.9962324206697195,
      "grad_norm": 1.1895700693130493,
      "learning_rate": 1.0678454987346952e-05,
      "loss": 0.0922,
      "step": 615200
    },
    {
      "epoch": 3.9968820033128716,
      "grad_norm": 0.7952201962471008,
      "learning_rate": 1.067154452892096e-05,
      "loss": 0.0996,
      "step": 615300
    },
    {
      "epoch": 3.997531585956023,
      "grad_norm": 1.060073971748352,
      "learning_rate": 1.066463407049497e-05,
      "loss": 0.099,
      "step": 615400
    },
    {
      "epoch": 3.9981811685991753,
      "grad_norm": 1.0061348676681519,
      "learning_rate": 1.0657723612068978e-05,
      "loss": 0.1013,
      "step": 615500
    },
    {
      "epoch": 3.998830751242327,
      "grad_norm": 1.4373387098312378,
      "learning_rate": 1.0650813153642986e-05,
      "loss": 0.0987,
      "step": 615600
    },
    {
      "epoch": 3.9994803338854785,
      "grad_norm": 0.7818349003791809,
      "learning_rate": 1.0643902695216995e-05,
      "loss": 0.1016,
      "step": 615700
    },
    {
      "epoch": 4.00012991652863,
      "grad_norm": 1.1857585906982422,
      "learning_rate": 1.0636992236791004e-05,
      "loss": 0.0945,
      "step": 615800
    },
    {
      "epoch": 4.000779499171782,
      "grad_norm": 0.9155710935592651,
      "learning_rate": 1.0630081778365014e-05,
      "loss": 0.0993,
      "step": 615900
    },
    {
      "epoch": 4.001429081814934,
      "grad_norm": 1.2534791231155396,
      "learning_rate": 1.0623171319939022e-05,
      "loss": 0.0896,
      "step": 616000
    },
    {
      "epoch": 4.002078664458086,
      "grad_norm": 1.2177025079727173,
      "learning_rate": 1.0616260861513032e-05,
      "loss": 0.0943,
      "step": 616100
    },
    {
      "epoch": 4.0027282471012375,
      "grad_norm": 0.8464306592941284,
      "learning_rate": 1.060935040308704e-05,
      "loss": 0.0989,
      "step": 616200
    },
    {
      "epoch": 4.003377829744389,
      "grad_norm": 1.3707826137542725,
      "learning_rate": 1.060243994466105e-05,
      "loss": 0.1025,
      "step": 616300
    },
    {
      "epoch": 4.004027412387541,
      "grad_norm": 0.814498782157898,
      "learning_rate": 1.0595529486235058e-05,
      "loss": 0.1068,
      "step": 616400
    },
    {
      "epoch": 4.004676995030692,
      "grad_norm": 0.6535230278968811,
      "learning_rate": 1.0588619027809067e-05,
      "loss": 0.0964,
      "step": 616500
    },
    {
      "epoch": 4.005326577673845,
      "grad_norm": 0.9181523323059082,
      "learning_rate": 1.0581708569383076e-05,
      "loss": 0.1,
      "step": 616600
    },
    {
      "epoch": 4.0059761603169965,
      "grad_norm": 0.9454764723777771,
      "learning_rate": 1.0574798110957085e-05,
      "loss": 0.0928,
      "step": 616700
    },
    {
      "epoch": 4.006625742960148,
      "grad_norm": 1.18780517578125,
      "learning_rate": 1.0567887652531094e-05,
      "loss": 0.0881,
      "step": 616800
    },
    {
      "epoch": 4.0072753256033,
      "grad_norm": 0.758051872253418,
      "learning_rate": 1.0560977194105103e-05,
      "loss": 0.0944,
      "step": 616900
    },
    {
      "epoch": 4.007924908246451,
      "grad_norm": 1.1841700077056885,
      "learning_rate": 1.0554066735679113e-05,
      "loss": 0.0949,
      "step": 617000
    },
    {
      "epoch": 4.008574490889603,
      "grad_norm": 1.0016251802444458,
      "learning_rate": 1.0547156277253122e-05,
      "loss": 0.0924,
      "step": 617100
    },
    {
      "epoch": 4.009224073532756,
      "grad_norm": 1.5832961797714233,
      "learning_rate": 1.054024581882713e-05,
      "loss": 0.0981,
      "step": 617200
    },
    {
      "epoch": 4.009873656175907,
      "grad_norm": 1.241133213043213,
      "learning_rate": 1.0533335360401139e-05,
      "loss": 0.1028,
      "step": 617300
    },
    {
      "epoch": 4.010523238819059,
      "grad_norm": 1.2876256704330444,
      "learning_rate": 1.0526424901975147e-05,
      "loss": 0.0955,
      "step": 617400
    },
    {
      "epoch": 4.01117282146221,
      "grad_norm": 1.75311279296875,
      "learning_rate": 1.0519514443549157e-05,
      "loss": 0.0981,
      "step": 617500
    },
    {
      "epoch": 4.011822404105362,
      "grad_norm": 0.9404552578926086,
      "learning_rate": 1.0512603985123165e-05,
      "loss": 0.0982,
      "step": 617600
    },
    {
      "epoch": 4.012471986748514,
      "grad_norm": 1.425970435142517,
      "learning_rate": 1.0505693526697175e-05,
      "loss": 0.0985,
      "step": 617700
    },
    {
      "epoch": 4.013121569391666,
      "grad_norm": 1.0134828090667725,
      "learning_rate": 1.0498783068271185e-05,
      "loss": 0.0983,
      "step": 617800
    },
    {
      "epoch": 4.013771152034818,
      "grad_norm": 1.162994146347046,
      "learning_rate": 1.0491872609845193e-05,
      "loss": 0.0943,
      "step": 617900
    },
    {
      "epoch": 4.0144207346779694,
      "grad_norm": 1.3602453470230103,
      "learning_rate": 1.0484962151419203e-05,
      "loss": 0.0974,
      "step": 618000
    },
    {
      "epoch": 4.015070317321121,
      "grad_norm": 1.4917752742767334,
      "learning_rate": 1.047805169299321e-05,
      "loss": 0.1,
      "step": 618100
    },
    {
      "epoch": 4.015719899964273,
      "grad_norm": 1.1138808727264404,
      "learning_rate": 1.0471141234567219e-05,
      "loss": 0.0993,
      "step": 618200
    },
    {
      "epoch": 4.016369482607424,
      "grad_norm": 1.1827729940414429,
      "learning_rate": 1.0464230776141227e-05,
      "loss": 0.0998,
      "step": 618300
    },
    {
      "epoch": 4.017019065250577,
      "grad_norm": 0.9382609724998474,
      "learning_rate": 1.0457320317715237e-05,
      "loss": 0.1024,
      "step": 618400
    },
    {
      "epoch": 4.0176686478937285,
      "grad_norm": 1.4535799026489258,
      "learning_rate": 1.0450409859289247e-05,
      "loss": 0.0984,
      "step": 618500
    },
    {
      "epoch": 4.01831823053688,
      "grad_norm": 1.7878799438476562,
      "learning_rate": 1.0443499400863255e-05,
      "loss": 0.1026,
      "step": 618600
    },
    {
      "epoch": 4.018967813180032,
      "grad_norm": 1.2549448013305664,
      "learning_rate": 1.0436588942437265e-05,
      "loss": 0.1054,
      "step": 618700
    },
    {
      "epoch": 4.019617395823183,
      "grad_norm": 1.1197144985198975,
      "learning_rate": 1.0429678484011273e-05,
      "loss": 0.0929,
      "step": 618800
    },
    {
      "epoch": 4.020266978466335,
      "grad_norm": 1.3654797077178955,
      "learning_rate": 1.0422768025585283e-05,
      "loss": 0.1012,
      "step": 618900
    },
    {
      "epoch": 4.0209165611094875,
      "grad_norm": 0.8013331890106201,
      "learning_rate": 1.0415857567159291e-05,
      "loss": 0.1,
      "step": 619000
    },
    {
      "epoch": 4.021566143752639,
      "grad_norm": 0.9528802037239075,
      "learning_rate": 1.0408947108733299e-05,
      "loss": 0.0933,
      "step": 619100
    },
    {
      "epoch": 4.022215726395791,
      "grad_norm": 0.5977745652198792,
      "learning_rate": 1.0402036650307309e-05,
      "loss": 0.1011,
      "step": 619200
    },
    {
      "epoch": 4.022865309038942,
      "grad_norm": 0.9879841804504395,
      "learning_rate": 1.0395126191881317e-05,
      "loss": 0.0923,
      "step": 619300
    },
    {
      "epoch": 4.023514891682094,
      "grad_norm": 1.5692298412322998,
      "learning_rate": 1.0388215733455327e-05,
      "loss": 0.1003,
      "step": 619400
    },
    {
      "epoch": 4.024164474325246,
      "grad_norm": 1.0203065872192383,
      "learning_rate": 1.0381305275029335e-05,
      "loss": 0.1016,
      "step": 619500
    },
    {
      "epoch": 4.024814056968398,
      "grad_norm": 1.2814288139343262,
      "learning_rate": 1.0374394816603345e-05,
      "loss": 0.0951,
      "step": 619600
    },
    {
      "epoch": 4.02546363961155,
      "grad_norm": 1.4214435815811157,
      "learning_rate": 1.0367484358177353e-05,
      "loss": 0.096,
      "step": 619700
    },
    {
      "epoch": 4.026113222254701,
      "grad_norm": 0.9624254703521729,
      "learning_rate": 1.0360573899751363e-05,
      "loss": 0.101,
      "step": 619800
    },
    {
      "epoch": 4.026762804897853,
      "grad_norm": 1.7521437406539917,
      "learning_rate": 1.0353663441325371e-05,
      "loss": 0.0994,
      "step": 619900
    },
    {
      "epoch": 4.027412387541005,
      "grad_norm": 1.3637726306915283,
      "learning_rate": 1.034675298289938e-05,
      "loss": 0.0934,
      "step": 620000
    },
    {
      "epoch": 4.028061970184156,
      "grad_norm": 1.4204559326171875,
      "learning_rate": 1.0339842524473389e-05,
      "loss": 0.0973,
      "step": 620100
    },
    {
      "epoch": 4.028711552827309,
      "grad_norm": 0.8536220192909241,
      "learning_rate": 1.0332932066047397e-05,
      "loss": 0.0963,
      "step": 620200
    },
    {
      "epoch": 4.02936113547046,
      "grad_norm": 0.8336510062217712,
      "learning_rate": 1.0326021607621407e-05,
      "loss": 0.0958,
      "step": 620300
    },
    {
      "epoch": 4.030010718113612,
      "grad_norm": 0.9650218486785889,
      "learning_rate": 1.0319111149195417e-05,
      "loss": 0.1022,
      "step": 620400
    },
    {
      "epoch": 4.030660300756764,
      "grad_norm": 1.6944001913070679,
      "learning_rate": 1.0312200690769425e-05,
      "loss": 0.094,
      "step": 620500
    },
    {
      "epoch": 4.031309883399915,
      "grad_norm": 1.1028844118118286,
      "learning_rate": 1.0305290232343435e-05,
      "loss": 0.0984,
      "step": 620600
    },
    {
      "epoch": 4.031959466043068,
      "grad_norm": 0.9142544269561768,
      "learning_rate": 1.0298379773917443e-05,
      "loss": 0.0914,
      "step": 620700
    },
    {
      "epoch": 4.032609048686219,
      "grad_norm": 0.818110466003418,
      "learning_rate": 1.0291469315491451e-05,
      "loss": 0.1007,
      "step": 620800
    },
    {
      "epoch": 4.033258631329371,
      "grad_norm": 1.0560487508773804,
      "learning_rate": 1.028455885706546e-05,
      "loss": 0.0967,
      "step": 620900
    },
    {
      "epoch": 4.033908213972523,
      "grad_norm": 1.23997163772583,
      "learning_rate": 1.027764839863947e-05,
      "loss": 0.0946,
      "step": 621000
    },
    {
      "epoch": 4.034557796615674,
      "grad_norm": 1.1920742988586426,
      "learning_rate": 1.027073794021348e-05,
      "loss": 0.1036,
      "step": 621100
    },
    {
      "epoch": 4.035207379258826,
      "grad_norm": 1.6010403633117676,
      "learning_rate": 1.0263827481787487e-05,
      "loss": 0.0977,
      "step": 621200
    },
    {
      "epoch": 4.035856961901978,
      "grad_norm": 1.12551748752594,
      "learning_rate": 1.0256917023361497e-05,
      "loss": 0.0969,
      "step": 621300
    },
    {
      "epoch": 4.03650654454513,
      "grad_norm": 1.3317228555679321,
      "learning_rate": 1.0250006564935505e-05,
      "loss": 0.0989,
      "step": 621400
    },
    {
      "epoch": 4.037156127188282,
      "grad_norm": 0.7386319041252136,
      "learning_rate": 1.0243096106509515e-05,
      "loss": 0.098,
      "step": 621500
    },
    {
      "epoch": 4.037805709831433,
      "grad_norm": 0.7692798972129822,
      "learning_rate": 1.0236185648083522e-05,
      "loss": 0.095,
      "step": 621600
    },
    {
      "epoch": 4.038455292474585,
      "grad_norm": 1.5112566947937012,
      "learning_rate": 1.0229275189657531e-05,
      "loss": 0.0938,
      "step": 621700
    },
    {
      "epoch": 4.0391048751177365,
      "grad_norm": 0.5001369118690491,
      "learning_rate": 1.0222364731231541e-05,
      "loss": 0.0978,
      "step": 621800
    },
    {
      "epoch": 4.039754457760889,
      "grad_norm": 1.913097858428955,
      "learning_rate": 1.021545427280555e-05,
      "loss": 0.0969,
      "step": 621900
    },
    {
      "epoch": 4.040404040404041,
      "grad_norm": 1.667134404182434,
      "learning_rate": 1.020854381437956e-05,
      "loss": 0.1017,
      "step": 622000
    },
    {
      "epoch": 4.041053623047192,
      "grad_norm": 1.075247883796692,
      "learning_rate": 1.0201633355953568e-05,
      "loss": 0.1042,
      "step": 622100
    },
    {
      "epoch": 4.041703205690344,
      "grad_norm": 0.9782605767250061,
      "learning_rate": 1.0194722897527577e-05,
      "loss": 0.0976,
      "step": 622200
    },
    {
      "epoch": 4.0423527883334955,
      "grad_norm": 0.6823416352272034,
      "learning_rate": 1.0187812439101586e-05,
      "loss": 0.1025,
      "step": 622300
    },
    {
      "epoch": 4.043002370976647,
      "grad_norm": 1.0170531272888184,
      "learning_rate": 1.0180901980675595e-05,
      "loss": 0.0942,
      "step": 622400
    },
    {
      "epoch": 4.0436519536198,
      "grad_norm": 1.3410420417785645,
      "learning_rate": 1.0173991522249604e-05,
      "loss": 0.0931,
      "step": 622500
    },
    {
      "epoch": 4.044301536262951,
      "grad_norm": 1.0436512231826782,
      "learning_rate": 1.0167081063823612e-05,
      "loss": 0.1011,
      "step": 622600
    },
    {
      "epoch": 4.044951118906103,
      "grad_norm": 1.2038699388504028,
      "learning_rate": 1.0160170605397622e-05,
      "loss": 0.0987,
      "step": 622700
    },
    {
      "epoch": 4.0456007015492546,
      "grad_norm": 1.1464546918869019,
      "learning_rate": 1.015326014697163e-05,
      "loss": 0.0949,
      "step": 622800
    },
    {
      "epoch": 4.046250284192406,
      "grad_norm": 0.9120889902114868,
      "learning_rate": 1.014634968854564e-05,
      "loss": 0.09,
      "step": 622900
    },
    {
      "epoch": 4.046899866835558,
      "grad_norm": 1.1339575052261353,
      "learning_rate": 1.0139439230119648e-05,
      "loss": 0.0961,
      "step": 623000
    },
    {
      "epoch": 4.04754944947871,
      "grad_norm": 1.0728166103363037,
      "learning_rate": 1.0132528771693658e-05,
      "loss": 0.0975,
      "step": 623100
    },
    {
      "epoch": 4.048199032121862,
      "grad_norm": 1.342113733291626,
      "learning_rate": 1.0125618313267667e-05,
      "loss": 0.1031,
      "step": 623200
    },
    {
      "epoch": 4.048848614765014,
      "grad_norm": 1.6731626987457275,
      "learning_rate": 1.0118707854841674e-05,
      "loss": 0.0982,
      "step": 623300
    },
    {
      "epoch": 4.049498197408165,
      "grad_norm": 0.9875195026397705,
      "learning_rate": 1.0111797396415684e-05,
      "loss": 0.0996,
      "step": 623400
    },
    {
      "epoch": 4.050147780051317,
      "grad_norm": 1.4461711645126343,
      "learning_rate": 1.0104886937989692e-05,
      "loss": 0.0965,
      "step": 623500
    },
    {
      "epoch": 4.050797362694468,
      "grad_norm": 1.498897671699524,
      "learning_rate": 1.0097976479563702e-05,
      "loss": 0.095,
      "step": 623600
    },
    {
      "epoch": 4.051446945337621,
      "grad_norm": 0.8101398348808289,
      "learning_rate": 1.009106602113771e-05,
      "loss": 0.0938,
      "step": 623700
    },
    {
      "epoch": 4.052096527980773,
      "grad_norm": 1.7182328701019287,
      "learning_rate": 1.008415556271172e-05,
      "loss": 0.0977,
      "step": 623800
    },
    {
      "epoch": 4.052746110623924,
      "grad_norm": 1.5380430221557617,
      "learning_rate": 1.007724510428573e-05,
      "loss": 0.0971,
      "step": 623900
    },
    {
      "epoch": 4.053395693267076,
      "grad_norm": 0.7789759635925293,
      "learning_rate": 1.0070334645859738e-05,
      "loss": 0.0953,
      "step": 624000
    },
    {
      "epoch": 4.0540452759102275,
      "grad_norm": 1.2044456005096436,
      "learning_rate": 1.0063424187433746e-05,
      "loss": 0.1041,
      "step": 624100
    },
    {
      "epoch": 4.054694858553379,
      "grad_norm": 1.3023937940597534,
      "learning_rate": 1.0056513729007754e-05,
      "loss": 0.1007,
      "step": 624200
    },
    {
      "epoch": 4.055344441196532,
      "grad_norm": 1.1358678340911865,
      "learning_rate": 1.0049603270581764e-05,
      "loss": 0.1022,
      "step": 624300
    },
    {
      "epoch": 4.055994023839683,
      "grad_norm": 0.90733402967453,
      "learning_rate": 1.0042692812155774e-05,
      "loss": 0.0986,
      "step": 624400
    },
    {
      "epoch": 4.056643606482835,
      "grad_norm": 0.9633350968360901,
      "learning_rate": 1.0035782353729782e-05,
      "loss": 0.1,
      "step": 624500
    },
    {
      "epoch": 4.0572931891259865,
      "grad_norm": 1.1887121200561523,
      "learning_rate": 1.0028871895303792e-05,
      "loss": 0.0988,
      "step": 624600
    },
    {
      "epoch": 4.057942771769138,
      "grad_norm": 1.1949479579925537,
      "learning_rate": 1.00219614368778e-05,
      "loss": 0.0937,
      "step": 624700
    },
    {
      "epoch": 4.05859235441229,
      "grad_norm": 1.1747817993164062,
      "learning_rate": 1.001505097845181e-05,
      "loss": 0.0959,
      "step": 624800
    },
    {
      "epoch": 4.059241937055442,
      "grad_norm": 0.9789682030677795,
      "learning_rate": 1.0008140520025818e-05,
      "loss": 0.1001,
      "step": 624900
    },
    {
      "epoch": 4.059891519698594,
      "grad_norm": 1.3523184061050415,
      "learning_rate": 1.0001230061599826e-05,
      "loss": 0.0987,
      "step": 625000
    },
    {
      "epoch": 4.0605411023417455,
      "grad_norm": 0.9502987265586853,
      "learning_rate": 9.994319603173836e-06,
      "loss": 0.0988,
      "step": 625100
    },
    {
      "epoch": 4.061190684984897,
      "grad_norm": 1.6590569019317627,
      "learning_rate": 9.987409144747844e-06,
      "loss": 0.1,
      "step": 625200
    },
    {
      "epoch": 4.061840267628049,
      "grad_norm": 0.8205365538597107,
      "learning_rate": 9.980498686321854e-06,
      "loss": 0.099,
      "step": 625300
    },
    {
      "epoch": 4.0624898502712,
      "grad_norm": 1.107697606086731,
      "learning_rate": 9.973588227895862e-06,
      "loss": 0.1,
      "step": 625400
    },
    {
      "epoch": 4.063139432914353,
      "grad_norm": 1.274921178817749,
      "learning_rate": 9.966677769469872e-06,
      "loss": 0.0965,
      "step": 625500
    },
    {
      "epoch": 4.0637890155575045,
      "grad_norm": 1.5577213764190674,
      "learning_rate": 9.95976731104388e-06,
      "loss": 0.0994,
      "step": 625600
    },
    {
      "epoch": 4.064438598200656,
      "grad_norm": 0.8758738040924072,
      "learning_rate": 9.95285685261789e-06,
      "loss": 0.1005,
      "step": 625700
    },
    {
      "epoch": 4.065088180843808,
      "grad_norm": 0.899006187915802,
      "learning_rate": 9.945946394191898e-06,
      "loss": 0.0956,
      "step": 625800
    },
    {
      "epoch": 4.065737763486959,
      "grad_norm": 0.9061962366104126,
      "learning_rate": 9.939035935765906e-06,
      "loss": 0.0978,
      "step": 625900
    },
    {
      "epoch": 4.066387346130111,
      "grad_norm": 0.7642815113067627,
      "learning_rate": 9.932125477339916e-06,
      "loss": 0.0945,
      "step": 626000
    },
    {
      "epoch": 4.0670369287732635,
      "grad_norm": 1.5660021305084229,
      "learning_rate": 9.925215018913924e-06,
      "loss": 0.104,
      "step": 626100
    },
    {
      "epoch": 4.067686511416415,
      "grad_norm": 1.165886402130127,
      "learning_rate": 9.918304560487934e-06,
      "loss": 0.0986,
      "step": 626200
    },
    {
      "epoch": 4.068336094059567,
      "grad_norm": 1.3823899030685425,
      "learning_rate": 9.911394102061942e-06,
      "loss": 0.0933,
      "step": 626300
    },
    {
      "epoch": 4.068985676702718,
      "grad_norm": 0.9860575795173645,
      "learning_rate": 9.904483643635952e-06,
      "loss": 0.0965,
      "step": 626400
    },
    {
      "epoch": 4.06963525934587,
      "grad_norm": 0.7096673250198364,
      "learning_rate": 9.897573185209962e-06,
      "loss": 0.0996,
      "step": 626500
    },
    {
      "epoch": 4.070284841989022,
      "grad_norm": 1.0473780632019043,
      "learning_rate": 9.89066272678397e-06,
      "loss": 0.0964,
      "step": 626600
    },
    {
      "epoch": 4.070934424632174,
      "grad_norm": 1.2364439964294434,
      "learning_rate": 9.883752268357978e-06,
      "loss": 0.0952,
      "step": 626700
    },
    {
      "epoch": 4.071584007275326,
      "grad_norm": 1.2992745637893677,
      "learning_rate": 9.876841809931987e-06,
      "loss": 0.0968,
      "step": 626800
    },
    {
      "epoch": 4.072233589918477,
      "grad_norm": 1.0890676975250244,
      "learning_rate": 9.869931351505996e-06,
      "loss": 0.1003,
      "step": 626900
    },
    {
      "epoch": 4.072883172561629,
      "grad_norm": 0.6310179233551025,
      "learning_rate": 9.863020893080005e-06,
      "loss": 0.0968,
      "step": 627000
    },
    {
      "epoch": 4.073532755204781,
      "grad_norm": 1.207594871520996,
      "learning_rate": 9.856110434654014e-06,
      "loss": 0.0984,
      "step": 627100
    },
    {
      "epoch": 4.074182337847932,
      "grad_norm": 0.8880525827407837,
      "learning_rate": 9.849199976228024e-06,
      "loss": 0.0926,
      "step": 627200
    },
    {
      "epoch": 4.074831920491085,
      "grad_norm": 0.8952992558479309,
      "learning_rate": 9.842289517802032e-06,
      "loss": 0.0999,
      "step": 627300
    },
    {
      "epoch": 4.075481503134236,
      "grad_norm": 1.364291787147522,
      "learning_rate": 9.835379059376042e-06,
      "loss": 0.1002,
      "step": 627400
    },
    {
      "epoch": 4.076131085777388,
      "grad_norm": 0.8177933096885681,
      "learning_rate": 9.82846860095005e-06,
      "loss": 0.0968,
      "step": 627500
    },
    {
      "epoch": 4.07678066842054,
      "grad_norm": 1.2861989736557007,
      "learning_rate": 9.821558142524059e-06,
      "loss": 0.0942,
      "step": 627600
    },
    {
      "epoch": 4.077430251063691,
      "grad_norm": 1.1973482370376587,
      "learning_rate": 9.814647684098067e-06,
      "loss": 0.0939,
      "step": 627700
    },
    {
      "epoch": 4.078079833706843,
      "grad_norm": 1.8108537197113037,
      "learning_rate": 9.807737225672077e-06,
      "loss": 0.1005,
      "step": 627800
    },
    {
      "epoch": 4.078729416349995,
      "grad_norm": 1.5795210599899292,
      "learning_rate": 9.800826767246086e-06,
      "loss": 0.0976,
      "step": 627900
    },
    {
      "epoch": 4.079378998993147,
      "grad_norm": 1.4447678327560425,
      "learning_rate": 9.793916308820095e-06,
      "loss": 0.0971,
      "step": 628000
    },
    {
      "epoch": 4.080028581636299,
      "grad_norm": 1.1004401445388794,
      "learning_rate": 9.787005850394104e-06,
      "loss": 0.103,
      "step": 628100
    },
    {
      "epoch": 4.08067816427945,
      "grad_norm": 1.0158137083053589,
      "learning_rate": 9.780095391968113e-06,
      "loss": 0.0973,
      "step": 628200
    },
    {
      "epoch": 4.081327746922602,
      "grad_norm": 0.6868614554405212,
      "learning_rate": 9.773184933542122e-06,
      "loss": 0.0991,
      "step": 628300
    },
    {
      "epoch": 4.081977329565754,
      "grad_norm": 0.9143078327178955,
      "learning_rate": 9.76627447511613e-06,
      "loss": 0.1003,
      "step": 628400
    },
    {
      "epoch": 4.082626912208906,
      "grad_norm": 1.4797723293304443,
      "learning_rate": 9.759364016690139e-06,
      "loss": 0.101,
      "step": 628500
    },
    {
      "epoch": 4.083276494852058,
      "grad_norm": 1.5967594385147095,
      "learning_rate": 9.752453558264149e-06,
      "loss": 0.1004,
      "step": 628600
    },
    {
      "epoch": 4.083926077495209,
      "grad_norm": 1.7920211553573608,
      "learning_rate": 9.745543099838157e-06,
      "loss": 0.1002,
      "step": 628700
    },
    {
      "epoch": 4.084575660138361,
      "grad_norm": 0.591457724571228,
      "learning_rate": 9.738632641412167e-06,
      "loss": 0.0936,
      "step": 628800
    },
    {
      "epoch": 4.0852252427815126,
      "grad_norm": 1.0684226751327515,
      "learning_rate": 9.731722182986175e-06,
      "loss": 0.0923,
      "step": 628900
    },
    {
      "epoch": 4.085874825424665,
      "grad_norm": 0.5949205160140991,
      "learning_rate": 9.724811724560185e-06,
      "loss": 0.1015,
      "step": 629000
    },
    {
      "epoch": 4.086524408067817,
      "grad_norm": 1.0768312215805054,
      "learning_rate": 9.717901266134194e-06,
      "loss": 0.0944,
      "step": 629100
    },
    {
      "epoch": 4.087173990710968,
      "grad_norm": 0.838193416595459,
      "learning_rate": 9.710990807708203e-06,
      "loss": 0.0973,
      "step": 629200
    },
    {
      "epoch": 4.08782357335412,
      "grad_norm": 1.0522937774658203,
      "learning_rate": 9.70408034928221e-06,
      "loss": 0.0949,
      "step": 629300
    },
    {
      "epoch": 4.088473155997272,
      "grad_norm": 0.8872876763343811,
      "learning_rate": 9.697169890856219e-06,
      "loss": 0.1022,
      "step": 629400
    },
    {
      "epoch": 4.089122738640423,
      "grad_norm": 0.915227472782135,
      "learning_rate": 9.690259432430229e-06,
      "loss": 0.0951,
      "step": 629500
    },
    {
      "epoch": 4.089772321283576,
      "grad_norm": 0.7349240183830261,
      "learning_rate": 9.683348974004237e-06,
      "loss": 0.0936,
      "step": 629600
    },
    {
      "epoch": 4.090421903926727,
      "grad_norm": 0.8625869750976562,
      "learning_rate": 9.676438515578247e-06,
      "loss": 0.0931,
      "step": 629700
    },
    {
      "epoch": 4.091071486569879,
      "grad_norm": 1.134330153465271,
      "learning_rate": 9.669528057152257e-06,
      "loss": 0.0967,
      "step": 629800
    },
    {
      "epoch": 4.091721069213031,
      "grad_norm": 0.9703468680381775,
      "learning_rate": 9.662617598726265e-06,
      "loss": 0.0967,
      "step": 629900
    },
    {
      "epoch": 4.092370651856182,
      "grad_norm": 1.4277645349502563,
      "learning_rate": 9.655707140300275e-06,
      "loss": 0.1017,
      "step": 630000
    },
    {
      "epoch": 4.093020234499334,
      "grad_norm": 1.1708208322525024,
      "learning_rate": 9.648796681874283e-06,
      "loss": 0.0992,
      "step": 630100
    },
    {
      "epoch": 4.093669817142486,
      "grad_norm": 0.7430281043052673,
      "learning_rate": 9.641886223448291e-06,
      "loss": 0.094,
      "step": 630200
    },
    {
      "epoch": 4.094319399785638,
      "grad_norm": 1.4327994585037231,
      "learning_rate": 9.634975765022299e-06,
      "loss": 0.0976,
      "step": 630300
    },
    {
      "epoch": 4.09496898242879,
      "grad_norm": 0.818755567073822,
      "learning_rate": 9.628065306596309e-06,
      "loss": 0.097,
      "step": 630400
    },
    {
      "epoch": 4.095618565071941,
      "grad_norm": 1.1793020963668823,
      "learning_rate": 9.621154848170319e-06,
      "loss": 0.0946,
      "step": 630500
    },
    {
      "epoch": 4.096268147715093,
      "grad_norm": 1.1587473154067993,
      "learning_rate": 9.614244389744327e-06,
      "loss": 0.0978,
      "step": 630600
    },
    {
      "epoch": 4.0969177303582445,
      "grad_norm": 1.4885815382003784,
      "learning_rate": 9.607333931318337e-06,
      "loss": 0.0958,
      "step": 630700
    },
    {
      "epoch": 4.097567313001397,
      "grad_norm": 1.3656800985336304,
      "learning_rate": 9.600423472892345e-06,
      "loss": 0.1028,
      "step": 630800
    },
    {
      "epoch": 4.098216895644549,
      "grad_norm": 1.0734574794769287,
      "learning_rate": 9.593513014466355e-06,
      "loss": 0.1019,
      "step": 630900
    },
    {
      "epoch": 4.0988664782877,
      "grad_norm": 1.116992712020874,
      "learning_rate": 9.586602556040363e-06,
      "loss": 0.0989,
      "step": 631000
    },
    {
      "epoch": 4.099516060930852,
      "grad_norm": 1.5883029699325562,
      "learning_rate": 9.579692097614371e-06,
      "loss": 0.0953,
      "step": 631100
    },
    {
      "epoch": 4.1001656435740035,
      "grad_norm": 0.8269321322441101,
      "learning_rate": 9.572781639188381e-06,
      "loss": 0.1008,
      "step": 631200
    },
    {
      "epoch": 4.100815226217155,
      "grad_norm": 1.7095422744750977,
      "learning_rate": 9.56587118076239e-06,
      "loss": 0.0976,
      "step": 631300
    },
    {
      "epoch": 4.101464808860308,
      "grad_norm": 1.1071820259094238,
      "learning_rate": 9.558960722336399e-06,
      "loss": 0.0976,
      "step": 631400
    },
    {
      "epoch": 4.102114391503459,
      "grad_norm": 0.7973753213882446,
      "learning_rate": 9.552050263910407e-06,
      "loss": 0.0921,
      "step": 631500
    },
    {
      "epoch": 4.102763974146611,
      "grad_norm": 0.9135129451751709,
      "learning_rate": 9.545139805484417e-06,
      "loss": 0.0976,
      "step": 631600
    },
    {
      "epoch": 4.1034135567897625,
      "grad_norm": 0.9680647253990173,
      "learning_rate": 9.538229347058425e-06,
      "loss": 0.0943,
      "step": 631700
    },
    {
      "epoch": 4.104063139432914,
      "grad_norm": 1.3987741470336914,
      "learning_rate": 9.531318888632435e-06,
      "loss": 0.0906,
      "step": 631800
    },
    {
      "epoch": 4.104712722076066,
      "grad_norm": 1.185460090637207,
      "learning_rate": 9.524408430206443e-06,
      "loss": 0.0962,
      "step": 631900
    },
    {
      "epoch": 4.105362304719218,
      "grad_norm": 1.1675704717636108,
      "learning_rate": 9.517497971780451e-06,
      "loss": 0.0992,
      "step": 632000
    },
    {
      "epoch": 4.10601188736237,
      "grad_norm": 0.7246127128601074,
      "learning_rate": 9.510587513354461e-06,
      "loss": 0.096,
      "step": 632100
    },
    {
      "epoch": 4.1066614700055215,
      "grad_norm": 1.00441312789917,
      "learning_rate": 9.50367705492847e-06,
      "loss": 0.0959,
      "step": 632200
    },
    {
      "epoch": 4.107311052648673,
      "grad_norm": 0.8643437623977661,
      "learning_rate": 9.49676659650248e-06,
      "loss": 0.103,
      "step": 632300
    },
    {
      "epoch": 4.107960635291825,
      "grad_norm": 1.4423235654830933,
      "learning_rate": 9.489856138076487e-06,
      "loss": 0.099,
      "step": 632400
    },
    {
      "epoch": 4.108610217934976,
      "grad_norm": 1.4834421873092651,
      "learning_rate": 9.482945679650497e-06,
      "loss": 0.1006,
      "step": 632500
    },
    {
      "epoch": 4.109259800578129,
      "grad_norm": 1.4676332473754883,
      "learning_rate": 9.476035221224507e-06,
      "loss": 0.095,
      "step": 632600
    },
    {
      "epoch": 4.1099093832212805,
      "grad_norm": 1.0014667510986328,
      "learning_rate": 9.469124762798515e-06,
      "loss": 0.0952,
      "step": 632700
    },
    {
      "epoch": 4.110558965864432,
      "grad_norm": 1.446218490600586,
      "learning_rate": 9.462214304372523e-06,
      "loss": 0.0962,
      "step": 632800
    },
    {
      "epoch": 4.111208548507584,
      "grad_norm": 0.8769903182983398,
      "learning_rate": 9.455303845946532e-06,
      "loss": 0.0938,
      "step": 632900
    },
    {
      "epoch": 4.111858131150735,
      "grad_norm": 0.8432469367980957,
      "learning_rate": 9.448393387520541e-06,
      "loss": 0.0942,
      "step": 633000
    },
    {
      "epoch": 4.112507713793887,
      "grad_norm": 1.1673306226730347,
      "learning_rate": 9.441482929094551e-06,
      "loss": 0.0933,
      "step": 633100
    },
    {
      "epoch": 4.1131572964370395,
      "grad_norm": 1.0475561618804932,
      "learning_rate": 9.43457247066856e-06,
      "loss": 0.1017,
      "step": 633200
    },
    {
      "epoch": 4.113806879080191,
      "grad_norm": 0.6540389657020569,
      "learning_rate": 9.42766201224257e-06,
      "loss": 0.0945,
      "step": 633300
    },
    {
      "epoch": 4.114456461723343,
      "grad_norm": 1.2562110424041748,
      "learning_rate": 9.420751553816577e-06,
      "loss": 0.0935,
      "step": 633400
    },
    {
      "epoch": 4.115106044366494,
      "grad_norm": 1.6553573608398438,
      "learning_rate": 9.413841095390587e-06,
      "loss": 0.0907,
      "step": 633500
    },
    {
      "epoch": 4.115755627009646,
      "grad_norm": 0.4989212155342102,
      "learning_rate": 9.406930636964595e-06,
      "loss": 0.1016,
      "step": 633600
    },
    {
      "epoch": 4.116405209652798,
      "grad_norm": 1.033573865890503,
      "learning_rate": 9.400020178538604e-06,
      "loss": 0.0932,
      "step": 633700
    },
    {
      "epoch": 4.11705479229595,
      "grad_norm": 0.46171510219573975,
      "learning_rate": 9.393109720112613e-06,
      "loss": 0.0968,
      "step": 633800
    },
    {
      "epoch": 4.117704374939102,
      "grad_norm": 0.8697002530097961,
      "learning_rate": 9.386199261686622e-06,
      "loss": 0.0974,
      "step": 633900
    },
    {
      "epoch": 4.118353957582253,
      "grad_norm": 0.8565545082092285,
      "learning_rate": 9.379288803260631e-06,
      "loss": 0.0986,
      "step": 634000
    },
    {
      "epoch": 4.119003540225405,
      "grad_norm": 1.0481083393096924,
      "learning_rate": 9.37237834483464e-06,
      "loss": 0.0999,
      "step": 634100
    },
    {
      "epoch": 4.119653122868557,
      "grad_norm": 0.9410629272460938,
      "learning_rate": 9.36546788640865e-06,
      "loss": 0.0946,
      "step": 634200
    },
    {
      "epoch": 4.120302705511708,
      "grad_norm": 0.7474306225776672,
      "learning_rate": 9.358557427982658e-06,
      "loss": 0.0978,
      "step": 634300
    },
    {
      "epoch": 4.120952288154861,
      "grad_norm": 1.7104243040084839,
      "learning_rate": 9.351646969556667e-06,
      "loss": 0.102,
      "step": 634400
    },
    {
      "epoch": 4.121601870798012,
      "grad_norm": 0.720248818397522,
      "learning_rate": 9.344736511130676e-06,
      "loss": 0.0925,
      "step": 634500
    },
    {
      "epoch": 4.122251453441164,
      "grad_norm": 0.6885858774185181,
      "learning_rate": 9.337826052704684e-06,
      "loss": 0.0934,
      "step": 634600
    },
    {
      "epoch": 4.122901036084316,
      "grad_norm": 0.8040720820426941,
      "learning_rate": 9.330915594278694e-06,
      "loss": 0.0984,
      "step": 634700
    },
    {
      "epoch": 4.123550618727467,
      "grad_norm": 0.9686065316200256,
      "learning_rate": 9.324005135852702e-06,
      "loss": 0.0987,
      "step": 634800
    },
    {
      "epoch": 4.12420020137062,
      "grad_norm": 0.8341888189315796,
      "learning_rate": 9.317094677426712e-06,
      "loss": 0.093,
      "step": 634900
    },
    {
      "epoch": 4.1248497840137714,
      "grad_norm": 1.329530954360962,
      "learning_rate": 9.31018421900072e-06,
      "loss": 0.1006,
      "step": 635000
    },
    {
      "epoch": 4.125499366656923,
      "grad_norm": 0.8336672186851501,
      "learning_rate": 9.30327376057473e-06,
      "loss": 0.0972,
      "step": 635100
    },
    {
      "epoch": 4.126148949300075,
      "grad_norm": 0.7902320027351379,
      "learning_rate": 9.29636330214874e-06,
      "loss": 0.0927,
      "step": 635200
    },
    {
      "epoch": 4.126798531943226,
      "grad_norm": 1.2201502323150635,
      "learning_rate": 9.289452843722748e-06,
      "loss": 0.099,
      "step": 635300
    },
    {
      "epoch": 4.127448114586378,
      "grad_norm": 1.0967915058135986,
      "learning_rate": 9.282542385296756e-06,
      "loss": 0.0876,
      "step": 635400
    },
    {
      "epoch": 4.12809769722953,
      "grad_norm": 1.2550945281982422,
      "learning_rate": 9.275631926870764e-06,
      "loss": 0.0968,
      "step": 635500
    },
    {
      "epoch": 4.128747279872682,
      "grad_norm": 0.7137147188186646,
      "learning_rate": 9.268721468444774e-06,
      "loss": 0.1016,
      "step": 635600
    },
    {
      "epoch": 4.129396862515834,
      "grad_norm": 1.3312431573867798,
      "learning_rate": 9.261811010018782e-06,
      "loss": 0.0958,
      "step": 635700
    },
    {
      "epoch": 4.130046445158985,
      "grad_norm": 1.4284158945083618,
      "learning_rate": 9.254900551592792e-06,
      "loss": 0.0997,
      "step": 635800
    },
    {
      "epoch": 4.130696027802137,
      "grad_norm": 0.8673209547996521,
      "learning_rate": 9.247990093166802e-06,
      "loss": 0.0956,
      "step": 635900
    },
    {
      "epoch": 4.131345610445289,
      "grad_norm": 1.4851067066192627,
      "learning_rate": 9.24107963474081e-06,
      "loss": 0.1006,
      "step": 636000
    },
    {
      "epoch": 4.131995193088441,
      "grad_norm": 1.3693629503250122,
      "learning_rate": 9.23416917631482e-06,
      "loss": 0.0995,
      "step": 636100
    },
    {
      "epoch": 4.132644775731593,
      "grad_norm": 0.7316911816596985,
      "learning_rate": 9.227258717888828e-06,
      "loss": 0.0907,
      "step": 636200
    },
    {
      "epoch": 4.133294358374744,
      "grad_norm": 0.9368009567260742,
      "learning_rate": 9.220348259462836e-06,
      "loss": 0.1005,
      "step": 636300
    },
    {
      "epoch": 4.133943941017896,
      "grad_norm": 1.5207998752593994,
      "learning_rate": 9.213437801036844e-06,
      "loss": 0.0951,
      "step": 636400
    },
    {
      "epoch": 4.134593523661048,
      "grad_norm": 1.2334644794464111,
      "learning_rate": 9.206527342610854e-06,
      "loss": 0.0998,
      "step": 636500
    },
    {
      "epoch": 4.135243106304199,
      "grad_norm": 1.4731557369232178,
      "learning_rate": 9.199616884184864e-06,
      "loss": 0.0998,
      "step": 636600
    },
    {
      "epoch": 4.135892688947352,
      "grad_norm": 0.9429686069488525,
      "learning_rate": 9.192706425758872e-06,
      "loss": 0.0955,
      "step": 636700
    },
    {
      "epoch": 4.136542271590503,
      "grad_norm": 1.4231420755386353,
      "learning_rate": 9.185795967332882e-06,
      "loss": 0.0946,
      "step": 636800
    },
    {
      "epoch": 4.137191854233655,
      "grad_norm": 0.9012631773948669,
      "learning_rate": 9.17888550890689e-06,
      "loss": 0.0977,
      "step": 636900
    },
    {
      "epoch": 4.137841436876807,
      "grad_norm": 0.8442931175231934,
      "learning_rate": 9.1719750504809e-06,
      "loss": 0.099,
      "step": 637000
    },
    {
      "epoch": 4.138491019519958,
      "grad_norm": 1.402708888053894,
      "learning_rate": 9.165064592054908e-06,
      "loss": 0.0981,
      "step": 637100
    },
    {
      "epoch": 4.13914060216311,
      "grad_norm": 1.1524420976638794,
      "learning_rate": 9.158154133628916e-06,
      "loss": 0.1008,
      "step": 637200
    },
    {
      "epoch": 4.139790184806262,
      "grad_norm": 0.9629333019256592,
      "learning_rate": 9.151243675202926e-06,
      "loss": 0.0952,
      "step": 637300
    },
    {
      "epoch": 4.140439767449414,
      "grad_norm": 0.6718171238899231,
      "learning_rate": 9.144333216776934e-06,
      "loss": 0.0989,
      "step": 637400
    },
    {
      "epoch": 4.141089350092566,
      "grad_norm": 1.492851972579956,
      "learning_rate": 9.137422758350944e-06,
      "loss": 0.0956,
      "step": 637500
    },
    {
      "epoch": 4.141738932735717,
      "grad_norm": 0.9734117388725281,
      "learning_rate": 9.130512299924952e-06,
      "loss": 0.1011,
      "step": 637600
    },
    {
      "epoch": 4.142388515378869,
      "grad_norm": 0.7726348638534546,
      "learning_rate": 9.123601841498962e-06,
      "loss": 0.097,
      "step": 637700
    },
    {
      "epoch": 4.1430380980220205,
      "grad_norm": 1.0212228298187256,
      "learning_rate": 9.116691383072972e-06,
      "loss": 0.1007,
      "step": 637800
    },
    {
      "epoch": 4.143687680665173,
      "grad_norm": 1.034317970275879,
      "learning_rate": 9.10978092464698e-06,
      "loss": 0.1013,
      "step": 637900
    },
    {
      "epoch": 4.144337263308325,
      "grad_norm": 1.0376229286193848,
      "learning_rate": 9.102870466220988e-06,
      "loss": 0.0966,
      "step": 638000
    },
    {
      "epoch": 4.144986845951476,
      "grad_norm": 1.153740406036377,
      "learning_rate": 9.095960007794996e-06,
      "loss": 0.0988,
      "step": 638100
    },
    {
      "epoch": 4.145636428594628,
      "grad_norm": 1.0520741939544678,
      "learning_rate": 9.089049549369006e-06,
      "loss": 0.096,
      "step": 638200
    },
    {
      "epoch": 4.1462860112377795,
      "grad_norm": 1.3874571323394775,
      "learning_rate": 9.082139090943014e-06,
      "loss": 0.1015,
      "step": 638300
    },
    {
      "epoch": 4.146935593880931,
      "grad_norm": 1.0279123783111572,
      "learning_rate": 9.075228632517024e-06,
      "loss": 0.0978,
      "step": 638400
    },
    {
      "epoch": 4.147585176524084,
      "grad_norm": 0.9108569622039795,
      "learning_rate": 9.068318174091034e-06,
      "loss": 0.0913,
      "step": 638500
    },
    {
      "epoch": 4.148234759167235,
      "grad_norm": 1.6108514070510864,
      "learning_rate": 9.061407715665042e-06,
      "loss": 0.0977,
      "step": 638600
    },
    {
      "epoch": 4.148884341810387,
      "grad_norm": 1.2225552797317505,
      "learning_rate": 9.054497257239052e-06,
      "loss": 0.101,
      "step": 638700
    },
    {
      "epoch": 4.1495339244535385,
      "grad_norm": 1.3018369674682617,
      "learning_rate": 9.04758679881306e-06,
      "loss": 0.0997,
      "step": 638800
    },
    {
      "epoch": 4.15018350709669,
      "grad_norm": 0.625648021697998,
      "learning_rate": 9.040676340387068e-06,
      "loss": 0.0958,
      "step": 638900
    },
    {
      "epoch": 4.150833089739842,
      "grad_norm": 1.2446197271347046,
      "learning_rate": 9.033765881961077e-06,
      "loss": 0.0997,
      "step": 639000
    },
    {
      "epoch": 4.151482672382994,
      "grad_norm": 1.1105313301086426,
      "learning_rate": 9.026855423535086e-06,
      "loss": 0.099,
      "step": 639100
    },
    {
      "epoch": 4.152132255026146,
      "grad_norm": 0.9373545050621033,
      "learning_rate": 9.019944965109096e-06,
      "loss": 0.095,
      "step": 639200
    },
    {
      "epoch": 4.1527818376692975,
      "grad_norm": 0.9038324952125549,
      "learning_rate": 9.013034506683104e-06,
      "loss": 0.0941,
      "step": 639300
    },
    {
      "epoch": 4.153431420312449,
      "grad_norm": 1.3031361103057861,
      "learning_rate": 9.006124048257114e-06,
      "loss": 0.0999,
      "step": 639400
    },
    {
      "epoch": 4.154081002955601,
      "grad_norm": 1.1002477407455444,
      "learning_rate": 8.999213589831123e-06,
      "loss": 0.0994,
      "step": 639500
    },
    {
      "epoch": 4.154730585598752,
      "grad_norm": 1.153343915939331,
      "learning_rate": 8.992303131405132e-06,
      "loss": 0.0955,
      "step": 639600
    },
    {
      "epoch": 4.155380168241905,
      "grad_norm": 1.094553828239441,
      "learning_rate": 8.98539267297914e-06,
      "loss": 0.1009,
      "step": 639700
    },
    {
      "epoch": 4.1560297508850566,
      "grad_norm": 1.1776769161224365,
      "learning_rate": 8.978482214553149e-06,
      "loss": 0.0964,
      "step": 639800
    },
    {
      "epoch": 4.156679333528208,
      "grad_norm": 1.0673048496246338,
      "learning_rate": 8.971571756127159e-06,
      "loss": 0.0965,
      "step": 639900
    },
    {
      "epoch": 4.15732891617136,
      "grad_norm": 0.6314135193824768,
      "learning_rate": 8.964661297701167e-06,
      "loss": 0.1048,
      "step": 640000
    },
    {
      "epoch": 4.157978498814511,
      "grad_norm": 0.9315792322158813,
      "learning_rate": 8.957750839275177e-06,
      "loss": 0.0941,
      "step": 640100
    },
    {
      "epoch": 4.158628081457663,
      "grad_norm": 1.4463201761245728,
      "learning_rate": 8.950840380849185e-06,
      "loss": 0.1042,
      "step": 640200
    },
    {
      "epoch": 4.159277664100816,
      "grad_norm": 0.7376610040664673,
      "learning_rate": 8.943929922423195e-06,
      "loss": 0.0951,
      "step": 640300
    },
    {
      "epoch": 4.159927246743967,
      "grad_norm": 1.0482791662216187,
      "learning_rate": 8.937019463997203e-06,
      "loss": 0.0986,
      "step": 640400
    },
    {
      "epoch": 4.160576829387119,
      "grad_norm": 0.9202476739883423,
      "learning_rate": 8.930109005571213e-06,
      "loss": 0.0996,
      "step": 640500
    },
    {
      "epoch": 4.16122641203027,
      "grad_norm": 1.4899088144302368,
      "learning_rate": 8.92319854714522e-06,
      "loss": 0.094,
      "step": 640600
    },
    {
      "epoch": 4.161875994673422,
      "grad_norm": 0.6074584126472473,
      "learning_rate": 8.916288088719229e-06,
      "loss": 0.1029,
      "step": 640700
    },
    {
      "epoch": 4.162525577316574,
      "grad_norm": 0.6651577949523926,
      "learning_rate": 8.909377630293239e-06,
      "loss": 0.0996,
      "step": 640800
    },
    {
      "epoch": 4.163175159959726,
      "grad_norm": 1.4727890491485596,
      "learning_rate": 8.902467171867247e-06,
      "loss": 0.0971,
      "step": 640900
    },
    {
      "epoch": 4.163824742602878,
      "grad_norm": 1.0716590881347656,
      "learning_rate": 8.895556713441257e-06,
      "loss": 0.1021,
      "step": 641000
    },
    {
      "epoch": 4.1644743252460295,
      "grad_norm": 0.7805519700050354,
      "learning_rate": 8.888646255015267e-06,
      "loss": 0.0971,
      "step": 641100
    },
    {
      "epoch": 4.165123907889181,
      "grad_norm": 1.3959956169128418,
      "learning_rate": 8.881735796589275e-06,
      "loss": 0.0965,
      "step": 641200
    },
    {
      "epoch": 4.165773490532333,
      "grad_norm": 0.7941426634788513,
      "learning_rate": 8.874825338163285e-06,
      "loss": 0.0978,
      "step": 641300
    },
    {
      "epoch": 4.166423073175484,
      "grad_norm": 1.2843986749649048,
      "learning_rate": 8.867914879737293e-06,
      "loss": 0.0954,
      "step": 641400
    },
    {
      "epoch": 4.167072655818637,
      "grad_norm": 0.811346709728241,
      "learning_rate": 8.861004421311301e-06,
      "loss": 0.0943,
      "step": 641500
    },
    {
      "epoch": 4.1677222384617885,
      "grad_norm": 0.930556058883667,
      "learning_rate": 8.854093962885309e-06,
      "loss": 0.0969,
      "step": 641600
    },
    {
      "epoch": 4.16837182110494,
      "grad_norm": 1.0209108591079712,
      "learning_rate": 8.847183504459319e-06,
      "loss": 0.091,
      "step": 641700
    },
    {
      "epoch": 4.169021403748092,
      "grad_norm": 1.18836510181427,
      "learning_rate": 8.840273046033329e-06,
      "loss": 0.0957,
      "step": 641800
    },
    {
      "epoch": 4.169670986391243,
      "grad_norm": 0.9245148301124573,
      "learning_rate": 8.833362587607337e-06,
      "loss": 0.1003,
      "step": 641900
    },
    {
      "epoch": 4.170320569034395,
      "grad_norm": 1.022032380104065,
      "learning_rate": 8.826452129181347e-06,
      "loss": 0.1014,
      "step": 642000
    },
    {
      "epoch": 4.1709701516775475,
      "grad_norm": 1.0417594909667969,
      "learning_rate": 8.819541670755355e-06,
      "loss": 0.0999,
      "step": 642100
    },
    {
      "epoch": 4.171619734320699,
      "grad_norm": 0.9884617924690247,
      "learning_rate": 8.812631212329365e-06,
      "loss": 0.0984,
      "step": 642200
    },
    {
      "epoch": 4.172269316963851,
      "grad_norm": 1.354247808456421,
      "learning_rate": 8.805720753903373e-06,
      "loss": 0.1008,
      "step": 642300
    },
    {
      "epoch": 4.172918899607002,
      "grad_norm": 0.945500910282135,
      "learning_rate": 8.798810295477381e-06,
      "loss": 0.1067,
      "step": 642400
    },
    {
      "epoch": 4.173568482250154,
      "grad_norm": 1.0077619552612305,
      "learning_rate": 8.791899837051391e-06,
      "loss": 0.0912,
      "step": 642500
    },
    {
      "epoch": 4.1742180648933065,
      "grad_norm": 1.1608341932296753,
      "learning_rate": 8.784989378625399e-06,
      "loss": 0.1023,
      "step": 642600
    },
    {
      "epoch": 4.174867647536458,
      "grad_norm": 0.6325966715812683,
      "learning_rate": 8.778078920199409e-06,
      "loss": 0.0944,
      "step": 642700
    },
    {
      "epoch": 4.17551723017961,
      "grad_norm": 1.1217176914215088,
      "learning_rate": 8.771168461773417e-06,
      "loss": 0.1002,
      "step": 642800
    },
    {
      "epoch": 4.176166812822761,
      "grad_norm": 1.1059186458587646,
      "learning_rate": 8.764258003347427e-06,
      "loss": 0.0983,
      "step": 642900
    },
    {
      "epoch": 4.176816395465913,
      "grad_norm": 1.1573925018310547,
      "learning_rate": 8.757347544921435e-06,
      "loss": 0.0938,
      "step": 643000
    },
    {
      "epoch": 4.177465978109065,
      "grad_norm": 0.5949638485908508,
      "learning_rate": 8.750437086495445e-06,
      "loss": 0.0941,
      "step": 643100
    },
    {
      "epoch": 4.178115560752217,
      "grad_norm": 1.5784274339675903,
      "learning_rate": 8.743526628069453e-06,
      "loss": 0.0995,
      "step": 643200
    },
    {
      "epoch": 4.178765143395369,
      "grad_norm": 1.1723425388336182,
      "learning_rate": 8.736616169643461e-06,
      "loss": 0.0907,
      "step": 643300
    },
    {
      "epoch": 4.17941472603852,
      "grad_norm": 1.2232444286346436,
      "learning_rate": 8.729705711217471e-06,
      "loss": 0.103,
      "step": 643400
    },
    {
      "epoch": 4.180064308681672,
      "grad_norm": 1.357095718383789,
      "learning_rate": 8.72279525279148e-06,
      "loss": 0.0983,
      "step": 643500
    },
    {
      "epoch": 4.180713891324824,
      "grad_norm": 1.4689093828201294,
      "learning_rate": 8.71588479436549e-06,
      "loss": 0.0945,
      "step": 643600
    },
    {
      "epoch": 4.181363473967975,
      "grad_norm": 0.8818314075469971,
      "learning_rate": 8.708974335939497e-06,
      "loss": 0.0938,
      "step": 643700
    },
    {
      "epoch": 4.182013056611128,
      "grad_norm": 1.4067344665527344,
      "learning_rate": 8.702063877513507e-06,
      "loss": 0.0971,
      "step": 643800
    },
    {
      "epoch": 4.182662639254279,
      "grad_norm": 1.2931572198867798,
      "learning_rate": 8.695153419087517e-06,
      "loss": 0.0939,
      "step": 643900
    },
    {
      "epoch": 4.183312221897431,
      "grad_norm": 0.8202605843544006,
      "learning_rate": 8.688242960661525e-06,
      "loss": 0.0981,
      "step": 644000
    },
    {
      "epoch": 4.183961804540583,
      "grad_norm": 0.6454177498817444,
      "learning_rate": 8.681332502235533e-06,
      "loss": 0.1019,
      "step": 644100
    },
    {
      "epoch": 4.184611387183734,
      "grad_norm": 1.0543686151504517,
      "learning_rate": 8.674422043809542e-06,
      "loss": 0.1023,
      "step": 644200
    },
    {
      "epoch": 4.185260969826886,
      "grad_norm": 1.3219561576843262,
      "learning_rate": 8.667511585383551e-06,
      "loss": 0.0954,
      "step": 644300
    },
    {
      "epoch": 4.185910552470038,
      "grad_norm": 1.1280162334442139,
      "learning_rate": 8.66060112695756e-06,
      "loss": 0.0959,
      "step": 644400
    },
    {
      "epoch": 4.18656013511319,
      "grad_norm": 1.1760941743850708,
      "learning_rate": 8.65369066853157e-06,
      "loss": 0.0913,
      "step": 644500
    },
    {
      "epoch": 4.187209717756342,
      "grad_norm": 1.2560980319976807,
      "learning_rate": 8.64678021010558e-06,
      "loss": 0.096,
      "step": 644600
    },
    {
      "epoch": 4.187859300399493,
      "grad_norm": 1.0745373964309692,
      "learning_rate": 8.639869751679587e-06,
      "loss": 0.094,
      "step": 644700
    },
    {
      "epoch": 4.188508883042645,
      "grad_norm": 1.158263921737671,
      "learning_rate": 8.632959293253597e-06,
      "loss": 0.0886,
      "step": 644800
    },
    {
      "epoch": 4.1891584656857965,
      "grad_norm": 0.728539764881134,
      "learning_rate": 8.626048834827605e-06,
      "loss": 0.096,
      "step": 644900
    },
    {
      "epoch": 4.189808048328949,
      "grad_norm": 1.4065511226654053,
      "learning_rate": 8.619138376401614e-06,
      "loss": 0.0961,
      "step": 645000
    },
    {
      "epoch": 4.190457630972101,
      "grad_norm": 0.8869098424911499,
      "learning_rate": 8.612227917975622e-06,
      "loss": 0.1018,
      "step": 645100
    },
    {
      "epoch": 4.191107213615252,
      "grad_norm": 0.8094890713691711,
      "learning_rate": 8.605317459549632e-06,
      "loss": 0.0986,
      "step": 645200
    },
    {
      "epoch": 4.191756796258404,
      "grad_norm": 1.1038439273834229,
      "learning_rate": 8.598407001123641e-06,
      "loss": 0.1018,
      "step": 645300
    },
    {
      "epoch": 4.1924063789015555,
      "grad_norm": 1.2694330215454102,
      "learning_rate": 8.59149654269765e-06,
      "loss": 0.0989,
      "step": 645400
    },
    {
      "epoch": 4.193055961544707,
      "grad_norm": 1.2983641624450684,
      "learning_rate": 8.58458608427166e-06,
      "loss": 0.0995,
      "step": 645500
    },
    {
      "epoch": 4.19370554418786,
      "grad_norm": 1.13446843624115,
      "learning_rate": 8.577675625845668e-06,
      "loss": 0.095,
      "step": 645600
    },
    {
      "epoch": 4.194355126831011,
      "grad_norm": 1.0087999105453491,
      "learning_rate": 8.570765167419677e-06,
      "loss": 0.0995,
      "step": 645700
    },
    {
      "epoch": 4.195004709474163,
      "grad_norm": 0.6729720234870911,
      "learning_rate": 8.563854708993686e-06,
      "loss": 0.0982,
      "step": 645800
    },
    {
      "epoch": 4.1956542921173146,
      "grad_norm": 1.1850775480270386,
      "learning_rate": 8.556944250567694e-06,
      "loss": 0.1018,
      "step": 645900
    },
    {
      "epoch": 4.196303874760466,
      "grad_norm": 1.716914176940918,
      "learning_rate": 8.550033792141704e-06,
      "loss": 0.0971,
      "step": 646000
    },
    {
      "epoch": 4.196953457403618,
      "grad_norm": 0.737084686756134,
      "learning_rate": 8.543123333715712e-06,
      "loss": 0.0972,
      "step": 646100
    },
    {
      "epoch": 4.19760304004677,
      "grad_norm": 0.8989206552505493,
      "learning_rate": 8.536212875289722e-06,
      "loss": 0.0907,
      "step": 646200
    },
    {
      "epoch": 4.198252622689922,
      "grad_norm": 0.9131158590316772,
      "learning_rate": 8.52930241686373e-06,
      "loss": 0.0989,
      "step": 646300
    },
    {
      "epoch": 4.198902205333074,
      "grad_norm": 1.524783968925476,
      "learning_rate": 8.52239195843774e-06,
      "loss": 0.1039,
      "step": 646400
    },
    {
      "epoch": 4.199551787976225,
      "grad_norm": 1.6699016094207764,
      "learning_rate": 8.51548150001175e-06,
      "loss": 0.0951,
      "step": 646500
    },
    {
      "epoch": 4.200201370619377,
      "grad_norm": 0.8581557869911194,
      "learning_rate": 8.508571041585758e-06,
      "loss": 0.1,
      "step": 646600
    },
    {
      "epoch": 4.200850953262528,
      "grad_norm": 1.2087388038635254,
      "learning_rate": 8.501660583159766e-06,
      "loss": 0.0975,
      "step": 646700
    },
    {
      "epoch": 4.201500535905681,
      "grad_norm": 1.2085939645767212,
      "learning_rate": 8.494750124733774e-06,
      "loss": 0.0943,
      "step": 646800
    },
    {
      "epoch": 4.202150118548833,
      "grad_norm": 0.9430123567581177,
      "learning_rate": 8.487839666307784e-06,
      "loss": 0.1,
      "step": 646900
    },
    {
      "epoch": 4.202799701191984,
      "grad_norm": 1.6011046171188354,
      "learning_rate": 8.480929207881792e-06,
      "loss": 0.0977,
      "step": 647000
    },
    {
      "epoch": 4.203449283835136,
      "grad_norm": 0.776135265827179,
      "learning_rate": 8.474018749455802e-06,
      "loss": 0.0938,
      "step": 647100
    },
    {
      "epoch": 4.2040988664782875,
      "grad_norm": 1.2180215120315552,
      "learning_rate": 8.467108291029812e-06,
      "loss": 0.0929,
      "step": 647200
    },
    {
      "epoch": 4.204748449121439,
      "grad_norm": 1.532680630683899,
      "learning_rate": 8.46019783260382e-06,
      "loss": 0.0967,
      "step": 647300
    },
    {
      "epoch": 4.205398031764592,
      "grad_norm": 1.2470221519470215,
      "learning_rate": 8.45328737417783e-06,
      "loss": 0.0966,
      "step": 647400
    },
    {
      "epoch": 4.206047614407743,
      "grad_norm": 0.9434255361557007,
      "learning_rate": 8.446376915751838e-06,
      "loss": 0.0951,
      "step": 647500
    },
    {
      "epoch": 4.206697197050895,
      "grad_norm": 1.0499264001846313,
      "learning_rate": 8.439466457325846e-06,
      "loss": 0.0939,
      "step": 647600
    },
    {
      "epoch": 4.2073467796940465,
      "grad_norm": 0.9596453309059143,
      "learning_rate": 8.432555998899854e-06,
      "loss": 0.0995,
      "step": 647700
    },
    {
      "epoch": 4.207996362337198,
      "grad_norm": 0.6324622631072998,
      "learning_rate": 8.425645540473864e-06,
      "loss": 0.1044,
      "step": 647800
    },
    {
      "epoch": 4.20864594498035,
      "grad_norm": 0.7823340892791748,
      "learning_rate": 8.418735082047874e-06,
      "loss": 0.1008,
      "step": 647900
    },
    {
      "epoch": 4.209295527623502,
      "grad_norm": 1.311306357383728,
      "learning_rate": 8.411824623621882e-06,
      "loss": 0.1002,
      "step": 648000
    },
    {
      "epoch": 4.209945110266654,
      "grad_norm": 0.9425008296966553,
      "learning_rate": 8.404914165195892e-06,
      "loss": 0.0998,
      "step": 648100
    },
    {
      "epoch": 4.2105946929098055,
      "grad_norm": 1.2889118194580078,
      "learning_rate": 8.3980037067699e-06,
      "loss": 0.0934,
      "step": 648200
    },
    {
      "epoch": 4.211244275552957,
      "grad_norm": 1.1796635389328003,
      "learning_rate": 8.39109324834391e-06,
      "loss": 0.0964,
      "step": 648300
    },
    {
      "epoch": 4.211893858196109,
      "grad_norm": 0.7084142565727234,
      "learning_rate": 8.384182789917918e-06,
      "loss": 0.0966,
      "step": 648400
    },
    {
      "epoch": 4.21254344083926,
      "grad_norm": 0.573619544506073,
      "learning_rate": 8.377272331491926e-06,
      "loss": 0.0948,
      "step": 648500
    },
    {
      "epoch": 4.213193023482413,
      "grad_norm": 1.5218591690063477,
      "learning_rate": 8.370361873065936e-06,
      "loss": 0.101,
      "step": 648600
    },
    {
      "epoch": 4.2138426061255645,
      "grad_norm": 0.9123911261558533,
      "learning_rate": 8.363451414639944e-06,
      "loss": 0.0977,
      "step": 648700
    },
    {
      "epoch": 4.214492188768716,
      "grad_norm": 1.2800617218017578,
      "learning_rate": 8.356540956213954e-06,
      "loss": 0.0984,
      "step": 648800
    },
    {
      "epoch": 4.215141771411868,
      "grad_norm": 1.1437644958496094,
      "learning_rate": 8.349630497787962e-06,
      "loss": 0.0983,
      "step": 648900
    },
    {
      "epoch": 4.215791354055019,
      "grad_norm": 1.8258123397827148,
      "learning_rate": 8.342720039361972e-06,
      "loss": 0.0961,
      "step": 649000
    },
    {
      "epoch": 4.216440936698172,
      "grad_norm": 1.189899206161499,
      "learning_rate": 8.33580958093598e-06,
      "loss": 0.1063,
      "step": 649100
    },
    {
      "epoch": 4.2170905193413235,
      "grad_norm": 1.164640188217163,
      "learning_rate": 8.32889912250999e-06,
      "loss": 0.0916,
      "step": 649200
    },
    {
      "epoch": 4.217740101984475,
      "grad_norm": 1.5184168815612793,
      "learning_rate": 8.321988664083998e-06,
      "loss": 0.1008,
      "step": 649300
    },
    {
      "epoch": 4.218389684627627,
      "grad_norm": 1.2189708948135376,
      "learning_rate": 8.315078205658006e-06,
      "loss": 0.0999,
      "step": 649400
    },
    {
      "epoch": 4.219039267270778,
      "grad_norm": 1.6154849529266357,
      "learning_rate": 8.308167747232016e-06,
      "loss": 0.1002,
      "step": 649500
    },
    {
      "epoch": 4.21968884991393,
      "grad_norm": 1.020944595336914,
      "learning_rate": 8.301257288806024e-06,
      "loss": 0.092,
      "step": 649600
    },
    {
      "epoch": 4.220338432557082,
      "grad_norm": 1.3152934312820435,
      "learning_rate": 8.294346830380034e-06,
      "loss": 0.0936,
      "step": 649700
    },
    {
      "epoch": 4.220988015200234,
      "grad_norm": 0.919209897518158,
      "learning_rate": 8.287436371954044e-06,
      "loss": 0.0927,
      "step": 649800
    },
    {
      "epoch": 4.221637597843386,
      "grad_norm": 1.2601011991500854,
      "learning_rate": 8.280525913528052e-06,
      "loss": 0.0929,
      "step": 649900
    },
    {
      "epoch": 4.222287180486537,
      "grad_norm": 0.7948195338249207,
      "learning_rate": 8.273615455102062e-06,
      "loss": 0.0978,
      "step": 650000
    },
    {
      "epoch": 4.222936763129689,
      "grad_norm": 0.6330564618110657,
      "learning_rate": 8.26670499667607e-06,
      "loss": 0.0943,
      "step": 650100
    },
    {
      "epoch": 4.223586345772841,
      "grad_norm": 1.3779468536376953,
      "learning_rate": 8.259794538250078e-06,
      "loss": 0.0967,
      "step": 650200
    },
    {
      "epoch": 4.224235928415993,
      "grad_norm": 1.7259879112243652,
      "learning_rate": 8.252884079824087e-06,
      "loss": 0.0956,
      "step": 650300
    },
    {
      "epoch": 4.224885511059145,
      "grad_norm": 0.7962049245834351,
      "learning_rate": 8.245973621398096e-06,
      "loss": 0.0919,
      "step": 650400
    },
    {
      "epoch": 4.225535093702296,
      "grad_norm": 0.7281538844108582,
      "learning_rate": 8.239063162972106e-06,
      "loss": 0.0994,
      "step": 650500
    },
    {
      "epoch": 4.226184676345448,
      "grad_norm": 1.8489106893539429,
      "learning_rate": 8.232152704546114e-06,
      "loss": 0.0991,
      "step": 650600
    },
    {
      "epoch": 4.2268342589886,
      "grad_norm": 1.082655668258667,
      "learning_rate": 8.225242246120124e-06,
      "loss": 0.1001,
      "step": 650700
    },
    {
      "epoch": 4.227483841631751,
      "grad_norm": 0.9526157379150391,
      "learning_rate": 8.218331787694132e-06,
      "loss": 0.0999,
      "step": 650800
    },
    {
      "epoch": 4.228133424274904,
      "grad_norm": 1.08798086643219,
      "learning_rate": 8.211421329268142e-06,
      "loss": 0.0997,
      "step": 650900
    },
    {
      "epoch": 4.228783006918055,
      "grad_norm": 0.7284429669380188,
      "learning_rate": 8.20451087084215e-06,
      "loss": 0.095,
      "step": 651000
    },
    {
      "epoch": 4.229432589561207,
      "grad_norm": 1.4732334613800049,
      "learning_rate": 8.197600412416159e-06,
      "loss": 0.1014,
      "step": 651100
    },
    {
      "epoch": 4.230082172204359,
      "grad_norm": 1.5217186212539673,
      "learning_rate": 8.190689953990168e-06,
      "loss": 0.0989,
      "step": 651200
    },
    {
      "epoch": 4.23073175484751,
      "grad_norm": 0.8649536967277527,
      "learning_rate": 8.183779495564177e-06,
      "loss": 0.0947,
      "step": 651300
    },
    {
      "epoch": 4.231381337490662,
      "grad_norm": 0.9680532813072205,
      "learning_rate": 8.176869037138186e-06,
      "loss": 0.0967,
      "step": 651400
    },
    {
      "epoch": 4.232030920133814,
      "grad_norm": 1.054482340812683,
      "learning_rate": 8.169958578712195e-06,
      "loss": 0.0962,
      "step": 651500
    },
    {
      "epoch": 4.232680502776966,
      "grad_norm": 0.6295831799507141,
      "learning_rate": 8.163048120286204e-06,
      "loss": 0.0928,
      "step": 651600
    },
    {
      "epoch": 4.233330085420118,
      "grad_norm": 0.9940484762191772,
      "learning_rate": 8.156137661860213e-06,
      "loss": 0.093,
      "step": 651700
    },
    {
      "epoch": 4.233979668063269,
      "grad_norm": 1.0870589017868042,
      "learning_rate": 8.149227203434222e-06,
      "loss": 0.0955,
      "step": 651800
    },
    {
      "epoch": 4.234629250706421,
      "grad_norm": 0.9692158699035645,
      "learning_rate": 8.14231674500823e-06,
      "loss": 0.1002,
      "step": 651900
    },
    {
      "epoch": 4.235278833349573,
      "grad_norm": 0.945884644985199,
      "learning_rate": 8.135406286582239e-06,
      "loss": 0.1009,
      "step": 652000
    },
    {
      "epoch": 4.235928415992725,
      "grad_norm": 1.0325504541397095,
      "learning_rate": 8.128495828156249e-06,
      "loss": 0.0975,
      "step": 652100
    },
    {
      "epoch": 4.236577998635877,
      "grad_norm": 1.040637493133545,
      "learning_rate": 8.121585369730257e-06,
      "loss": 0.0977,
      "step": 652200
    },
    {
      "epoch": 4.237227581279028,
      "grad_norm": 1.4844248294830322,
      "learning_rate": 8.114674911304267e-06,
      "loss": 0.094,
      "step": 652300
    },
    {
      "epoch": 4.23787716392218,
      "grad_norm": 1.2140864133834839,
      "learning_rate": 8.107764452878275e-06,
      "loss": 0.0983,
      "step": 652400
    },
    {
      "epoch": 4.238526746565332,
      "grad_norm": 0.8560401201248169,
      "learning_rate": 8.100853994452285e-06,
      "loss": 0.0942,
      "step": 652500
    },
    {
      "epoch": 4.239176329208483,
      "grad_norm": 1.0010305643081665,
      "learning_rate": 8.093943536026295e-06,
      "loss": 0.0991,
      "step": 652600
    },
    {
      "epoch": 4.239825911851636,
      "grad_norm": 1.181441068649292,
      "learning_rate": 8.087033077600303e-06,
      "loss": 0.0912,
      "step": 652700
    },
    {
      "epoch": 4.240475494494787,
      "grad_norm": 0.7738689184188843,
      "learning_rate": 8.08012261917431e-06,
      "loss": 0.1041,
      "step": 652800
    },
    {
      "epoch": 4.241125077137939,
      "grad_norm": 1.1189457178115845,
      "learning_rate": 8.073212160748319e-06,
      "loss": 0.0968,
      "step": 652900
    },
    {
      "epoch": 4.241774659781091,
      "grad_norm": 1.0977832078933716,
      "learning_rate": 8.066301702322329e-06,
      "loss": 0.0981,
      "step": 653000
    },
    {
      "epoch": 4.242424242424242,
      "grad_norm": 1.225464940071106,
      "learning_rate": 8.059391243896337e-06,
      "loss": 0.1,
      "step": 653100
    },
    {
      "epoch": 4.243073825067394,
      "grad_norm": 0.9948422312736511,
      "learning_rate": 8.052480785470347e-06,
      "loss": 0.097,
      "step": 653200
    },
    {
      "epoch": 4.243723407710546,
      "grad_norm": 0.7404239177703857,
      "learning_rate": 8.045570327044357e-06,
      "loss": 0.096,
      "step": 653300
    },
    {
      "epoch": 4.244372990353698,
      "grad_norm": 0.4114776849746704,
      "learning_rate": 8.038659868618365e-06,
      "loss": 0.0959,
      "step": 653400
    },
    {
      "epoch": 4.24502257299685,
      "grad_norm": 0.9685119390487671,
      "learning_rate": 8.031749410192375e-06,
      "loss": 0.0972,
      "step": 653500
    },
    {
      "epoch": 4.245672155640001,
      "grad_norm": 1.3054248094558716,
      "learning_rate": 8.024838951766383e-06,
      "loss": 0.0979,
      "step": 653600
    },
    {
      "epoch": 4.246321738283153,
      "grad_norm": 1.216511607170105,
      "learning_rate": 8.017928493340391e-06,
      "loss": 0.099,
      "step": 653700
    },
    {
      "epoch": 4.2469713209263045,
      "grad_norm": 1.1037864685058594,
      "learning_rate": 8.011018034914401e-06,
      "loss": 0.093,
      "step": 653800
    },
    {
      "epoch": 4.247620903569457,
      "grad_norm": 0.7517801523208618,
      "learning_rate": 8.004107576488409e-06,
      "loss": 0.0952,
      "step": 653900
    },
    {
      "epoch": 4.248270486212609,
      "grad_norm": 0.7289639115333557,
      "learning_rate": 7.997197118062419e-06,
      "loss": 0.0936,
      "step": 654000
    },
    {
      "epoch": 4.24892006885576,
      "grad_norm": 1.1314303874969482,
      "learning_rate": 7.990286659636427e-06,
      "loss": 0.0911,
      "step": 654100
    },
    {
      "epoch": 4.249569651498912,
      "grad_norm": 1.2355624437332153,
      "learning_rate": 7.983376201210437e-06,
      "loss": 0.1013,
      "step": 654200
    },
    {
      "epoch": 4.2502192341420635,
      "grad_norm": 1.590366005897522,
      "learning_rate": 7.976465742784445e-06,
      "loss": 0.1,
      "step": 654300
    },
    {
      "epoch": 4.250868816785215,
      "grad_norm": 0.9057264924049377,
      "learning_rate": 7.969555284358455e-06,
      "loss": 0.0977,
      "step": 654400
    },
    {
      "epoch": 4.251518399428368,
      "grad_norm": 1.3698575496673584,
      "learning_rate": 7.962644825932463e-06,
      "loss": 0.0942,
      "step": 654500
    },
    {
      "epoch": 4.252167982071519,
      "grad_norm": 1.3827357292175293,
      "learning_rate": 7.955734367506471e-06,
      "loss": 0.0988,
      "step": 654600
    },
    {
      "epoch": 4.252817564714671,
      "grad_norm": 1.4622918367385864,
      "learning_rate": 7.948823909080481e-06,
      "loss": 0.0945,
      "step": 654700
    },
    {
      "epoch": 4.2534671473578225,
      "grad_norm": 0.4693499207496643,
      "learning_rate": 7.94191345065449e-06,
      "loss": 0.0988,
      "step": 654800
    },
    {
      "epoch": 4.254116730000974,
      "grad_norm": 1.0716197490692139,
      "learning_rate": 7.935002992228499e-06,
      "loss": 0.1023,
      "step": 654900
    },
    {
      "epoch": 4.254766312644126,
      "grad_norm": 1.2462583780288696,
      "learning_rate": 7.928092533802507e-06,
      "loss": 0.0932,
      "step": 655000
    },
    {
      "epoch": 4.255415895287278,
      "grad_norm": 0.9877831935882568,
      "learning_rate": 7.921182075376517e-06,
      "loss": 0.0983,
      "step": 655100
    },
    {
      "epoch": 4.25606547793043,
      "grad_norm": 1.067135214805603,
      "learning_rate": 7.914271616950527e-06,
      "loss": 0.0955,
      "step": 655200
    },
    {
      "epoch": 4.2567150605735815,
      "grad_norm": 1.2859394550323486,
      "learning_rate": 7.907361158524535e-06,
      "loss": 0.0931,
      "step": 655300
    },
    {
      "epoch": 4.257364643216733,
      "grad_norm": 0.8921752572059631,
      "learning_rate": 7.900450700098543e-06,
      "loss": 0.0967,
      "step": 655400
    },
    {
      "epoch": 4.258014225859885,
      "grad_norm": 0.9873196482658386,
      "learning_rate": 7.893540241672551e-06,
      "loss": 0.1013,
      "step": 655500
    },
    {
      "epoch": 4.258663808503036,
      "grad_norm": 1.5499929189682007,
      "learning_rate": 7.886629783246561e-06,
      "loss": 0.1003,
      "step": 655600
    },
    {
      "epoch": 4.259313391146189,
      "grad_norm": 1.3415312767028809,
      "learning_rate": 7.87971932482057e-06,
      "loss": 0.0967,
      "step": 655700
    },
    {
      "epoch": 4.2599629737893405,
      "grad_norm": 1.101709008216858,
      "learning_rate": 7.87280886639458e-06,
      "loss": 0.1,
      "step": 655800
    },
    {
      "epoch": 4.260612556432492,
      "grad_norm": 1.4650148153305054,
      "learning_rate": 7.865898407968589e-06,
      "loss": 0.0979,
      "step": 655900
    },
    {
      "epoch": 4.261262139075644,
      "grad_norm": 0.9380031824111938,
      "learning_rate": 7.858987949542597e-06,
      "loss": 0.093,
      "step": 656000
    },
    {
      "epoch": 4.261911721718795,
      "grad_norm": 0.793497622013092,
      "learning_rate": 7.852077491116607e-06,
      "loss": 0.1039,
      "step": 656100
    },
    {
      "epoch": 4.262561304361947,
      "grad_norm": 1.5142840147018433,
      "learning_rate": 7.845167032690615e-06,
      "loss": 0.1023,
      "step": 656200
    },
    {
      "epoch": 4.2632108870050995,
      "grad_norm": 0.9889542460441589,
      "learning_rate": 7.838256574264623e-06,
      "loss": 0.0958,
      "step": 656300
    },
    {
      "epoch": 4.263860469648251,
      "grad_norm": 0.9975409507751465,
      "learning_rate": 7.831346115838632e-06,
      "loss": 0.0965,
      "step": 656400
    },
    {
      "epoch": 4.264510052291403,
      "grad_norm": 1.4398351907730103,
      "learning_rate": 7.824435657412641e-06,
      "loss": 0.0956,
      "step": 656500
    },
    {
      "epoch": 4.265159634934554,
      "grad_norm": 1.2549002170562744,
      "learning_rate": 7.817525198986651e-06,
      "loss": 0.1003,
      "step": 656600
    },
    {
      "epoch": 4.265809217577706,
      "grad_norm": 1.5740008354187012,
      "learning_rate": 7.81061474056066e-06,
      "loss": 0.1006,
      "step": 656700
    },
    {
      "epoch": 4.2664588002208585,
      "grad_norm": 0.8935866951942444,
      "learning_rate": 7.80370428213467e-06,
      "loss": 0.0954,
      "step": 656800
    },
    {
      "epoch": 4.26710838286401,
      "grad_norm": 2.1295454502105713,
      "learning_rate": 7.796793823708678e-06,
      "loss": 0.0957,
      "step": 656900
    },
    {
      "epoch": 4.267757965507162,
      "grad_norm": 0.824175238609314,
      "learning_rate": 7.789883365282687e-06,
      "loss": 0.1004,
      "step": 657000
    },
    {
      "epoch": 4.268407548150313,
      "grad_norm": 1.0098179578781128,
      "learning_rate": 7.782972906856696e-06,
      "loss": 0.0946,
      "step": 657100
    },
    {
      "epoch": 4.269057130793465,
      "grad_norm": 0.8753063678741455,
      "learning_rate": 7.776062448430704e-06,
      "loss": 0.0958,
      "step": 657200
    },
    {
      "epoch": 4.269706713436617,
      "grad_norm": 0.7672966122627258,
      "learning_rate": 7.769151990004714e-06,
      "loss": 0.098,
      "step": 657300
    },
    {
      "epoch": 4.270356296079768,
      "grad_norm": 1.5966304540634155,
      "learning_rate": 7.762241531578722e-06,
      "loss": 0.0982,
      "step": 657400
    },
    {
      "epoch": 4.271005878722921,
      "grad_norm": 1.3359674215316772,
      "learning_rate": 7.755331073152732e-06,
      "loss": 0.1023,
      "step": 657500
    },
    {
      "epoch": 4.271655461366072,
      "grad_norm": 1.53920578956604,
      "learning_rate": 7.74842061472674e-06,
      "loss": 0.1011,
      "step": 657600
    },
    {
      "epoch": 4.272305044009224,
      "grad_norm": 0.9551641941070557,
      "learning_rate": 7.74151015630075e-06,
      "loss": 0.0938,
      "step": 657700
    },
    {
      "epoch": 4.272954626652376,
      "grad_norm": 0.5674955248832703,
      "learning_rate": 7.734599697874758e-06,
      "loss": 0.0933,
      "step": 657800
    },
    {
      "epoch": 4.273604209295527,
      "grad_norm": 1.4868342876434326,
      "learning_rate": 7.727689239448768e-06,
      "loss": 0.0939,
      "step": 657900
    },
    {
      "epoch": 4.27425379193868,
      "grad_norm": 1.1562507152557373,
      "learning_rate": 7.720778781022776e-06,
      "loss": 0.0989,
      "step": 658000
    },
    {
      "epoch": 4.2749033745818315,
      "grad_norm": 1.119246006011963,
      "learning_rate": 7.713868322596784e-06,
      "loss": 0.102,
      "step": 658100
    },
    {
      "epoch": 4.275552957224983,
      "grad_norm": 1.5803848505020142,
      "learning_rate": 7.706957864170794e-06,
      "loss": 0.0996,
      "step": 658200
    },
    {
      "epoch": 4.276202539868135,
      "grad_norm": 0.9661988615989685,
      "learning_rate": 7.700047405744802e-06,
      "loss": 0.0984,
      "step": 658300
    },
    {
      "epoch": 4.276852122511286,
      "grad_norm": 1.101425290107727,
      "learning_rate": 7.693136947318812e-06,
      "loss": 0.0984,
      "step": 658400
    },
    {
      "epoch": 4.277501705154438,
      "grad_norm": 1.3938204050064087,
      "learning_rate": 7.686226488892822e-06,
      "loss": 0.0968,
      "step": 658500
    },
    {
      "epoch": 4.2781512877975905,
      "grad_norm": 1.2870925664901733,
      "learning_rate": 7.67931603046683e-06,
      "loss": 0.097,
      "step": 658600
    },
    {
      "epoch": 4.278800870440742,
      "grad_norm": 0.8316357731819153,
      "learning_rate": 7.67240557204084e-06,
      "loss": 0.0962,
      "step": 658700
    },
    {
      "epoch": 4.279450453083894,
      "grad_norm": 0.9782447814941406,
      "learning_rate": 7.665495113614848e-06,
      "loss": 0.1015,
      "step": 658800
    },
    {
      "epoch": 4.280100035727045,
      "grad_norm": 1.0030865669250488,
      "learning_rate": 7.658584655188856e-06,
      "loss": 0.0933,
      "step": 658900
    },
    {
      "epoch": 4.280749618370197,
      "grad_norm": 1.2780741453170776,
      "learning_rate": 7.651674196762864e-06,
      "loss": 0.0957,
      "step": 659000
    },
    {
      "epoch": 4.281399201013349,
      "grad_norm": 1.4693268537521362,
      "learning_rate": 7.644763738336874e-06,
      "loss": 0.0987,
      "step": 659100
    },
    {
      "epoch": 4.282048783656501,
      "grad_norm": 0.7304136157035828,
      "learning_rate": 7.637853279910884e-06,
      "loss": 0.0946,
      "step": 659200
    },
    {
      "epoch": 4.282698366299653,
      "grad_norm": 1.063406229019165,
      "learning_rate": 7.630942821484892e-06,
      "loss": 0.0929,
      "step": 659300
    },
    {
      "epoch": 4.283347948942804,
      "grad_norm": 1.3442730903625488,
      "learning_rate": 7.624032363058902e-06,
      "loss": 0.0901,
      "step": 659400
    },
    {
      "epoch": 4.283997531585956,
      "grad_norm": 1.092037320137024,
      "learning_rate": 7.61712190463291e-06,
      "loss": 0.0984,
      "step": 659500
    },
    {
      "epoch": 4.284647114229108,
      "grad_norm": 1.0569427013397217,
      "learning_rate": 7.610211446206919e-06,
      "loss": 0.0993,
      "step": 659600
    },
    {
      "epoch": 4.285296696872259,
      "grad_norm": 1.3937478065490723,
      "learning_rate": 7.603300987780927e-06,
      "loss": 0.0951,
      "step": 659700
    },
    {
      "epoch": 4.285946279515412,
      "grad_norm": 1.1646723747253418,
      "learning_rate": 7.596390529354937e-06,
      "loss": 0.0965,
      "step": 659800
    },
    {
      "epoch": 4.286595862158563,
      "grad_norm": 1.1650341749191284,
      "learning_rate": 7.589480070928946e-06,
      "loss": 0.0986,
      "step": 659900
    },
    {
      "epoch": 4.287245444801715,
      "grad_norm": 0.8241813778877258,
      "learning_rate": 7.582569612502954e-06,
      "loss": 0.0984,
      "step": 660000
    },
    {
      "epoch": 4.287895027444867,
      "grad_norm": 0.7157384157180786,
      "learning_rate": 7.575659154076964e-06,
      "loss": 0.0993,
      "step": 660100
    },
    {
      "epoch": 4.288544610088018,
      "grad_norm": 0.8760324716567993,
      "learning_rate": 7.568748695650972e-06,
      "loss": 0.0986,
      "step": 660200
    },
    {
      "epoch": 4.28919419273117,
      "grad_norm": 0.7971282005310059,
      "learning_rate": 7.561838237224982e-06,
      "loss": 0.0972,
      "step": 660300
    },
    {
      "epoch": 4.289843775374322,
      "grad_norm": 1.9283740520477295,
      "learning_rate": 7.55492777879899e-06,
      "loss": 0.0998,
      "step": 660400
    },
    {
      "epoch": 4.290493358017474,
      "grad_norm": 0.9962508082389832,
      "learning_rate": 7.548017320372999e-06,
      "loss": 0.0943,
      "step": 660500
    },
    {
      "epoch": 4.291142940660626,
      "grad_norm": 1.014650821685791,
      "learning_rate": 7.541106861947009e-06,
      "loss": 0.0964,
      "step": 660600
    },
    {
      "epoch": 4.291792523303777,
      "grad_norm": 1.1180574893951416,
      "learning_rate": 7.534196403521017e-06,
      "loss": 0.0915,
      "step": 660700
    },
    {
      "epoch": 4.292442105946929,
      "grad_norm": 1.1905131340026855,
      "learning_rate": 7.527285945095026e-06,
      "loss": 0.0968,
      "step": 660800
    },
    {
      "epoch": 4.2930916885900805,
      "grad_norm": 1.3883439302444458,
      "learning_rate": 7.520375486669034e-06,
      "loss": 0.097,
      "step": 660900
    },
    {
      "epoch": 4.293741271233233,
      "grad_norm": 0.9252652525901794,
      "learning_rate": 7.513465028243044e-06,
      "loss": 0.0977,
      "step": 661000
    },
    {
      "epoch": 4.294390853876385,
      "grad_norm": 1.212682843208313,
      "learning_rate": 7.506554569817052e-06,
      "loss": 0.1029,
      "step": 661100
    },
    {
      "epoch": 4.295040436519536,
      "grad_norm": 0.5722832679748535,
      "learning_rate": 7.499644111391062e-06,
      "loss": 0.0998,
      "step": 661200
    },
    {
      "epoch": 4.295690019162688,
      "grad_norm": 0.7190833687782288,
      "learning_rate": 7.492733652965071e-06,
      "loss": 0.0951,
      "step": 661300
    },
    {
      "epoch": 4.2963396018058395,
      "grad_norm": 1.2005977630615234,
      "learning_rate": 7.485823194539079e-06,
      "loss": 0.0977,
      "step": 661400
    },
    {
      "epoch": 4.296989184448991,
      "grad_norm": 0.9766339659690857,
      "learning_rate": 7.478912736113089e-06,
      "loss": 0.0988,
      "step": 661500
    },
    {
      "epoch": 4.297638767092144,
      "grad_norm": 1.1029232740402222,
      "learning_rate": 7.472002277687097e-06,
      "loss": 0.0968,
      "step": 661600
    },
    {
      "epoch": 4.298288349735295,
      "grad_norm": 1.2693133354187012,
      "learning_rate": 7.465091819261106e-06,
      "loss": 0.0984,
      "step": 661700
    },
    {
      "epoch": 4.298937932378447,
      "grad_norm": 1.0632506608963013,
      "learning_rate": 7.4581813608351145e-06,
      "loss": 0.0962,
      "step": 661800
    },
    {
      "epoch": 4.2995875150215985,
      "grad_norm": 0.9204384684562683,
      "learning_rate": 7.451270902409124e-06,
      "loss": 0.0993,
      "step": 661900
    },
    {
      "epoch": 4.30023709766475,
      "grad_norm": 1.0309953689575195,
      "learning_rate": 7.444360443983134e-06,
      "loss": 0.0943,
      "step": 662000
    },
    {
      "epoch": 4.300886680307902,
      "grad_norm": 1.118209958076477,
      "learning_rate": 7.437449985557142e-06,
      "loss": 0.0994,
      "step": 662100
    },
    {
      "epoch": 4.301536262951054,
      "grad_norm": 0.8919601440429688,
      "learning_rate": 7.430539527131151e-06,
      "loss": 0.0926,
      "step": 662200
    },
    {
      "epoch": 4.302185845594206,
      "grad_norm": 1.132313847541809,
      "learning_rate": 7.4236290687051595e-06,
      "loss": 0.0913,
      "step": 662300
    },
    {
      "epoch": 4.3028354282373575,
      "grad_norm": 1.1561459302902222,
      "learning_rate": 7.416718610279169e-06,
      "loss": 0.0933,
      "step": 662400
    },
    {
      "epoch": 4.303485010880509,
      "grad_norm": 1.2385499477386475,
      "learning_rate": 7.409808151853178e-06,
      "loss": 0.1,
      "step": 662500
    },
    {
      "epoch": 4.304134593523661,
      "grad_norm": 0.86726313829422,
      "learning_rate": 7.4028976934271865e-06,
      "loss": 0.0958,
      "step": 662600
    },
    {
      "epoch": 4.304784176166812,
      "grad_norm": 1.4280810356140137,
      "learning_rate": 7.395987235001196e-06,
      "loss": 0.0875,
      "step": 662700
    },
    {
      "epoch": 4.305433758809965,
      "grad_norm": 1.0993198156356812,
      "learning_rate": 7.3890767765752046e-06,
      "loss": 0.0946,
      "step": 662800
    },
    {
      "epoch": 4.3060833414531166,
      "grad_norm": 1.0411007404327393,
      "learning_rate": 7.382166318149214e-06,
      "loss": 0.0959,
      "step": 662900
    },
    {
      "epoch": 4.306732924096268,
      "grad_norm": 1.1155431270599365,
      "learning_rate": 7.375255859723222e-06,
      "loss": 0.0991,
      "step": 663000
    },
    {
      "epoch": 4.30738250673942,
      "grad_norm": 1.0656864643096924,
      "learning_rate": 7.3683454012972316e-06,
      "loss": 0.0935,
      "step": 663100
    },
    {
      "epoch": 4.308032089382571,
      "grad_norm": 1.7529923915863037,
      "learning_rate": 7.361434942871241e-06,
      "loss": 0.0946,
      "step": 663200
    },
    {
      "epoch": 4.308681672025724,
      "grad_norm": 1.6493018865585327,
      "learning_rate": 7.35452448444525e-06,
      "loss": 0.0939,
      "step": 663300
    },
    {
      "epoch": 4.309331254668876,
      "grad_norm": 1.3779810667037964,
      "learning_rate": 7.347614026019259e-06,
      "loss": 0.0986,
      "step": 663400
    },
    {
      "epoch": 4.309980837312027,
      "grad_norm": 1.1639574766159058,
      "learning_rate": 7.340703567593267e-06,
      "loss": 0.0909,
      "step": 663500
    },
    {
      "epoch": 4.310630419955179,
      "grad_norm": 1.158986210823059,
      "learning_rate": 7.333793109167277e-06,
      "loss": 0.0939,
      "step": 663600
    },
    {
      "epoch": 4.31128000259833,
      "grad_norm": 1.0759402513504028,
      "learning_rate": 7.326882650741285e-06,
      "loss": 0.0926,
      "step": 663700
    },
    {
      "epoch": 4.311929585241482,
      "grad_norm": 0.6926782727241516,
      "learning_rate": 7.319972192315295e-06,
      "loss": 0.0962,
      "step": 663800
    },
    {
      "epoch": 4.312579167884634,
      "grad_norm": 1.0152636766433716,
      "learning_rate": 7.313061733889304e-06,
      "loss": 0.0969,
      "step": 663900
    },
    {
      "epoch": 4.313228750527786,
      "grad_norm": 1.365494966506958,
      "learning_rate": 7.306151275463312e-06,
      "loss": 0.0951,
      "step": 664000
    },
    {
      "epoch": 4.313878333170938,
      "grad_norm": 1.0056434869766235,
      "learning_rate": 7.299240817037322e-06,
      "loss": 0.0954,
      "step": 664100
    },
    {
      "epoch": 4.3145279158140895,
      "grad_norm": 1.2863188982009888,
      "learning_rate": 7.29233035861133e-06,
      "loss": 0.1009,
      "step": 664200
    },
    {
      "epoch": 4.315177498457241,
      "grad_norm": 1.6625832319259644,
      "learning_rate": 7.285419900185339e-06,
      "loss": 0.0917,
      "step": 664300
    },
    {
      "epoch": 4.315827081100393,
      "grad_norm": 0.9841279983520508,
      "learning_rate": 7.278509441759347e-06,
      "loss": 0.1013,
      "step": 664400
    },
    {
      "epoch": 4.316476663743545,
      "grad_norm": 1.431268572807312,
      "learning_rate": 7.271598983333357e-06,
      "loss": 0.0907,
      "step": 664500
    },
    {
      "epoch": 4.317126246386697,
      "grad_norm": 0.9275861978530884,
      "learning_rate": 7.264688524907367e-06,
      "loss": 0.0978,
      "step": 664600
    },
    {
      "epoch": 4.3177758290298485,
      "grad_norm": 1.3133362531661987,
      "learning_rate": 7.257778066481374e-06,
      "loss": 0.0979,
      "step": 664700
    },
    {
      "epoch": 4.318425411673,
      "grad_norm": 0.6963015198707581,
      "learning_rate": 7.250867608055384e-06,
      "loss": 0.0941,
      "step": 664800
    },
    {
      "epoch": 4.319074994316152,
      "grad_norm": 0.9217345714569092,
      "learning_rate": 7.243957149629392e-06,
      "loss": 0.0935,
      "step": 664900
    },
    {
      "epoch": 4.319724576959303,
      "grad_norm": 1.7798413038253784,
      "learning_rate": 7.237046691203402e-06,
      "loss": 0.0957,
      "step": 665000
    },
    {
      "epoch": 4.320374159602455,
      "grad_norm": 0.9938739538192749,
      "learning_rate": 7.23013623277741e-06,
      "loss": 0.0949,
      "step": 665100
    },
    {
      "epoch": 4.3210237422456075,
      "grad_norm": 0.8173944354057312,
      "learning_rate": 7.223225774351419e-06,
      "loss": 0.0945,
      "step": 665200
    },
    {
      "epoch": 4.321673324888759,
      "grad_norm": 0.8827131390571594,
      "learning_rate": 7.216315315925429e-06,
      "loss": 0.0995,
      "step": 665300
    },
    {
      "epoch": 4.322322907531911,
      "grad_norm": 1.2490521669387817,
      "learning_rate": 7.209404857499437e-06,
      "loss": 0.1027,
      "step": 665400
    },
    {
      "epoch": 4.322972490175062,
      "grad_norm": 1.1799218654632568,
      "learning_rate": 7.202494399073447e-06,
      "loss": 0.0971,
      "step": 665500
    },
    {
      "epoch": 4.323622072818214,
      "grad_norm": 1.059211015701294,
      "learning_rate": 7.195583940647454e-06,
      "loss": 0.0964,
      "step": 665600
    },
    {
      "epoch": 4.3242716554613665,
      "grad_norm": 0.7039003968238831,
      "learning_rate": 7.188673482221464e-06,
      "loss": 0.1023,
      "step": 665700
    },
    {
      "epoch": 4.324921238104518,
      "grad_norm": 1.7125884294509888,
      "learning_rate": 7.181763023795472e-06,
      "loss": 0.096,
      "step": 665800
    },
    {
      "epoch": 4.32557082074767,
      "grad_norm": 1.4550632238388062,
      "learning_rate": 7.174852565369482e-06,
      "loss": 0.0941,
      "step": 665900
    },
    {
      "epoch": 4.326220403390821,
      "grad_norm": 1.3563838005065918,
      "learning_rate": 7.167942106943491e-06,
      "loss": 0.1004,
      "step": 666000
    },
    {
      "epoch": 4.326869986033973,
      "grad_norm": 0.8551186919212341,
      "learning_rate": 7.161031648517499e-06,
      "loss": 0.0966,
      "step": 666100
    },
    {
      "epoch": 4.327519568677125,
      "grad_norm": 0.9605496525764465,
      "learning_rate": 7.154121190091509e-06,
      "loss": 0.0939,
      "step": 666200
    },
    {
      "epoch": 4.328169151320277,
      "grad_norm": 1.0317214727401733,
      "learning_rate": 7.147210731665517e-06,
      "loss": 0.0953,
      "step": 666300
    },
    {
      "epoch": 4.328818733963429,
      "grad_norm": 1.2050487995147705,
      "learning_rate": 7.140300273239526e-06,
      "loss": 0.0988,
      "step": 666400
    },
    {
      "epoch": 4.32946831660658,
      "grad_norm": 1.3091456890106201,
      "learning_rate": 7.133389814813536e-06,
      "loss": 0.0959,
      "step": 666500
    },
    {
      "epoch": 4.330117899249732,
      "grad_norm": 0.9763104319572449,
      "learning_rate": 7.126479356387544e-06,
      "loss": 0.1007,
      "step": 666600
    },
    {
      "epoch": 4.330767481892884,
      "grad_norm": 1.0725252628326416,
      "learning_rate": 7.119568897961554e-06,
      "loss": 0.0951,
      "step": 666700
    },
    {
      "epoch": 4.331417064536035,
      "grad_norm": 1.1191651821136475,
      "learning_rate": 7.112658439535562e-06,
      "loss": 0.0929,
      "step": 666800
    },
    {
      "epoch": 4.332066647179188,
      "grad_norm": 1.1475658416748047,
      "learning_rate": 7.105747981109571e-06,
      "loss": 0.0995,
      "step": 666900
    },
    {
      "epoch": 4.332716229822339,
      "grad_norm": 1.1321594715118408,
      "learning_rate": 7.098837522683579e-06,
      "loss": 0.0949,
      "step": 667000
    },
    {
      "epoch": 4.333365812465491,
      "grad_norm": 1.201990008354187,
      "learning_rate": 7.091927064257589e-06,
      "loss": 0.0937,
      "step": 667100
    },
    {
      "epoch": 4.334015395108643,
      "grad_norm": 0.9226722717285156,
      "learning_rate": 7.085016605831598e-06,
      "loss": 0.0941,
      "step": 667200
    },
    {
      "epoch": 4.334664977751794,
      "grad_norm": 0.7859634757041931,
      "learning_rate": 7.078106147405606e-06,
      "loss": 0.0979,
      "step": 667300
    },
    {
      "epoch": 4.335314560394946,
      "grad_norm": 1.0348106622695923,
      "learning_rate": 7.071195688979616e-06,
      "loss": 0.1006,
      "step": 667400
    },
    {
      "epoch": 4.335964143038098,
      "grad_norm": 0.8305310010910034,
      "learning_rate": 7.064285230553624e-06,
      "loss": 0.0978,
      "step": 667500
    },
    {
      "epoch": 4.33661372568125,
      "grad_norm": 0.9232677817344666,
      "learning_rate": 7.057374772127634e-06,
      "loss": 0.092,
      "step": 667600
    },
    {
      "epoch": 4.337263308324402,
      "grad_norm": 0.5583968758583069,
      "learning_rate": 7.050464313701642e-06,
      "loss": 0.1016,
      "step": 667700
    },
    {
      "epoch": 4.337912890967553,
      "grad_norm": 1.3501636981964111,
      "learning_rate": 7.043553855275651e-06,
      "loss": 0.0932,
      "step": 667800
    },
    {
      "epoch": 4.338562473610705,
      "grad_norm": 1.053309679031372,
      "learning_rate": 7.036643396849661e-06,
      "loss": 0.0923,
      "step": 667900
    },
    {
      "epoch": 4.3392120562538565,
      "grad_norm": 1.6166399717330933,
      "learning_rate": 7.029732938423669e-06,
      "loss": 0.1027,
      "step": 668000
    },
    {
      "epoch": 4.339861638897009,
      "grad_norm": 1.1479545831680298,
      "learning_rate": 7.0228224799976784e-06,
      "loss": 0.0994,
      "step": 668100
    },
    {
      "epoch": 4.340511221540161,
      "grad_norm": 1.070124864578247,
      "learning_rate": 7.015912021571687e-06,
      "loss": 0.097,
      "step": 668200
    },
    {
      "epoch": 4.341160804183312,
      "grad_norm": 0.8644547462463379,
      "learning_rate": 7.0090015631456964e-06,
      "loss": 0.0952,
      "step": 668300
    },
    {
      "epoch": 4.341810386826464,
      "grad_norm": 1.106423258781433,
      "learning_rate": 7.002091104719705e-06,
      "loss": 0.098,
      "step": 668400
    },
    {
      "epoch": 4.3424599694696155,
      "grad_norm": 0.5398789644241333,
      "learning_rate": 6.9951806462937144e-06,
      "loss": 0.0972,
      "step": 668500
    },
    {
      "epoch": 4.343109552112767,
      "grad_norm": 1.2467586994171143,
      "learning_rate": 6.9882701878677235e-06,
      "loss": 0.0953,
      "step": 668600
    },
    {
      "epoch": 4.34375913475592,
      "grad_norm": 0.8222053050994873,
      "learning_rate": 6.981359729441732e-06,
      "loss": 0.0912,
      "step": 668700
    },
    {
      "epoch": 4.344408717399071,
      "grad_norm": 1.090441107749939,
      "learning_rate": 6.9744492710157415e-06,
      "loss": 0.0931,
      "step": 668800
    },
    {
      "epoch": 4.345058300042223,
      "grad_norm": 1.2483559846878052,
      "learning_rate": 6.96753881258975e-06,
      "loss": 0.093,
      "step": 668900
    },
    {
      "epoch": 4.345707882685375,
      "grad_norm": 1.014758586883545,
      "learning_rate": 6.960628354163759e-06,
      "loss": 0.0921,
      "step": 669000
    },
    {
      "epoch": 4.346357465328526,
      "grad_norm": 1.1499252319335938,
      "learning_rate": 6.953717895737767e-06,
      "loss": 0.0956,
      "step": 669100
    },
    {
      "epoch": 4.347007047971678,
      "grad_norm": 1.279183268547058,
      "learning_rate": 6.946807437311777e-06,
      "loss": 0.0946,
      "step": 669200
    },
    {
      "epoch": 4.34765663061483,
      "grad_norm": 0.60578453540802,
      "learning_rate": 6.9398969788857865e-06,
      "loss": 0.0945,
      "step": 669300
    },
    {
      "epoch": 4.348306213257982,
      "grad_norm": 1.365165114402771,
      "learning_rate": 6.932986520459795e-06,
      "loss": 0.1037,
      "step": 669400
    },
    {
      "epoch": 4.348955795901134,
      "grad_norm": 0.9923354387283325,
      "learning_rate": 6.926076062033804e-06,
      "loss": 0.0958,
      "step": 669500
    },
    {
      "epoch": 4.349605378544285,
      "grad_norm": 0.8148377537727356,
      "learning_rate": 6.919165603607812e-06,
      "loss": 0.0967,
      "step": 669600
    },
    {
      "epoch": 4.350254961187437,
      "grad_norm": 0.8767761588096619,
      "learning_rate": 6.912255145181822e-06,
      "loss": 0.095,
      "step": 669700
    },
    {
      "epoch": 4.3509045438305884,
      "grad_norm": 1.2159061431884766,
      "learning_rate": 6.90534468675583e-06,
      "loss": 0.0988,
      "step": 669800
    },
    {
      "epoch": 4.351554126473741,
      "grad_norm": 1.2255358695983887,
      "learning_rate": 6.898434228329839e-06,
      "loss": 0.0946,
      "step": 669900
    },
    {
      "epoch": 4.352203709116893,
      "grad_norm": 1.6370278596878052,
      "learning_rate": 6.891523769903849e-06,
      "loss": 0.0985,
      "step": 670000
    },
    {
      "epoch": 4.352853291760044,
      "grad_norm": 0.9125999808311462,
      "learning_rate": 6.884613311477857e-06,
      "loss": 0.095,
      "step": 670100
    },
    {
      "epoch": 4.353502874403196,
      "grad_norm": 0.755318284034729,
      "learning_rate": 6.877702853051867e-06,
      "loss": 0.0984,
      "step": 670200
    },
    {
      "epoch": 4.3541524570463475,
      "grad_norm": 0.9306960105895996,
      "learning_rate": 6.870792394625875e-06,
      "loss": 0.0951,
      "step": 670300
    },
    {
      "epoch": 4.354802039689499,
      "grad_norm": 0.9142801761627197,
      "learning_rate": 6.863881936199884e-06,
      "loss": 0.0948,
      "step": 670400
    },
    {
      "epoch": 4.355451622332652,
      "grad_norm": 0.8900631070137024,
      "learning_rate": 6.856971477773892e-06,
      "loss": 0.096,
      "step": 670500
    },
    {
      "epoch": 4.356101204975803,
      "grad_norm": 0.9311962127685547,
      "learning_rate": 6.850061019347902e-06,
      "loss": 0.1,
      "step": 670600
    },
    {
      "epoch": 4.356750787618955,
      "grad_norm": 1.4915153980255127,
      "learning_rate": 6.843150560921911e-06,
      "loss": 0.0927,
      "step": 670700
    },
    {
      "epoch": 4.3574003702621065,
      "grad_norm": 1.1815599203109741,
      "learning_rate": 6.836240102495919e-06,
      "loss": 0.0981,
      "step": 670800
    },
    {
      "epoch": 4.358049952905258,
      "grad_norm": 0.9650911688804626,
      "learning_rate": 6.829329644069929e-06,
      "loss": 0.097,
      "step": 670900
    },
    {
      "epoch": 4.358699535548411,
      "grad_norm": 1.2819795608520508,
      "learning_rate": 6.822419185643937e-06,
      "loss": 0.0997,
      "step": 671000
    },
    {
      "epoch": 4.359349118191562,
      "grad_norm": 1.1151421070098877,
      "learning_rate": 6.815508727217947e-06,
      "loss": 0.0954,
      "step": 671100
    },
    {
      "epoch": 4.359998700834714,
      "grad_norm": 1.0058598518371582,
      "learning_rate": 6.808598268791956e-06,
      "loss": 0.0975,
      "step": 671200
    },
    {
      "epoch": 4.3606482834778655,
      "grad_norm": 1.023298740386963,
      "learning_rate": 6.801687810365964e-06,
      "loss": 0.0953,
      "step": 671300
    },
    {
      "epoch": 4.361297866121017,
      "grad_norm": 1.02019464969635,
      "learning_rate": 6.794777351939974e-06,
      "loss": 0.1038,
      "step": 671400
    },
    {
      "epoch": 4.361947448764169,
      "grad_norm": 1.394142985343933,
      "learning_rate": 6.787866893513982e-06,
      "loss": 0.097,
      "step": 671500
    },
    {
      "epoch": 4.36259703140732,
      "grad_norm": 0.8114206194877625,
      "learning_rate": 6.780956435087991e-06,
      "loss": 0.0936,
      "step": 671600
    },
    {
      "epoch": 4.363246614050473,
      "grad_norm": 1.874745488166809,
      "learning_rate": 6.774045976661999e-06,
      "loss": 0.0969,
      "step": 671700
    },
    {
      "epoch": 4.3638961966936245,
      "grad_norm": 1.1289706230163574,
      "learning_rate": 6.767135518236009e-06,
      "loss": 0.095,
      "step": 671800
    },
    {
      "epoch": 4.364545779336776,
      "grad_norm": 1.3257954120635986,
      "learning_rate": 6.760225059810019e-06,
      "loss": 0.0974,
      "step": 671900
    },
    {
      "epoch": 4.365195361979928,
      "grad_norm": 0.8767296075820923,
      "learning_rate": 6.753314601384027e-06,
      "loss": 0.0939,
      "step": 672000
    },
    {
      "epoch": 4.365844944623079,
      "grad_norm": 0.8429815173149109,
      "learning_rate": 6.746404142958036e-06,
      "loss": 0.0971,
      "step": 672100
    },
    {
      "epoch": 4.366494527266232,
      "grad_norm": 0.98976069688797,
      "learning_rate": 6.739493684532044e-06,
      "loss": 0.0955,
      "step": 672200
    },
    {
      "epoch": 4.3671441099093835,
      "grad_norm": 0.8345144987106323,
      "learning_rate": 6.732583226106054e-06,
      "loss": 0.1,
      "step": 672300
    },
    {
      "epoch": 4.367793692552535,
      "grad_norm": 0.667497456073761,
      "learning_rate": 6.725672767680062e-06,
      "loss": 0.0937,
      "step": 672400
    },
    {
      "epoch": 4.368443275195687,
      "grad_norm": 1.303876280784607,
      "learning_rate": 6.718762309254071e-06,
      "loss": 0.0993,
      "step": 672500
    },
    {
      "epoch": 4.369092857838838,
      "grad_norm": 0.9871429800987244,
      "learning_rate": 6.711851850828081e-06,
      "loss": 0.0966,
      "step": 672600
    },
    {
      "epoch": 4.36974244048199,
      "grad_norm": 0.9297304749488831,
      "learning_rate": 6.704941392402089e-06,
      "loss": 0.0937,
      "step": 672700
    },
    {
      "epoch": 4.370392023125142,
      "grad_norm": 0.8838821053504944,
      "learning_rate": 6.698030933976099e-06,
      "loss": 0.0923,
      "step": 672800
    },
    {
      "epoch": 4.371041605768294,
      "grad_norm": 0.9536962509155273,
      "learning_rate": 6.691120475550107e-06,
      "loss": 0.0983,
      "step": 672900
    },
    {
      "epoch": 4.371691188411446,
      "grad_norm": 1.0112003087997437,
      "learning_rate": 6.684210017124116e-06,
      "loss": 0.0926,
      "step": 673000
    },
    {
      "epoch": 4.372340771054597,
      "grad_norm": 1.2890361547470093,
      "learning_rate": 6.6772995586981244e-06,
      "loss": 0.0974,
      "step": 673100
    },
    {
      "epoch": 4.372990353697749,
      "grad_norm": 1.28128981590271,
      "learning_rate": 6.670389100272134e-06,
      "loss": 0.0967,
      "step": 673200
    },
    {
      "epoch": 4.373639936340901,
      "grad_norm": 1.1781890392303467,
      "learning_rate": 6.663478641846143e-06,
      "loss": 0.0932,
      "step": 673300
    },
    {
      "epoch": 4.374289518984053,
      "grad_norm": 1.13014554977417,
      "learning_rate": 6.6565681834201515e-06,
      "loss": 0.0949,
      "step": 673400
    },
    {
      "epoch": 4.374939101627205,
      "grad_norm": 1.161839485168457,
      "learning_rate": 6.649657724994161e-06,
      "loss": 0.1007,
      "step": 673500
    },
    {
      "epoch": 4.375588684270356,
      "grad_norm": 1.4810208082199097,
      "learning_rate": 6.6427472665681695e-06,
      "loss": 0.0925,
      "step": 673600
    },
    {
      "epoch": 4.376238266913508,
      "grad_norm": 0.9307944774627686,
      "learning_rate": 6.635836808142179e-06,
      "loss": 0.099,
      "step": 673700
    },
    {
      "epoch": 4.37688784955666,
      "grad_norm": 1.03525972366333,
      "learning_rate": 6.6289263497161875e-06,
      "loss": 0.0971,
      "step": 673800
    },
    {
      "epoch": 4.377537432199811,
      "grad_norm": 1.2548185586929321,
      "learning_rate": 6.6220158912901965e-06,
      "loss": 0.0954,
      "step": 673900
    },
    {
      "epoch": 4.378187014842964,
      "grad_norm": 0.7856144905090332,
      "learning_rate": 6.615105432864206e-06,
      "loss": 0.0905,
      "step": 674000
    },
    {
      "epoch": 4.378836597486115,
      "grad_norm": 0.6034381985664368,
      "learning_rate": 6.6081949744382145e-06,
      "loss": 0.0997,
      "step": 674100
    },
    {
      "epoch": 4.379486180129267,
      "grad_norm": 1.1907384395599365,
      "learning_rate": 6.6012845160122235e-06,
      "loss": 0.0963,
      "step": 674200
    },
    {
      "epoch": 4.380135762772419,
      "grad_norm": 0.83491051197052,
      "learning_rate": 6.594374057586232e-06,
      "loss": 0.0962,
      "step": 674300
    },
    {
      "epoch": 4.38078534541557,
      "grad_norm": 1.2486814260482788,
      "learning_rate": 6.5874635991602415e-06,
      "loss": 0.0933,
      "step": 674400
    },
    {
      "epoch": 4.381434928058722,
      "grad_norm": 1.3104112148284912,
      "learning_rate": 6.58055314073425e-06,
      "loss": 0.0907,
      "step": 674500
    },
    {
      "epoch": 4.382084510701874,
      "grad_norm": 0.9902766346931458,
      "learning_rate": 6.5736426823082595e-06,
      "loss": 0.0944,
      "step": 674600
    },
    {
      "epoch": 4.382734093345026,
      "grad_norm": 1.067488193511963,
      "learning_rate": 6.5667322238822685e-06,
      "loss": 0.0962,
      "step": 674700
    },
    {
      "epoch": 4.383383675988178,
      "grad_norm": 1.7486791610717773,
      "learning_rate": 6.559821765456277e-06,
      "loss": 0.0964,
      "step": 674800
    },
    {
      "epoch": 4.384033258631329,
      "grad_norm": 0.5880919098854065,
      "learning_rate": 6.5529113070302865e-06,
      "loss": 0.0964,
      "step": 674900
    },
    {
      "epoch": 4.384682841274481,
      "grad_norm": 1.2075382471084595,
      "learning_rate": 6.546000848604295e-06,
      "loss": 0.0971,
      "step": 675000
    },
    {
      "epoch": 4.385332423917633,
      "grad_norm": 1.041725516319275,
      "learning_rate": 6.539090390178304e-06,
      "loss": 0.0905,
      "step": 675100
    },
    {
      "epoch": 4.385982006560785,
      "grad_norm": 1.0417996644973755,
      "learning_rate": 6.5321799317523135e-06,
      "loss": 0.0964,
      "step": 675200
    },
    {
      "epoch": 4.386631589203937,
      "grad_norm": 1.2349624633789062,
      "learning_rate": 6.525269473326322e-06,
      "loss": 0.093,
      "step": 675300
    },
    {
      "epoch": 4.387281171847088,
      "grad_norm": 0.44849690794944763,
      "learning_rate": 6.5183590149003315e-06,
      "loss": 0.0912,
      "step": 675400
    },
    {
      "epoch": 4.38793075449024,
      "grad_norm": 0.952225387096405,
      "learning_rate": 6.51144855647434e-06,
      "loss": 0.1006,
      "step": 675500
    },
    {
      "epoch": 4.388580337133392,
      "grad_norm": 1.0755627155303955,
      "learning_rate": 6.504538098048349e-06,
      "loss": 0.0999,
      "step": 675600
    },
    {
      "epoch": 4.389229919776543,
      "grad_norm": 0.8540875315666199,
      "learning_rate": 6.497627639622357e-06,
      "loss": 0.0977,
      "step": 675700
    },
    {
      "epoch": 4.389879502419696,
      "grad_norm": 1.0474450588226318,
      "learning_rate": 6.490717181196367e-06,
      "loss": 0.0983,
      "step": 675800
    },
    {
      "epoch": 4.390529085062847,
      "grad_norm": 1.2545242309570312,
      "learning_rate": 6.483806722770376e-06,
      "loss": 0.1006,
      "step": 675900
    },
    {
      "epoch": 4.391178667705999,
      "grad_norm": 1.3039658069610596,
      "learning_rate": 6.476896264344384e-06,
      "loss": 0.0995,
      "step": 676000
    },
    {
      "epoch": 4.391828250349151,
      "grad_norm": 0.9019880294799805,
      "learning_rate": 6.469985805918394e-06,
      "loss": 0.0991,
      "step": 676100
    },
    {
      "epoch": 4.392477832992302,
      "grad_norm": 0.8512480854988098,
      "learning_rate": 6.463075347492402e-06,
      "loss": 0.0973,
      "step": 676200
    },
    {
      "epoch": 4.393127415635454,
      "grad_norm": 0.7336551547050476,
      "learning_rate": 6.456164889066412e-06,
      "loss": 0.0899,
      "step": 676300
    },
    {
      "epoch": 4.393776998278606,
      "grad_norm": 0.972173810005188,
      "learning_rate": 6.44925443064042e-06,
      "loss": 0.1028,
      "step": 676400
    },
    {
      "epoch": 4.394426580921758,
      "grad_norm": 0.8709760904312134,
      "learning_rate": 6.442343972214429e-06,
      "loss": 0.101,
      "step": 676500
    },
    {
      "epoch": 4.39507616356491,
      "grad_norm": 1.2128599882125854,
      "learning_rate": 6.435433513788439e-06,
      "loss": 0.0968,
      "step": 676600
    },
    {
      "epoch": 4.395725746208061,
      "grad_norm": 1.0382883548736572,
      "learning_rate": 6.428523055362447e-06,
      "loss": 0.0946,
      "step": 676700
    },
    {
      "epoch": 4.396375328851213,
      "grad_norm": 1.1961642503738403,
      "learning_rate": 6.421612596936456e-06,
      "loss": 0.0952,
      "step": 676800
    },
    {
      "epoch": 4.3970249114943645,
      "grad_norm": 0.9050377011299133,
      "learning_rate": 6.414702138510464e-06,
      "loss": 0.0971,
      "step": 676900
    },
    {
      "epoch": 4.397674494137517,
      "grad_norm": 1.3244699239730835,
      "learning_rate": 6.407791680084474e-06,
      "loss": 0.0993,
      "step": 677000
    },
    {
      "epoch": 4.398324076780669,
      "grad_norm": 1.0174921751022339,
      "learning_rate": 6.400881221658482e-06,
      "loss": 0.0977,
      "step": 677100
    },
    {
      "epoch": 4.39897365942382,
      "grad_norm": 1.144974708557129,
      "learning_rate": 6.393970763232492e-06,
      "loss": 0.1053,
      "step": 677200
    },
    {
      "epoch": 4.399623242066972,
      "grad_norm": 1.2100780010223389,
      "learning_rate": 6.387060304806501e-06,
      "loss": 0.0982,
      "step": 677300
    },
    {
      "epoch": 4.4002728247101235,
      "grad_norm": 1.0695230960845947,
      "learning_rate": 6.380149846380509e-06,
      "loss": 0.1018,
      "step": 677400
    },
    {
      "epoch": 4.400922407353275,
      "grad_norm": 0.7210928797721863,
      "learning_rate": 6.373239387954519e-06,
      "loss": 0.0976,
      "step": 677500
    },
    {
      "epoch": 4.401571989996428,
      "grad_norm": 0.7600118517875671,
      "learning_rate": 6.366328929528527e-06,
      "loss": 0.0945,
      "step": 677600
    },
    {
      "epoch": 4.402221572639579,
      "grad_norm": 1.1365710496902466,
      "learning_rate": 6.359418471102536e-06,
      "loss": 0.0902,
      "step": 677700
    },
    {
      "epoch": 4.402871155282731,
      "grad_norm": 1.5451196432113647,
      "learning_rate": 6.352508012676544e-06,
      "loss": 0.0944,
      "step": 677800
    },
    {
      "epoch": 4.4035207379258825,
      "grad_norm": 1.2456532716751099,
      "learning_rate": 6.345597554250554e-06,
      "loss": 0.098,
      "step": 677900
    },
    {
      "epoch": 4.404170320569034,
      "grad_norm": 1.1373169422149658,
      "learning_rate": 6.338687095824564e-06,
      "loss": 0.1007,
      "step": 678000
    },
    {
      "epoch": 4.404819903212186,
      "grad_norm": 1.0976508855819702,
      "learning_rate": 6.331776637398572e-06,
      "loss": 0.0984,
      "step": 678100
    },
    {
      "epoch": 4.405469485855338,
      "grad_norm": 1.3544881343841553,
      "learning_rate": 6.324866178972581e-06,
      "loss": 0.0962,
      "step": 678200
    },
    {
      "epoch": 4.40611906849849,
      "grad_norm": 1.6050134897232056,
      "learning_rate": 6.317955720546589e-06,
      "loss": 0.0976,
      "step": 678300
    },
    {
      "epoch": 4.4067686511416415,
      "grad_norm": 0.8691399693489075,
      "learning_rate": 6.311045262120599e-06,
      "loss": 0.0938,
      "step": 678400
    },
    {
      "epoch": 4.407418233784793,
      "grad_norm": 1.1709798574447632,
      "learning_rate": 6.304134803694607e-06,
      "loss": 0.0973,
      "step": 678500
    },
    {
      "epoch": 4.408067816427945,
      "grad_norm": 1.0134001970291138,
      "learning_rate": 6.297224345268616e-06,
      "loss": 0.0975,
      "step": 678600
    },
    {
      "epoch": 4.408717399071097,
      "grad_norm": 1.1061779260635376,
      "learning_rate": 6.290313886842626e-06,
      "loss": 0.0923,
      "step": 678700
    },
    {
      "epoch": 4.409366981714249,
      "grad_norm": 1.017553687095642,
      "learning_rate": 6.283403428416634e-06,
      "loss": 0.0965,
      "step": 678800
    },
    {
      "epoch": 4.4100165643574005,
      "grad_norm": 0.8206527829170227,
      "learning_rate": 6.276492969990644e-06,
      "loss": 0.0961,
      "step": 678900
    },
    {
      "epoch": 4.410666147000552,
      "grad_norm": 1.2260998487472534,
      "learning_rate": 6.269582511564652e-06,
      "loss": 0.1015,
      "step": 679000
    },
    {
      "epoch": 4.411315729643704,
      "grad_norm": 1.1681126356124878,
      "learning_rate": 6.262672053138661e-06,
      "loss": 0.1041,
      "step": 679100
    },
    {
      "epoch": 4.411965312286855,
      "grad_norm": 1.5490797758102417,
      "learning_rate": 6.255761594712671e-06,
      "loss": 0.0915,
      "step": 679200
    },
    {
      "epoch": 4.412614894930007,
      "grad_norm": 1.096186637878418,
      "learning_rate": 6.248851136286679e-06,
      "loss": 0.1015,
      "step": 679300
    },
    {
      "epoch": 4.4132644775731595,
      "grad_norm": 1.3675072193145752,
      "learning_rate": 6.241940677860688e-06,
      "loss": 0.0947,
      "step": 679400
    },
    {
      "epoch": 4.413914060216311,
      "grad_norm": 1.278507947921753,
      "learning_rate": 6.2350302194346965e-06,
      "loss": 0.0958,
      "step": 679500
    },
    {
      "epoch": 4.414563642859463,
      "grad_norm": 1.3518941402435303,
      "learning_rate": 6.228119761008706e-06,
      "loss": 0.0973,
      "step": 679600
    },
    {
      "epoch": 4.415213225502614,
      "grad_norm": 1.4547182321548462,
      "learning_rate": 6.221209302582715e-06,
      "loss": 0.1017,
      "step": 679700
    },
    {
      "epoch": 4.415862808145766,
      "grad_norm": 1.1635617017745972,
      "learning_rate": 6.214298844156724e-06,
      "loss": 0.0896,
      "step": 679800
    },
    {
      "epoch": 4.4165123907889186,
      "grad_norm": 1.145256519317627,
      "learning_rate": 6.2073883857307325e-06,
      "loss": 0.0943,
      "step": 679900
    },
    {
      "epoch": 4.41716197343207,
      "grad_norm": 1.6165608167648315,
      "learning_rate": 6.2004779273047415e-06,
      "loss": 0.0949,
      "step": 680000
    },
    {
      "epoch": 4.417811556075222,
      "grad_norm": 1.443487524986267,
      "learning_rate": 6.1935674688787505e-06,
      "loss": 0.0967,
      "step": 680100
    },
    {
      "epoch": 4.418461138718373,
      "grad_norm": 0.6139103770256042,
      "learning_rate": 6.18665701045276e-06,
      "loss": 0.0932,
      "step": 680200
    },
    {
      "epoch": 4.419110721361525,
      "grad_norm": 1.8592039346694946,
      "learning_rate": 6.1797465520267686e-06,
      "loss": 0.0951,
      "step": 680300
    },
    {
      "epoch": 4.419760304004677,
      "grad_norm": 1.3886265754699707,
      "learning_rate": 6.1728360936007776e-06,
      "loss": 0.099,
      "step": 680400
    },
    {
      "epoch": 4.420409886647828,
      "grad_norm": 0.8649255037307739,
      "learning_rate": 6.1659256351747866e-06,
      "loss": 0.0991,
      "step": 680500
    },
    {
      "epoch": 4.421059469290981,
      "grad_norm": 1.177916407585144,
      "learning_rate": 6.1590151767487956e-06,
      "loss": 0.0955,
      "step": 680600
    },
    {
      "epoch": 4.421709051934132,
      "grad_norm": 1.359249234199524,
      "learning_rate": 6.1521047183228046e-06,
      "loss": 0.0923,
      "step": 680700
    },
    {
      "epoch": 4.422358634577284,
      "grad_norm": 1.7031301259994507,
      "learning_rate": 6.145194259896813e-06,
      "loss": 0.0963,
      "step": 680800
    },
    {
      "epoch": 4.423008217220436,
      "grad_norm": 0.8576372861862183,
      "learning_rate": 6.138283801470823e-06,
      "loss": 0.0933,
      "step": 680900
    },
    {
      "epoch": 4.423657799863587,
      "grad_norm": 1.2734220027923584,
      "learning_rate": 6.131373343044832e-06,
      "loss": 0.0964,
      "step": 681000
    },
    {
      "epoch": 4.42430738250674,
      "grad_norm": 1.266552448272705,
      "learning_rate": 6.124462884618841e-06,
      "loss": 0.0958,
      "step": 681100
    },
    {
      "epoch": 4.4249569651498915,
      "grad_norm": 1.3667837381362915,
      "learning_rate": 6.117552426192849e-06,
      "loss": 0.1006,
      "step": 681200
    },
    {
      "epoch": 4.425606547793043,
      "grad_norm": 1.1933326721191406,
      "learning_rate": 6.110641967766858e-06,
      "loss": 0.0888,
      "step": 681300
    },
    {
      "epoch": 4.426256130436195,
      "grad_norm": 1.3186118602752686,
      "learning_rate": 6.103731509340867e-06,
      "loss": 0.0892,
      "step": 681400
    },
    {
      "epoch": 4.426905713079346,
      "grad_norm": 1.065231442451477,
      "learning_rate": 6.096821050914876e-06,
      "loss": 0.0923,
      "step": 681500
    },
    {
      "epoch": 4.427555295722498,
      "grad_norm": 0.5805611610412598,
      "learning_rate": 6.089910592488885e-06,
      "loss": 0.1008,
      "step": 681600
    },
    {
      "epoch": 4.4282048783656505,
      "grad_norm": 1.0666693449020386,
      "learning_rate": 6.083000134062894e-06,
      "loss": 0.0974,
      "step": 681700
    },
    {
      "epoch": 4.428854461008802,
      "grad_norm": 0.8908264636993408,
      "learning_rate": 6.076089675636903e-06,
      "loss": 0.0945,
      "step": 681800
    },
    {
      "epoch": 4.429504043651954,
      "grad_norm": 1.0606753826141357,
      "learning_rate": 6.069179217210912e-06,
      "loss": 0.0908,
      "step": 681900
    },
    {
      "epoch": 4.430153626295105,
      "grad_norm": 0.9793412089347839,
      "learning_rate": 6.062268758784921e-06,
      "loss": 0.1085,
      "step": 682000
    },
    {
      "epoch": 4.430803208938257,
      "grad_norm": 2.141507148742676,
      "learning_rate": 6.055358300358929e-06,
      "loss": 0.1005,
      "step": 682100
    },
    {
      "epoch": 4.431452791581409,
      "grad_norm": 0.9410107731819153,
      "learning_rate": 6.048447841932938e-06,
      "loss": 0.0933,
      "step": 682200
    },
    {
      "epoch": 4.432102374224561,
      "grad_norm": 0.9729675054550171,
      "learning_rate": 6.041537383506948e-06,
      "loss": 0.0962,
      "step": 682300
    },
    {
      "epoch": 4.432751956867713,
      "grad_norm": 1.2351139783859253,
      "learning_rate": 6.034626925080957e-06,
      "loss": 0.0971,
      "step": 682400
    },
    {
      "epoch": 4.433401539510864,
      "grad_norm": 1.179647445678711,
      "learning_rate": 6.027716466654965e-06,
      "loss": 0.0951,
      "step": 682500
    },
    {
      "epoch": 4.434051122154016,
      "grad_norm": 1.8150269985198975,
      "learning_rate": 6.020806008228974e-06,
      "loss": 0.097,
      "step": 682600
    },
    {
      "epoch": 4.434700704797168,
      "grad_norm": 1.0512257814407349,
      "learning_rate": 6.013895549802983e-06,
      "loss": 0.0989,
      "step": 682700
    },
    {
      "epoch": 4.435350287440319,
      "grad_norm": 1.0860097408294678,
      "learning_rate": 6.006985091376992e-06,
      "loss": 0.0936,
      "step": 682800
    },
    {
      "epoch": 4.435999870083472,
      "grad_norm": 1.1671332120895386,
      "learning_rate": 6.000074632951001e-06,
      "loss": 0.0993,
      "step": 682900
    },
    {
      "epoch": 4.436649452726623,
      "grad_norm": 1.445826530456543,
      "learning_rate": 5.99316417452501e-06,
      "loss": 0.0966,
      "step": 683000
    },
    {
      "epoch": 4.437299035369775,
      "grad_norm": 1.1363508701324463,
      "learning_rate": 5.986253716099019e-06,
      "loss": 0.0959,
      "step": 683100
    },
    {
      "epoch": 4.437948618012927,
      "grad_norm": 1.1663119792938232,
      "learning_rate": 5.979343257673028e-06,
      "loss": 0.0945,
      "step": 683200
    },
    {
      "epoch": 4.438598200656078,
      "grad_norm": 0.6907684206962585,
      "learning_rate": 5.972432799247037e-06,
      "loss": 0.0985,
      "step": 683300
    },
    {
      "epoch": 4.43924778329923,
      "grad_norm": 0.916189432144165,
      "learning_rate": 5.965522340821045e-06,
      "loss": 0.099,
      "step": 683400
    },
    {
      "epoch": 4.439897365942382,
      "grad_norm": 0.8359137177467346,
      "learning_rate": 5.958611882395054e-06,
      "loss": 0.0972,
      "step": 683500
    },
    {
      "epoch": 4.440546948585534,
      "grad_norm": 0.8613153100013733,
      "learning_rate": 5.951701423969064e-06,
      "loss": 0.0915,
      "step": 683600
    },
    {
      "epoch": 4.441196531228686,
      "grad_norm": 0.8682157397270203,
      "learning_rate": 5.944790965543073e-06,
      "loss": 0.0952,
      "step": 683700
    },
    {
      "epoch": 4.441846113871837,
      "grad_norm": 1.309051752090454,
      "learning_rate": 5.937880507117081e-06,
      "loss": 0.0984,
      "step": 683800
    },
    {
      "epoch": 4.442495696514989,
      "grad_norm": 1.5379562377929688,
      "learning_rate": 5.93097004869109e-06,
      "loss": 0.0948,
      "step": 683900
    },
    {
      "epoch": 4.4431452791581405,
      "grad_norm": 1.140956163406372,
      "learning_rate": 5.924059590265099e-06,
      "loss": 0.0907,
      "step": 684000
    },
    {
      "epoch": 4.443794861801293,
      "grad_norm": 1.1177663803100586,
      "learning_rate": 5.917149131839108e-06,
      "loss": 0.098,
      "step": 684100
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 1.2849560976028442,
      "learning_rate": 5.910238673413117e-06,
      "loss": 0.0979,
      "step": 684200
    },
    {
      "epoch": 4.445094027087596,
      "grad_norm": 1.3425867557525635,
      "learning_rate": 5.903328214987126e-06,
      "loss": 0.0957,
      "step": 684300
    },
    {
      "epoch": 4.445743609730748,
      "grad_norm": 1.325095295906067,
      "learning_rate": 5.896417756561135e-06,
      "loss": 0.0976,
      "step": 684400
    },
    {
      "epoch": 4.4463931923738995,
      "grad_norm": 1.100691318511963,
      "learning_rate": 5.889507298135144e-06,
      "loss": 0.0893,
      "step": 684500
    },
    {
      "epoch": 4.447042775017051,
      "grad_norm": 1.4272469282150269,
      "learning_rate": 5.882596839709153e-06,
      "loss": 0.0959,
      "step": 684600
    },
    {
      "epoch": 4.447692357660204,
      "grad_norm": 0.8294117450714111,
      "learning_rate": 5.875686381283161e-06,
      "loss": 0.0999,
      "step": 684700
    },
    {
      "epoch": 4.448341940303355,
      "grad_norm": 1.0415760278701782,
      "learning_rate": 5.86877592285717e-06,
      "loss": 0.0964,
      "step": 684800
    },
    {
      "epoch": 4.448991522946507,
      "grad_norm": 1.045997977256775,
      "learning_rate": 5.86186546443118e-06,
      "loss": 0.0882,
      "step": 684900
    },
    {
      "epoch": 4.4496411055896585,
      "grad_norm": 1.3861750364303589,
      "learning_rate": 5.854955006005189e-06,
      "loss": 0.0981,
      "step": 685000
    },
    {
      "epoch": 4.45029068823281,
      "grad_norm": 0.6055705547332764,
      "learning_rate": 5.848044547579197e-06,
      "loss": 0.094,
      "step": 685100
    },
    {
      "epoch": 4.450940270875962,
      "grad_norm": 1.4988961219787598,
      "learning_rate": 5.841134089153206e-06,
      "loss": 0.0952,
      "step": 685200
    },
    {
      "epoch": 4.451589853519114,
      "grad_norm": 1.229823112487793,
      "learning_rate": 5.834223630727215e-06,
      "loss": 0.0909,
      "step": 685300
    },
    {
      "epoch": 4.452239436162266,
      "grad_norm": 0.5638148784637451,
      "learning_rate": 5.827313172301224e-06,
      "loss": 0.095,
      "step": 685400
    },
    {
      "epoch": 4.4528890188054175,
      "grad_norm": 1.260928750038147,
      "learning_rate": 5.820402713875233e-06,
      "loss": 0.0939,
      "step": 685500
    },
    {
      "epoch": 4.453538601448569,
      "grad_norm": 0.9079626798629761,
      "learning_rate": 5.8134922554492424e-06,
      "loss": 0.0978,
      "step": 685600
    },
    {
      "epoch": 4.454188184091721,
      "grad_norm": 0.879446268081665,
      "learning_rate": 5.8065817970232514e-06,
      "loss": 0.0995,
      "step": 685700
    },
    {
      "epoch": 4.454837766734872,
      "grad_norm": 0.9304457306861877,
      "learning_rate": 5.7996713385972604e-06,
      "loss": 0.0987,
      "step": 685800
    },
    {
      "epoch": 4.455487349378025,
      "grad_norm": 0.8112916946411133,
      "learning_rate": 5.7927608801712694e-06,
      "loss": 0.1011,
      "step": 685900
    },
    {
      "epoch": 4.4561369320211766,
      "grad_norm": 0.748323380947113,
      "learning_rate": 5.785850421745278e-06,
      "loss": 0.0966,
      "step": 686000
    },
    {
      "epoch": 4.456786514664328,
      "grad_norm": 1.2082234621047974,
      "learning_rate": 5.778939963319287e-06,
      "loss": 0.0976,
      "step": 686100
    },
    {
      "epoch": 4.45743609730748,
      "grad_norm": 1.1075409650802612,
      "learning_rate": 5.772029504893296e-06,
      "loss": 0.092,
      "step": 686200
    },
    {
      "epoch": 4.458085679950631,
      "grad_norm": 1.1105574369430542,
      "learning_rate": 5.7651190464673055e-06,
      "loss": 0.0953,
      "step": 686300
    },
    {
      "epoch": 4.458735262593784,
      "grad_norm": 1.0397533178329468,
      "learning_rate": 5.758208588041314e-06,
      "loss": 0.0983,
      "step": 686400
    },
    {
      "epoch": 4.459384845236936,
      "grad_norm": 1.4749207496643066,
      "learning_rate": 5.751298129615323e-06,
      "loss": 0.096,
      "step": 686500
    },
    {
      "epoch": 4.460034427880087,
      "grad_norm": 0.9691475629806519,
      "learning_rate": 5.744387671189332e-06,
      "loss": 0.1006,
      "step": 686600
    },
    {
      "epoch": 4.460684010523239,
      "grad_norm": 1.2834726572036743,
      "learning_rate": 5.737477212763341e-06,
      "loss": 0.0973,
      "step": 686700
    },
    {
      "epoch": 4.46133359316639,
      "grad_norm": 1.2949340343475342,
      "learning_rate": 5.73056675433735e-06,
      "loss": 0.0922,
      "step": 686800
    },
    {
      "epoch": 4.461983175809542,
      "grad_norm": 0.8298782110214233,
      "learning_rate": 5.723656295911359e-06,
      "loss": 0.0958,
      "step": 686900
    },
    {
      "epoch": 4.462632758452694,
      "grad_norm": 1.276063084602356,
      "learning_rate": 5.716745837485368e-06,
      "loss": 0.0935,
      "step": 687000
    },
    {
      "epoch": 4.463282341095846,
      "grad_norm": 1.0732084512710571,
      "learning_rate": 5.709835379059377e-06,
      "loss": 0.0961,
      "step": 687100
    },
    {
      "epoch": 4.463931923738998,
      "grad_norm": 1.023444652557373,
      "learning_rate": 5.702924920633386e-06,
      "loss": 0.0991,
      "step": 687200
    },
    {
      "epoch": 4.4645815063821495,
      "grad_norm": 0.7476071715354919,
      "learning_rate": 5.696014462207394e-06,
      "loss": 0.0988,
      "step": 687300
    },
    {
      "epoch": 4.465231089025301,
      "grad_norm": 1.4254472255706787,
      "learning_rate": 5.689104003781403e-06,
      "loss": 0.0972,
      "step": 687400
    },
    {
      "epoch": 4.465880671668453,
      "grad_norm": 1.1605967283248901,
      "learning_rate": 5.682193545355412e-06,
      "loss": 0.0961,
      "step": 687500
    },
    {
      "epoch": 4.466530254311605,
      "grad_norm": 1.5645967721939087,
      "learning_rate": 5.675283086929422e-06,
      "loss": 0.0959,
      "step": 687600
    },
    {
      "epoch": 4.467179836954757,
      "grad_norm": 0.8416428565979004,
      "learning_rate": 5.66837262850343e-06,
      "loss": 0.1009,
      "step": 687700
    },
    {
      "epoch": 4.4678294195979085,
      "grad_norm": 1.1555429697036743,
      "learning_rate": 5.661462170077439e-06,
      "loss": 0.0966,
      "step": 687800
    },
    {
      "epoch": 4.46847900224106,
      "grad_norm": 1.0194120407104492,
      "learning_rate": 5.654551711651448e-06,
      "loss": 0.0975,
      "step": 687900
    },
    {
      "epoch": 4.469128584884212,
      "grad_norm": 1.5346499681472778,
      "learning_rate": 5.647641253225457e-06,
      "loss": 0.0969,
      "step": 688000
    },
    {
      "epoch": 4.469778167527363,
      "grad_norm": 0.6751954555511475,
      "learning_rate": 5.640730794799466e-06,
      "loss": 0.0917,
      "step": 688100
    },
    {
      "epoch": 4.470427750170516,
      "grad_norm": 1.98824942111969,
      "learning_rate": 5.633820336373474e-06,
      "loss": 0.0922,
      "step": 688200
    },
    {
      "epoch": 4.4710773328136675,
      "grad_norm": 1.0273106098175049,
      "learning_rate": 5.626909877947484e-06,
      "loss": 0.1014,
      "step": 688300
    },
    {
      "epoch": 4.471726915456819,
      "grad_norm": 0.9651207327842712,
      "learning_rate": 5.619999419521493e-06,
      "loss": 0.0951,
      "step": 688400
    },
    {
      "epoch": 4.472376498099971,
      "grad_norm": 1.2068610191345215,
      "learning_rate": 5.613088961095502e-06,
      "loss": 0.0967,
      "step": 688500
    },
    {
      "epoch": 4.473026080743122,
      "grad_norm": 0.746416449546814,
      "learning_rate": 5.60617850266951e-06,
      "loss": 0.0963,
      "step": 688600
    },
    {
      "epoch": 4.473675663386274,
      "grad_norm": 1.0742206573486328,
      "learning_rate": 5.599268044243519e-06,
      "loss": 0.0954,
      "step": 688700
    },
    {
      "epoch": 4.4743252460294265,
      "grad_norm": 0.9333294630050659,
      "learning_rate": 5.592357585817528e-06,
      "loss": 0.0988,
      "step": 688800
    },
    {
      "epoch": 4.474974828672578,
      "grad_norm": 0.9186649918556213,
      "learning_rate": 5.585447127391538e-06,
      "loss": 0.098,
      "step": 688900
    },
    {
      "epoch": 4.47562441131573,
      "grad_norm": 0.9935231804847717,
      "learning_rate": 5.578536668965546e-06,
      "loss": 0.1032,
      "step": 689000
    },
    {
      "epoch": 4.476273993958881,
      "grad_norm": 1.0475451946258545,
      "learning_rate": 5.571626210539555e-06,
      "loss": 0.0938,
      "step": 689100
    },
    {
      "epoch": 4.476923576602033,
      "grad_norm": 0.6585573554039001,
      "learning_rate": 5.564715752113564e-06,
      "loss": 0.0997,
      "step": 689200
    },
    {
      "epoch": 4.477573159245185,
      "grad_norm": 1.1568700075149536,
      "learning_rate": 5.557805293687573e-06,
      "loss": 0.0995,
      "step": 689300
    },
    {
      "epoch": 4.478222741888337,
      "grad_norm": 1.1951292753219604,
      "learning_rate": 5.550894835261582e-06,
      "loss": 0.0971,
      "step": 689400
    },
    {
      "epoch": 4.478872324531489,
      "grad_norm": 0.7390587329864502,
      "learning_rate": 5.54398437683559e-06,
      "loss": 0.0985,
      "step": 689500
    },
    {
      "epoch": 4.47952190717464,
      "grad_norm": 1.2594482898712158,
      "learning_rate": 5.5370739184096e-06,
      "loss": 0.0975,
      "step": 689600
    },
    {
      "epoch": 4.480171489817792,
      "grad_norm": 1.062875509262085,
      "learning_rate": 5.530163459983609e-06,
      "loss": 0.0984,
      "step": 689700
    },
    {
      "epoch": 4.480821072460944,
      "grad_norm": 0.747655987739563,
      "learning_rate": 5.523253001557618e-06,
      "loss": 0.0945,
      "step": 689800
    },
    {
      "epoch": 4.481470655104095,
      "grad_norm": 1.0004980564117432,
      "learning_rate": 5.516342543131626e-06,
      "loss": 0.091,
      "step": 689900
    },
    {
      "epoch": 4.482120237747248,
      "grad_norm": 0.7705898880958557,
      "learning_rate": 5.509432084705635e-06,
      "loss": 0.098,
      "step": 690000
    },
    {
      "epoch": 4.482769820390399,
      "grad_norm": 0.9825323820114136,
      "learning_rate": 5.502521626279644e-06,
      "loss": 0.0961,
      "step": 690100
    },
    {
      "epoch": 4.483419403033551,
      "grad_norm": 0.9955146908760071,
      "learning_rate": 5.495611167853653e-06,
      "loss": 0.1023,
      "step": 690200
    },
    {
      "epoch": 4.484068985676703,
      "grad_norm": 0.83783358335495,
      "learning_rate": 5.488700709427662e-06,
      "loss": 0.0985,
      "step": 690300
    },
    {
      "epoch": 4.484718568319854,
      "grad_norm": 1.5574285984039307,
      "learning_rate": 5.481790251001671e-06,
      "loss": 0.0972,
      "step": 690400
    },
    {
      "epoch": 4.485368150963006,
      "grad_norm": 1.2202039957046509,
      "learning_rate": 5.47487979257568e-06,
      "loss": 0.0992,
      "step": 690500
    },
    {
      "epoch": 4.486017733606158,
      "grad_norm": 1.1751798391342163,
      "learning_rate": 5.467969334149689e-06,
      "loss": 0.0974,
      "step": 690600
    },
    {
      "epoch": 4.48666731624931,
      "grad_norm": 1.712206482887268,
      "learning_rate": 5.461058875723698e-06,
      "loss": 0.0966,
      "step": 690700
    },
    {
      "epoch": 4.487316898892462,
      "grad_norm": 1.2080544233322144,
      "learning_rate": 5.4541484172977064e-06,
      "loss": 0.1001,
      "step": 690800
    },
    {
      "epoch": 4.487966481535613,
      "grad_norm": 1.0218572616577148,
      "learning_rate": 5.447237958871716e-06,
      "loss": 0.0968,
      "step": 690900
    },
    {
      "epoch": 4.488616064178765,
      "grad_norm": 1.1057108640670776,
      "learning_rate": 5.440327500445725e-06,
      "loss": 0.0981,
      "step": 691000
    },
    {
      "epoch": 4.4892656468219165,
      "grad_norm": 1.2865989208221436,
      "learning_rate": 5.433417042019734e-06,
      "loss": 0.0961,
      "step": 691100
    },
    {
      "epoch": 4.489915229465069,
      "grad_norm": 1.711391806602478,
      "learning_rate": 5.4265065835937425e-06,
      "loss": 0.0974,
      "step": 691200
    },
    {
      "epoch": 4.490564812108221,
      "grad_norm": 1.1578012704849243,
      "learning_rate": 5.4195961251677515e-06,
      "loss": 0.0937,
      "step": 691300
    },
    {
      "epoch": 4.491214394751372,
      "grad_norm": 1.091047763824463,
      "learning_rate": 5.4126856667417605e-06,
      "loss": 0.0953,
      "step": 691400
    },
    {
      "epoch": 4.491863977394524,
      "grad_norm": 0.7669752836227417,
      "learning_rate": 5.4057752083157695e-06,
      "loss": 0.0943,
      "step": 691500
    },
    {
      "epoch": 4.4925135600376755,
      "grad_norm": 1.055203914642334,
      "learning_rate": 5.3988647498897785e-06,
      "loss": 0.0964,
      "step": 691600
    },
    {
      "epoch": 4.493163142680827,
      "grad_norm": 1.6219160556793213,
      "learning_rate": 5.3919542914637875e-06,
      "loss": 0.0988,
      "step": 691700
    },
    {
      "epoch": 4.49381272532398,
      "grad_norm": 0.7244765162467957,
      "learning_rate": 5.3850438330377965e-06,
      "loss": 0.0877,
      "step": 691800
    },
    {
      "epoch": 4.494462307967131,
      "grad_norm": 1.329380750656128,
      "learning_rate": 5.3781333746118055e-06,
      "loss": 0.0948,
      "step": 691900
    },
    {
      "epoch": 4.495111890610283,
      "grad_norm": 1.5222529172897339,
      "learning_rate": 5.371222916185814e-06,
      "loss": 0.0905,
      "step": 692000
    },
    {
      "epoch": 4.495761473253435,
      "grad_norm": 1.6677066087722778,
      "learning_rate": 5.364312457759823e-06,
      "loss": 0.1012,
      "step": 692100
    },
    {
      "epoch": 4.496411055896586,
      "grad_norm": 0.9810776710510254,
      "learning_rate": 5.357401999333832e-06,
      "loss": 0.0972,
      "step": 692200
    },
    {
      "epoch": 4.497060638539738,
      "grad_norm": 1.0934118032455444,
      "learning_rate": 5.3504915409078415e-06,
      "loss": 0.0987,
      "step": 692300
    },
    {
      "epoch": 4.49771022118289,
      "grad_norm": 0.8805454969406128,
      "learning_rate": 5.3435810824818505e-06,
      "loss": 0.0918,
      "step": 692400
    },
    {
      "epoch": 4.498359803826042,
      "grad_norm": 1.0260696411132812,
      "learning_rate": 5.336670624055859e-06,
      "loss": 0.0936,
      "step": 692500
    },
    {
      "epoch": 4.499009386469194,
      "grad_norm": 1.3858596086502075,
      "learning_rate": 5.329760165629868e-06,
      "loss": 0.0977,
      "step": 692600
    },
    {
      "epoch": 4.499658969112345,
      "grad_norm": 0.8773204684257507,
      "learning_rate": 5.322849707203877e-06,
      "loss": 0.0916,
      "step": 692700
    },
    {
      "epoch": 4.500308551755497,
      "grad_norm": 1.2143219709396362,
      "learning_rate": 5.315939248777886e-06,
      "loss": 0.0938,
      "step": 692800
    },
    {
      "epoch": 4.500958134398649,
      "grad_norm": 1.6797840595245361,
      "learning_rate": 5.309028790351895e-06,
      "loss": 0.1011,
      "step": 692900
    },
    {
      "epoch": 4.501607717041801,
      "grad_norm": 0.7204388976097107,
      "learning_rate": 5.302118331925904e-06,
      "loss": 0.0998,
      "step": 693000
    },
    {
      "epoch": 4.502257299684953,
      "grad_norm": 1.5616867542266846,
      "learning_rate": 5.295207873499913e-06,
      "loss": 0.1018,
      "step": 693100
    },
    {
      "epoch": 4.502906882328104,
      "grad_norm": 0.8539031744003296,
      "learning_rate": 5.288297415073922e-06,
      "loss": 0.0945,
      "step": 693200
    },
    {
      "epoch": 4.503556464971256,
      "grad_norm": 0.9073624014854431,
      "learning_rate": 5.28138695664793e-06,
      "loss": 0.0966,
      "step": 693300
    },
    {
      "epoch": 4.5042060476144075,
      "grad_norm": 1.212213158607483,
      "learning_rate": 5.274476498221939e-06,
      "loss": 0.0948,
      "step": 693400
    },
    {
      "epoch": 4.504855630257559,
      "grad_norm": 1.2997475862503052,
      "learning_rate": 5.267566039795948e-06,
      "loss": 0.0966,
      "step": 693500
    },
    {
      "epoch": 4.505505212900712,
      "grad_norm": 1.0842106342315674,
      "learning_rate": 5.260655581369958e-06,
      "loss": 0.097,
      "step": 693600
    },
    {
      "epoch": 4.506154795543863,
      "grad_norm": 1.4409894943237305,
      "learning_rate": 5.253745122943966e-06,
      "loss": 0.0961,
      "step": 693700
    },
    {
      "epoch": 4.506804378187015,
      "grad_norm": 0.8633385300636292,
      "learning_rate": 5.246834664517975e-06,
      "loss": 0.094,
      "step": 693800
    },
    {
      "epoch": 4.5074539608301665,
      "grad_norm": 1.3844196796417236,
      "learning_rate": 5.239924206091984e-06,
      "loss": 0.0957,
      "step": 693900
    },
    {
      "epoch": 4.508103543473318,
      "grad_norm": 0.9669196009635925,
      "learning_rate": 5.233013747665993e-06,
      "loss": 0.0933,
      "step": 694000
    },
    {
      "epoch": 4.508753126116471,
      "grad_norm": 0.9080513715744019,
      "learning_rate": 5.226103289240002e-06,
      "loss": 0.0953,
      "step": 694100
    },
    {
      "epoch": 4.509402708759622,
      "grad_norm": 1.3218486309051514,
      "learning_rate": 5.21919283081401e-06,
      "loss": 0.0916,
      "step": 694200
    },
    {
      "epoch": 4.510052291402774,
      "grad_norm": 1.2421290874481201,
      "learning_rate": 5.21228237238802e-06,
      "loss": 0.0952,
      "step": 694300
    },
    {
      "epoch": 4.5107018740459255,
      "grad_norm": 1.4186209440231323,
      "learning_rate": 5.205371913962029e-06,
      "loss": 0.0931,
      "step": 694400
    },
    {
      "epoch": 4.511351456689077,
      "grad_norm": 1.0659807920455933,
      "learning_rate": 5.198461455536038e-06,
      "loss": 0.0985,
      "step": 694500
    },
    {
      "epoch": 4.512001039332229,
      "grad_norm": 1.103327751159668,
      "learning_rate": 5.191550997110046e-06,
      "loss": 0.0938,
      "step": 694600
    },
    {
      "epoch": 4.51265062197538,
      "grad_norm": 1.1765395402908325,
      "learning_rate": 5.184640538684055e-06,
      "loss": 0.0904,
      "step": 694700
    },
    {
      "epoch": 4.513300204618533,
      "grad_norm": 0.832619845867157,
      "learning_rate": 5.177730080258064e-06,
      "loss": 0.0934,
      "step": 694800
    },
    {
      "epoch": 4.5139497872616845,
      "grad_norm": 1.0883485078811646,
      "learning_rate": 5.170819621832074e-06,
      "loss": 0.0986,
      "step": 694900
    },
    {
      "epoch": 4.514599369904836,
      "grad_norm": 1.3916850090026855,
      "learning_rate": 5.163909163406082e-06,
      "loss": 0.0974,
      "step": 695000
    },
    {
      "epoch": 4.515248952547988,
      "grad_norm": 0.74250328540802,
      "learning_rate": 5.156998704980091e-06,
      "loss": 0.0973,
      "step": 695100
    },
    {
      "epoch": 4.515898535191139,
      "grad_norm": 0.7635409235954285,
      "learning_rate": 5.1500882465541e-06,
      "loss": 0.0921,
      "step": 695200
    },
    {
      "epoch": 4.516548117834292,
      "grad_norm": 1.3372682332992554,
      "learning_rate": 5.143177788128109e-06,
      "loss": 0.0997,
      "step": 695300
    },
    {
      "epoch": 4.5171977004774435,
      "grad_norm": 1.0503004789352417,
      "learning_rate": 5.136267329702118e-06,
      "loss": 0.0951,
      "step": 695400
    },
    {
      "epoch": 4.517847283120595,
      "grad_norm": 0.7050402164459229,
      "learning_rate": 5.129356871276126e-06,
      "loss": 0.0936,
      "step": 695500
    },
    {
      "epoch": 4.518496865763747,
      "grad_norm": 1.6422088146209717,
      "learning_rate": 5.122446412850136e-06,
      "loss": 0.097,
      "step": 695600
    },
    {
      "epoch": 4.519146448406898,
      "grad_norm": 0.4314432442188263,
      "learning_rate": 5.115535954424145e-06,
      "loss": 0.0968,
      "step": 695700
    },
    {
      "epoch": 4.51979603105005,
      "grad_norm": 1.1651853322982788,
      "learning_rate": 5.108625495998154e-06,
      "loss": 0.0914,
      "step": 695800
    },
    {
      "epoch": 4.520445613693202,
      "grad_norm": 1.0396190881729126,
      "learning_rate": 5.101715037572162e-06,
      "loss": 0.0958,
      "step": 695900
    },
    {
      "epoch": 4.521095196336354,
      "grad_norm": 1.0009987354278564,
      "learning_rate": 5.094804579146171e-06,
      "loss": 0.1001,
      "step": 696000
    },
    {
      "epoch": 4.521744778979506,
      "grad_norm": 0.7863181233406067,
      "learning_rate": 5.08789412072018e-06,
      "loss": 0.0976,
      "step": 696100
    },
    {
      "epoch": 4.522394361622657,
      "grad_norm": 1.0737146139144897,
      "learning_rate": 5.080983662294189e-06,
      "loss": 0.0946,
      "step": 696200
    },
    {
      "epoch": 4.523043944265809,
      "grad_norm": 1.0512837171554565,
      "learning_rate": 5.074073203868198e-06,
      "loss": 0.0896,
      "step": 696300
    },
    {
      "epoch": 4.523693526908961,
      "grad_norm": 0.816602885723114,
      "learning_rate": 5.067162745442207e-06,
      "loss": 0.0931,
      "step": 696400
    },
    {
      "epoch": 4.524343109552113,
      "grad_norm": 0.9771474599838257,
      "learning_rate": 5.060252287016216e-06,
      "loss": 0.0922,
      "step": 696500
    },
    {
      "epoch": 4.524992692195265,
      "grad_norm": 0.7234483361244202,
      "learning_rate": 5.053341828590225e-06,
      "loss": 0.0914,
      "step": 696600
    },
    {
      "epoch": 4.525642274838416,
      "grad_norm": 0.8876039385795593,
      "learning_rate": 5.046431370164234e-06,
      "loss": 0.0965,
      "step": 696700
    },
    {
      "epoch": 4.526291857481568,
      "grad_norm": 0.9451612830162048,
      "learning_rate": 5.0395209117382425e-06,
      "loss": 0.0883,
      "step": 696800
    },
    {
      "epoch": 4.52694144012472,
      "grad_norm": 0.8894289135932922,
      "learning_rate": 5.0326104533122515e-06,
      "loss": 0.0908,
      "step": 696900
    },
    {
      "epoch": 4.527591022767871,
      "grad_norm": 1.3212977647781372,
      "learning_rate": 5.025699994886261e-06,
      "loss": 0.099,
      "step": 697000
    },
    {
      "epoch": 4.528240605411024,
      "grad_norm": 0.6668788194656372,
      "learning_rate": 5.01878953646027e-06,
      "loss": 0.0956,
      "step": 697100
    },
    {
      "epoch": 4.528890188054175,
      "grad_norm": 0.931365430355072,
      "learning_rate": 5.0118790780342785e-06,
      "loss": 0.1005,
      "step": 697200
    },
    {
      "epoch": 4.529539770697327,
      "grad_norm": 0.9659512042999268,
      "learning_rate": 5.0049686196082875e-06,
      "loss": 0.0976,
      "step": 697300
    },
    {
      "epoch": 4.530189353340479,
      "grad_norm": 1.4206979274749756,
      "learning_rate": 4.9980581611822965e-06,
      "loss": 0.0926,
      "step": 697400
    },
    {
      "epoch": 4.53083893598363,
      "grad_norm": 1.771923303604126,
      "learning_rate": 4.9911477027563055e-06,
      "loss": 0.0927,
      "step": 697500
    },
    {
      "epoch": 4.531488518626782,
      "grad_norm": 1.252407431602478,
      "learning_rate": 4.9842372443303145e-06,
      "loss": 0.0918,
      "step": 697600
    },
    {
      "epoch": 4.532138101269934,
      "grad_norm": 1.1335358619689941,
      "learning_rate": 4.9773267859043235e-06,
      "loss": 0.0963,
      "step": 697700
    },
    {
      "epoch": 4.532787683913086,
      "grad_norm": 1.4055242538452148,
      "learning_rate": 4.9704163274783326e-06,
      "loss": 0.0993,
      "step": 697800
    },
    {
      "epoch": 4.533437266556238,
      "grad_norm": 0.935819149017334,
      "learning_rate": 4.9635058690523416e-06,
      "loss": 0.0968,
      "step": 697900
    },
    {
      "epoch": 4.534086849199389,
      "grad_norm": 0.9563872218132019,
      "learning_rate": 4.9565954106263506e-06,
      "loss": 0.0959,
      "step": 698000
    },
    {
      "epoch": 4.534736431842541,
      "grad_norm": 1.1496694087982178,
      "learning_rate": 4.949684952200359e-06,
      "loss": 0.0968,
      "step": 698100
    },
    {
      "epoch": 4.535386014485693,
      "grad_norm": 1.0172048807144165,
      "learning_rate": 4.942774493774368e-06,
      "loss": 0.0984,
      "step": 698200
    },
    {
      "epoch": 4.536035597128845,
      "grad_norm": 1.3870741128921509,
      "learning_rate": 4.935864035348378e-06,
      "loss": 0.097,
      "step": 698300
    },
    {
      "epoch": 4.536685179771997,
      "grad_norm": 0.7481374144554138,
      "learning_rate": 4.928953576922387e-06,
      "loss": 0.0919,
      "step": 698400
    },
    {
      "epoch": 4.537334762415148,
      "grad_norm": 0.6453864574432373,
      "learning_rate": 4.922043118496395e-06,
      "loss": 0.0938,
      "step": 698500
    },
    {
      "epoch": 4.5379843450583,
      "grad_norm": 1.532794713973999,
      "learning_rate": 4.915132660070404e-06,
      "loss": 0.0906,
      "step": 698600
    },
    {
      "epoch": 4.538633927701452,
      "grad_norm": 1.2183784246444702,
      "learning_rate": 4.908222201644413e-06,
      "loss": 0.1073,
      "step": 698700
    },
    {
      "epoch": 4.539283510344603,
      "grad_norm": 0.5413864254951477,
      "learning_rate": 4.901311743218422e-06,
      "loss": 0.0926,
      "step": 698800
    },
    {
      "epoch": 4.539933092987756,
      "grad_norm": 0.7816950678825378,
      "learning_rate": 4.894401284792431e-06,
      "loss": 0.0978,
      "step": 698900
    },
    {
      "epoch": 4.540582675630907,
      "grad_norm": 1.2299338579177856,
      "learning_rate": 4.88749082636644e-06,
      "loss": 0.0954,
      "step": 699000
    },
    {
      "epoch": 4.541232258274059,
      "grad_norm": 0.8364110589027405,
      "learning_rate": 4.880580367940449e-06,
      "loss": 0.089,
      "step": 699100
    },
    {
      "epoch": 4.541881840917211,
      "grad_norm": 1.161346197128296,
      "learning_rate": 4.873669909514458e-06,
      "loss": 0.0952,
      "step": 699200
    },
    {
      "epoch": 4.542531423560362,
      "grad_norm": 0.9108268618583679,
      "learning_rate": 4.866759451088467e-06,
      "loss": 0.0952,
      "step": 699300
    },
    {
      "epoch": 4.543181006203515,
      "grad_norm": 1.491918683052063,
      "learning_rate": 4.859848992662475e-06,
      "loss": 0.0919,
      "step": 699400
    },
    {
      "epoch": 4.543830588846666,
      "grad_norm": 1.1767771244049072,
      "learning_rate": 4.852938534236484e-06,
      "loss": 0.0913,
      "step": 699500
    },
    {
      "epoch": 4.544480171489818,
      "grad_norm": 1.298520803451538,
      "learning_rate": 4.846028075810494e-06,
      "loss": 0.0976,
      "step": 699600
    },
    {
      "epoch": 4.54512975413297,
      "grad_norm": 1.0210967063903809,
      "learning_rate": 4.839117617384503e-06,
      "loss": 0.0954,
      "step": 699700
    },
    {
      "epoch": 4.545779336776121,
      "grad_norm": 1.2599669694900513,
      "learning_rate": 4.832207158958511e-06,
      "loss": 0.0938,
      "step": 699800
    },
    {
      "epoch": 4.546428919419273,
      "grad_norm": 0.9847099781036377,
      "learning_rate": 4.82529670053252e-06,
      "loss": 0.0963,
      "step": 699900
    },
    {
      "epoch": 4.5470785020624245,
      "grad_norm": 0.7705590724945068,
      "learning_rate": 4.818386242106529e-06,
      "loss": 0.0971,
      "step": 700000
    },
    {
      "epoch": 4.547728084705577,
      "grad_norm": 1.1719467639923096,
      "learning_rate": 4.811475783680538e-06,
      "loss": 0.0968,
      "step": 700100
    },
    {
      "epoch": 4.548377667348729,
      "grad_norm": 1.0800788402557373,
      "learning_rate": 4.804565325254547e-06,
      "loss": 0.0909,
      "step": 700200
    },
    {
      "epoch": 4.54902724999188,
      "grad_norm": 1.4855157136917114,
      "learning_rate": 4.797654866828556e-06,
      "loss": 0.0975,
      "step": 700300
    },
    {
      "epoch": 4.549676832635032,
      "grad_norm": 1.5256831645965576,
      "learning_rate": 4.790744408402565e-06,
      "loss": 0.096,
      "step": 700400
    },
    {
      "epoch": 4.5503264152781835,
      "grad_norm": 1.216852068901062,
      "learning_rate": 4.783833949976574e-06,
      "loss": 0.0892,
      "step": 700500
    },
    {
      "epoch": 4.550975997921336,
      "grad_norm": 0.7255640029907227,
      "learning_rate": 4.776923491550583e-06,
      "loss": 0.0977,
      "step": 700600
    },
    {
      "epoch": 4.551625580564488,
      "grad_norm": 1.3914417028427124,
      "learning_rate": 4.770013033124591e-06,
      "loss": 0.0983,
      "step": 700700
    },
    {
      "epoch": 4.552275163207639,
      "grad_norm": 1.0553147792816162,
      "learning_rate": 4.7631025746986e-06,
      "loss": 0.0973,
      "step": 700800
    },
    {
      "epoch": 4.552924745850791,
      "grad_norm": 0.8571547865867615,
      "learning_rate": 4.756192116272609e-06,
      "loss": 0.0924,
      "step": 700900
    },
    {
      "epoch": 4.5535743284939425,
      "grad_norm": 1.4850411415100098,
      "learning_rate": 4.749281657846619e-06,
      "loss": 0.0947,
      "step": 701000
    },
    {
      "epoch": 4.554223911137094,
      "grad_norm": 1.4018049240112305,
      "learning_rate": 4.742371199420627e-06,
      "loss": 0.0919,
      "step": 701100
    },
    {
      "epoch": 4.554873493780246,
      "grad_norm": 0.725141704082489,
      "learning_rate": 4.735460740994636e-06,
      "loss": 0.0985,
      "step": 701200
    },
    {
      "epoch": 4.555523076423398,
      "grad_norm": 1.4179986715316772,
      "learning_rate": 4.728550282568645e-06,
      "loss": 0.0883,
      "step": 701300
    },
    {
      "epoch": 4.55617265906655,
      "grad_norm": 0.8964293599128723,
      "learning_rate": 4.721639824142654e-06,
      "loss": 0.0946,
      "step": 701400
    },
    {
      "epoch": 4.5568222417097015,
      "grad_norm": 1.0232077836990356,
      "learning_rate": 4.714729365716663e-06,
      "loss": 0.0914,
      "step": 701500
    },
    {
      "epoch": 4.557471824352853,
      "grad_norm": 0.7516505122184753,
      "learning_rate": 4.707818907290672e-06,
      "loss": 0.0985,
      "step": 701600
    },
    {
      "epoch": 4.558121406996005,
      "grad_norm": 1.4699667692184448,
      "learning_rate": 4.700908448864681e-06,
      "loss": 0.1007,
      "step": 701700
    },
    {
      "epoch": 4.558770989639157,
      "grad_norm": 0.8646759986877441,
      "learning_rate": 4.69399799043869e-06,
      "loss": 0.0934,
      "step": 701800
    },
    {
      "epoch": 4.559420572282309,
      "grad_norm": 1.0633260011672974,
      "learning_rate": 4.687087532012699e-06,
      "loss": 0.0932,
      "step": 701900
    },
    {
      "epoch": 4.5600701549254605,
      "grad_norm": 1.9560518264770508,
      "learning_rate": 4.680177073586707e-06,
      "loss": 0.0966,
      "step": 702000
    },
    {
      "epoch": 4.560719737568612,
      "grad_norm": 1.416425108909607,
      "learning_rate": 4.673266615160716e-06,
      "loss": 0.0924,
      "step": 702100
    },
    {
      "epoch": 4.561369320211764,
      "grad_norm": 1.5758470296859741,
      "learning_rate": 4.666356156734725e-06,
      "loss": 0.0927,
      "step": 702200
    },
    {
      "epoch": 4.562018902854915,
      "grad_norm": 0.6858153939247131,
      "learning_rate": 4.659445698308735e-06,
      "loss": 0.0897,
      "step": 702300
    },
    {
      "epoch": 4.562668485498067,
      "grad_norm": 0.8121143579483032,
      "learning_rate": 4.652535239882743e-06,
      "loss": 0.0943,
      "step": 702400
    },
    {
      "epoch": 4.5633180681412195,
      "grad_norm": 0.7445027232170105,
      "learning_rate": 4.645624781456752e-06,
      "loss": 0.099,
      "step": 702500
    },
    {
      "epoch": 4.563967650784371,
      "grad_norm": 1.167885661125183,
      "learning_rate": 4.638714323030761e-06,
      "loss": 0.0991,
      "step": 702600
    },
    {
      "epoch": 4.564617233427523,
      "grad_norm": 0.7362090349197388,
      "learning_rate": 4.63180386460477e-06,
      "loss": 0.0947,
      "step": 702700
    },
    {
      "epoch": 4.565266816070674,
      "grad_norm": 1.064462423324585,
      "learning_rate": 4.624893406178779e-06,
      "loss": 0.0963,
      "step": 702800
    },
    {
      "epoch": 4.565916398713826,
      "grad_norm": 0.8865175843238831,
      "learning_rate": 4.6179829477527876e-06,
      "loss": 0.0952,
      "step": 702900
    },
    {
      "epoch": 4.5665659813569786,
      "grad_norm": 1.0217642784118652,
      "learning_rate": 4.611072489326797e-06,
      "loss": 0.1051,
      "step": 703000
    },
    {
      "epoch": 4.56721556400013,
      "grad_norm": 0.7330219149589539,
      "learning_rate": 4.6041620309008064e-06,
      "loss": 0.0903,
      "step": 703100
    },
    {
      "epoch": 4.567865146643282,
      "grad_norm": 1.200217366218567,
      "learning_rate": 4.5972515724748154e-06,
      "loss": 0.1034,
      "step": 703200
    },
    {
      "epoch": 4.568514729286433,
      "grad_norm": 1.0249626636505127,
      "learning_rate": 4.590341114048824e-06,
      "loss": 0.1017,
      "step": 703300
    },
    {
      "epoch": 4.569164311929585,
      "grad_norm": 1.129211187362671,
      "learning_rate": 4.583430655622833e-06,
      "loss": 0.0903,
      "step": 703400
    },
    {
      "epoch": 4.569813894572737,
      "grad_norm": 1.8433688879013062,
      "learning_rate": 4.576520197196842e-06,
      "loss": 0.0946,
      "step": 703500
    },
    {
      "epoch": 4.570463477215888,
      "grad_norm": 1.1805440187454224,
      "learning_rate": 4.5696097387708514e-06,
      "loss": 0.0898,
      "step": 703600
    },
    {
      "epoch": 4.571113059859041,
      "grad_norm": 1.230747938156128,
      "learning_rate": 4.56269928034486e-06,
      "loss": 0.0921,
      "step": 703700
    },
    {
      "epoch": 4.571762642502192,
      "grad_norm": 1.192343831062317,
      "learning_rate": 4.555788821918869e-06,
      "loss": 0.0924,
      "step": 703800
    },
    {
      "epoch": 4.572412225145344,
      "grad_norm": 1.1008967161178589,
      "learning_rate": 4.548878363492878e-06,
      "loss": 0.0923,
      "step": 703900
    },
    {
      "epoch": 4.573061807788496,
      "grad_norm": 0.7917842268943787,
      "learning_rate": 4.541967905066887e-06,
      "loss": 0.0893,
      "step": 704000
    },
    {
      "epoch": 4.573711390431647,
      "grad_norm": 1.0654306411743164,
      "learning_rate": 4.535057446640896e-06,
      "loss": 0.0995,
      "step": 704100
    },
    {
      "epoch": 4.5743609730748,
      "grad_norm": 1.5065497159957886,
      "learning_rate": 4.528146988214904e-06,
      "loss": 0.0931,
      "step": 704200
    },
    {
      "epoch": 4.5750105557179515,
      "grad_norm": 1.147530436515808,
      "learning_rate": 4.521236529788914e-06,
      "loss": 0.0938,
      "step": 704300
    },
    {
      "epoch": 4.575660138361103,
      "grad_norm": 0.9219931960105896,
      "learning_rate": 4.514326071362923e-06,
      "loss": 0.0915,
      "step": 704400
    },
    {
      "epoch": 4.576309721004255,
      "grad_norm": 1.042130947113037,
      "learning_rate": 4.507415612936932e-06,
      "loss": 0.094,
      "step": 704500
    },
    {
      "epoch": 4.576959303647406,
      "grad_norm": 0.9857470393180847,
      "learning_rate": 4.50050515451094e-06,
      "loss": 0.0927,
      "step": 704600
    },
    {
      "epoch": 4.577608886290558,
      "grad_norm": 1.3465410470962524,
      "learning_rate": 4.493594696084949e-06,
      "loss": 0.0917,
      "step": 704700
    },
    {
      "epoch": 4.5782584689337105,
      "grad_norm": 1.3793545961380005,
      "learning_rate": 4.486684237658958e-06,
      "loss": 0.1,
      "step": 704800
    },
    {
      "epoch": 4.578908051576862,
      "grad_norm": 1.2872810363769531,
      "learning_rate": 4.479773779232967e-06,
      "loss": 0.0959,
      "step": 704900
    },
    {
      "epoch": 4.579557634220014,
      "grad_norm": 0.875090479850769,
      "learning_rate": 4.472863320806976e-06,
      "loss": 0.0975,
      "step": 705000
    },
    {
      "epoch": 4.580207216863165,
      "grad_norm": 1.458107590675354,
      "learning_rate": 4.465952862380985e-06,
      "loss": 0.0967,
      "step": 705100
    },
    {
      "epoch": 4.580856799506317,
      "grad_norm": 1.0848948955535889,
      "learning_rate": 4.459042403954994e-06,
      "loss": 0.0944,
      "step": 705200
    },
    {
      "epoch": 4.581506382149469,
      "grad_norm": 0.5766737461090088,
      "learning_rate": 4.452131945529003e-06,
      "loss": 0.0902,
      "step": 705300
    },
    {
      "epoch": 4.582155964792621,
      "grad_norm": 0.9665215611457825,
      "learning_rate": 4.445221487103012e-06,
      "loss": 0.0986,
      "step": 705400
    },
    {
      "epoch": 4.582805547435773,
      "grad_norm": 0.848518431186676,
      "learning_rate": 4.43831102867702e-06,
      "loss": 0.0912,
      "step": 705500
    },
    {
      "epoch": 4.583455130078924,
      "grad_norm": 1.0739774703979492,
      "learning_rate": 4.43140057025103e-06,
      "loss": 0.0926,
      "step": 705600
    },
    {
      "epoch": 4.584104712722076,
      "grad_norm": 1.091023564338684,
      "learning_rate": 4.424490111825039e-06,
      "loss": 0.0953,
      "step": 705700
    },
    {
      "epoch": 4.584754295365228,
      "grad_norm": 1.276658535003662,
      "learning_rate": 4.417579653399048e-06,
      "loss": 0.0931,
      "step": 705800
    },
    {
      "epoch": 4.585403878008379,
      "grad_norm": 0.8647196888923645,
      "learning_rate": 4.410669194973056e-06,
      "loss": 0.0965,
      "step": 705900
    },
    {
      "epoch": 4.586053460651532,
      "grad_norm": 1.1349989175796509,
      "learning_rate": 4.403758736547065e-06,
      "loss": 0.0935,
      "step": 706000
    },
    {
      "epoch": 4.586703043294683,
      "grad_norm": 0.9043565988540649,
      "learning_rate": 4.396848278121074e-06,
      "loss": 0.0938,
      "step": 706100
    },
    {
      "epoch": 4.587352625937835,
      "grad_norm": 1.0590107440948486,
      "learning_rate": 4.389937819695083e-06,
      "loss": 0.094,
      "step": 706200
    },
    {
      "epoch": 4.588002208580987,
      "grad_norm": 1.2735651731491089,
      "learning_rate": 4.383027361269092e-06,
      "loss": 0.0943,
      "step": 706300
    },
    {
      "epoch": 4.588651791224138,
      "grad_norm": 1.011149525642395,
      "learning_rate": 4.376116902843101e-06,
      "loss": 0.0945,
      "step": 706400
    },
    {
      "epoch": 4.58930137386729,
      "grad_norm": 1.702256441116333,
      "learning_rate": 4.36920644441711e-06,
      "loss": 0.0909,
      "step": 706500
    },
    {
      "epoch": 4.589950956510442,
      "grad_norm": 1.363856554031372,
      "learning_rate": 4.362295985991119e-06,
      "loss": 0.0927,
      "step": 706600
    },
    {
      "epoch": 4.590600539153594,
      "grad_norm": 0.9248530864715576,
      "learning_rate": 4.355385527565128e-06,
      "loss": 0.0981,
      "step": 706700
    },
    {
      "epoch": 4.591250121796746,
      "grad_norm": 1.630152702331543,
      "learning_rate": 4.348475069139136e-06,
      "loss": 0.0969,
      "step": 706800
    },
    {
      "epoch": 4.591899704439897,
      "grad_norm": 0.6551589965820312,
      "learning_rate": 4.341564610713145e-06,
      "loss": 0.0927,
      "step": 706900
    },
    {
      "epoch": 4.592549287083049,
      "grad_norm": 1.2833671569824219,
      "learning_rate": 4.334654152287155e-06,
      "loss": 0.1005,
      "step": 707000
    },
    {
      "epoch": 4.593198869726201,
      "grad_norm": 0.9302086234092712,
      "learning_rate": 4.327743693861164e-06,
      "loss": 0.0995,
      "step": 707100
    },
    {
      "epoch": 4.593848452369353,
      "grad_norm": 1.1040741205215454,
      "learning_rate": 4.320833235435172e-06,
      "loss": 0.0992,
      "step": 707200
    },
    {
      "epoch": 4.594498035012505,
      "grad_norm": 0.8905569911003113,
      "learning_rate": 4.313922777009181e-06,
      "loss": 0.0948,
      "step": 707300
    },
    {
      "epoch": 4.595147617655656,
      "grad_norm": 0.881422221660614,
      "learning_rate": 4.30701231858319e-06,
      "loss": 0.0909,
      "step": 707400
    },
    {
      "epoch": 4.595797200298808,
      "grad_norm": 1.3423906564712524,
      "learning_rate": 4.300101860157199e-06,
      "loss": 0.0924,
      "step": 707500
    },
    {
      "epoch": 4.5964467829419595,
      "grad_norm": 1.1676697731018066,
      "learning_rate": 4.293191401731208e-06,
      "loss": 0.0978,
      "step": 707600
    },
    {
      "epoch": 4.597096365585111,
      "grad_norm": 1.3800815343856812,
      "learning_rate": 4.286280943305217e-06,
      "loss": 0.0947,
      "step": 707700
    },
    {
      "epoch": 4.597745948228264,
      "grad_norm": 1.1943897008895874,
      "learning_rate": 4.279370484879226e-06,
      "loss": 0.1021,
      "step": 707800
    },
    {
      "epoch": 4.598395530871415,
      "grad_norm": 1.1050834655761719,
      "learning_rate": 4.272460026453235e-06,
      "loss": 0.0966,
      "step": 707900
    },
    {
      "epoch": 4.599045113514567,
      "grad_norm": 1.0576273202896118,
      "learning_rate": 4.265549568027244e-06,
      "loss": 0.0942,
      "step": 708000
    },
    {
      "epoch": 4.5996946961577185,
      "grad_norm": 1.4394437074661255,
      "learning_rate": 4.2586391096012524e-06,
      "loss": 0.0986,
      "step": 708100
    },
    {
      "epoch": 4.60034427880087,
      "grad_norm": 0.8967189192771912,
      "learning_rate": 4.2517286511752614e-06,
      "loss": 0.0973,
      "step": 708200
    },
    {
      "epoch": 4.600993861444023,
      "grad_norm": 1.1421973705291748,
      "learning_rate": 4.244818192749271e-06,
      "loss": 0.0939,
      "step": 708300
    },
    {
      "epoch": 4.601643444087174,
      "grad_norm": 1.256426215171814,
      "learning_rate": 4.23790773432328e-06,
      "loss": 0.0983,
      "step": 708400
    },
    {
      "epoch": 4.602293026730326,
      "grad_norm": 1.2279324531555176,
      "learning_rate": 4.2309972758972885e-06,
      "loss": 0.0978,
      "step": 708500
    },
    {
      "epoch": 4.6029426093734775,
      "grad_norm": 0.9027447700500488,
      "learning_rate": 4.2240868174712975e-06,
      "loss": 0.0933,
      "step": 708600
    },
    {
      "epoch": 4.603592192016629,
      "grad_norm": 1.0411953926086426,
      "learning_rate": 4.2171763590453065e-06,
      "loss": 0.0983,
      "step": 708700
    },
    {
      "epoch": 4.604241774659781,
      "grad_norm": 1.7965834140777588,
      "learning_rate": 4.2102659006193155e-06,
      "loss": 0.0995,
      "step": 708800
    },
    {
      "epoch": 4.604891357302932,
      "grad_norm": 0.7757323384284973,
      "learning_rate": 4.2033554421933245e-06,
      "loss": 0.0945,
      "step": 708900
    },
    {
      "epoch": 4.605540939946085,
      "grad_norm": 1.3337674140930176,
      "learning_rate": 4.1964449837673335e-06,
      "loss": 0.0956,
      "step": 709000
    },
    {
      "epoch": 4.606190522589237,
      "grad_norm": 1.0405354499816895,
      "learning_rate": 4.1895345253413425e-06,
      "loss": 0.0963,
      "step": 709100
    },
    {
      "epoch": 4.606840105232388,
      "grad_norm": 1.414090633392334,
      "learning_rate": 4.1826240669153515e-06,
      "loss": 0.0986,
      "step": 709200
    },
    {
      "epoch": 4.60748968787554,
      "grad_norm": 1.1440455913543701,
      "learning_rate": 4.1757136084893605e-06,
      "loss": 0.097,
      "step": 709300
    },
    {
      "epoch": 4.608139270518691,
      "grad_norm": 0.8145436644554138,
      "learning_rate": 4.168803150063369e-06,
      "loss": 0.0953,
      "step": 709400
    },
    {
      "epoch": 4.608788853161844,
      "grad_norm": 0.5381441712379456,
      "learning_rate": 4.161892691637378e-06,
      "loss": 0.0873,
      "step": 709500
    },
    {
      "epoch": 4.609438435804996,
      "grad_norm": 1.1912258863449097,
      "learning_rate": 4.154982233211387e-06,
      "loss": 0.0918,
      "step": 709600
    },
    {
      "epoch": 4.610088018448147,
      "grad_norm": 1.0539501905441284,
      "learning_rate": 4.1480717747853965e-06,
      "loss": 0.0922,
      "step": 709700
    },
    {
      "epoch": 4.610737601091299,
      "grad_norm": 0.5711049437522888,
      "learning_rate": 4.141161316359405e-06,
      "loss": 0.0955,
      "step": 709800
    },
    {
      "epoch": 4.6113871837344504,
      "grad_norm": 1.16175377368927,
      "learning_rate": 4.134250857933414e-06,
      "loss": 0.0949,
      "step": 709900
    },
    {
      "epoch": 4.612036766377602,
      "grad_norm": 1.2181602716445923,
      "learning_rate": 4.127340399507423e-06,
      "loss": 0.0963,
      "step": 710000
    },
    {
      "epoch": 4.612686349020754,
      "grad_norm": 1.492555856704712,
      "learning_rate": 4.120429941081432e-06,
      "loss": 0.1025,
      "step": 710100
    },
    {
      "epoch": 4.613335931663906,
      "grad_norm": 1.43411123752594,
      "learning_rate": 4.113519482655441e-06,
      "loss": 0.0936,
      "step": 710200
    },
    {
      "epoch": 4.613985514307058,
      "grad_norm": 1.2813509702682495,
      "learning_rate": 4.10660902422945e-06,
      "loss": 0.091,
      "step": 710300
    },
    {
      "epoch": 4.6146350969502095,
      "grad_norm": 0.6392526626586914,
      "learning_rate": 4.099698565803459e-06,
      "loss": 0.0978,
      "step": 710400
    },
    {
      "epoch": 4.615284679593361,
      "grad_norm": 1.3549522161483765,
      "learning_rate": 4.092788107377468e-06,
      "loss": 0.0986,
      "step": 710500
    },
    {
      "epoch": 4.615934262236513,
      "grad_norm": 1.4159022569656372,
      "learning_rate": 4.085877648951477e-06,
      "loss": 0.094,
      "step": 710600
    },
    {
      "epoch": 4.616583844879665,
      "grad_norm": 0.9995265603065491,
      "learning_rate": 4.078967190525485e-06,
      "loss": 0.0953,
      "step": 710700
    },
    {
      "epoch": 4.617233427522817,
      "grad_norm": 1.2025798559188843,
      "learning_rate": 4.072056732099494e-06,
      "loss": 0.0941,
      "step": 710800
    },
    {
      "epoch": 4.6178830101659685,
      "grad_norm": 0.7991068363189697,
      "learning_rate": 4.065146273673503e-06,
      "loss": 0.0878,
      "step": 710900
    },
    {
      "epoch": 4.61853259280912,
      "grad_norm": 1.4065226316452026,
      "learning_rate": 4.058235815247513e-06,
      "loss": 0.0935,
      "step": 711000
    },
    {
      "epoch": 4.619182175452272,
      "grad_norm": 1.3030930757522583,
      "learning_rate": 4.051325356821521e-06,
      "loss": 0.0944,
      "step": 711100
    },
    {
      "epoch": 4.619831758095423,
      "grad_norm": 0.8866491317749023,
      "learning_rate": 4.04441489839553e-06,
      "loss": 0.0928,
      "step": 711200
    },
    {
      "epoch": 4.620481340738575,
      "grad_norm": 1.2472456693649292,
      "learning_rate": 4.037504439969539e-06,
      "loss": 0.0896,
      "step": 711300
    },
    {
      "epoch": 4.6211309233817275,
      "grad_norm": 1.1870068311691284,
      "learning_rate": 4.030593981543548e-06,
      "loss": 0.0897,
      "step": 711400
    },
    {
      "epoch": 4.621780506024879,
      "grad_norm": 1.0484026670455933,
      "learning_rate": 4.023683523117557e-06,
      "loss": 0.0934,
      "step": 711500
    },
    {
      "epoch": 4.622430088668031,
      "grad_norm": 1.330542802810669,
      "learning_rate": 4.016773064691565e-06,
      "loss": 0.0928,
      "step": 711600
    },
    {
      "epoch": 4.623079671311182,
      "grad_norm": 0.8286606669425964,
      "learning_rate": 4.009862606265575e-06,
      "loss": 0.096,
      "step": 711700
    },
    {
      "epoch": 4.623729253954334,
      "grad_norm": 1.3561346530914307,
      "learning_rate": 4.002952147839584e-06,
      "loss": 0.0932,
      "step": 711800
    },
    {
      "epoch": 4.6243788365974865,
      "grad_norm": 1.1283618211746216,
      "learning_rate": 3.996041689413593e-06,
      "loss": 0.093,
      "step": 711900
    },
    {
      "epoch": 4.625028419240638,
      "grad_norm": 1.0376416444778442,
      "learning_rate": 3.989131230987601e-06,
      "loss": 0.0934,
      "step": 712000
    },
    {
      "epoch": 4.62567800188379,
      "grad_norm": 1.040267825126648,
      "learning_rate": 3.98222077256161e-06,
      "loss": 0.0917,
      "step": 712100
    },
    {
      "epoch": 4.626327584526941,
      "grad_norm": 1.2668228149414062,
      "learning_rate": 3.975310314135619e-06,
      "loss": 0.0984,
      "step": 712200
    },
    {
      "epoch": 4.626977167170093,
      "grad_norm": 1.5599048137664795,
      "learning_rate": 3.968399855709629e-06,
      "loss": 0.0983,
      "step": 712300
    },
    {
      "epoch": 4.627626749813245,
      "grad_norm": 1.0065635442733765,
      "learning_rate": 3.961489397283637e-06,
      "loss": 0.0944,
      "step": 712400
    },
    {
      "epoch": 4.628276332456397,
      "grad_norm": 1.0128458738327026,
      "learning_rate": 3.954578938857646e-06,
      "loss": 0.0917,
      "step": 712500
    },
    {
      "epoch": 4.628925915099549,
      "grad_norm": 1.1874172687530518,
      "learning_rate": 3.947668480431655e-06,
      "loss": 0.0921,
      "step": 712600
    },
    {
      "epoch": 4.6295754977427,
      "grad_norm": 1.0109152793884277,
      "learning_rate": 3.940758022005664e-06,
      "loss": 0.1004,
      "step": 712700
    },
    {
      "epoch": 4.630225080385852,
      "grad_norm": 1.1440985202789307,
      "learning_rate": 3.933847563579673e-06,
      "loss": 0.0883,
      "step": 712800
    },
    {
      "epoch": 4.630874663029004,
      "grad_norm": 1.019036054611206,
      "learning_rate": 3.926937105153681e-06,
      "loss": 0.0891,
      "step": 712900
    },
    {
      "epoch": 4.631524245672155,
      "grad_norm": 0.7815062999725342,
      "learning_rate": 3.920026646727691e-06,
      "loss": 0.0948,
      "step": 713000
    },
    {
      "epoch": 4.632173828315308,
      "grad_norm": 0.9638440012931824,
      "learning_rate": 3.9131161883017e-06,
      "loss": 0.1011,
      "step": 713100
    },
    {
      "epoch": 4.632823410958459,
      "grad_norm": 0.6943498849868774,
      "learning_rate": 3.906205729875709e-06,
      "loss": 0.0987,
      "step": 713200
    },
    {
      "epoch": 4.633472993601611,
      "grad_norm": 1.0541753768920898,
      "learning_rate": 3.899295271449717e-06,
      "loss": 0.0912,
      "step": 713300
    },
    {
      "epoch": 4.634122576244763,
      "grad_norm": 1.0643894672393799,
      "learning_rate": 3.892384813023726e-06,
      "loss": 0.0956,
      "step": 713400
    },
    {
      "epoch": 4.634772158887914,
      "grad_norm": 0.8075672388076782,
      "learning_rate": 3.885474354597735e-06,
      "loss": 0.0937,
      "step": 713500
    },
    {
      "epoch": 4.635421741531067,
      "grad_norm": 0.7125802636146545,
      "learning_rate": 3.878563896171744e-06,
      "loss": 0.1014,
      "step": 713600
    },
    {
      "epoch": 4.636071324174218,
      "grad_norm": 1.1752891540527344,
      "learning_rate": 3.871653437745753e-06,
      "loss": 0.0892,
      "step": 713700
    },
    {
      "epoch": 4.63672090681737,
      "grad_norm": 1.1520739793777466,
      "learning_rate": 3.864742979319762e-06,
      "loss": 0.0935,
      "step": 713800
    },
    {
      "epoch": 4.637370489460522,
      "grad_norm": 1.3785406351089478,
      "learning_rate": 3.857832520893771e-06,
      "loss": 0.0982,
      "step": 713900
    },
    {
      "epoch": 4.638020072103673,
      "grad_norm": 1.237931489944458,
      "learning_rate": 3.85092206246778e-06,
      "loss": 0.0953,
      "step": 714000
    },
    {
      "epoch": 4.638669654746825,
      "grad_norm": 1.0419045686721802,
      "learning_rate": 3.844011604041789e-06,
      "loss": 0.0979,
      "step": 714100
    },
    {
      "epoch": 4.6393192373899765,
      "grad_norm": 0.8495579957962036,
      "learning_rate": 3.8371011456157975e-06,
      "loss": 0.0916,
      "step": 714200
    },
    {
      "epoch": 4.639968820033129,
      "grad_norm": 1.8484277725219727,
      "learning_rate": 3.830190687189807e-06,
      "loss": 0.0965,
      "step": 714300
    },
    {
      "epoch": 4.640618402676281,
      "grad_norm": 0.9662638902664185,
      "learning_rate": 3.823280228763816e-06,
      "loss": 0.0908,
      "step": 714400
    },
    {
      "epoch": 4.641267985319432,
      "grad_norm": 0.5489895939826965,
      "learning_rate": 3.816369770337825e-06,
      "loss": 0.0995,
      "step": 714500
    },
    {
      "epoch": 4.641917567962584,
      "grad_norm": 1.3791885375976562,
      "learning_rate": 3.809459311911834e-06,
      "loss": 0.0974,
      "step": 714600
    },
    {
      "epoch": 4.6425671506057355,
      "grad_norm": 1.224909782409668,
      "learning_rate": 3.8025488534858425e-06,
      "loss": 0.0964,
      "step": 714700
    },
    {
      "epoch": 4.643216733248888,
      "grad_norm": 0.908527672290802,
      "learning_rate": 3.7956383950598515e-06,
      "loss": 0.0933,
      "step": 714800
    },
    {
      "epoch": 4.64386631589204,
      "grad_norm": 0.7582452893257141,
      "learning_rate": 3.78872793663386e-06,
      "loss": 0.0929,
      "step": 714900
    },
    {
      "epoch": 4.644515898535191,
      "grad_norm": 1.4700506925582886,
      "learning_rate": 3.78181747820787e-06,
      "loss": 0.0959,
      "step": 715000
    },
    {
      "epoch": 4.645165481178343,
      "grad_norm": 1.2182163000106812,
      "learning_rate": 3.7749070197818785e-06,
      "loss": 0.0982,
      "step": 715100
    },
    {
      "epoch": 4.645815063821495,
      "grad_norm": 0.9018182754516602,
      "learning_rate": 3.7679965613558875e-06,
      "loss": 0.0915,
      "step": 715200
    },
    {
      "epoch": 4.646464646464646,
      "grad_norm": 1.0948894023895264,
      "learning_rate": 3.761086102929896e-06,
      "loss": 0.0977,
      "step": 715300
    },
    {
      "epoch": 4.647114229107798,
      "grad_norm": 1.228317379951477,
      "learning_rate": 3.754175644503905e-06,
      "loss": 0.0952,
      "step": 715400
    },
    {
      "epoch": 4.64776381175095,
      "grad_norm": 1.176026701927185,
      "learning_rate": 3.747265186077914e-06,
      "loss": 0.0987,
      "step": 715500
    },
    {
      "epoch": 4.648413394394102,
      "grad_norm": 0.7343321442604065,
      "learning_rate": 3.7403547276519227e-06,
      "loss": 0.0993,
      "step": 715600
    },
    {
      "epoch": 4.649062977037254,
      "grad_norm": 1.0000046491622925,
      "learning_rate": 3.733444269225932e-06,
      "loss": 0.0946,
      "step": 715700
    },
    {
      "epoch": 4.649712559680405,
      "grad_norm": 0.855783224105835,
      "learning_rate": 3.726533810799941e-06,
      "loss": 0.0969,
      "step": 715800
    },
    {
      "epoch": 4.650362142323557,
      "grad_norm": 1.4510678052902222,
      "learning_rate": 3.71962335237395e-06,
      "loss": 0.0989,
      "step": 715900
    },
    {
      "epoch": 4.651011724966709,
      "grad_norm": 1.0440068244934082,
      "learning_rate": 3.7127128939479587e-06,
      "loss": 0.0889,
      "step": 716000
    },
    {
      "epoch": 4.651661307609861,
      "grad_norm": 0.8547024130821228,
      "learning_rate": 3.7058024355219677e-06,
      "loss": 0.1027,
      "step": 716100
    },
    {
      "epoch": 4.652310890253013,
      "grad_norm": 1.1641031503677368,
      "learning_rate": 3.6988919770959763e-06,
      "loss": 0.0991,
      "step": 716200
    },
    {
      "epoch": 4.652960472896164,
      "grad_norm": 1.3502966165542603,
      "learning_rate": 3.691981518669986e-06,
      "loss": 0.0851,
      "step": 716300
    },
    {
      "epoch": 4.653610055539316,
      "grad_norm": 1.445859432220459,
      "learning_rate": 3.6850710602439948e-06,
      "loss": 0.0966,
      "step": 716400
    },
    {
      "epoch": 4.6542596381824675,
      "grad_norm": 0.8720345497131348,
      "learning_rate": 3.6781606018180038e-06,
      "loss": 0.0943,
      "step": 716500
    },
    {
      "epoch": 4.654909220825619,
      "grad_norm": 0.9217386245727539,
      "learning_rate": 3.6712501433920123e-06,
      "loss": 0.0907,
      "step": 716600
    },
    {
      "epoch": 4.655558803468772,
      "grad_norm": 1.6194902658462524,
      "learning_rate": 3.6643396849660213e-06,
      "loss": 0.099,
      "step": 716700
    },
    {
      "epoch": 4.656208386111923,
      "grad_norm": 1.035373568534851,
      "learning_rate": 3.6574292265400304e-06,
      "loss": 0.0932,
      "step": 716800
    },
    {
      "epoch": 4.656857968755075,
      "grad_norm": 1.5302411317825317,
      "learning_rate": 3.650518768114039e-06,
      "loss": 0.0964,
      "step": 716900
    },
    {
      "epoch": 4.6575075513982265,
      "grad_norm": 1.0437089204788208,
      "learning_rate": 3.6436083096880484e-06,
      "loss": 0.091,
      "step": 717000
    },
    {
      "epoch": 4.658157134041378,
      "grad_norm": 1.2281839847564697,
      "learning_rate": 3.6366978512620574e-06,
      "loss": 0.0978,
      "step": 717100
    },
    {
      "epoch": 4.658806716684531,
      "grad_norm": 0.9970833659172058,
      "learning_rate": 3.6297873928360664e-06,
      "loss": 0.0959,
      "step": 717200
    },
    {
      "epoch": 4.659456299327682,
      "grad_norm": 1.2039116621017456,
      "learning_rate": 3.622876934410075e-06,
      "loss": 0.0983,
      "step": 717300
    },
    {
      "epoch": 4.660105881970834,
      "grad_norm": 0.6134418249130249,
      "learning_rate": 3.615966475984084e-06,
      "loss": 0.0976,
      "step": 717400
    },
    {
      "epoch": 4.6607554646139855,
      "grad_norm": 1.0482832193374634,
      "learning_rate": 3.6090560175580925e-06,
      "loss": 0.0999,
      "step": 717500
    },
    {
      "epoch": 4.661405047257137,
      "grad_norm": 0.9247840642929077,
      "learning_rate": 3.6021455591321015e-06,
      "loss": 0.0949,
      "step": 717600
    },
    {
      "epoch": 4.662054629900289,
      "grad_norm": 1.4281611442565918,
      "learning_rate": 3.595235100706111e-06,
      "loss": 0.0899,
      "step": 717700
    },
    {
      "epoch": 4.66270421254344,
      "grad_norm": 1.391161322593689,
      "learning_rate": 3.58832464228012e-06,
      "loss": 0.0915,
      "step": 717800
    },
    {
      "epoch": 4.663353795186593,
      "grad_norm": 1.25870943069458,
      "learning_rate": 3.5814141838541286e-06,
      "loss": 0.103,
      "step": 717900
    },
    {
      "epoch": 4.6640033778297445,
      "grad_norm": 1.3726023435592651,
      "learning_rate": 3.5745037254281376e-06,
      "loss": 0.092,
      "step": 718000
    },
    {
      "epoch": 4.664652960472896,
      "grad_norm": 1.3023746013641357,
      "learning_rate": 3.5675932670021466e-06,
      "loss": 0.0995,
      "step": 718100
    },
    {
      "epoch": 4.665302543116048,
      "grad_norm": 0.8882494568824768,
      "learning_rate": 3.560682808576155e-06,
      "loss": 0.0992,
      "step": 718200
    },
    {
      "epoch": 4.665952125759199,
      "grad_norm": 0.8738191723823547,
      "learning_rate": 3.5537723501501646e-06,
      "loss": 0.0886,
      "step": 718300
    },
    {
      "epoch": 4.666601708402352,
      "grad_norm": 0.9018057584762573,
      "learning_rate": 3.5468618917241736e-06,
      "loss": 0.0952,
      "step": 718400
    },
    {
      "epoch": 4.6672512910455035,
      "grad_norm": 0.8232229351997375,
      "learning_rate": 3.5399514332981826e-06,
      "loss": 0.0886,
      "step": 718500
    },
    {
      "epoch": 4.667900873688655,
      "grad_norm": 0.783628523349762,
      "learning_rate": 3.533040974872191e-06,
      "loss": 0.099,
      "step": 718600
    },
    {
      "epoch": 4.668550456331807,
      "grad_norm": 0.8600204586982727,
      "learning_rate": 3.5261305164462e-06,
      "loss": 0.0964,
      "step": 718700
    },
    {
      "epoch": 4.669200038974958,
      "grad_norm": 1.2386753559112549,
      "learning_rate": 3.5192200580202088e-06,
      "loss": 0.0936,
      "step": 718800
    },
    {
      "epoch": 4.66984962161811,
      "grad_norm": 0.9593948125839233,
      "learning_rate": 3.5123095995942178e-06,
      "loss": 0.1,
      "step": 718900
    },
    {
      "epoch": 4.6704992042612625,
      "grad_norm": 0.8917960524559021,
      "learning_rate": 3.505399141168227e-06,
      "loss": 0.0976,
      "step": 719000
    },
    {
      "epoch": 4.671148786904414,
      "grad_norm": 1.6743677854537964,
      "learning_rate": 3.498488682742236e-06,
      "loss": 0.0944,
      "step": 719100
    },
    {
      "epoch": 4.671798369547566,
      "grad_norm": 0.9166469573974609,
      "learning_rate": 3.4915782243162448e-06,
      "loss": 0.0898,
      "step": 719200
    },
    {
      "epoch": 4.672447952190717,
      "grad_norm": 0.788042426109314,
      "learning_rate": 3.4846677658902538e-06,
      "loss": 0.097,
      "step": 719300
    },
    {
      "epoch": 4.673097534833869,
      "grad_norm": 1.4958338737487793,
      "learning_rate": 3.4777573074642628e-06,
      "loss": 0.0918,
      "step": 719400
    },
    {
      "epoch": 4.673747117477021,
      "grad_norm": 1.23654043674469,
      "learning_rate": 3.4708468490382714e-06,
      "loss": 0.0993,
      "step": 719500
    },
    {
      "epoch": 4.674396700120173,
      "grad_norm": 1.2202500104904175,
      "learning_rate": 3.4639363906122804e-06,
      "loss": 0.0961,
      "step": 719600
    },
    {
      "epoch": 4.675046282763325,
      "grad_norm": 1.9570300579071045,
      "learning_rate": 3.45702593218629e-06,
      "loss": 0.0937,
      "step": 719700
    },
    {
      "epoch": 4.675695865406476,
      "grad_norm": 1.130887746810913,
      "learning_rate": 3.450115473760299e-06,
      "loss": 0.1029,
      "step": 719800
    },
    {
      "epoch": 4.676345448049628,
      "grad_norm": 1.3104698657989502,
      "learning_rate": 3.4432050153343074e-06,
      "loss": 0.1062,
      "step": 719900
    },
    {
      "epoch": 4.67699503069278,
      "grad_norm": 1.1128129959106445,
      "learning_rate": 3.4362945569083164e-06,
      "loss": 0.0898,
      "step": 720000
    },
    {
      "epoch": 4.677644613335931,
      "grad_norm": 1.0617002248764038,
      "learning_rate": 3.429384098482325e-06,
      "loss": 0.097,
      "step": 720100
    },
    {
      "epoch": 4.678294195979084,
      "grad_norm": 1.1587680578231812,
      "learning_rate": 3.422473640056334e-06,
      "loss": 0.0991,
      "step": 720200
    },
    {
      "epoch": 4.678943778622235,
      "grad_norm": 1.3587040901184082,
      "learning_rate": 3.4155631816303434e-06,
      "loss": 0.0992,
      "step": 720300
    },
    {
      "epoch": 4.679593361265387,
      "grad_norm": 0.8591988682746887,
      "learning_rate": 3.4086527232043524e-06,
      "loss": 0.0974,
      "step": 720400
    },
    {
      "epoch": 4.680242943908539,
      "grad_norm": 0.7342727184295654,
      "learning_rate": 3.401742264778361e-06,
      "loss": 0.0981,
      "step": 720500
    },
    {
      "epoch": 4.68089252655169,
      "grad_norm": 0.9824767112731934,
      "learning_rate": 3.39483180635237e-06,
      "loss": 0.1005,
      "step": 720600
    },
    {
      "epoch": 4.681542109194842,
      "grad_norm": 0.9244788289070129,
      "learning_rate": 3.387921347926379e-06,
      "loss": 0.0996,
      "step": 720700
    },
    {
      "epoch": 4.682191691837994,
      "grad_norm": 0.9282527565956116,
      "learning_rate": 3.3810108895003876e-06,
      "loss": 0.0983,
      "step": 720800
    },
    {
      "epoch": 4.682841274481146,
      "grad_norm": 0.7731322646141052,
      "learning_rate": 3.3741004310743966e-06,
      "loss": 0.0949,
      "step": 720900
    },
    {
      "epoch": 4.683490857124298,
      "grad_norm": 1.145416498184204,
      "learning_rate": 3.367189972648406e-06,
      "loss": 0.0915,
      "step": 721000
    },
    {
      "epoch": 4.684140439767449,
      "grad_norm": 1.024747371673584,
      "learning_rate": 3.360279514222415e-06,
      "loss": 0.097,
      "step": 721100
    },
    {
      "epoch": 4.684790022410601,
      "grad_norm": 1.4761505126953125,
      "learning_rate": 3.3533690557964236e-06,
      "loss": 0.097,
      "step": 721200
    },
    {
      "epoch": 4.6854396050537535,
      "grad_norm": 1.2647850513458252,
      "learning_rate": 3.3464585973704326e-06,
      "loss": 0.0924,
      "step": 721300
    },
    {
      "epoch": 4.686089187696905,
      "grad_norm": 1.163499116897583,
      "learning_rate": 3.339548138944441e-06,
      "loss": 0.0933,
      "step": 721400
    },
    {
      "epoch": 4.686738770340057,
      "grad_norm": 0.8544826507568359,
      "learning_rate": 3.33263768051845e-06,
      "loss": 0.0932,
      "step": 721500
    },
    {
      "epoch": 4.687388352983208,
      "grad_norm": 0.8299188613891602,
      "learning_rate": 3.325727222092459e-06,
      "loss": 0.0956,
      "step": 721600
    },
    {
      "epoch": 4.68803793562636,
      "grad_norm": 1.081471562385559,
      "learning_rate": 3.3188167636664686e-06,
      "loss": 0.0941,
      "step": 721700
    },
    {
      "epoch": 4.688687518269512,
      "grad_norm": 0.5132495760917664,
      "learning_rate": 3.311906305240477e-06,
      "loss": 0.1003,
      "step": 721800
    },
    {
      "epoch": 4.689337100912663,
      "grad_norm": 1.5298207998275757,
      "learning_rate": 3.3049958468144862e-06,
      "loss": 0.0957,
      "step": 721900
    },
    {
      "epoch": 4.689986683555816,
      "grad_norm": 0.796849250793457,
      "learning_rate": 3.2980853883884952e-06,
      "loss": 0.0998,
      "step": 722000
    },
    {
      "epoch": 4.690636266198967,
      "grad_norm": 1.3704338073730469,
      "learning_rate": 3.291174929962504e-06,
      "loss": 0.0988,
      "step": 722100
    },
    {
      "epoch": 4.691285848842119,
      "grad_norm": 1.4769623279571533,
      "learning_rate": 3.284264471536513e-06,
      "loss": 0.0913,
      "step": 722200
    },
    {
      "epoch": 4.691935431485271,
      "grad_norm": 1.3324588537216187,
      "learning_rate": 3.2773540131105214e-06,
      "loss": 0.0925,
      "step": 722300
    },
    {
      "epoch": 4.692585014128422,
      "grad_norm": 1.2056092023849487,
      "learning_rate": 3.2704435546845312e-06,
      "loss": 0.1025,
      "step": 722400
    },
    {
      "epoch": 4.693234596771575,
      "grad_norm": 1.385273814201355,
      "learning_rate": 3.26353309625854e-06,
      "loss": 0.0959,
      "step": 722500
    },
    {
      "epoch": 4.693884179414726,
      "grad_norm": 1.082129955291748,
      "learning_rate": 3.256622637832549e-06,
      "loss": 0.0876,
      "step": 722600
    },
    {
      "epoch": 4.694533762057878,
      "grad_norm": 1.4102994203567505,
      "learning_rate": 3.2497121794065574e-06,
      "loss": 0.1005,
      "step": 722700
    },
    {
      "epoch": 4.69518334470103,
      "grad_norm": 0.5857559442520142,
      "learning_rate": 3.2428017209805664e-06,
      "loss": 0.0916,
      "step": 722800
    },
    {
      "epoch": 4.695832927344181,
      "grad_norm": 1.3490605354309082,
      "learning_rate": 3.2358912625545754e-06,
      "loss": 0.1003,
      "step": 722900
    },
    {
      "epoch": 4.696482509987333,
      "grad_norm": 1.6030884981155396,
      "learning_rate": 3.228980804128585e-06,
      "loss": 0.1004,
      "step": 723000
    },
    {
      "epoch": 4.6971320926304845,
      "grad_norm": 0.7657061219215393,
      "learning_rate": 3.2220703457025934e-06,
      "loss": 0.0958,
      "step": 723100
    },
    {
      "epoch": 4.697781675273637,
      "grad_norm": 1.0814595222473145,
      "learning_rate": 3.2151598872766024e-06,
      "loss": 0.0956,
      "step": 723200
    },
    {
      "epoch": 4.698431257916789,
      "grad_norm": 1.1366310119628906,
      "learning_rate": 3.2082494288506114e-06,
      "loss": 0.0993,
      "step": 723300
    },
    {
      "epoch": 4.69908084055994,
      "grad_norm": 0.9262809753417969,
      "learning_rate": 3.20133897042462e-06,
      "loss": 0.094,
      "step": 723400
    },
    {
      "epoch": 4.699730423203092,
      "grad_norm": 1.283385157585144,
      "learning_rate": 3.194428511998629e-06,
      "loss": 0.0879,
      "step": 723500
    },
    {
      "epoch": 4.7003800058462435,
      "grad_norm": 1.0026991367340088,
      "learning_rate": 3.1875180535726376e-06,
      "loss": 0.0966,
      "step": 723600
    },
    {
      "epoch": 4.701029588489396,
      "grad_norm": 1.1991788148880005,
      "learning_rate": 3.1806075951466475e-06,
      "loss": 0.0966,
      "step": 723700
    },
    {
      "epoch": 4.701679171132548,
      "grad_norm": 1.0322415828704834,
      "learning_rate": 3.173697136720656e-06,
      "loss": 0.0932,
      "step": 723800
    },
    {
      "epoch": 4.702328753775699,
      "grad_norm": 1.4670898914337158,
      "learning_rate": 3.166786678294665e-06,
      "loss": 0.0966,
      "step": 723900
    },
    {
      "epoch": 4.702978336418851,
      "grad_norm": 1.2006089687347412,
      "learning_rate": 3.1598762198686736e-06,
      "loss": 0.1042,
      "step": 724000
    },
    {
      "epoch": 4.7036279190620025,
      "grad_norm": 2.0410945415496826,
      "learning_rate": 3.1529657614426826e-06,
      "loss": 0.0932,
      "step": 724100
    },
    {
      "epoch": 4.704277501705154,
      "grad_norm": 0.9907563924789429,
      "learning_rate": 3.1460553030166916e-06,
      "loss": 0.0962,
      "step": 724200
    },
    {
      "epoch": 4.704927084348306,
      "grad_norm": 1.0552598237991333,
      "learning_rate": 3.1391448445907002e-06,
      "loss": 0.0883,
      "step": 724300
    },
    {
      "epoch": 4.705576666991458,
      "grad_norm": 1.262869119644165,
      "learning_rate": 3.1322343861647096e-06,
      "loss": 0.0946,
      "step": 724400
    },
    {
      "epoch": 4.70622624963461,
      "grad_norm": 1.1128919124603271,
      "learning_rate": 3.1253239277387186e-06,
      "loss": 0.0979,
      "step": 724500
    },
    {
      "epoch": 4.7068758322777615,
      "grad_norm": 0.955475389957428,
      "learning_rate": 3.1184134693127277e-06,
      "loss": 0.097,
      "step": 724600
    },
    {
      "epoch": 4.707525414920913,
      "grad_norm": 1.0640052556991577,
      "learning_rate": 3.1115030108867362e-06,
      "loss": 0.0929,
      "step": 724700
    },
    {
      "epoch": 4.708174997564065,
      "grad_norm": 1.546982765197754,
      "learning_rate": 3.1045925524607452e-06,
      "loss": 0.0933,
      "step": 724800
    },
    {
      "epoch": 4.708824580207217,
      "grad_norm": 0.9693741798400879,
      "learning_rate": 3.0976820940347542e-06,
      "loss": 0.101,
      "step": 724900
    },
    {
      "epoch": 4.709474162850369,
      "grad_norm": 0.8291673064231873,
      "learning_rate": 3.0907716356087633e-06,
      "loss": 0.098,
      "step": 725000
    },
    {
      "epoch": 4.7101237454935205,
      "grad_norm": 0.7718383073806763,
      "learning_rate": 3.083861177182772e-06,
      "loss": 0.0992,
      "step": 725100
    },
    {
      "epoch": 4.710773328136672,
      "grad_norm": 0.7922889590263367,
      "learning_rate": 3.0769507187567813e-06,
      "loss": 0.0962,
      "step": 725200
    },
    {
      "epoch": 4.711422910779824,
      "grad_norm": 1.116481065750122,
      "learning_rate": 3.07004026033079e-06,
      "loss": 0.0916,
      "step": 725300
    },
    {
      "epoch": 4.712072493422975,
      "grad_norm": 1.1578865051269531,
      "learning_rate": 3.063129801904799e-06,
      "loss": 0.0967,
      "step": 725400
    },
    {
      "epoch": 4.712722076066127,
      "grad_norm": 0.7229180932044983,
      "learning_rate": 3.056219343478808e-06,
      "loss": 0.0969,
      "step": 725500
    },
    {
      "epoch": 4.7133716587092795,
      "grad_norm": 1.2784514427185059,
      "learning_rate": 3.049308885052817e-06,
      "loss": 0.0944,
      "step": 725600
    },
    {
      "epoch": 4.714021241352431,
      "grad_norm": 0.8282673954963684,
      "learning_rate": 3.042398426626826e-06,
      "loss": 0.0971,
      "step": 725700
    },
    {
      "epoch": 4.714670823995583,
      "grad_norm": 1.186413049697876,
      "learning_rate": 3.0354879682008344e-06,
      "loss": 0.093,
      "step": 725800
    },
    {
      "epoch": 4.715320406638734,
      "grad_norm": 1.022216796875,
      "learning_rate": 3.028577509774844e-06,
      "loss": 0.0924,
      "step": 725900
    },
    {
      "epoch": 4.715969989281886,
      "grad_norm": 0.9616873860359192,
      "learning_rate": 3.0216670513488525e-06,
      "loss": 0.0945,
      "step": 726000
    },
    {
      "epoch": 4.716619571925039,
      "grad_norm": 1.5451802015304565,
      "learning_rate": 3.0147565929228615e-06,
      "loss": 0.0926,
      "step": 726100
    },
    {
      "epoch": 4.71726915456819,
      "grad_norm": 0.6438581943511963,
      "learning_rate": 3.0078461344968705e-06,
      "loss": 0.0942,
      "step": 726200
    },
    {
      "epoch": 4.717918737211342,
      "grad_norm": 0.9394086599349976,
      "learning_rate": 3.0009356760708795e-06,
      "loss": 0.0948,
      "step": 726300
    },
    {
      "epoch": 4.718568319854493,
      "grad_norm": 0.8605859875679016,
      "learning_rate": 2.994025217644888e-06,
      "loss": 0.0901,
      "step": 726400
    },
    {
      "epoch": 4.719217902497645,
      "grad_norm": 1.0612680912017822,
      "learning_rate": 2.9871147592188975e-06,
      "loss": 0.0969,
      "step": 726500
    },
    {
      "epoch": 4.719867485140797,
      "grad_norm": 1.197866678237915,
      "learning_rate": 2.980204300792906e-06,
      "loss": 0.099,
      "step": 726600
    },
    {
      "epoch": 4.720517067783949,
      "grad_norm": 0.9479207396507263,
      "learning_rate": 2.973293842366915e-06,
      "loss": 0.101,
      "step": 726700
    },
    {
      "epoch": 4.721166650427101,
      "grad_norm": 0.7935817241668701,
      "learning_rate": 2.9663833839409236e-06,
      "loss": 0.0926,
      "step": 726800
    },
    {
      "epoch": 4.7218162330702524,
      "grad_norm": 1.2190587520599365,
      "learning_rate": 2.959472925514933e-06,
      "loss": 0.0966,
      "step": 726900
    },
    {
      "epoch": 4.722465815713404,
      "grad_norm": 1.057141900062561,
      "learning_rate": 2.952562467088942e-06,
      "loss": 0.0966,
      "step": 727000
    },
    {
      "epoch": 4.723115398356556,
      "grad_norm": 1.2472673654556274,
      "learning_rate": 2.9456520086629507e-06,
      "loss": 0.0976,
      "step": 727100
    },
    {
      "epoch": 4.723764980999707,
      "grad_norm": 0.7050833106040955,
      "learning_rate": 2.93874155023696e-06,
      "loss": 0.0895,
      "step": 727200
    },
    {
      "epoch": 4.72441456364286,
      "grad_norm": 1.1672247648239136,
      "learning_rate": 2.9318310918109687e-06,
      "loss": 0.0954,
      "step": 727300
    },
    {
      "epoch": 4.7250641462860115,
      "grad_norm": 1.1759835481643677,
      "learning_rate": 2.9249206333849777e-06,
      "loss": 0.0858,
      "step": 727400
    },
    {
      "epoch": 4.725713728929163,
      "grad_norm": 1.4468741416931152,
      "learning_rate": 2.9180101749589867e-06,
      "loss": 0.0901,
      "step": 727500
    },
    {
      "epoch": 4.726363311572315,
      "grad_norm": 1.0018305778503418,
      "learning_rate": 2.9110997165329957e-06,
      "loss": 0.0933,
      "step": 727600
    },
    {
      "epoch": 4.727012894215466,
      "grad_norm": 0.9944208860397339,
      "learning_rate": 2.9041892581070043e-06,
      "loss": 0.0937,
      "step": 727700
    },
    {
      "epoch": 4.727662476858618,
      "grad_norm": 0.8863462805747986,
      "learning_rate": 2.8972787996810133e-06,
      "loss": 0.0977,
      "step": 727800
    },
    {
      "epoch": 4.7283120595017705,
      "grad_norm": 1.0108503103256226,
      "learning_rate": 2.8903683412550223e-06,
      "loss": 0.0894,
      "step": 727900
    },
    {
      "epoch": 4.728961642144922,
      "grad_norm": 1.0393415689468384,
      "learning_rate": 2.8834578828290313e-06,
      "loss": 0.1008,
      "step": 728000
    },
    {
      "epoch": 4.729611224788074,
      "grad_norm": 2.0146255493164062,
      "learning_rate": 2.87654742440304e-06,
      "loss": 0.0964,
      "step": 728100
    },
    {
      "epoch": 4.730260807431225,
      "grad_norm": 1.0365151166915894,
      "learning_rate": 2.8696369659770493e-06,
      "loss": 0.0911,
      "step": 728200
    },
    {
      "epoch": 4.730910390074377,
      "grad_norm": 0.7848278284072876,
      "learning_rate": 2.862726507551058e-06,
      "loss": 0.09,
      "step": 728300
    },
    {
      "epoch": 4.731559972717529,
      "grad_norm": 1.7646455764770508,
      "learning_rate": 2.855816049125067e-06,
      "loss": 0.0969,
      "step": 728400
    },
    {
      "epoch": 4.732209555360681,
      "grad_norm": 0.793849527835846,
      "learning_rate": 2.848905590699076e-06,
      "loss": 0.0989,
      "step": 728500
    },
    {
      "epoch": 4.732859138003833,
      "grad_norm": 1.0689905881881714,
      "learning_rate": 2.841995132273085e-06,
      "loss": 0.0961,
      "step": 728600
    },
    {
      "epoch": 4.733508720646984,
      "grad_norm": 1.1481380462646484,
      "learning_rate": 2.835084673847094e-06,
      "loss": 0.0942,
      "step": 728700
    },
    {
      "epoch": 4.734158303290136,
      "grad_norm": 1.3663568496704102,
      "learning_rate": 2.8281742154211025e-06,
      "loss": 0.0953,
      "step": 728800
    },
    {
      "epoch": 4.734807885933288,
      "grad_norm": 0.7965823411941528,
      "learning_rate": 2.821263756995112e-06,
      "loss": 0.0942,
      "step": 728900
    },
    {
      "epoch": 4.73545746857644,
      "grad_norm": 0.7849305868148804,
      "learning_rate": 2.8143532985691205e-06,
      "loss": 0.0953,
      "step": 729000
    },
    {
      "epoch": 4.736107051219592,
      "grad_norm": 0.899448812007904,
      "learning_rate": 2.8074428401431295e-06,
      "loss": 0.096,
      "step": 729100
    },
    {
      "epoch": 4.736756633862743,
      "grad_norm": 1.4839028120040894,
      "learning_rate": 2.8005323817171385e-06,
      "loss": 0.0993,
      "step": 729200
    },
    {
      "epoch": 4.737406216505895,
      "grad_norm": 0.750991702079773,
      "learning_rate": 2.7936219232911475e-06,
      "loss": 0.0983,
      "step": 729300
    },
    {
      "epoch": 4.738055799149047,
      "grad_norm": 1.1102988719940186,
      "learning_rate": 2.786711464865156e-06,
      "loss": 0.0947,
      "step": 729400
    },
    {
      "epoch": 4.738705381792198,
      "grad_norm": 1.2111765146255493,
      "learning_rate": 2.7798010064391655e-06,
      "loss": 0.0938,
      "step": 729500
    },
    {
      "epoch": 4.73935496443535,
      "grad_norm": 1.2810672521591187,
      "learning_rate": 2.772890548013174e-06,
      "loss": 0.0896,
      "step": 729600
    },
    {
      "epoch": 4.740004547078502,
      "grad_norm": 1.2759071588516235,
      "learning_rate": 2.765980089587183e-06,
      "loss": 0.0945,
      "step": 729700
    },
    {
      "epoch": 4.740654129721654,
      "grad_norm": 0.6918911933898926,
      "learning_rate": 2.759069631161192e-06,
      "loss": 0.0916,
      "step": 729800
    },
    {
      "epoch": 4.741303712364806,
      "grad_norm": 0.6480728387832642,
      "learning_rate": 2.752159172735201e-06,
      "loss": 0.0971,
      "step": 729900
    },
    {
      "epoch": 4.741953295007957,
      "grad_norm": 0.8264232873916626,
      "learning_rate": 2.74524871430921e-06,
      "loss": 0.0882,
      "step": 730000
    },
    {
      "epoch": 4.742602877651109,
      "grad_norm": 1.4163457155227661,
      "learning_rate": 2.7383382558832187e-06,
      "loss": 0.0965,
      "step": 730100
    },
    {
      "epoch": 4.743252460294261,
      "grad_norm": 1.0914807319641113,
      "learning_rate": 2.731427797457228e-06,
      "loss": 0.0884,
      "step": 730200
    },
    {
      "epoch": 4.743902042937413,
      "grad_norm": 0.9169358015060425,
      "learning_rate": 2.7245173390312367e-06,
      "loss": 0.0986,
      "step": 730300
    },
    {
      "epoch": 4.744551625580565,
      "grad_norm": 1.1666502952575684,
      "learning_rate": 2.7176068806052457e-06,
      "loss": 0.0944,
      "step": 730400
    },
    {
      "epoch": 4.745201208223716,
      "grad_norm": 1.4056750535964966,
      "learning_rate": 2.7106964221792547e-06,
      "loss": 0.089,
      "step": 730500
    },
    {
      "epoch": 4.745850790866868,
      "grad_norm": 0.8686571717262268,
      "learning_rate": 2.7037859637532637e-06,
      "loss": 0.093,
      "step": 730600
    },
    {
      "epoch": 4.7465003735100195,
      "grad_norm": 0.9426587224006653,
      "learning_rate": 2.6968755053272723e-06,
      "loss": 0.1012,
      "step": 730700
    },
    {
      "epoch": 4.747149956153171,
      "grad_norm": 1.2275396585464478,
      "learning_rate": 2.6899650469012813e-06,
      "loss": 0.0918,
      "step": 730800
    },
    {
      "epoch": 4.747799538796324,
      "grad_norm": 1.2041023969650269,
      "learning_rate": 2.6830545884752903e-06,
      "loss": 0.0965,
      "step": 730900
    },
    {
      "epoch": 4.748449121439475,
      "grad_norm": 1.5844370126724243,
      "learning_rate": 2.6761441300492993e-06,
      "loss": 0.0959,
      "step": 731000
    },
    {
      "epoch": 4.749098704082627,
      "grad_norm": 1.1021442413330078,
      "learning_rate": 2.6692336716233083e-06,
      "loss": 0.0931,
      "step": 731100
    },
    {
      "epoch": 4.7497482867257785,
      "grad_norm": 0.8405254483222961,
      "learning_rate": 2.6623232131973173e-06,
      "loss": 0.0917,
      "step": 731200
    },
    {
      "epoch": 4.75039786936893,
      "grad_norm": 0.9503516554832458,
      "learning_rate": 2.6554127547713263e-06,
      "loss": 0.102,
      "step": 731300
    },
    {
      "epoch": 4.751047452012083,
      "grad_norm": 1.3492363691329956,
      "learning_rate": 2.648502296345335e-06,
      "loss": 0.0899,
      "step": 731400
    },
    {
      "epoch": 4.751697034655234,
      "grad_norm": 1.9161937236785889,
      "learning_rate": 2.6415918379193443e-06,
      "loss": 0.0899,
      "step": 731500
    },
    {
      "epoch": 4.752346617298386,
      "grad_norm": 1.183591604232788,
      "learning_rate": 2.634681379493353e-06,
      "loss": 0.0943,
      "step": 731600
    },
    {
      "epoch": 4.7529961999415375,
      "grad_norm": 0.9938146471977234,
      "learning_rate": 2.627770921067362e-06,
      "loss": 0.0939,
      "step": 731700
    },
    {
      "epoch": 4.753645782584689,
      "grad_norm": 1.203187108039856,
      "learning_rate": 2.6208604626413705e-06,
      "loss": 0.0918,
      "step": 731800
    },
    {
      "epoch": 4.754295365227841,
      "grad_norm": 1.3121051788330078,
      "learning_rate": 2.61395000421538e-06,
      "loss": 0.0953,
      "step": 731900
    },
    {
      "epoch": 4.754944947870992,
      "grad_norm": 0.5025312900543213,
      "learning_rate": 2.6070395457893885e-06,
      "loss": 0.0975,
      "step": 732000
    },
    {
      "epoch": 4.755594530514145,
      "grad_norm": 1.0021666288375854,
      "learning_rate": 2.6001290873633975e-06,
      "loss": 0.0975,
      "step": 732100
    },
    {
      "epoch": 4.756244113157297,
      "grad_norm": 0.902342677116394,
      "learning_rate": 2.5932186289374065e-06,
      "loss": 0.0905,
      "step": 732200
    },
    {
      "epoch": 4.756893695800448,
      "grad_norm": 1.258109450340271,
      "learning_rate": 2.5863081705114155e-06,
      "loss": 0.0912,
      "step": 732300
    },
    {
      "epoch": 4.7575432784436,
      "grad_norm": 1.0522282123565674,
      "learning_rate": 2.5793977120854245e-06,
      "loss": 0.0877,
      "step": 732400
    },
    {
      "epoch": 4.758192861086751,
      "grad_norm": 1.3372502326965332,
      "learning_rate": 2.5724872536594335e-06,
      "loss": 0.0984,
      "step": 732500
    },
    {
      "epoch": 4.758842443729904,
      "grad_norm": 0.6622630953788757,
      "learning_rate": 2.5655767952334425e-06,
      "loss": 0.097,
      "step": 732600
    },
    {
      "epoch": 4.759492026373056,
      "grad_norm": 0.9312435388565063,
      "learning_rate": 2.558666336807451e-06,
      "loss": 0.094,
      "step": 732700
    },
    {
      "epoch": 4.760141609016207,
      "grad_norm": 0.882922351360321,
      "learning_rate": 2.55175587838146e-06,
      "loss": 0.0951,
      "step": 732800
    },
    {
      "epoch": 4.760791191659359,
      "grad_norm": 1.1364502906799316,
      "learning_rate": 2.544845419955469e-06,
      "loss": 0.0991,
      "step": 732900
    },
    {
      "epoch": 4.7614407743025104,
      "grad_norm": 0.8207071423530579,
      "learning_rate": 2.537934961529478e-06,
      "loss": 0.0941,
      "step": 733000
    },
    {
      "epoch": 4.762090356945662,
      "grad_norm": 1.3184329271316528,
      "learning_rate": 2.5310245031034867e-06,
      "loss": 0.0981,
      "step": 733100
    },
    {
      "epoch": 4.762739939588814,
      "grad_norm": 1.1025784015655518,
      "learning_rate": 2.524114044677496e-06,
      "loss": 0.0903,
      "step": 733200
    },
    {
      "epoch": 4.763389522231966,
      "grad_norm": 1.2168101072311401,
      "learning_rate": 2.5172035862515047e-06,
      "loss": 0.0963,
      "step": 733300
    },
    {
      "epoch": 4.764039104875118,
      "grad_norm": 1.4794968366622925,
      "learning_rate": 2.5102931278255137e-06,
      "loss": 0.0963,
      "step": 733400
    },
    {
      "epoch": 4.7646886875182695,
      "grad_norm": 0.7401661276817322,
      "learning_rate": 2.5033826693995227e-06,
      "loss": 0.0948,
      "step": 733500
    },
    {
      "epoch": 4.765338270161421,
      "grad_norm": 1.2730385065078735,
      "learning_rate": 2.4964722109735317e-06,
      "loss": 0.0927,
      "step": 733600
    },
    {
      "epoch": 4.765987852804573,
      "grad_norm": 0.96402907371521,
      "learning_rate": 2.4895617525475407e-06,
      "loss": 0.098,
      "step": 733700
    },
    {
      "epoch": 4.766637435447725,
      "grad_norm": 0.9334203600883484,
      "learning_rate": 2.4826512941215493e-06,
      "loss": 0.0933,
      "step": 733800
    },
    {
      "epoch": 4.767287018090877,
      "grad_norm": 1.7749896049499512,
      "learning_rate": 2.4757408356955588e-06,
      "loss": 0.0967,
      "step": 733900
    },
    {
      "epoch": 4.7679366007340285,
      "grad_norm": 0.9997277855873108,
      "learning_rate": 2.4688303772695673e-06,
      "loss": 0.0929,
      "step": 734000
    },
    {
      "epoch": 4.76858618337718,
      "grad_norm": 0.6936133503913879,
      "learning_rate": 2.4619199188435763e-06,
      "loss": 0.086,
      "step": 734100
    },
    {
      "epoch": 4.769235766020332,
      "grad_norm": 0.9021124839782715,
      "learning_rate": 2.4550094604175853e-06,
      "loss": 0.0963,
      "step": 734200
    },
    {
      "epoch": 4.769885348663483,
      "grad_norm": 1.02765691280365,
      "learning_rate": 2.4480990019915944e-06,
      "loss": 0.0955,
      "step": 734300
    },
    {
      "epoch": 4.770534931306636,
      "grad_norm": 0.8649891018867493,
      "learning_rate": 2.441188543565603e-06,
      "loss": 0.0973,
      "step": 734400
    },
    {
      "epoch": 4.7711845139497875,
      "grad_norm": 1.6805000305175781,
      "learning_rate": 2.4342780851396124e-06,
      "loss": 0.0921,
      "step": 734500
    },
    {
      "epoch": 4.771834096592939,
      "grad_norm": 1.556989073753357,
      "learning_rate": 2.427367626713621e-06,
      "loss": 0.0927,
      "step": 734600
    },
    {
      "epoch": 4.772483679236091,
      "grad_norm": 1.0081508159637451,
      "learning_rate": 2.42045716828763e-06,
      "loss": 0.091,
      "step": 734700
    },
    {
      "epoch": 4.773133261879242,
      "grad_norm": 1.2741808891296387,
      "learning_rate": 2.413546709861639e-06,
      "loss": 0.0984,
      "step": 734800
    },
    {
      "epoch": 4.773782844522394,
      "grad_norm": 1.1716654300689697,
      "learning_rate": 2.406636251435648e-06,
      "loss": 0.0983,
      "step": 734900
    },
    {
      "epoch": 4.7744324271655465,
      "grad_norm": 0.8201473951339722,
      "learning_rate": 2.399725793009657e-06,
      "loss": 0.0898,
      "step": 735000
    },
    {
      "epoch": 4.775082009808698,
      "grad_norm": 1.3697497844696045,
      "learning_rate": 2.3928153345836655e-06,
      "loss": 0.0968,
      "step": 735100
    },
    {
      "epoch": 4.77573159245185,
      "grad_norm": 0.8512844443321228,
      "learning_rate": 2.385904876157675e-06,
      "loss": 0.0918,
      "step": 735200
    },
    {
      "epoch": 4.776381175095001,
      "grad_norm": 1.3628112077713013,
      "learning_rate": 2.3789944177316836e-06,
      "loss": 0.0944,
      "step": 735300
    },
    {
      "epoch": 4.777030757738153,
      "grad_norm": 1.1090798377990723,
      "learning_rate": 2.3720839593056926e-06,
      "loss": 0.0935,
      "step": 735400
    },
    {
      "epoch": 4.777680340381305,
      "grad_norm": 0.5778771638870239,
      "learning_rate": 2.365173500879701e-06,
      "loss": 0.0917,
      "step": 735500
    },
    {
      "epoch": 4.778329923024457,
      "grad_norm": 0.9777208566665649,
      "learning_rate": 2.3582630424537106e-06,
      "loss": 0.0953,
      "step": 735600
    },
    {
      "epoch": 4.778979505667609,
      "grad_norm": 1.3528163433074951,
      "learning_rate": 2.351352584027719e-06,
      "loss": 0.094,
      "step": 735700
    },
    {
      "epoch": 4.77962908831076,
      "grad_norm": 1.2315181493759155,
      "learning_rate": 2.344442125601728e-06,
      "loss": 0.1017,
      "step": 735800
    },
    {
      "epoch": 4.780278670953912,
      "grad_norm": 1.0299710035324097,
      "learning_rate": 2.337531667175737e-06,
      "loss": 0.0972,
      "step": 735900
    },
    {
      "epoch": 4.780928253597064,
      "grad_norm": 1.0332798957824707,
      "learning_rate": 2.330621208749746e-06,
      "loss": 0.1016,
      "step": 736000
    },
    {
      "epoch": 4.781577836240215,
      "grad_norm": 1.1580674648284912,
      "learning_rate": 2.323710750323755e-06,
      "loss": 0.0938,
      "step": 736100
    },
    {
      "epoch": 4.782227418883368,
      "grad_norm": 1.0733450651168823,
      "learning_rate": 2.316800291897764e-06,
      "loss": 0.0975,
      "step": 736200
    },
    {
      "epoch": 4.782877001526519,
      "grad_norm": 1.0333714485168457,
      "learning_rate": 2.309889833471773e-06,
      "loss": 0.0958,
      "step": 736300
    },
    {
      "epoch": 4.783526584169671,
      "grad_norm": 0.7363932132720947,
      "learning_rate": 2.3029793750457818e-06,
      "loss": 0.0932,
      "step": 736400
    },
    {
      "epoch": 4.784176166812823,
      "grad_norm": 0.9687997698783875,
      "learning_rate": 2.2960689166197908e-06,
      "loss": 0.0961,
      "step": 736500
    },
    {
      "epoch": 4.784825749455974,
      "grad_norm": 0.9158524870872498,
      "learning_rate": 2.2891584581937998e-06,
      "loss": 0.0871,
      "step": 736600
    },
    {
      "epoch": 4.785475332099127,
      "grad_norm": 1.3210676908493042,
      "learning_rate": 2.2822479997678088e-06,
      "loss": 0.0993,
      "step": 736700
    },
    {
      "epoch": 4.786124914742278,
      "grad_norm": 0.9057472348213196,
      "learning_rate": 2.2753375413418174e-06,
      "loss": 0.0901,
      "step": 736800
    },
    {
      "epoch": 4.78677449738543,
      "grad_norm": 1.4569419622421265,
      "learning_rate": 2.2684270829158268e-06,
      "loss": 0.0914,
      "step": 736900
    },
    {
      "epoch": 4.787424080028582,
      "grad_norm": 0.5839865803718567,
      "learning_rate": 2.2615166244898354e-06,
      "loss": 0.0985,
      "step": 737000
    },
    {
      "epoch": 4.788073662671733,
      "grad_norm": 1.7484742403030396,
      "learning_rate": 2.2546061660638444e-06,
      "loss": 0.0915,
      "step": 737100
    },
    {
      "epoch": 4.788723245314885,
      "grad_norm": 1.1986382007598877,
      "learning_rate": 2.2476957076378534e-06,
      "loss": 0.0925,
      "step": 737200
    },
    {
      "epoch": 4.7893728279580365,
      "grad_norm": 1.6839524507522583,
      "learning_rate": 2.2407852492118624e-06,
      "loss": 0.1,
      "step": 737300
    },
    {
      "epoch": 4.790022410601189,
      "grad_norm": 1.3747905492782593,
      "learning_rate": 2.2338747907858714e-06,
      "loss": 0.098,
      "step": 737400
    },
    {
      "epoch": 4.790671993244341,
      "grad_norm": 1.3699495792388916,
      "learning_rate": 2.22696433235988e-06,
      "loss": 0.099,
      "step": 737500
    },
    {
      "epoch": 4.791321575887492,
      "grad_norm": 0.9321739077568054,
      "learning_rate": 2.2200538739338894e-06,
      "loss": 0.0927,
      "step": 737600
    },
    {
      "epoch": 4.791971158530644,
      "grad_norm": 0.7675871253013611,
      "learning_rate": 2.213143415507898e-06,
      "loss": 0.0943,
      "step": 737700
    },
    {
      "epoch": 4.7926207411737956,
      "grad_norm": 1.912637710571289,
      "learning_rate": 2.206232957081907e-06,
      "loss": 0.1006,
      "step": 737800
    },
    {
      "epoch": 4.793270323816948,
      "grad_norm": 1.0766246318817139,
      "learning_rate": 2.199322498655916e-06,
      "loss": 0.0984,
      "step": 737900
    },
    {
      "epoch": 4.7939199064601,
      "grad_norm": 0.9032758474349976,
      "learning_rate": 2.192412040229925e-06,
      "loss": 0.0908,
      "step": 738000
    },
    {
      "epoch": 4.794569489103251,
      "grad_norm": 1.3312698602676392,
      "learning_rate": 2.1855015818039336e-06,
      "loss": 0.1009,
      "step": 738100
    },
    {
      "epoch": 4.795219071746403,
      "grad_norm": 1.4385199546813965,
      "learning_rate": 2.178591123377943e-06,
      "loss": 0.096,
      "step": 738200
    },
    {
      "epoch": 4.795868654389555,
      "grad_norm": 0.9586856961250305,
      "learning_rate": 2.1716806649519516e-06,
      "loss": 0.0962,
      "step": 738300
    },
    {
      "epoch": 4.796518237032706,
      "grad_norm": 0.8534148335456848,
      "learning_rate": 2.1647702065259606e-06,
      "loss": 0.097,
      "step": 738400
    },
    {
      "epoch": 4.797167819675858,
      "grad_norm": 1.007216215133667,
      "learning_rate": 2.1578597480999696e-06,
      "loss": 0.0954,
      "step": 738500
    },
    {
      "epoch": 4.79781740231901,
      "grad_norm": 0.725195050239563,
      "learning_rate": 2.1509492896739786e-06,
      "loss": 0.0918,
      "step": 738600
    },
    {
      "epoch": 4.798466984962162,
      "grad_norm": 1.069722294807434,
      "learning_rate": 2.1440388312479876e-06,
      "loss": 0.0963,
      "step": 738700
    },
    {
      "epoch": 4.799116567605314,
      "grad_norm": 1.3449896574020386,
      "learning_rate": 2.137128372821996e-06,
      "loss": 0.0942,
      "step": 738800
    },
    {
      "epoch": 4.799766150248465,
      "grad_norm": 0.8670264482498169,
      "learning_rate": 2.1302179143960056e-06,
      "loss": 0.096,
      "step": 738900
    },
    {
      "epoch": 4.800415732891617,
      "grad_norm": 0.9448659420013428,
      "learning_rate": 2.123307455970014e-06,
      "loss": 0.0886,
      "step": 739000
    },
    {
      "epoch": 4.801065315534769,
      "grad_norm": 1.14006507396698,
      "learning_rate": 2.116396997544023e-06,
      "loss": 0.0946,
      "step": 739100
    },
    {
      "epoch": 4.801714898177921,
      "grad_norm": 0.7850201725959778,
      "learning_rate": 2.109486539118032e-06,
      "loss": 0.0957,
      "step": 739200
    },
    {
      "epoch": 4.802364480821073,
      "grad_norm": 0.9624933004379272,
      "learning_rate": 2.102576080692041e-06,
      "loss": 0.0955,
      "step": 739300
    },
    {
      "epoch": 4.803014063464224,
      "grad_norm": 1.050512671470642,
      "learning_rate": 2.09566562226605e-06,
      "loss": 0.0908,
      "step": 739400
    },
    {
      "epoch": 4.803663646107376,
      "grad_norm": 1.3429661989212036,
      "learning_rate": 2.088755163840059e-06,
      "loss": 0.0936,
      "step": 739500
    },
    {
      "epoch": 4.8043132287505275,
      "grad_norm": 0.7895116209983826,
      "learning_rate": 2.081844705414068e-06,
      "loss": 0.0913,
      "step": 739600
    },
    {
      "epoch": 4.804962811393679,
      "grad_norm": 1.0058380365371704,
      "learning_rate": 2.074934246988077e-06,
      "loss": 0.0988,
      "step": 739700
    },
    {
      "epoch": 4.805612394036832,
      "grad_norm": 1.1019607782363892,
      "learning_rate": 2.068023788562086e-06,
      "loss": 0.0947,
      "step": 739800
    },
    {
      "epoch": 4.806261976679983,
      "grad_norm": 1.0091956853866577,
      "learning_rate": 2.061113330136095e-06,
      "loss": 0.0961,
      "step": 739900
    },
    {
      "epoch": 4.806911559323135,
      "grad_norm": 1.088381052017212,
      "learning_rate": 2.054202871710104e-06,
      "loss": 0.0953,
      "step": 740000
    },
    {
      "epoch": 4.8075611419662865,
      "grad_norm": 1.1659164428710938,
      "learning_rate": 2.0472924132841124e-06,
      "loss": 0.0956,
      "step": 740100
    },
    {
      "epoch": 4.808210724609438,
      "grad_norm": 0.9111987948417664,
      "learning_rate": 2.040381954858122e-06,
      "loss": 0.0952,
      "step": 740200
    },
    {
      "epoch": 4.808860307252591,
      "grad_norm": 1.2356483936309814,
      "learning_rate": 2.0334714964321304e-06,
      "loss": 0.098,
      "step": 740300
    },
    {
      "epoch": 4.809509889895742,
      "grad_norm": 1.0255184173583984,
      "learning_rate": 2.0265610380061394e-06,
      "loss": 0.0976,
      "step": 740400
    },
    {
      "epoch": 4.810159472538894,
      "grad_norm": 0.8135207891464233,
      "learning_rate": 2.019650579580148e-06,
      "loss": 0.0986,
      "step": 740500
    },
    {
      "epoch": 4.8108090551820455,
      "grad_norm": 1.0309128761291504,
      "learning_rate": 2.0127401211541574e-06,
      "loss": 0.0924,
      "step": 740600
    },
    {
      "epoch": 4.811458637825197,
      "grad_norm": 1.1561721563339233,
      "learning_rate": 2.005829662728166e-06,
      "loss": 0.0957,
      "step": 740700
    },
    {
      "epoch": 4.812108220468349,
      "grad_norm": 1.0391119718551636,
      "learning_rate": 1.998919204302175e-06,
      "loss": 0.0928,
      "step": 740800
    },
    {
      "epoch": 4.8127578031115,
      "grad_norm": 1.1437357664108276,
      "learning_rate": 1.992008745876184e-06,
      "loss": 0.0971,
      "step": 740900
    },
    {
      "epoch": 4.813407385754653,
      "grad_norm": 1.0261567831039429,
      "learning_rate": 1.985098287450193e-06,
      "loss": 0.0941,
      "step": 741000
    },
    {
      "epoch": 4.8140569683978045,
      "grad_norm": 0.9895569682121277,
      "learning_rate": 1.9781878290242016e-06,
      "loss": 0.0975,
      "step": 741100
    },
    {
      "epoch": 4.814706551040956,
      "grad_norm": 1.0204089879989624,
      "learning_rate": 1.971277370598211e-06,
      "loss": 0.0936,
      "step": 741200
    },
    {
      "epoch": 4.815356133684108,
      "grad_norm": 1.261064052581787,
      "learning_rate": 1.9643669121722196e-06,
      "loss": 0.0994,
      "step": 741300
    },
    {
      "epoch": 4.816005716327259,
      "grad_norm": 0.5948711633682251,
      "learning_rate": 1.9574564537462286e-06,
      "loss": 0.0923,
      "step": 741400
    },
    {
      "epoch": 4.816655298970412,
      "grad_norm": 0.8541837930679321,
      "learning_rate": 1.9505459953202376e-06,
      "loss": 0.0931,
      "step": 741500
    },
    {
      "epoch": 4.8173048816135635,
      "grad_norm": 0.9639360904693604,
      "learning_rate": 1.9436355368942466e-06,
      "loss": 0.0943,
      "step": 741600
    },
    {
      "epoch": 4.817954464256715,
      "grad_norm": 1.522275686264038,
      "learning_rate": 1.9367250784682556e-06,
      "loss": 0.0928,
      "step": 741700
    },
    {
      "epoch": 4.818604046899867,
      "grad_norm": 1.5890263319015503,
      "learning_rate": 1.9298146200422642e-06,
      "loss": 0.0952,
      "step": 741800
    },
    {
      "epoch": 4.819253629543018,
      "grad_norm": 1.8805596828460693,
      "learning_rate": 1.9229041616162736e-06,
      "loss": 0.0943,
      "step": 741900
    },
    {
      "epoch": 4.81990321218617,
      "grad_norm": 0.9128136038780212,
      "learning_rate": 1.9159937031902822e-06,
      "loss": 0.0962,
      "step": 742000
    },
    {
      "epoch": 4.8205527948293225,
      "grad_norm": 0.8724809885025024,
      "learning_rate": 1.9090832447642912e-06,
      "loss": 0.0916,
      "step": 742100
    },
    {
      "epoch": 4.821202377472474,
      "grad_norm": 1.4398951530456543,
      "learning_rate": 1.9021727863383004e-06,
      "loss": 0.0948,
      "step": 742200
    },
    {
      "epoch": 4.821851960115626,
      "grad_norm": 0.8257511854171753,
      "learning_rate": 1.8952623279123092e-06,
      "loss": 0.0903,
      "step": 742300
    },
    {
      "epoch": 4.822501542758777,
      "grad_norm": 1.080485224723816,
      "learning_rate": 1.888351869486318e-06,
      "loss": 0.0916,
      "step": 742400
    },
    {
      "epoch": 4.823151125401929,
      "grad_norm": 1.266872763633728,
      "learning_rate": 1.8814414110603268e-06,
      "loss": 0.0949,
      "step": 742500
    },
    {
      "epoch": 4.823800708045081,
      "grad_norm": 1.3197910785675049,
      "learning_rate": 1.874530952634336e-06,
      "loss": 0.0959,
      "step": 742600
    },
    {
      "epoch": 4.824450290688233,
      "grad_norm": 1.2267760038375854,
      "learning_rate": 1.8676204942083448e-06,
      "loss": 0.0962,
      "step": 742700
    },
    {
      "epoch": 4.825099873331385,
      "grad_norm": 1.3781205415725708,
      "learning_rate": 1.8607100357823536e-06,
      "loss": 0.1014,
      "step": 742800
    },
    {
      "epoch": 4.825749455974536,
      "grad_norm": 0.7219851613044739,
      "learning_rate": 1.8537995773563628e-06,
      "loss": 0.1008,
      "step": 742900
    },
    {
      "epoch": 4.826399038617688,
      "grad_norm": 1.4593311548233032,
      "learning_rate": 1.8468891189303716e-06,
      "loss": 0.0963,
      "step": 743000
    },
    {
      "epoch": 4.82704862126084,
      "grad_norm": 1.0676274299621582,
      "learning_rate": 1.8399786605043806e-06,
      "loss": 0.1001,
      "step": 743100
    },
    {
      "epoch": 4.827698203903992,
      "grad_norm": 0.851840078830719,
      "learning_rate": 1.8330682020783896e-06,
      "loss": 0.0948,
      "step": 743200
    },
    {
      "epoch": 4.828347786547144,
      "grad_norm": 0.9527637958526611,
      "learning_rate": 1.8261577436523987e-06,
      "loss": 0.0945,
      "step": 743300
    },
    {
      "epoch": 4.828997369190295,
      "grad_norm": 1.1084524393081665,
      "learning_rate": 1.8192472852264074e-06,
      "loss": 0.0958,
      "step": 743400
    },
    {
      "epoch": 4.829646951833447,
      "grad_norm": 0.40715065598487854,
      "learning_rate": 1.8123368268004162e-06,
      "loss": 0.0956,
      "step": 743500
    },
    {
      "epoch": 4.830296534476599,
      "grad_norm": 0.8053811192512512,
      "learning_rate": 1.8054263683744255e-06,
      "loss": 0.0962,
      "step": 743600
    },
    {
      "epoch": 4.83094611711975,
      "grad_norm": 0.9463703632354736,
      "learning_rate": 1.7985159099484342e-06,
      "loss": 0.0908,
      "step": 743700
    },
    {
      "epoch": 4.831595699762902,
      "grad_norm": 1.5680170059204102,
      "learning_rate": 1.791605451522443e-06,
      "loss": 0.095,
      "step": 743800
    },
    {
      "epoch": 4.832245282406054,
      "grad_norm": 0.8182092905044556,
      "learning_rate": 1.7846949930964523e-06,
      "loss": 0.0864,
      "step": 743900
    },
    {
      "epoch": 4.832894865049206,
      "grad_norm": 0.834756076335907,
      "learning_rate": 1.777784534670461e-06,
      "loss": 0.0932,
      "step": 744000
    },
    {
      "epoch": 4.833544447692358,
      "grad_norm": 0.8750346302986145,
      "learning_rate": 1.7708740762444698e-06,
      "loss": 0.0919,
      "step": 744100
    },
    {
      "epoch": 4.834194030335509,
      "grad_norm": 1.2681288719177246,
      "learning_rate": 1.763963617818479e-06,
      "loss": 0.0946,
      "step": 744200
    },
    {
      "epoch": 4.834843612978661,
      "grad_norm": 1.4626718759536743,
      "learning_rate": 1.7570531593924879e-06,
      "loss": 0.0984,
      "step": 744300
    },
    {
      "epoch": 4.8354931956218135,
      "grad_norm": 0.9752869606018066,
      "learning_rate": 1.7501427009664966e-06,
      "loss": 0.0958,
      "step": 744400
    },
    {
      "epoch": 4.836142778264965,
      "grad_norm": 0.7909160852432251,
      "learning_rate": 1.7432322425405057e-06,
      "loss": 0.0944,
      "step": 744500
    },
    {
      "epoch": 4.836792360908117,
      "grad_norm": 1.1908947229385376,
      "learning_rate": 1.7363217841145149e-06,
      "loss": 0.0959,
      "step": 744600
    },
    {
      "epoch": 4.837441943551268,
      "grad_norm": 0.9023386836051941,
      "learning_rate": 1.7294113256885237e-06,
      "loss": 0.1019,
      "step": 744700
    },
    {
      "epoch": 4.83809152619442,
      "grad_norm": 1.6641041040420532,
      "learning_rate": 1.7225008672625325e-06,
      "loss": 0.0925,
      "step": 744800
    },
    {
      "epoch": 4.838741108837572,
      "grad_norm": 0.9269335865974426,
      "learning_rate": 1.7155904088365417e-06,
      "loss": 0.0944,
      "step": 744900
    },
    {
      "epoch": 4.839390691480723,
      "grad_norm": 1.2375589609146118,
      "learning_rate": 1.7086799504105505e-06,
      "loss": 0.0853,
      "step": 745000
    },
    {
      "epoch": 4.840040274123876,
      "grad_norm": 0.7995964884757996,
      "learning_rate": 1.7017694919845593e-06,
      "loss": 0.0994,
      "step": 745100
    },
    {
      "epoch": 4.840689856767027,
      "grad_norm": 1.1815778017044067,
      "learning_rate": 1.6948590335585685e-06,
      "loss": 0.0912,
      "step": 745200
    },
    {
      "epoch": 4.841339439410179,
      "grad_norm": 1.0315011739730835,
      "learning_rate": 1.6879485751325773e-06,
      "loss": 0.092,
      "step": 745300
    },
    {
      "epoch": 4.841989022053331,
      "grad_norm": 1.3073093891143799,
      "learning_rate": 1.681038116706586e-06,
      "loss": 0.0943,
      "step": 745400
    },
    {
      "epoch": 4.842638604696482,
      "grad_norm": 1.1069402694702148,
      "learning_rate": 1.6741276582805949e-06,
      "loss": 0.0935,
      "step": 745500
    },
    {
      "epoch": 4.843288187339635,
      "grad_norm": 1.238297939300537,
      "learning_rate": 1.667217199854604e-06,
      "loss": 0.0998,
      "step": 745600
    },
    {
      "epoch": 4.843937769982786,
      "grad_norm": 1.2794255018234253,
      "learning_rate": 1.6603067414286129e-06,
      "loss": 0.096,
      "step": 745700
    },
    {
      "epoch": 4.844587352625938,
      "grad_norm": 0.7374837398529053,
      "learning_rate": 1.6533962830026219e-06,
      "loss": 0.0917,
      "step": 745800
    },
    {
      "epoch": 4.84523693526909,
      "grad_norm": 1.1178838014602661,
      "learning_rate": 1.6464858245766309e-06,
      "loss": 0.0963,
      "step": 745900
    },
    {
      "epoch": 4.845886517912241,
      "grad_norm": 0.9414046406745911,
      "learning_rate": 1.6395753661506399e-06,
      "loss": 0.0866,
      "step": 746000
    },
    {
      "epoch": 4.846536100555393,
      "grad_norm": 1.2021024227142334,
      "learning_rate": 1.6326649077246487e-06,
      "loss": 0.0898,
      "step": 746100
    },
    {
      "epoch": 4.8471856831985445,
      "grad_norm": 1.277597427368164,
      "learning_rate": 1.6257544492986579e-06,
      "loss": 0.096,
      "step": 746200
    },
    {
      "epoch": 4.847835265841697,
      "grad_norm": 1.406576156616211,
      "learning_rate": 1.6188439908726667e-06,
      "loss": 0.0919,
      "step": 746300
    },
    {
      "epoch": 4.848484848484849,
      "grad_norm": 0.8420237898826599,
      "learning_rate": 1.6119335324466755e-06,
      "loss": 0.0925,
      "step": 746400
    },
    {
      "epoch": 4.849134431128,
      "grad_norm": 1.324792742729187,
      "learning_rate": 1.6050230740206843e-06,
      "loss": 0.0974,
      "step": 746500
    },
    {
      "epoch": 4.849784013771152,
      "grad_norm": 1.610723853111267,
      "learning_rate": 1.5981126155946935e-06,
      "loss": 0.096,
      "step": 746600
    },
    {
      "epoch": 4.8504335964143035,
      "grad_norm": 1.414937138557434,
      "learning_rate": 1.5912021571687023e-06,
      "loss": 0.0969,
      "step": 746700
    },
    {
      "epoch": 4.851083179057456,
      "grad_norm": 0.5984268188476562,
      "learning_rate": 1.584291698742711e-06,
      "loss": 0.0944,
      "step": 746800
    },
    {
      "epoch": 4.851732761700608,
      "grad_norm": 1.3783676624298096,
      "learning_rate": 1.5773812403167203e-06,
      "loss": 0.0979,
      "step": 746900
    },
    {
      "epoch": 4.852382344343759,
      "grad_norm": 0.813412070274353,
      "learning_rate": 1.570470781890729e-06,
      "loss": 0.0941,
      "step": 747000
    },
    {
      "epoch": 4.853031926986911,
      "grad_norm": 1.423689842224121,
      "learning_rate": 1.563560323464738e-06,
      "loss": 0.0948,
      "step": 747100
    },
    {
      "epoch": 4.8536815096300625,
      "grad_norm": 0.7760607600212097,
      "learning_rate": 1.556649865038747e-06,
      "loss": 0.1028,
      "step": 747200
    },
    {
      "epoch": 4.854331092273214,
      "grad_norm": 0.837932288646698,
      "learning_rate": 1.549739406612756e-06,
      "loss": 0.0998,
      "step": 747300
    },
    {
      "epoch": 4.854980674916366,
      "grad_norm": 0.7558243870735168,
      "learning_rate": 1.5428289481867649e-06,
      "loss": 0.0985,
      "step": 747400
    },
    {
      "epoch": 4.855630257559518,
      "grad_norm": 0.9704067707061768,
      "learning_rate": 1.535918489760774e-06,
      "loss": 0.0988,
      "step": 747500
    },
    {
      "epoch": 4.85627984020267,
      "grad_norm": 1.009438157081604,
      "learning_rate": 1.5290080313347827e-06,
      "loss": 0.1003,
      "step": 747600
    },
    {
      "epoch": 4.8569294228458215,
      "grad_norm": 1.0712565183639526,
      "learning_rate": 1.5220975729087917e-06,
      "loss": 0.0954,
      "step": 747700
    },
    {
      "epoch": 4.857579005488973,
      "grad_norm": 1.2288284301757812,
      "learning_rate": 1.5151871144828007e-06,
      "loss": 0.0965,
      "step": 747800
    },
    {
      "epoch": 4.858228588132125,
      "grad_norm": 1.2803486585617065,
      "learning_rate": 1.5082766560568095e-06,
      "loss": 0.0945,
      "step": 747900
    },
    {
      "epoch": 4.858878170775277,
      "grad_norm": 1.1327121257781982,
      "learning_rate": 1.5013661976308185e-06,
      "loss": 0.0921,
      "step": 748000
    },
    {
      "epoch": 4.859527753418429,
      "grad_norm": 1.1382907629013062,
      "learning_rate": 1.4944557392048273e-06,
      "loss": 0.1011,
      "step": 748100
    },
    {
      "epoch": 4.8601773360615805,
      "grad_norm": 0.9908719658851624,
      "learning_rate": 1.4875452807788363e-06,
      "loss": 0.0945,
      "step": 748200
    },
    {
      "epoch": 4.860826918704732,
      "grad_norm": 1.0142163038253784,
      "learning_rate": 1.4806348223528453e-06,
      "loss": 0.0968,
      "step": 748300
    },
    {
      "epoch": 4.861476501347884,
      "grad_norm": 0.8282093405723572,
      "learning_rate": 1.4737243639268543e-06,
      "loss": 0.0971,
      "step": 748400
    },
    {
      "epoch": 4.862126083991035,
      "grad_norm": 1.3802850246429443,
      "learning_rate": 1.4668139055008633e-06,
      "loss": 0.0961,
      "step": 748500
    },
    {
      "epoch": 4.862775666634188,
      "grad_norm": 0.9096836447715759,
      "learning_rate": 1.459903447074872e-06,
      "loss": 0.0954,
      "step": 748600
    },
    {
      "epoch": 4.8634252492773395,
      "grad_norm": 0.9551687836647034,
      "learning_rate": 1.4529929886488811e-06,
      "loss": 0.0987,
      "step": 748700
    },
    {
      "epoch": 4.864074831920491,
      "grad_norm": 0.5592803955078125,
      "learning_rate": 1.4460825302228901e-06,
      "loss": 0.0932,
      "step": 748800
    },
    {
      "epoch": 4.864724414563643,
      "grad_norm": 0.875477135181427,
      "learning_rate": 1.439172071796899e-06,
      "loss": 0.0931,
      "step": 748900
    },
    {
      "epoch": 4.865373997206794,
      "grad_norm": 1.4085811376571655,
      "learning_rate": 1.432261613370908e-06,
      "loss": 0.0959,
      "step": 749000
    },
    {
      "epoch": 4.866023579849946,
      "grad_norm": 0.7749162316322327,
      "learning_rate": 1.4253511549449167e-06,
      "loss": 0.0982,
      "step": 749100
    },
    {
      "epoch": 4.866673162493099,
      "grad_norm": 0.8506231307983398,
      "learning_rate": 1.4184406965189257e-06,
      "loss": 0.0899,
      "step": 749200
    },
    {
      "epoch": 4.86732274513625,
      "grad_norm": 1.2426166534423828,
      "learning_rate": 1.4115302380929347e-06,
      "loss": 0.0998,
      "step": 749300
    },
    {
      "epoch": 4.867972327779402,
      "grad_norm": 1.2703832387924194,
      "learning_rate": 1.4046197796669435e-06,
      "loss": 0.0979,
      "step": 749400
    },
    {
      "epoch": 4.868621910422553,
      "grad_norm": 1.0698055028915405,
      "learning_rate": 1.3977093212409525e-06,
      "loss": 0.0914,
      "step": 749500
    },
    {
      "epoch": 4.869271493065705,
      "grad_norm": 1.1083688735961914,
      "learning_rate": 1.3907988628149615e-06,
      "loss": 0.0897,
      "step": 749600
    },
    {
      "epoch": 4.869921075708857,
      "grad_norm": 1.043243646621704,
      "learning_rate": 1.3838884043889705e-06,
      "loss": 0.0949,
      "step": 749700
    },
    {
      "epoch": 4.870570658352009,
      "grad_norm": 1.1655552387237549,
      "learning_rate": 1.3769779459629795e-06,
      "loss": 0.0929,
      "step": 749800
    },
    {
      "epoch": 4.871220240995161,
      "grad_norm": 1.3602628707885742,
      "learning_rate": 1.3700674875369883e-06,
      "loss": 0.0931,
      "step": 749900
    },
    {
      "epoch": 4.8718698236383124,
      "grad_norm": 1.383306622505188,
      "learning_rate": 1.3631570291109973e-06,
      "loss": 0.0965,
      "step": 750000
    },
    {
      "epoch": 4.872519406281464,
      "grad_norm": 1.5371326208114624,
      "learning_rate": 1.3562465706850061e-06,
      "loss": 0.0952,
      "step": 750100
    },
    {
      "epoch": 4.873168988924616,
      "grad_norm": 0.893181324005127,
      "learning_rate": 1.3493361122590151e-06,
      "loss": 0.0971,
      "step": 750200
    },
    {
      "epoch": 4.873818571567767,
      "grad_norm": 1.0679570436477661,
      "learning_rate": 1.3424256538330241e-06,
      "loss": 0.0928,
      "step": 750300
    },
    {
      "epoch": 4.87446815421092,
      "grad_norm": 0.9550638794898987,
      "learning_rate": 1.335515195407033e-06,
      "loss": 0.0947,
      "step": 750400
    },
    {
      "epoch": 4.8751177368540715,
      "grad_norm": 1.08548903465271,
      "learning_rate": 1.328604736981042e-06,
      "loss": 0.0909,
      "step": 750500
    },
    {
      "epoch": 4.875767319497223,
      "grad_norm": 0.8204517960548401,
      "learning_rate": 1.3216942785550507e-06,
      "loss": 0.0957,
      "step": 750600
    },
    {
      "epoch": 4.876416902140375,
      "grad_norm": 0.5902315974235535,
      "learning_rate": 1.3147838201290597e-06,
      "loss": 0.0893,
      "step": 750700
    },
    {
      "epoch": 4.877066484783526,
      "grad_norm": 1.2487157583236694,
      "learning_rate": 1.3078733617030687e-06,
      "loss": 0.0926,
      "step": 750800
    },
    {
      "epoch": 4.877716067426679,
      "grad_norm": 1.0023043155670166,
      "learning_rate": 1.3009629032770777e-06,
      "loss": 0.092,
      "step": 750900
    },
    {
      "epoch": 4.8783656500698305,
      "grad_norm": 0.8648493885993958,
      "learning_rate": 1.2940524448510867e-06,
      "loss": 0.0965,
      "step": 751000
    },
    {
      "epoch": 4.879015232712982,
      "grad_norm": 1.0117993354797363,
      "learning_rate": 1.2871419864250955e-06,
      "loss": 0.0956,
      "step": 751100
    },
    {
      "epoch": 4.879664815356134,
      "grad_norm": 0.6494762897491455,
      "learning_rate": 1.2802315279991045e-06,
      "loss": 0.0923,
      "step": 751200
    },
    {
      "epoch": 4.880314397999285,
      "grad_norm": 0.8018348217010498,
      "learning_rate": 1.2733210695731135e-06,
      "loss": 0.0984,
      "step": 751300
    },
    {
      "epoch": 4.880963980642437,
      "grad_norm": 1.2448058128356934,
      "learning_rate": 1.2664106111471223e-06,
      "loss": 0.0987,
      "step": 751400
    },
    {
      "epoch": 4.881613563285589,
      "grad_norm": 1.1792662143707275,
      "learning_rate": 1.2595001527211313e-06,
      "loss": 0.098,
      "step": 751500
    },
    {
      "epoch": 4.882263145928741,
      "grad_norm": 0.9099470973014832,
      "learning_rate": 1.2525896942951401e-06,
      "loss": 0.0939,
      "step": 751600
    },
    {
      "epoch": 4.882912728571893,
      "grad_norm": 1.5727792978286743,
      "learning_rate": 1.2456792358691491e-06,
      "loss": 0.094,
      "step": 751700
    },
    {
      "epoch": 4.883562311215044,
      "grad_norm": 0.9559677243232727,
      "learning_rate": 1.2387687774431581e-06,
      "loss": 0.0894,
      "step": 751800
    },
    {
      "epoch": 4.884211893858196,
      "grad_norm": 1.4080640077590942,
      "learning_rate": 1.231858319017167e-06,
      "loss": 0.0984,
      "step": 751900
    },
    {
      "epoch": 4.884861476501348,
      "grad_norm": 0.9804694652557373,
      "learning_rate": 1.224947860591176e-06,
      "loss": 0.0943,
      "step": 752000
    },
    {
      "epoch": 4.8855110591445,
      "grad_norm": 1.1427818536758423,
      "learning_rate": 1.2180374021651847e-06,
      "loss": 0.0927,
      "step": 752100
    },
    {
      "epoch": 4.886160641787652,
      "grad_norm": 0.7312296032905579,
      "learning_rate": 1.2111269437391937e-06,
      "loss": 0.0937,
      "step": 752200
    },
    {
      "epoch": 4.886810224430803,
      "grad_norm": 1.421781063079834,
      "learning_rate": 1.2042164853132027e-06,
      "loss": 0.0926,
      "step": 752300
    },
    {
      "epoch": 4.887459807073955,
      "grad_norm": 1.5311279296875,
      "learning_rate": 1.1973060268872117e-06,
      "loss": 0.0947,
      "step": 752400
    },
    {
      "epoch": 4.888109389717107,
      "grad_norm": 1.023913860321045,
      "learning_rate": 1.1903955684612208e-06,
      "loss": 0.0939,
      "step": 752500
    },
    {
      "epoch": 4.888758972360258,
      "grad_norm": 1.0801423788070679,
      "learning_rate": 1.1834851100352295e-06,
      "loss": 0.0933,
      "step": 752600
    },
    {
      "epoch": 4.88940855500341,
      "grad_norm": 0.7594757080078125,
      "learning_rate": 1.1765746516092385e-06,
      "loss": 0.0991,
      "step": 752700
    },
    {
      "epoch": 4.890058137646562,
      "grad_norm": 1.3905925750732422,
      "learning_rate": 1.1696641931832476e-06,
      "loss": 0.0937,
      "step": 752800
    },
    {
      "epoch": 4.890707720289714,
      "grad_norm": 1.370419979095459,
      "learning_rate": 1.1627537347572563e-06,
      "loss": 0.1024,
      "step": 752900
    },
    {
      "epoch": 4.891357302932866,
      "grad_norm": 1.26432466506958,
      "learning_rate": 1.1558432763312654e-06,
      "loss": 0.0913,
      "step": 753000
    },
    {
      "epoch": 4.892006885576017,
      "grad_norm": 1.338906168937683,
      "learning_rate": 1.1489328179052741e-06,
      "loss": 0.0941,
      "step": 753100
    },
    {
      "epoch": 4.892656468219169,
      "grad_norm": 1.4495220184326172,
      "learning_rate": 1.1420223594792832e-06,
      "loss": 0.087,
      "step": 753200
    },
    {
      "epoch": 4.893306050862321,
      "grad_norm": 1.1989986896514893,
      "learning_rate": 1.1351119010532922e-06,
      "loss": 0.0899,
      "step": 753300
    },
    {
      "epoch": 4.893955633505473,
      "grad_norm": 1.1209436655044556,
      "learning_rate": 1.128201442627301e-06,
      "loss": 0.0901,
      "step": 753400
    },
    {
      "epoch": 4.894605216148625,
      "grad_norm": 1.096922755241394,
      "learning_rate": 1.12129098420131e-06,
      "loss": 0.0968,
      "step": 753500
    },
    {
      "epoch": 4.895254798791776,
      "grad_norm": 1.5858170986175537,
      "learning_rate": 1.114380525775319e-06,
      "loss": 0.0969,
      "step": 753600
    },
    {
      "epoch": 4.895904381434928,
      "grad_norm": 0.8010159730911255,
      "learning_rate": 1.107470067349328e-06,
      "loss": 0.0982,
      "step": 753700
    },
    {
      "epoch": 4.8965539640780795,
      "grad_norm": 0.940028727054596,
      "learning_rate": 1.100559608923337e-06,
      "loss": 0.0936,
      "step": 753800
    },
    {
      "epoch": 4.897203546721231,
      "grad_norm": 1.248734951019287,
      "learning_rate": 1.0936491504973458e-06,
      "loss": 0.1017,
      "step": 753900
    },
    {
      "epoch": 4.897853129364384,
      "grad_norm": 1.2433202266693115,
      "learning_rate": 1.0867386920713548e-06,
      "loss": 0.0916,
      "step": 754000
    },
    {
      "epoch": 4.898502712007535,
      "grad_norm": 1.00365149974823,
      "learning_rate": 1.0798282336453636e-06,
      "loss": 0.0994,
      "step": 754100
    },
    {
      "epoch": 4.899152294650687,
      "grad_norm": 1.194404125213623,
      "learning_rate": 1.0729177752193726e-06,
      "loss": 0.0968,
      "step": 754200
    },
    {
      "epoch": 4.8998018772938385,
      "grad_norm": 0.6843734979629517,
      "learning_rate": 1.0660073167933816e-06,
      "loss": 0.0901,
      "step": 754300
    },
    {
      "epoch": 4.90045145993699,
      "grad_norm": 1.232979655265808,
      "learning_rate": 1.0590968583673904e-06,
      "loss": 0.097,
      "step": 754400
    },
    {
      "epoch": 4.901101042580143,
      "grad_norm": 1.0754188299179077,
      "learning_rate": 1.0521863999413994e-06,
      "loss": 0.0958,
      "step": 754500
    },
    {
      "epoch": 4.901750625223294,
      "grad_norm": 1.004146933555603,
      "learning_rate": 1.0452759415154082e-06,
      "loss": 0.0934,
      "step": 754600
    },
    {
      "epoch": 4.902400207866446,
      "grad_norm": 1.2419633865356445,
      "learning_rate": 1.0383654830894172e-06,
      "loss": 0.0892,
      "step": 754700
    },
    {
      "epoch": 4.9030497905095975,
      "grad_norm": 1.2104078531265259,
      "learning_rate": 1.0314550246634262e-06,
      "loss": 0.0975,
      "step": 754800
    },
    {
      "epoch": 4.903699373152749,
      "grad_norm": 0.791407585144043,
      "learning_rate": 1.0245445662374352e-06,
      "loss": 0.0963,
      "step": 754900
    },
    {
      "epoch": 4.904348955795901,
      "grad_norm": 0.7883208990097046,
      "learning_rate": 1.0176341078114442e-06,
      "loss": 0.0953,
      "step": 755000
    },
    {
      "epoch": 4.904998538439052,
      "grad_norm": 0.9500600695610046,
      "learning_rate": 1.010723649385453e-06,
      "loss": 0.0913,
      "step": 755100
    },
    {
      "epoch": 4.905648121082205,
      "grad_norm": 0.8095575571060181,
      "learning_rate": 1.003813190959462e-06,
      "loss": 0.0972,
      "step": 755200
    },
    {
      "epoch": 4.906297703725357,
      "grad_norm": 1.341426134109497,
      "learning_rate": 9.969027325334708e-07,
      "loss": 0.0895,
      "step": 755300
    },
    {
      "epoch": 4.906947286368508,
      "grad_norm": 0.8428245186805725,
      "learning_rate": 9.899922741074798e-07,
      "loss": 0.0923,
      "step": 755400
    },
    {
      "epoch": 4.90759686901166,
      "grad_norm": 1.3340598344802856,
      "learning_rate": 9.830818156814888e-07,
      "loss": 0.0899,
      "step": 755500
    },
    {
      "epoch": 4.908246451654811,
      "grad_norm": 1.630423903465271,
      "learning_rate": 9.761713572554976e-07,
      "loss": 0.0922,
      "step": 755600
    },
    {
      "epoch": 4.908896034297964,
      "grad_norm": 0.6617357134819031,
      "learning_rate": 9.692608988295066e-07,
      "loss": 0.0901,
      "step": 755700
    },
    {
      "epoch": 4.909545616941116,
      "grad_norm": 1.483992576599121,
      "learning_rate": 9.623504404035154e-07,
      "loss": 0.0987,
      "step": 755800
    },
    {
      "epoch": 4.910195199584267,
      "grad_norm": 1.1936789751052856,
      "learning_rate": 9.554399819775244e-07,
      "loss": 0.0934,
      "step": 755900
    },
    {
      "epoch": 4.910844782227419,
      "grad_norm": 1.1745960712432861,
      "learning_rate": 9.485295235515335e-07,
      "loss": 0.0956,
      "step": 756000
    },
    {
      "epoch": 4.9114943648705705,
      "grad_norm": 1.2970879077911377,
      "learning_rate": 9.416190651255423e-07,
      "loss": 0.0981,
      "step": 756100
    },
    {
      "epoch": 4.912143947513722,
      "grad_norm": 0.8968815207481384,
      "learning_rate": 9.347086066995513e-07,
      "loss": 0.0959,
      "step": 756200
    },
    {
      "epoch": 4.912793530156875,
      "grad_norm": 1.3201625347137451,
      "learning_rate": 9.277981482735602e-07,
      "loss": 0.0942,
      "step": 756300
    },
    {
      "epoch": 4.913443112800026,
      "grad_norm": 1.5073623657226562,
      "learning_rate": 9.208876898475692e-07,
      "loss": 0.0989,
      "step": 756400
    },
    {
      "epoch": 4.914092695443178,
      "grad_norm": 1.3306432962417603,
      "learning_rate": 9.139772314215782e-07,
      "loss": 0.0891,
      "step": 756500
    },
    {
      "epoch": 4.9147422780863295,
      "grad_norm": 1.1172798871994019,
      "learning_rate": 9.07066772995587e-07,
      "loss": 0.0912,
      "step": 756600
    },
    {
      "epoch": 4.915391860729481,
      "grad_norm": 1.0360468626022339,
      "learning_rate": 9.00156314569596e-07,
      "loss": 0.0951,
      "step": 756700
    },
    {
      "epoch": 4.916041443372633,
      "grad_norm": 1.180833339691162,
      "learning_rate": 8.932458561436048e-07,
      "loss": 0.0961,
      "step": 756800
    },
    {
      "epoch": 4.916691026015785,
      "grad_norm": 1.0324798822402954,
      "learning_rate": 8.863353977176138e-07,
      "loss": 0.0974,
      "step": 756900
    },
    {
      "epoch": 4.917340608658937,
      "grad_norm": 0.7155190110206604,
      "learning_rate": 8.794249392916228e-07,
      "loss": 0.0858,
      "step": 757000
    },
    {
      "epoch": 4.9179901913020885,
      "grad_norm": 1.2060643434524536,
      "learning_rate": 8.725144808656317e-07,
      "loss": 0.0855,
      "step": 757100
    },
    {
      "epoch": 4.91863977394524,
      "grad_norm": 1.506914734840393,
      "learning_rate": 8.656040224396407e-07,
      "loss": 0.0947,
      "step": 757200
    },
    {
      "epoch": 4.919289356588392,
      "grad_norm": 0.4998745918273926,
      "learning_rate": 8.586935640136495e-07,
      "loss": 0.0918,
      "step": 757300
    },
    {
      "epoch": 4.919938939231543,
      "grad_norm": 1.3198721408843994,
      "learning_rate": 8.517831055876585e-07,
      "loss": 0.0982,
      "step": 757400
    },
    {
      "epoch": 4.920588521874696,
      "grad_norm": 1.0260694026947021,
      "learning_rate": 8.448726471616675e-07,
      "loss": 0.0971,
      "step": 757500
    },
    {
      "epoch": 4.9212381045178475,
      "grad_norm": 1.2157562971115112,
      "learning_rate": 8.379621887356763e-07,
      "loss": 0.0935,
      "step": 757600
    },
    {
      "epoch": 4.921887687160999,
      "grad_norm": 1.6463265419006348,
      "learning_rate": 8.310517303096853e-07,
      "loss": 0.0965,
      "step": 757700
    },
    {
      "epoch": 4.922537269804151,
      "grad_norm": 0.6987533569335938,
      "learning_rate": 8.241412718836942e-07,
      "loss": 0.1015,
      "step": 757800
    },
    {
      "epoch": 4.923186852447302,
      "grad_norm": 1.3912153244018555,
      "learning_rate": 8.172308134577032e-07,
      "loss": 0.0966,
      "step": 757900
    },
    {
      "epoch": 4.923836435090454,
      "grad_norm": 1.0366772413253784,
      "learning_rate": 8.103203550317122e-07,
      "loss": 0.0935,
      "step": 758000
    },
    {
      "epoch": 4.9244860177336065,
      "grad_norm": 0.9664760828018188,
      "learning_rate": 8.03409896605721e-07,
      "loss": 0.0976,
      "step": 758100
    },
    {
      "epoch": 4.925135600376758,
      "grad_norm": 0.686836838722229,
      "learning_rate": 7.9649943817973e-07,
      "loss": 0.0947,
      "step": 758200
    },
    {
      "epoch": 4.92578518301991,
      "grad_norm": 1.1378662586212158,
      "learning_rate": 7.895889797537389e-07,
      "loss": 0.0902,
      "step": 758300
    },
    {
      "epoch": 4.926434765663061,
      "grad_norm": 1.0263742208480835,
      "learning_rate": 7.826785213277479e-07,
      "loss": 0.0944,
      "step": 758400
    },
    {
      "epoch": 4.927084348306213,
      "grad_norm": 0.9785309433937073,
      "learning_rate": 7.757680629017568e-07,
      "loss": 0.091,
      "step": 758500
    },
    {
      "epoch": 4.9277339309493655,
      "grad_norm": 1.021004557609558,
      "learning_rate": 7.688576044757658e-07,
      "loss": 0.0988,
      "step": 758600
    },
    {
      "epoch": 4.928383513592517,
      "grad_norm": 0.6997320055961609,
      "learning_rate": 7.619471460497747e-07,
      "loss": 0.0985,
      "step": 758700
    },
    {
      "epoch": 4.929033096235669,
      "grad_norm": 1.3546173572540283,
      "learning_rate": 7.550366876237836e-07,
      "loss": 0.0929,
      "step": 758800
    },
    {
      "epoch": 4.92968267887882,
      "grad_norm": 1.3041735887527466,
      "learning_rate": 7.481262291977925e-07,
      "loss": 0.096,
      "step": 758900
    },
    {
      "epoch": 4.930332261521972,
      "grad_norm": 0.8251325488090515,
      "learning_rate": 7.412157707718015e-07,
      "loss": 0.094,
      "step": 759000
    },
    {
      "epoch": 4.930981844165124,
      "grad_norm": 0.9592748880386353,
      "learning_rate": 7.343053123458104e-07,
      "loss": 0.0956,
      "step": 759100
    },
    {
      "epoch": 4.931631426808275,
      "grad_norm": 1.4968478679656982,
      "learning_rate": 7.273948539198194e-07,
      "loss": 0.0913,
      "step": 759200
    },
    {
      "epoch": 4.932281009451428,
      "grad_norm": 1.2799451351165771,
      "learning_rate": 7.204843954938283e-07,
      "loss": 0.0981,
      "step": 759300
    },
    {
      "epoch": 4.932930592094579,
      "grad_norm": 0.6229979395866394,
      "learning_rate": 7.135739370678372e-07,
      "loss": 0.0975,
      "step": 759400
    },
    {
      "epoch": 4.933580174737731,
      "grad_norm": 0.5244733095169067,
      "learning_rate": 7.066634786418461e-07,
      "loss": 0.0972,
      "step": 759500
    },
    {
      "epoch": 4.934229757380883,
      "grad_norm": 0.9115732908248901,
      "learning_rate": 6.997530202158551e-07,
      "loss": 0.0865,
      "step": 759600
    },
    {
      "epoch": 4.934879340024034,
      "grad_norm": 1.0922402143478394,
      "learning_rate": 6.928425617898641e-07,
      "loss": 0.1027,
      "step": 759700
    },
    {
      "epoch": 4.935528922667187,
      "grad_norm": 0.6231839656829834,
      "learning_rate": 6.85932103363873e-07,
      "loss": 0.0979,
      "step": 759800
    },
    {
      "epoch": 4.936178505310338,
      "grad_norm": 0.8636009097099304,
      "learning_rate": 6.790216449378819e-07,
      "loss": 0.0926,
      "step": 759900
    },
    {
      "epoch": 4.93682808795349,
      "grad_norm": 1.3323569297790527,
      "learning_rate": 6.721111865118908e-07,
      "loss": 0.0952,
      "step": 760000
    },
    {
      "epoch": 4.937477670596642,
      "grad_norm": 1.1657676696777344,
      "learning_rate": 6.652007280858997e-07,
      "loss": 0.0926,
      "step": 760100
    },
    {
      "epoch": 4.938127253239793,
      "grad_norm": 1.1721059083938599,
      "learning_rate": 6.582902696599087e-07,
      "loss": 0.0983,
      "step": 760200
    },
    {
      "epoch": 4.938776835882945,
      "grad_norm": 1.655184268951416,
      "learning_rate": 6.513798112339176e-07,
      "loss": 0.0935,
      "step": 760300
    },
    {
      "epoch": 4.9394264185260965,
      "grad_norm": 1.3791214227676392,
      "learning_rate": 6.444693528079266e-07,
      "loss": 0.0955,
      "step": 760400
    },
    {
      "epoch": 4.940076001169249,
      "grad_norm": 1.3304107189178467,
      "learning_rate": 6.375588943819355e-07,
      "loss": 0.0913,
      "step": 760500
    },
    {
      "epoch": 4.940725583812401,
      "grad_norm": 1.4385571479797363,
      "learning_rate": 6.306484359559444e-07,
      "loss": 0.095,
      "step": 760600
    },
    {
      "epoch": 4.941375166455552,
      "grad_norm": 1.0185505151748657,
      "learning_rate": 6.237379775299534e-07,
      "loss": 0.0929,
      "step": 760700
    },
    {
      "epoch": 4.942024749098704,
      "grad_norm": 1.009095311164856,
      "learning_rate": 6.168275191039623e-07,
      "loss": 0.0944,
      "step": 760800
    },
    {
      "epoch": 4.9426743317418556,
      "grad_norm": 0.9436134099960327,
      "learning_rate": 6.099170606779712e-07,
      "loss": 0.0957,
      "step": 760900
    },
    {
      "epoch": 4.943323914385008,
      "grad_norm": 1.1687875986099243,
      "learning_rate": 6.030066022519802e-07,
      "loss": 0.0913,
      "step": 761000
    },
    {
      "epoch": 4.94397349702816,
      "grad_norm": 1.0108963251113892,
      "learning_rate": 5.960961438259891e-07,
      "loss": 0.0894,
      "step": 761100
    },
    {
      "epoch": 4.944623079671311,
      "grad_norm": 0.9459832310676575,
      "learning_rate": 5.891856853999981e-07,
      "loss": 0.0977,
      "step": 761200
    },
    {
      "epoch": 4.945272662314463,
      "grad_norm": 0.8972187638282776,
      "learning_rate": 5.82275226974007e-07,
      "loss": 0.091,
      "step": 761300
    },
    {
      "epoch": 4.945922244957615,
      "grad_norm": 1.139697790145874,
      "learning_rate": 5.753647685480159e-07,
      "loss": 0.097,
      "step": 761400
    },
    {
      "epoch": 4.946571827600766,
      "grad_norm": 1.5524516105651855,
      "learning_rate": 5.684543101220248e-07,
      "loss": 0.0957,
      "step": 761500
    },
    {
      "epoch": 4.947221410243918,
      "grad_norm": 1.136534571647644,
      "learning_rate": 5.615438516960338e-07,
      "loss": 0.0908,
      "step": 761600
    },
    {
      "epoch": 4.94787099288707,
      "grad_norm": 1.1173864603042603,
      "learning_rate": 5.546333932700428e-07,
      "loss": 0.0932,
      "step": 761700
    },
    {
      "epoch": 4.948520575530222,
      "grad_norm": 1.276147484779358,
      "learning_rate": 5.477229348440517e-07,
      "loss": 0.0958,
      "step": 761800
    },
    {
      "epoch": 4.949170158173374,
      "grad_norm": 0.9299277067184448,
      "learning_rate": 5.408124764180606e-07,
      "loss": 0.0938,
      "step": 761900
    },
    {
      "epoch": 4.949819740816525,
      "grad_norm": 0.6193356513977051,
      "learning_rate": 5.339020179920695e-07,
      "loss": 0.093,
      "step": 762000
    },
    {
      "epoch": 4.950469323459677,
      "grad_norm": 1.5427576303482056,
      "learning_rate": 5.269915595660784e-07,
      "loss": 0.0913,
      "step": 762100
    },
    {
      "epoch": 4.951118906102829,
      "grad_norm": 1.3699778318405151,
      "learning_rate": 5.200811011400874e-07,
      "loss": 0.092,
      "step": 762200
    },
    {
      "epoch": 4.951768488745981,
      "grad_norm": 1.304444432258606,
      "learning_rate": 5.131706427140965e-07,
      "loss": 0.0921,
      "step": 762300
    },
    {
      "epoch": 4.952418071389133,
      "grad_norm": 0.8535709381103516,
      "learning_rate": 5.062601842881054e-07,
      "loss": 0.1001,
      "step": 762400
    },
    {
      "epoch": 4.953067654032284,
      "grad_norm": 1.1673834323883057,
      "learning_rate": 4.993497258621143e-07,
      "loss": 0.0965,
      "step": 762500
    },
    {
      "epoch": 4.953717236675436,
      "grad_norm": 1.7209882736206055,
      "learning_rate": 4.924392674361232e-07,
      "loss": 0.0946,
      "step": 762600
    },
    {
      "epoch": 4.9543668193185875,
      "grad_norm": 1.0917409658432007,
      "learning_rate": 4.855288090101322e-07,
      "loss": 0.0901,
      "step": 762700
    },
    {
      "epoch": 4.955016401961739,
      "grad_norm": 1.5597058534622192,
      "learning_rate": 4.786183505841411e-07,
      "loss": 0.0952,
      "step": 762800
    },
    {
      "epoch": 4.955665984604892,
      "grad_norm": 1.1874374151229858,
      "learning_rate": 4.7170789215815e-07,
      "loss": 0.0991,
      "step": 762900
    },
    {
      "epoch": 4.956315567248043,
      "grad_norm": 1.7749789953231812,
      "learning_rate": 4.647974337321589e-07,
      "loss": 0.0947,
      "step": 763000
    },
    {
      "epoch": 4.956965149891195,
      "grad_norm": 0.7604479193687439,
      "learning_rate": 4.5788697530616786e-07,
      "loss": 0.0896,
      "step": 763100
    },
    {
      "epoch": 4.9576147325343465,
      "grad_norm": 1.1156948804855347,
      "learning_rate": 4.5097651688017686e-07,
      "loss": 0.0979,
      "step": 763200
    },
    {
      "epoch": 4.958264315177498,
      "grad_norm": 1.161298155784607,
      "learning_rate": 4.4406605845418576e-07,
      "loss": 0.0907,
      "step": 763300
    },
    {
      "epoch": 4.958913897820651,
      "grad_norm": 0.9390655159950256,
      "learning_rate": 4.371556000281947e-07,
      "loss": 0.0919,
      "step": 763400
    },
    {
      "epoch": 4.959563480463802,
      "grad_norm": 0.7022086977958679,
      "learning_rate": 4.302451416022036e-07,
      "loss": 0.0911,
      "step": 763500
    },
    {
      "epoch": 4.960213063106954,
      "grad_norm": 1.0330537557601929,
      "learning_rate": 4.233346831762125e-07,
      "loss": 0.0935,
      "step": 763600
    },
    {
      "epoch": 4.9608626457501055,
      "grad_norm": 1.1842553615570068,
      "learning_rate": 4.164242247502215e-07,
      "loss": 0.096,
      "step": 763700
    },
    {
      "epoch": 4.961512228393257,
      "grad_norm": 1.6142518520355225,
      "learning_rate": 4.0951376632423047e-07,
      "loss": 0.091,
      "step": 763800
    },
    {
      "epoch": 4.962161811036409,
      "grad_norm": 1.1081658601760864,
      "learning_rate": 4.0260330789823937e-07,
      "loss": 0.0919,
      "step": 763900
    },
    {
      "epoch": 4.962811393679561,
      "grad_norm": 1.5002772808074951,
      "learning_rate": 3.956928494722483e-07,
      "loss": 0.0915,
      "step": 764000
    },
    {
      "epoch": 4.963460976322713,
      "grad_norm": 1.485068917274475,
      "learning_rate": 3.8878239104625727e-07,
      "loss": 0.0916,
      "step": 764100
    },
    {
      "epoch": 4.9641105589658645,
      "grad_norm": 1.009713888168335,
      "learning_rate": 3.8187193262026617e-07,
      "loss": 0.0914,
      "step": 764200
    },
    {
      "epoch": 4.964760141609016,
      "grad_norm": 1.1459858417510986,
      "learning_rate": 3.749614741942751e-07,
      "loss": 0.0843,
      "step": 764300
    },
    {
      "epoch": 4.965409724252168,
      "grad_norm": 1.834215760231018,
      "learning_rate": 3.680510157682841e-07,
      "loss": 0.0935,
      "step": 764400
    },
    {
      "epoch": 4.966059306895319,
      "grad_norm": 1.2733078002929688,
      "learning_rate": 3.61140557342293e-07,
      "loss": 0.0919,
      "step": 764500
    },
    {
      "epoch": 4.966708889538472,
      "grad_norm": 0.9374325275421143,
      "learning_rate": 3.542300989163019e-07,
      "loss": 0.1003,
      "step": 764600
    },
    {
      "epoch": 4.9673584721816235,
      "grad_norm": 0.9346233606338501,
      "learning_rate": 3.473196404903109e-07,
      "loss": 0.0919,
      "step": 764700
    },
    {
      "epoch": 4.968008054824775,
      "grad_norm": 1.0506654977798462,
      "learning_rate": 3.404091820643198e-07,
      "loss": 0.0924,
      "step": 764800
    },
    {
      "epoch": 4.968657637467927,
      "grad_norm": 1.1185860633850098,
      "learning_rate": 3.3349872363832873e-07,
      "loss": 0.0916,
      "step": 764900
    },
    {
      "epoch": 4.969307220111078,
      "grad_norm": 0.7107995748519897,
      "learning_rate": 3.265882652123377e-07,
      "loss": 0.0943,
      "step": 765000
    },
    {
      "epoch": 4.96995680275423,
      "grad_norm": 1.2710356712341309,
      "learning_rate": 3.1967780678634663e-07,
      "loss": 0.0952,
      "step": 765100
    },
    {
      "epoch": 4.9706063853973825,
      "grad_norm": 0.8597143292427063,
      "learning_rate": 3.1276734836035553e-07,
      "loss": 0.0926,
      "step": 765200
    },
    {
      "epoch": 4.971255968040534,
      "grad_norm": 1.3666496276855469,
      "learning_rate": 3.058568899343645e-07,
      "loss": 0.0913,
      "step": 765300
    },
    {
      "epoch": 4.971905550683686,
      "grad_norm": 1.3085980415344238,
      "learning_rate": 2.9894643150837344e-07,
      "loss": 0.092,
      "step": 765400
    },
    {
      "epoch": 4.972555133326837,
      "grad_norm": 0.6554669141769409,
      "learning_rate": 2.9203597308238234e-07,
      "loss": 0.0953,
      "step": 765500
    },
    {
      "epoch": 4.973204715969989,
      "grad_norm": 1.7902462482452393,
      "learning_rate": 2.851255146563913e-07,
      "loss": 0.0914,
      "step": 765600
    },
    {
      "epoch": 4.973854298613141,
      "grad_norm": 1.1577016115188599,
      "learning_rate": 2.7821505623040024e-07,
      "loss": 0.0914,
      "step": 765700
    },
    {
      "epoch": 4.974503881256293,
      "grad_norm": 1.1024423837661743,
      "learning_rate": 2.7130459780440914e-07,
      "loss": 0.0996,
      "step": 765800
    },
    {
      "epoch": 4.975153463899445,
      "grad_norm": 0.5911573767662048,
      "learning_rate": 2.643941393784181e-07,
      "loss": 0.0934,
      "step": 765900
    },
    {
      "epoch": 4.975803046542596,
      "grad_norm": 0.9494019746780396,
      "learning_rate": 2.5748368095242704e-07,
      "loss": 0.0925,
      "step": 766000
    },
    {
      "epoch": 4.976452629185748,
      "grad_norm": 1.1169402599334717,
      "learning_rate": 2.50573222526436e-07,
      "loss": 0.0954,
      "step": 766100
    },
    {
      "epoch": 4.9771022118289,
      "grad_norm": 1.5208208560943604,
      "learning_rate": 2.436627641004449e-07,
      "loss": 0.0982,
      "step": 766200
    },
    {
      "epoch": 4.977751794472052,
      "grad_norm": 0.5942235589027405,
      "learning_rate": 2.3675230567445382e-07,
      "loss": 0.0961,
      "step": 766300
    },
    {
      "epoch": 4.978401377115204,
      "grad_norm": 0.795782208442688,
      "learning_rate": 2.298418472484628e-07,
      "loss": 0.0986,
      "step": 766400
    },
    {
      "epoch": 4.979050959758355,
      "grad_norm": 1.0537909269332886,
      "learning_rate": 2.2293138882247172e-07,
      "loss": 0.0911,
      "step": 766500
    },
    {
      "epoch": 4.979700542401507,
      "grad_norm": 1.6214193105697632,
      "learning_rate": 2.1602093039648067e-07,
      "loss": 0.0868,
      "step": 766600
    },
    {
      "epoch": 4.980350125044659,
      "grad_norm": 1.3145054578781128,
      "learning_rate": 2.091104719704896e-07,
      "loss": 0.0978,
      "step": 766700
    },
    {
      "epoch": 4.98099970768781,
      "grad_norm": 0.9930841326713562,
      "learning_rate": 2.022000135444985e-07,
      "loss": 0.097,
      "step": 766800
    },
    {
      "epoch": 4.981649290330962,
      "grad_norm": 1.084164023399353,
      "learning_rate": 1.9528955511850745e-07,
      "loss": 0.0946,
      "step": 766900
    },
    {
      "epoch": 4.9822988729741144,
      "grad_norm": 1.4588557481765747,
      "learning_rate": 1.883790966925164e-07,
      "loss": 0.0909,
      "step": 767000
    },
    {
      "epoch": 4.982948455617266,
      "grad_norm": 0.9347730875015259,
      "learning_rate": 1.8146863826652533e-07,
      "loss": 0.091,
      "step": 767100
    },
    {
      "epoch": 4.983598038260418,
      "grad_norm": 1.2397578954696655,
      "learning_rate": 1.7455817984053428e-07,
      "loss": 0.092,
      "step": 767200
    },
    {
      "epoch": 4.984247620903569,
      "grad_norm": 1.4636950492858887,
      "learning_rate": 1.676477214145432e-07,
      "loss": 0.0974,
      "step": 767300
    },
    {
      "epoch": 4.984897203546721,
      "grad_norm": 1.2786341905593872,
      "learning_rate": 1.6073726298855213e-07,
      "loss": 0.0938,
      "step": 767400
    },
    {
      "epoch": 4.9855467861898735,
      "grad_norm": 1.460591197013855,
      "learning_rate": 1.5382680456256108e-07,
      "loss": 0.0956,
      "step": 767500
    },
    {
      "epoch": 4.986196368833025,
      "grad_norm": 1.1787906885147095,
      "learning_rate": 1.4691634613657e-07,
      "loss": 0.0918,
      "step": 767600
    },
    {
      "epoch": 4.986845951476177,
      "grad_norm": 1.0251259803771973,
      "learning_rate": 1.4000588771057896e-07,
      "loss": 0.0972,
      "step": 767700
    },
    {
      "epoch": 4.987495534119328,
      "grad_norm": 0.9016215205192566,
      "learning_rate": 1.3309542928458789e-07,
      "loss": 0.1002,
      "step": 767800
    },
    {
      "epoch": 4.98814511676248,
      "grad_norm": 1.6653201580047607,
      "learning_rate": 1.261849708585968e-07,
      "loss": 0.0957,
      "step": 767900
    },
    {
      "epoch": 4.988794699405632,
      "grad_norm": 0.6108306050300598,
      "learning_rate": 1.1927451243260576e-07,
      "loss": 0.0889,
      "step": 768000
    },
    {
      "epoch": 4.989444282048783,
      "grad_norm": 1.1740320920944214,
      "learning_rate": 1.123640540066147e-07,
      "loss": 0.1053,
      "step": 768100
    },
    {
      "epoch": 4.990093864691936,
      "grad_norm": 1.3513201475143433,
      "learning_rate": 1.0545359558062364e-07,
      "loss": 0.0938,
      "step": 768200
    },
    {
      "epoch": 4.990743447335087,
      "grad_norm": 1.0971379280090332,
      "learning_rate": 9.854313715463255e-08,
      "loss": 0.0938,
      "step": 768300
    },
    {
      "epoch": 4.991393029978239,
      "grad_norm": 1.6261667013168335,
      "learning_rate": 9.16326787286415e-08,
      "loss": 0.0935,
      "step": 768400
    },
    {
      "epoch": 4.992042612621391,
      "grad_norm": 1.6763596534729004,
      "learning_rate": 8.472222030265044e-08,
      "loss": 0.0961,
      "step": 768500
    },
    {
      "epoch": 4.992692195264542,
      "grad_norm": 0.7881144285202026,
      "learning_rate": 7.781176187665938e-08,
      "loss": 0.0956,
      "step": 768600
    },
    {
      "epoch": 4.993341777907695,
      "grad_norm": 1.2112771272659302,
      "learning_rate": 7.090130345066831e-08,
      "loss": 0.093,
      "step": 768700
    },
    {
      "epoch": 4.993991360550846,
      "grad_norm": 0.9759792685508728,
      "learning_rate": 6.399084502467725e-08,
      "loss": 0.096,
      "step": 768800
    },
    {
      "epoch": 4.994640943193998,
      "grad_norm": 1.0242894887924194,
      "learning_rate": 5.7080386598686185e-08,
      "loss": 0.0944,
      "step": 768900
    },
    {
      "epoch": 4.99529052583715,
      "grad_norm": 0.9722064137458801,
      "learning_rate": 5.0169928172695124e-08,
      "loss": 0.0955,
      "step": 769000
    },
    {
      "epoch": 4.995940108480301,
      "grad_norm": 0.9204592704772949,
      "learning_rate": 4.3259469746704056e-08,
      "loss": 0.0965,
      "step": 769100
    },
    {
      "epoch": 4.996589691123453,
      "grad_norm": 0.8560224175453186,
      "learning_rate": 3.6349011320712995e-08,
      "loss": 0.0933,
      "step": 769200
    },
    {
      "epoch": 4.9972392737666045,
      "grad_norm": 0.7620701789855957,
      "learning_rate": 2.943855289472193e-08,
      "loss": 0.0954,
      "step": 769300
    },
    {
      "epoch": 4.997888856409757,
      "grad_norm": 1.4871174097061157,
      "learning_rate": 2.252809446873087e-08,
      "loss": 0.0937,
      "step": 769400
    },
    {
      "epoch": 4.998538439052909,
      "grad_norm": 1.196077585220337,
      "learning_rate": 1.5617636042739805e-08,
      "loss": 0.0962,
      "step": 769500
    },
    {
      "epoch": 4.99918802169606,
      "grad_norm": 0.8614132404327393,
      "learning_rate": 8.70717761674874e-09,
      "loss": 0.0995,
      "step": 769600
    },
    {
      "epoch": 4.999837604339212,
      "grad_norm": 0.8445954322814941,
      "learning_rate": 1.7967191907576764e-09,
      "loss": 0.0905,
      "step": 769700
    }
  ],
  "logging_steps": 100,
  "max_steps": 769725,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1350979897342976e+17,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
