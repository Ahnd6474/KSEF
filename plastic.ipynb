{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be96a56646559a26",
   "metadata": {},
   "source": [
    "# Plastic Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feaf6ce639fffd7",
   "metadata": {},
   "source": [
    "We attempt to design new plastic molecule using LDM.\n",
    "Our main goal is on\n",
    "1. Straw\n",
    "2. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba3d462540fb66",
   "metadata": {},
   "source": [
    "## Setup enviroment\n",
    "!git clone https://github.com/Ahnd6474/KSEF\n",
    "\n",
    "%cd your_shit/GitHub/KSEF\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "id": "10c1e0914dd7ef6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T23:08:53.233802Z",
     "start_time": "2025-11-30T23:08:40.672658Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Iterator, Optional\n",
    "import pyarrow\n",
    "import torch\n",
    "from geoldm.configs import get_dataset_info\n",
    "from geoldm.qm9 import dataset, load_model, sampling, visualize_molecule_3d"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "9c5b80a769160d02",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "1bb23c5d1539f3ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T23:09:51.670159Z",
     "start_time": "2025-11-30T23:09:49.273321Z"
    }
   },
   "source": [
    "df=pd.read_parquet('data/plastic.parquet')\n",
    "df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         comp          Tg          Tm          Td           YM       TS_y  \\\n",
       "0         0.0  246.341049  354.051544  524.699646   773.208252  25.838213   \n",
       "1         0.1  313.798401  449.257751  502.616241  1866.776855  44.379429   \n",
       "2         0.2  306.681305  446.618713  502.605896  1769.267456  44.743877   \n",
       "3         0.3  298.431488  442.813019  503.664642  1653.973633  44.287182   \n",
       "4         0.4  290.073944  436.398560  505.013275  1508.705688  42.840149   \n",
       "...       ...         ...         ...         ...          ...        ...   \n",
       "1373498   0.0  325.252380  462.438263  502.465759  2680.520020  39.839355   \n",
       "1373499   0.0  369.788818  509.966248  587.491455  2227.901367  33.554047   \n",
       "1373500   0.0  344.375183  457.250885  574.055664  1724.992798  50.262169   \n",
       "1373501   0.0  393.653473  536.671326  700.403137  1942.565186  44.805931   \n",
       "1373502   0.0  317.276978  469.612549  686.914612  1508.900513  54.058247   \n",
       "\n",
       "              TS_b       eps_b   perm_O2   perm_CO2    perm_He   perm_N2  \\\n",
       "0        32.409279  103.836082 -0.074397  -0.109570   0.493981 -0.071000   \n",
       "1        38.010651    7.215131 -0.043089  -0.006026   0.462577 -0.049233   \n",
       "2        37.998493    8.727348 -0.050900  -0.028856   0.455602 -0.060321   \n",
       "3        37.676380   10.997544 -0.057812  -0.048113   0.458959 -0.069126   \n",
       "4        36.907192   14.239414 -0.065853  -0.065725   0.461121 -0.075789   \n",
       "...            ...         ...       ...        ...        ...       ...   \n",
       "1373498  68.497833   30.515642  0.011635  -0.060768   0.179605  0.002471   \n",
       "1373499  37.865055    1.950041  2.547638  11.337365  21.572718  0.559539   \n",
       "1373500  46.697941   27.839941  0.099392   0.332474   1.581858 -0.005775   \n",
       "1373501  65.647507   27.729816  0.016741   0.138303   1.767082 -0.054595   \n",
       "1373502  40.808701    4.400261 -0.004739  -0.059302   0.252653  0.014024   \n",
       "\n",
       "         perm_CH4    perm_H2  \\\n",
       "0       -0.061771   0.296051   \n",
       "1       -0.031425   0.342873   \n",
       "2       -0.039934   0.322175   \n",
       "3       -0.047438   0.314363   \n",
       "4       -0.055182   0.302125   \n",
       "...           ...        ...   \n",
       "1373498  0.001415   0.056135   \n",
       "1373499  0.881040  24.865118   \n",
       "1373500  0.070204   1.482780   \n",
       "1373501 -0.069961   1.336078   \n",
       "1373502  0.044955   0.000020   \n",
       "\n",
       "                                                    smiles    num_side  \\\n",
       "0                         [[*]OCCC(=O)[*], [*]OCCC(=O)[*]]  [0.0, 0.0]   \n",
       "1                      [[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]  [0.0, 0.0]   \n",
       "2                      [[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]  [0.0, 0.0]   \n",
       "3                      [[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]  [0.0, 0.0]   \n",
       "4                      [[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]  [0.0, 0.0]   \n",
       "...                                                    ...         ...   \n",
       "1373498                         [[*]CC([*])O, [*]CC([*])O]  [nan, nan]   \n",
       "1373499           [[*]CC([*])c1ccccc1, [*]CC([*])c1ccccc1]  [nan, nan]   \n",
       "1373500                       [[*]CC([*])Cl, [*]CC([*])Cl]  [nan, nan]   \n",
       "1373501  [[*]CCOC(=O)c1ccc2cc(C(=O)O[*])ccc2c1, [*]CCOC...  [nan, nan]   \n",
       "1373502             [[*]CCCCCC(=O)N[*], [*]CCCCCC(=O)N[*]]  [nan, nan]   \n",
       "\n",
       "           num_back     end_group             names  \n",
       "0        [3.0, 3.0]          [, ]        [pha, pha]  \n",
       "1        [3.0, 3.0]         [, O]        [pha, pha]  \n",
       "2        [3.0, 3.0]         [, O]        [pha, pha]  \n",
       "3        [3.0, 3.0]         [, O]        [pha, pha]  \n",
       "4        [3.0, 3.0]         [, O]        [pha, pha]  \n",
       "...             ...           ...               ...  \n",
       "1373498  [nan, nan]  [None, None]        [PVA, PVA]  \n",
       "1373499  [nan, nan]  [None, None]          [PS, PS]  \n",
       "1373500  [nan, nan]  [None, None]        [PVC, PVC]  \n",
       "1373501  [nan, nan]  [None, None]        [PEN, PEN]  \n",
       "1373502  [nan, nan]  [None, None]  [Nylon6, Nylon6]  \n",
       "\n",
       "[1373503 rows x 19 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comp</th>\n",
       "      <th>Tg</th>\n",
       "      <th>Tm</th>\n",
       "      <th>Td</th>\n",
       "      <th>YM</th>\n",
       "      <th>TS_y</th>\n",
       "      <th>TS_b</th>\n",
       "      <th>eps_b</th>\n",
       "      <th>perm_O2</th>\n",
       "      <th>perm_CO2</th>\n",
       "      <th>perm_He</th>\n",
       "      <th>perm_N2</th>\n",
       "      <th>perm_CH4</th>\n",
       "      <th>perm_H2</th>\n",
       "      <th>smiles</th>\n",
       "      <th>num_side</th>\n",
       "      <th>num_back</th>\n",
       "      <th>end_group</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>246.341049</td>\n",
       "      <td>354.051544</td>\n",
       "      <td>524.699646</td>\n",
       "      <td>773.208252</td>\n",
       "      <td>25.838213</td>\n",
       "      <td>32.409279</td>\n",
       "      <td>103.836082</td>\n",
       "      <td>-0.074397</td>\n",
       "      <td>-0.109570</td>\n",
       "      <td>0.493981</td>\n",
       "      <td>-0.071000</td>\n",
       "      <td>-0.061771</td>\n",
       "      <td>0.296051</td>\n",
       "      <td>[[*]OCCC(=O)[*], [*]OCCC(=O)[*]]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[3.0, 3.0]</td>\n",
       "      <td>[, ]</td>\n",
       "      <td>[pha, pha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>313.798401</td>\n",
       "      <td>449.257751</td>\n",
       "      <td>502.616241</td>\n",
       "      <td>1866.776855</td>\n",
       "      <td>44.379429</td>\n",
       "      <td>38.010651</td>\n",
       "      <td>7.215131</td>\n",
       "      <td>-0.043089</td>\n",
       "      <td>-0.006026</td>\n",
       "      <td>0.462577</td>\n",
       "      <td>-0.049233</td>\n",
       "      <td>-0.031425</td>\n",
       "      <td>0.342873</td>\n",
       "      <td>[[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[3.0, 3.0]</td>\n",
       "      <td>[, O]</td>\n",
       "      <td>[pha, pha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>306.681305</td>\n",
       "      <td>446.618713</td>\n",
       "      <td>502.605896</td>\n",
       "      <td>1769.267456</td>\n",
       "      <td>44.743877</td>\n",
       "      <td>37.998493</td>\n",
       "      <td>8.727348</td>\n",
       "      <td>-0.050900</td>\n",
       "      <td>-0.028856</td>\n",
       "      <td>0.455602</td>\n",
       "      <td>-0.060321</td>\n",
       "      <td>-0.039934</td>\n",
       "      <td>0.322175</td>\n",
       "      <td>[[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[3.0, 3.0]</td>\n",
       "      <td>[, O]</td>\n",
       "      <td>[pha, pha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>298.431488</td>\n",
       "      <td>442.813019</td>\n",
       "      <td>503.664642</td>\n",
       "      <td>1653.973633</td>\n",
       "      <td>44.287182</td>\n",
       "      <td>37.676380</td>\n",
       "      <td>10.997544</td>\n",
       "      <td>-0.057812</td>\n",
       "      <td>-0.048113</td>\n",
       "      <td>0.458959</td>\n",
       "      <td>-0.069126</td>\n",
       "      <td>-0.047438</td>\n",
       "      <td>0.314363</td>\n",
       "      <td>[[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[3.0, 3.0]</td>\n",
       "      <td>[, O]</td>\n",
       "      <td>[pha, pha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>290.073944</td>\n",
       "      <td>436.398560</td>\n",
       "      <td>505.013275</td>\n",
       "      <td>1508.705688</td>\n",
       "      <td>42.840149</td>\n",
       "      <td>36.907192</td>\n",
       "      <td>14.239414</td>\n",
       "      <td>-0.065853</td>\n",
       "      <td>-0.065725</td>\n",
       "      <td>0.461121</td>\n",
       "      <td>-0.075789</td>\n",
       "      <td>-0.055182</td>\n",
       "      <td>0.302125</td>\n",
       "      <td>[[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[3.0, 3.0]</td>\n",
       "      <td>[, O]</td>\n",
       "      <td>[pha, pha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373498</th>\n",
       "      <td>0.0</td>\n",
       "      <td>325.252380</td>\n",
       "      <td>462.438263</td>\n",
       "      <td>502.465759</td>\n",
       "      <td>2680.520020</td>\n",
       "      <td>39.839355</td>\n",
       "      <td>68.497833</td>\n",
       "      <td>30.515642</td>\n",
       "      <td>0.011635</td>\n",
       "      <td>-0.060768</td>\n",
       "      <td>0.179605</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.056135</td>\n",
       "      <td>[[*]CC([*])O, [*]CC([*])O]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[PVA, PVA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>369.788818</td>\n",
       "      <td>509.966248</td>\n",
       "      <td>587.491455</td>\n",
       "      <td>2227.901367</td>\n",
       "      <td>33.554047</td>\n",
       "      <td>37.865055</td>\n",
       "      <td>1.950041</td>\n",
       "      <td>2.547638</td>\n",
       "      <td>11.337365</td>\n",
       "      <td>21.572718</td>\n",
       "      <td>0.559539</td>\n",
       "      <td>0.881040</td>\n",
       "      <td>24.865118</td>\n",
       "      <td>[[*]CC([*])c1ccccc1, [*]CC([*])c1ccccc1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[PS, PS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373500</th>\n",
       "      <td>0.0</td>\n",
       "      <td>344.375183</td>\n",
       "      <td>457.250885</td>\n",
       "      <td>574.055664</td>\n",
       "      <td>1724.992798</td>\n",
       "      <td>50.262169</td>\n",
       "      <td>46.697941</td>\n",
       "      <td>27.839941</td>\n",
       "      <td>0.099392</td>\n",
       "      <td>0.332474</td>\n",
       "      <td>1.581858</td>\n",
       "      <td>-0.005775</td>\n",
       "      <td>0.070204</td>\n",
       "      <td>1.482780</td>\n",
       "      <td>[[*]CC([*])Cl, [*]CC([*])Cl]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[PVC, PVC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373501</th>\n",
       "      <td>0.0</td>\n",
       "      <td>393.653473</td>\n",
       "      <td>536.671326</td>\n",
       "      <td>700.403137</td>\n",
       "      <td>1942.565186</td>\n",
       "      <td>44.805931</td>\n",
       "      <td>65.647507</td>\n",
       "      <td>27.729816</td>\n",
       "      <td>0.016741</td>\n",
       "      <td>0.138303</td>\n",
       "      <td>1.767082</td>\n",
       "      <td>-0.054595</td>\n",
       "      <td>-0.069961</td>\n",
       "      <td>1.336078</td>\n",
       "      <td>[[*]CCOC(=O)c1ccc2cc(C(=O)O[*])ccc2c1, [*]CCOC...</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[PEN, PEN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373502</th>\n",
       "      <td>0.0</td>\n",
       "      <td>317.276978</td>\n",
       "      <td>469.612549</td>\n",
       "      <td>686.914612</td>\n",
       "      <td>1508.900513</td>\n",
       "      <td>54.058247</td>\n",
       "      <td>40.808701</td>\n",
       "      <td>4.400261</td>\n",
       "      <td>-0.004739</td>\n",
       "      <td>-0.059302</td>\n",
       "      <td>0.252653</td>\n",
       "      <td>0.014024</td>\n",
       "      <td>0.044955</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>[[*]CCCCCC(=O)N[*], [*]CCCCCC(=O)N[*]]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[Nylon6, Nylon6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1373503 rows × 19 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "bb66463e2017a951",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T23:09:57.505839Z",
     "start_time": "2025-11-30T23:09:56.946317Z"
    }
   },
   "source": [
    "df.describe()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               comp            Tg            Tm            Td            YM  \\\n",
       "count  1.373503e+06  1.373503e+06  1.373503e+06  1.373503e+06  1.373503e+06   \n",
       "mean   4.997987e-01  2.725839e+02  3.529060e+02  5.769558e+02  5.405507e+02   \n",
       "std    2.583418e-01  2.701165e+01  3.713276e+01  2.686434e+01  4.882776e+02   \n",
       "min    0.000000e+00  2.020853e+02  2.794441e+02  4.246918e+02  9.102757e+01   \n",
       "25%    3.000000e-01  2.543430e+02  3.266345e+02  5.673138e+02  1.744876e+02   \n",
       "50%    5.000000e-01  2.699055e+02  3.456665e+02  5.827677e+02  3.266322e+02   \n",
       "75%    7.000000e-01  2.883478e+02  3.725729e+02  5.933018e+02  7.870456e+02   \n",
       "max    9.000000e-01  4.033092e+02  5.529254e+02  7.004031e+02  3.081163e+03   \n",
       "\n",
       "               TS_y          TS_b         eps_b       perm_O2      perm_CO2  \\\n",
       "count  1.373503e+06  1.373503e+06  1.373503e+06  1.373503e+06  1.373503e+06   \n",
       "mean   2.073843e+01  2.460826e+01  2.115185e+02  8.110294e-01  4.332410e+00   \n",
       "std    1.366027e+01  1.229784e+01  1.205617e+02  8.184573e-01  3.972706e+00   \n",
       "min    3.014123e+00  4.558969e+00  4.952888e-01 -9.569907e-02 -1.267213e-01   \n",
       "25%    8.976903e+00  1.493889e+01  1.076303e+02  3.791428e-01  1.926220e+00   \n",
       "50%    1.742608e+01  2.180278e+01  2.111784e+02  6.681085e-01  3.541990e+00   \n",
       "75%    3.001149e+01  3.317859e+01  3.043738e+02  1.066986e+00  5.741556e+00   \n",
       "max    7.795893e+01  1.298841e+02  5.666409e+02  3.425747e+01  1.253688e+02   \n",
       "\n",
       "            perm_He       perm_N2      perm_CH4       perm_H2  \n",
       "count  1.373503e+06  1.373503e+06  1.373503e+06  1.373503e+06  \n",
       "mean   7.026700e+00  1.901237e-01  4.661335e-01  7.116223e+00  \n",
       "std    4.018037e+00  2.560846e-01  5.097858e-01  4.612499e+00  \n",
       "min    1.728067e-01 -1.294968e-01 -1.326250e-01  2.026558e-05  \n",
       "25%    4.530061e+00  5.799794e-02  1.869081e-01  4.429616e+00  \n",
       "50%    6.793644e+00  1.420151e-01  3.605243e-01  6.750260e+00  \n",
       "75%    9.030967e+00  2.563907e-01  6.126437e-01  9.138975e+00  \n",
       "max    1.188544e+02  8.895615e+00  1.651693e+01  1.671498e+02  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comp</th>\n",
       "      <th>Tg</th>\n",
       "      <th>Tm</th>\n",
       "      <th>Td</th>\n",
       "      <th>YM</th>\n",
       "      <th>TS_y</th>\n",
       "      <th>TS_b</th>\n",
       "      <th>eps_b</th>\n",
       "      <th>perm_O2</th>\n",
       "      <th>perm_CO2</th>\n",
       "      <th>perm_He</th>\n",
       "      <th>perm_N2</th>\n",
       "      <th>perm_CH4</th>\n",
       "      <th>perm_H2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.997987e-01</td>\n",
       "      <td>2.725839e+02</td>\n",
       "      <td>3.529060e+02</td>\n",
       "      <td>5.769558e+02</td>\n",
       "      <td>5.405507e+02</td>\n",
       "      <td>2.073843e+01</td>\n",
       "      <td>2.460826e+01</td>\n",
       "      <td>2.115185e+02</td>\n",
       "      <td>8.110294e-01</td>\n",
       "      <td>4.332410e+00</td>\n",
       "      <td>7.026700e+00</td>\n",
       "      <td>1.901237e-01</td>\n",
       "      <td>4.661335e-01</td>\n",
       "      <td>7.116223e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.583418e-01</td>\n",
       "      <td>2.701165e+01</td>\n",
       "      <td>3.713276e+01</td>\n",
       "      <td>2.686434e+01</td>\n",
       "      <td>4.882776e+02</td>\n",
       "      <td>1.366027e+01</td>\n",
       "      <td>1.229784e+01</td>\n",
       "      <td>1.205617e+02</td>\n",
       "      <td>8.184573e-01</td>\n",
       "      <td>3.972706e+00</td>\n",
       "      <td>4.018037e+00</td>\n",
       "      <td>2.560846e-01</td>\n",
       "      <td>5.097858e-01</td>\n",
       "      <td>4.612499e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.020853e+02</td>\n",
       "      <td>2.794441e+02</td>\n",
       "      <td>4.246918e+02</td>\n",
       "      <td>9.102757e+01</td>\n",
       "      <td>3.014123e+00</td>\n",
       "      <td>4.558969e+00</td>\n",
       "      <td>4.952888e-01</td>\n",
       "      <td>-9.569907e-02</td>\n",
       "      <td>-1.267213e-01</td>\n",
       "      <td>1.728067e-01</td>\n",
       "      <td>-1.294968e-01</td>\n",
       "      <td>-1.326250e-01</td>\n",
       "      <td>2.026558e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>2.543430e+02</td>\n",
       "      <td>3.266345e+02</td>\n",
       "      <td>5.673138e+02</td>\n",
       "      <td>1.744876e+02</td>\n",
       "      <td>8.976903e+00</td>\n",
       "      <td>1.493889e+01</td>\n",
       "      <td>1.076303e+02</td>\n",
       "      <td>3.791428e-01</td>\n",
       "      <td>1.926220e+00</td>\n",
       "      <td>4.530061e+00</td>\n",
       "      <td>5.799794e-02</td>\n",
       "      <td>1.869081e-01</td>\n",
       "      <td>4.429616e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>2.699055e+02</td>\n",
       "      <td>3.456665e+02</td>\n",
       "      <td>5.827677e+02</td>\n",
       "      <td>3.266322e+02</td>\n",
       "      <td>1.742608e+01</td>\n",
       "      <td>2.180278e+01</td>\n",
       "      <td>2.111784e+02</td>\n",
       "      <td>6.681085e-01</td>\n",
       "      <td>3.541990e+00</td>\n",
       "      <td>6.793644e+00</td>\n",
       "      <td>1.420151e-01</td>\n",
       "      <td>3.605243e-01</td>\n",
       "      <td>6.750260e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000e-01</td>\n",
       "      <td>2.883478e+02</td>\n",
       "      <td>3.725729e+02</td>\n",
       "      <td>5.933018e+02</td>\n",
       "      <td>7.870456e+02</td>\n",
       "      <td>3.001149e+01</td>\n",
       "      <td>3.317859e+01</td>\n",
       "      <td>3.043738e+02</td>\n",
       "      <td>1.066986e+00</td>\n",
       "      <td>5.741556e+00</td>\n",
       "      <td>9.030967e+00</td>\n",
       "      <td>2.563907e-01</td>\n",
       "      <td>6.126437e-01</td>\n",
       "      <td>9.138975e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000e-01</td>\n",
       "      <td>4.033092e+02</td>\n",
       "      <td>5.529254e+02</td>\n",
       "      <td>7.004031e+02</td>\n",
       "      <td>3.081163e+03</td>\n",
       "      <td>7.795893e+01</td>\n",
       "      <td>1.298841e+02</td>\n",
       "      <td>5.666409e+02</td>\n",
       "      <td>3.425747e+01</td>\n",
       "      <td>1.253688e+02</td>\n",
       "      <td>1.188544e+02</td>\n",
       "      <td>8.895615e+00</td>\n",
       "      <td>1.651693e+01</td>\n",
       "      <td>1.671498e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "5152fb9dcbe4c51d",
   "metadata": {},
   "source": [
    "## Property Prediction\n",
    "We use MLP multi regressor to predict property of plastic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980fc32927142b91",
   "metadata": {},
   "source": [
    "### Load Model and helpers"
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-01T06:32:18.332723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import ast\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from geoldm import encode, smiles_to_3d\n",
    "from geoldm.configs import get_dataset_info\n",
    "from geoldm.qm9 import dataset as qm9_dataset\n",
    "\n",
    "# -------------------------\n",
    "# 설정\n",
    "# -------------------------\n",
    "\n",
    "DATA_PATH = Path(\"data/plastic.parquet\")               # 입력 데이터\n",
    "CHECKPOINT_DIR = Path(\"./qm9_latent2\")            # GeoLDM QM9 checkpoint 디렉토리\n",
    "LATENT_CACHE_PATH = Path(\"latents.pt\")            # SMILES -> latent dict 저장 위치\n",
    "MODEL_CKPT_PATH = Path(\"mixture_mlp.pt\")          # 학습된 MLP 저장 위치\n",
    "GRID_CSV_PATH = Path(\"results1/mixture_grid_search.csv\")   # 그리드 서치 결과 CSV\n",
    "\n",
    "TARGET_COLUMNS = [\n",
    "    \"Tg\",\n",
    "    \"Tm\",\n",
    "    \"Td\",\n",
    "    \"YM\",\n",
    "    \"TS_y\",\n",
    "    \"TS_b\",\n",
    "    \"eps_b\",\n",
    "    \"perm_O2\",\n",
    "    \"perm_CO2\",\n",
    "    \"perm_He\",\n",
    "    \"perm_N2\",\n",
    "    \"perm_CH4\",\n",
    "    \"perm_H2\",\n",
    "]\n",
    "\n",
    "# grid search에서 사용할 alpha 값들\n",
    "ALPHA_GRID = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "EPOCHS = 100\n",
    "EPOCHS2=500\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "VALID_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "# 조성비가 별도 컬럼에 있을 때 그 컬럼명\n",
    "ALPHA_COLUMN_NAME = \"comp\"   # 필요하면 여기 바꿔라\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# GeoLDM 로딩 / 3D 변환\n",
    "# -------------------------\n",
    "\n",
    "def load_qm9_latent_diffusion(checkpoint_dir: Path):\n",
    "    \"\"\"\n",
    "    GeoLDM latent diffusion 모델 로드 (무조건 CPU).\n",
    "    \"\"\"\n",
    "    args_path = checkpoint_dir / \"args.pickle\"\n",
    "    with args_path.open(\"rb\") as handle:\n",
    "        args = pickle.load(handle)\n",
    "\n",
    "    # GeoLDM은 CPU로만 돌리게 강제\n",
    "    setattr(args, \"cuda\", False)\n",
    "\n",
    "    dataset_info = get_dataset_info(args.dataset, args.remove_h)\n",
    "    dataloaders, _ = qm9_dataset.retrieve_dataloaders(args)\n",
    "    train_loader = dataloaders[\"train\"]\n",
    "\n",
    "    from geoldm import load_model\n",
    "\n",
    "    model, nodes_dist, _ = load_model(\n",
    "        stage=\"latent_diffusion\",\n",
    "        args=args,\n",
    "        dataset_info=dataset_info,\n",
    "        dataloader_train=train_loader,\n",
    "        checkpoint_path=checkpoint_dir,\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    return model, dataset_info, nodes_dist, device\n",
    "\n",
    "\n",
    "def conformer_to_tensors(\n",
    "    conformer: Dict,\n",
    "    dataset_info: Dict,\n",
    "    device: torch.device,\n",
    ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    RDKit conformer 하나를 GeoLDM 인코더가 먹을 수 있는 텐서로 변환.\n",
    "    - 좌표는 원점 기준으로 센터링해서 mean-zero 조건 만족.\n",
    "    - QM9이 모르는 원소(S, Cl 등)는 일단 O로 매핑.\n",
    "    \"\"\"\n",
    "    atom_decoder: List[str] = dataset_info[\"atom_decoder\"]  # ['H','C','N','O','F'] 같은 것\n",
    "    atom_encoder = {symbol: idx for idx, symbol in enumerate(atom_decoder)}\n",
    "\n",
    "    mapped_indices: List[int] = []\n",
    "    for symbol in conformer[\"atom_symbols\"]:\n",
    "        if symbol not in atom_encoder:\n",
    "            print(f\"[WARN] Unknown atom type {symbol!r} -> mapped to 'O'\")\n",
    "            if \"O\" in atom_encoder:\n",
    "                symbol = \"O\"\n",
    "            else:\n",
    "                symbol = atom_decoder[0]\n",
    "        mapped_indices.append(atom_encoder[symbol])\n",
    "\n",
    "    atom_indices = torch.tensor(mapped_indices, dtype=torch.long, device=device)\n",
    "    one_hot = F.one_hot(atom_indices, num_classes=len(atom_decoder)).float()\n",
    "\n",
    "    positions = torch.tensor(\n",
    "        conformer[\"coordinates\"], dtype=torch.float32, device=device\n",
    "    )\n",
    "    positions = positions - positions.mean(dim=0, keepdim=True)\n",
    "\n",
    "    x = positions.unsqueeze(0)  # (1, N, 3)\n",
    "    h = {\n",
    "        \"categorical\": one_hot.unsqueeze(0),  # (1, N, C)\n",
    "        \"integer\": torch.zeros(one_hot.shape[0], 1, device=device).unsqueeze(0),\n",
    "    }\n",
    "\n",
    "    node_mask = torch.ones(x.shape[0], x.shape[1], 1, device=device)\n",
    "    edge_mask = node_mask.squeeze(-1)[..., None] * node_mask.squeeze(-1)[:, None]\n",
    "    return x, h, node_mask, edge_mask\n",
    "\n",
    "\n",
    "def clean_bigsmiles(big: str) -> str:\n",
    "    \"\"\"\n",
    "    BigSMILES의 [*] anchor를 간단히 제거.\n",
    "    필요하면 여기서 더 복잡하게 전처리해도 됨.\n",
    "    \"\"\"\n",
    "    return big.replace(\"[*]\", \"\")\n",
    "\n",
    "\n",
    "def build_latent_cache():\n",
    "    \"\"\"\n",
    "    plastic.parquet에서 고유 분자들(BigSMILES)을 뽑아서\n",
    "    GeoLDM encoder로 latent 풀링벡터를 만든 뒤\n",
    "    {원본 BigSMILES: pooled_latent} dict를 LATENT_CACHE_PATH에 저장.\n",
    "    \"\"\"\n",
    "    print(\">> plastic.parquet 로드 중...\")\n",
    "    df = pd.read_parquet(DATA_PATH)\n",
    "\n",
    "    # 1) 고유 분자 문자열 수집\n",
    "    unique_mols = set()\n",
    "    for raw in df[\"smiles\"]:\n",
    "        item = raw\n",
    "        if isinstance(item, str):\n",
    "            item = ast.literal_eval(item)\n",
    "        elif isinstance(item, np.ndarray):\n",
    "            item = item.tolist()\n",
    "\n",
    "        if not isinstance(item, (list, tuple)) or len(item) < 2:\n",
    "            raise ValueError(f\"예상과 다른 형태의 값: {repr(item)}\")\n",
    "\n",
    "        mol1, mol2 = item[0], item[1]\n",
    "        unique_mols.add(mol1)\n",
    "        unique_mols.add(mol2)\n",
    "\n",
    "    unique_mols = sorted(unique_mols)\n",
    "    print(f\">> 고유 분자 수: {len(unique_mols)}\")\n",
    "\n",
    "    # 2) GeoLDM 로드 (CPU)\n",
    "    print(\">> GeoLDM latent diffusion 모델 로드 중 (CPU)...\")\n",
    "    model, dataset_info, nodes_dist, device = load_qm9_latent_diffusion(CHECKPOINT_DIR)\n",
    "    model.eval()\n",
    "\n",
    "    latent_dict: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "    # 3) 각 분자에 대해 BigSMILES -> (anchor 제거) -> 3D -> latent\n",
    "    print(\">> 각 분자 latent 계산 중...\")\n",
    "    with torch.no_grad():\n",
    "        for big_smi in tqdm(unique_mols, desc=\"Encoding molecules\"):\n",
    "            cleaned = clean_bigsmiles(big_smi)\n",
    "            try:\n",
    "                conformers = smiles_to_3d(cleaned)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] 3D 생성 실패: {big_smi} ({e}), skip\")\n",
    "                continue\n",
    "\n",
    "            if len(conformers) == 0:\n",
    "                print(f\"[WARN] 3D conformer 없음: {big_smi}, skip\")\n",
    "                continue\n",
    "\n",
    "            conformer = conformers[0]\n",
    "            x, h, node_mask, edge_mask = conformer_to_tensors(\n",
    "                conformer, dataset_info, device\n",
    "            )\n",
    "\n",
    "            # GeoLDM latent encoder\n",
    "            z_x, z_sigma_x, z_h, z_sigma_h = encode(\n",
    "                model, x, h, node_mask=node_mask, edge_mask=edge_mask\n",
    "            )\n",
    "            # 노드 차원 mean pooling 후 concat\n",
    "            z_x_mean = z_x.mean(dim=1).squeeze(0)  # (Dx,)\n",
    "            z_h_mean = z_h.mean(dim=1).squeeze(0)  # (Dh,)\n",
    "            pooled = torch.cat([z_x_mean, z_h_mean], dim=-1).cpu()\n",
    "\n",
    "            latent_dict[big_smi] = pooled\n",
    "\n",
    "    print(\">> latent dict 저장 중:\", LATENT_CACHE_PATH)\n",
    "    torch.save(latent_dict, LATENT_CACHE_PATH)\n",
    "    print(\">> latent dict 저장 완료.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Dataset / MLP\n",
    "# -------------------------\n",
    "\n",
    "class MixtureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    한 샘플: (z_A, z_B, alpha) -> feature 벡터, target은 scaled property.\n",
    "    alpha는\n",
    "      - 우선 df[ALPHA_COLUMN_NAME]에서 찾고,\n",
    "      - 없으면 smiles 리스트의 3번째 원소(item[2])에서 찾는다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        latent_dict: Dict[str, torch.Tensor],\n",
    "        target_means: np.ndarray,\n",
    "        target_stds: np.ndarray,\n",
    "    ):\n",
    "        self.rows = []\n",
    "        self.latent_dict = latent_dict\n",
    "\n",
    "        has_alpha_col = ALPHA_COLUMN_NAME in df.columns\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            raw = row[\"smiles\"]\n",
    "            item = raw\n",
    "            if isinstance(item, str):\n",
    "                item = ast.literal_eval(item)\n",
    "            elif isinstance(item, np.ndarray):\n",
    "                item = item.tolist()\n",
    "\n",
    "            if not isinstance(item, (list, tuple)) or len(item) < 2:\n",
    "                # 최소한 분자 2개는 있어야 한다\n",
    "                continue\n",
    "\n",
    "            mol1, mol2 = item[0], item[1]\n",
    "\n",
    "            # alpha 추출\n",
    "            if has_alpha_col:\n",
    "                alpha = float(row[ALPHA_COLUMN_NAME])\n",
    "            elif len(item) >= 3:\n",
    "                alpha = float(item[2])\n",
    "            else:\n",
    "                # 조성비를 어디서도 못 찾으면 스킵\n",
    "                continue\n",
    "\n",
    "            # latent 없는 분자는 스킵\n",
    "            if mol1 not in latent_dict or mol2 not in latent_dict:\n",
    "                continue\n",
    "\n",
    "            y = row[TARGET_COLUMNS].values.astype(np.float32)\n",
    "            self.rows.append((mol1, mol2, alpha, y))\n",
    "\n",
    "        self.target_means = target_means.astype(np.float32)\n",
    "        self.target_stds = target_stds.astype(np.float32)\n",
    "\n",
    "        if len(self.rows) > 0:\n",
    "            some_latent = next(iter(latent_dict.values()))\n",
    "            self.latent_dim = some_latent.numel()\n",
    "        else:\n",
    "            self.latent_dim = 0\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        mol1, mol2, alpha, y = self.rows[idx]\n",
    "        z_A = self.latent_dict[mol1].float()\n",
    "        z_B = self.latent_dict[mol2].float()\n",
    "        alpha = float(alpha)\n",
    "\n",
    "        # mixture feature\n",
    "        h_mix = alpha * z_A + (1.0 - alpha) * z_B\n",
    "        h_int = torch.abs(z_A - z_B)\n",
    "        alpha_tensor = torch.tensor([alpha], dtype=torch.float32)\n",
    "\n",
    "        # [z_A, z_B, h_mix, h_int, alpha]\n",
    "        x = torch.cat([z_A, z_B, h_mix, h_int, alpha_tensor], dim=0)\n",
    "\n",
    "        # target scaling\n",
    "        y_scaled = (y - self.target_means) / self.target_stds\n",
    "        y_scaled = torch.from_numpy(y_scaled)\n",
    "\n",
    "        return x, y_scaled\n",
    "\n",
    "\n",
    "class MixtureMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    2~3층 정도의 단순 MLP. 여기서는 512, 256 두 층 사용.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_sizes=(512, 256)):\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers += [nn.Linear(prev, h), nn.ReLU()]\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 학습 / 평가 / 그리드 서치\n",
    "# -------------------------\n",
    "\n",
    "def compute_target_stats(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    TARGET_COLUMNS에 대한 mean / std 계산.\n",
    "    \"\"\"\n",
    "    y = df[TARGET_COLUMNS].values.astype(np.float32)\n",
    "    means = y.mean(axis=0)\n",
    "    stds = y.std(axis=0) + 1e-8  # 0 방지\n",
    "    return means, stds\n",
    "\n",
    "\n",
    "def train_mixture_mlp():\n",
    "    \"\"\"\n",
    "    기존 scaled MSE로 학습된 모델이 있으면 그 체크포인트에서 이어서\n",
    "    Kendall-style uncertainty-weighted loss로 파인튜닝한다.\n",
    "    \"\"\"\n",
    "    print(\">> plastic.parquet 로드 중...\")\n",
    "    df = pd.read_parquet(DATA_PATH)\n",
    "\n",
    "    print(\">> latent dict 로드 중...\")\n",
    "    latent_dict: Dict[str, torch.Tensor] = torch.load(LATENT_CACHE_PATH)\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) 타깃 스케일링 통계 + ckpt 로드 (있으면 이어서)\n",
    "    # -------------------------\n",
    "    ckpt = None\n",
    "    if MODEL_CKPT_PATH.exists():\n",
    "        print(\">> 기존 체크포인트 발견: scaled MSE로 학습된 모델에서 이어서 학습합니다.\")\n",
    "        ckpt = torch.load(\n",
    "        MODEL_CKPT_PATH,\n",
    "        map_location=\"cpu\",\n",
    "        weights_only=False,  # <<< 추가\n",
    "        )\n",
    "        target_means = ckpt[\"target_means\"]\n",
    "        target_stds = ckpt[\"target_stds\"]\n",
    "        ckpt_input_dim = ckpt[\"input_dim\"]\n",
    "        output_dim = ckpt[\"output_dim\"]\n",
    "    else:\n",
    "        print(\">> 기존 체크포인트 없음: 새로 target 통계 계산 후 처음부터 학습합니다.\")\n",
    "        target_means, target_stds = compute_target_stats(df)\n",
    "        ckpt_input_dim = None\n",
    "        output_dim = len(TARGET_COLUMNS)\n",
    "\n",
    "    print(\">> target means:\", target_means)\n",
    "    print(\">> target stds:\", target_stds)\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) Dataset 구성\n",
    "    # -------------------------\n",
    "    dataset = MixtureDataset(df, latent_dict, target_means, target_stds)\n",
    "    n_total = len(dataset)\n",
    "    print(\">> 전체 유효 샘플 수:\", n_total)\n",
    "\n",
    "    if n_total == 0:\n",
    "        raise RuntimeError(\n",
    "            \"MixtureDataset이 비어 있음. \"\n",
    "            \"smiles에 [mol1, mol2] + comp(조성비) 구조가 맞는지, \"\n",
    "            \"GeoLDM latent가 제대로 만들어졌는지 확인해라.\"\n",
    "        )\n",
    "\n",
    "    n_valid = int(n_total * VALID_RATIO)\n",
    "    n_test = int(n_total * TEST_RATIO)\n",
    "    n_train = n_total - n_valid - n_test\n",
    "\n",
    "    train_dataset, valid_dataset, test_dataset = random_split(\n",
    "        dataset, [n_train, n_valid, n_test],\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "\n",
    "    input_dim = dataset[0][0].numel()\n",
    "    print(\">> input_dim:\", input_dim, \"output_dim:\", output_dim)\n",
    "\n",
    "    if ckpt_input_dim is not None and input_dim != ckpt_input_dim:\n",
    "        raise RuntimeError(\n",
    "            f\"입력 차원이 기존 ckpt({ckpt_input_dim})와 다름: {input_dim}\"\n",
    "        )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) 모델 + log_vars 세팅 (이어붙이기)\n",
    "    # -------------------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MixtureMLP(input_dim=input_dim, output_dim=output_dim, hidden_sizes=(512, 256))\n",
    "    model.to(device)\n",
    "\n",
    "    # Kendall et al. uncertainty weighting용 파라미터: 길이 T = output_dim\n",
    "    log_vars = nn.Parameter(torch.zeros(output_dim, device=device))\n",
    "\n",
    "    # 기존 ckpt가 있으면 가중치 로드 (scaled MSE로 학습된 상태에서 시작)\n",
    "    if ckpt is not None:\n",
    "        model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "        # 만약 이후에 또 이어붙이기 할 때를 대비해 ckpt에 log_vars가 있으면 복원\n",
    "        if \"log_vars\" in ckpt:\n",
    "            with torch.no_grad():\n",
    "                log_vars.copy_(ckpt[\"log_vars\"].to(device))\n",
    "        print(\">> 기존 모델 가중치 로드 완료.\")\n",
    "\n",
    "    # 모델 파라미터 + log_vars를 같이 최적화\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(model.parameters()) + [log_vars],\n",
    "        lr=LR,\n",
    "    )\n",
    "\n",
    "    writer = SummaryWriter(log_dir=\"runs/polymer_mixture_uncertainty\")\n",
    "\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    global_step = 0\n",
    "\n",
    "    # -------------------------\n",
    "    # 4) Training loop (Kendall loss)\n",
    "    # -------------------------\n",
    "    for epoch in range(1, EPOCHS2 + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        n_train_batches = 0\n",
    "\n",
    "        for x_batch, y_batch in tqdm(\n",
    "            train_loader, desc=f\"Train epoch {epoch}\", leave=False\n",
    "        ):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)         # (B, T)\n",
    "            err = y_pred - y_batch          # (B, T)\n",
    "\n",
    "            # 태스크별 MSE (배치 평균, scaled space)\n",
    "            mse_per_task = (err ** 2).mean(dim=0)   # (T,)\n",
    "\n",
    "            # Kendall et al. 2018 스타일:\n",
    "            # L = 0.5 * Σ_t [ exp(-s_t) * MSE_t + s_t ],  s_t = log_vars[t]\n",
    "            loss = 0.5 * torch.sum(torch.exp(-log_vars) * mse_per_task + log_vars)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            n_train_batches += 1\n",
    "\n",
    "            writer.add_scalar(\"train/batch_loss\", loss.item(), global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        train_loss /= max(n_train_batches, 1)\n",
    "\n",
    "        # log_vars 모니터링\n",
    "        for i, name in enumerate(TARGET_COLUMNS):\n",
    "            writer.add_scalar(f\"train/log_var_{name}\", log_vars[i].item(), epoch)\n",
    "\n",
    "        # -------------------------\n",
    "        # 5) Validation (같은 loss 정의)\n",
    "        # -------------------------\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        n_valid_batches = 0\n",
    "        per_target_sqerr = np.zeros(output_dim, dtype=np.float64)\n",
    "        n_valid_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in tqdm(\n",
    "                valid_loader, desc=f\"Valid epoch {epoch}\", leave=False\n",
    "            ):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                y_pred = model(x_batch)\n",
    "                err = y_pred - y_batch\n",
    "                mse_per_task = (err ** 2).mean(dim=0)   # (T,)\n",
    "\n",
    "                val_loss_tensor = 0.5 * torch.sum(torch.exp(-log_vars) * mse_per_task + log_vars)\n",
    "                loss_val = val_loss_tensor\n",
    "\n",
    "                valid_loss += loss_val.item()\n",
    "                n_valid_batches += 1\n",
    "\n",
    "                # 타깃별 MSE 통계용 (scaled space)\n",
    "                err_np = err.cpu().numpy()\n",
    "                per_target_sqerr += (err_np ** 2).sum(axis=0)\n",
    "                n_valid_samples += err_np.shape[0]\n",
    "\n",
    "        valid_loss /= max(n_valid_batches, 1)\n",
    "        per_target_mse_scaled = per_target_sqerr / max(n_valid_samples, 1)\n",
    "        per_target_mse_original = (target_stds ** 2) * per_target_mse_scaled\n",
    "\n",
    "        print(\n",
    "            f\"[Epoch {epoch}] train_loss={train_loss:.4f}, \"\n",
    "            f\"valid_loss={valid_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        writer.add_scalar(\"train/epoch_loss\", train_loss, epoch)\n",
    "        writer.add_scalar(\"valid/epoch_loss\", valid_loss, epoch)\n",
    "\n",
    "        for i, name in enumerate(TARGET_COLUMNS):\n",
    "            writer.add_scalar(f\"valid/MSE_scaled_{name}\", per_target_mse_scaled[i], epoch)\n",
    "            writer.add_scalar(f\"valid/MSE_original_{name}\", per_target_mse_original[i], epoch)\n",
    "\n",
    "        # best 모델 저장 (이제는 log_vars도 같이 저장)\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            print(f\"  >> 베스트 모델 갱신 (valid_loss={valid_loss:.4f})\")\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"input_dim\": input_dim,\n",
    "                    \"output_dim\": output_dim,\n",
    "                    \"target_means\": target_means,\n",
    "                    \"target_stds\": target_stds,\n",
    "                    \"log_vars\": log_vars.detach().cpu(),\n",
    "                },\n",
    "                MODEL_CKPT_PATH,\n",
    "            )\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    # -------------------------\n",
    "    # 6) Test 평가 (per-target MSE, 원래 스케일)\n",
    "    # -------------------------\n",
    "    print(\">> 최종 모델 로드 후 test set 평가\")\n",
    "    ckpt_final = torch.load(\n",
    "    MODEL_CKPT_PATH,\n",
    "    map_location=device,\n",
    "    weights_only=False,  # <<< 추가\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(ckpt_final[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    per_target_sqerr = np.zeros(output_dim, dtype=np.float64)\n",
    "    n_test_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in tqdm(test_loader, desc=\"Test\", leave=False):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            err = (y_pred - y_batch).cpu().numpy()\n",
    "            per_target_sqerr += (err ** 2).sum(axis=0)\n",
    "            n_test_samples += err.shape[0]\n",
    "\n",
    "    per_target_mse_scaled = per_target_sqerr / max(n_test_samples, 1)\n",
    "    per_target_mse_original = (target_stds ** 2) * per_target_mse_scaled\n",
    "\n",
    "    print(\">> Test MSE (원래 스케일 기준):\")\n",
    "    for i, name in enumerate(TARGET_COLUMNS):\n",
    "        print(f\"  {name}: {per_target_mse_original[i]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def grid_search_all_pairs():\n",
    "    \"\"\"\n",
    "    latent_dict에 들어있는 모든 분자 조합(i <= j)과\n",
    "    ALPHA_GRID의 alpha 값에 대해 MLP 예측을 수행하고\n",
    "    결과를 CSV로 저장한다.\n",
    "    \"\"\"\n",
    "    print(\">> latent dict 로드 중...\")\n",
    "    latent_dict: Dict[str, torch.Tensor] = torch.load(LATENT_CACHE_PATH)\n",
    "    mol_list = sorted(latent_dict.keys())\n",
    "    N = len(mol_list)\n",
    "    print(\">> 분자 수:\", N)\n",
    "\n",
    "    print(\">> MLP 체크포인트 로드 중...\")\n",
    "    ckpt = torch.load(\n",
    "    MODEL_CKPT_PATH,\n",
    "    map_location=\"cpu\",\n",
    "    weights_only=False,  # <<< 추가\n",
    "    )\n",
    "\n",
    "    input_dim = ckpt[\"input_dim\"]\n",
    "    output_dim = ckpt[\"output_dim\"]\n",
    "    target_means = ckpt[\"target_means\"]\n",
    "    target_stds = ckpt[\"target_stds\"]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MixtureMLP(input_dim=input_dim, output_dim=output_dim, hidden_sizes=(512, 256))\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    rows_out = []\n",
    "\n",
    "    total_pairs = N * (N + 1) // 2\n",
    "    print(\">> 모든 분자 조합 × alpha grid 예측 중...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=total_pairs, desc=\"Pairs\") as pbar:\n",
    "            for i in range(N):\n",
    "                mol1 = mol_list[i]\n",
    "                z_A_full = latent_dict[mol1].to(device).float()\n",
    "                for j in range(i, N):\n",
    "                    mol2 = mol_list[j]\n",
    "                    z_B_full = latent_dict[mol2].to(device).float()\n",
    "\n",
    "                    features = []\n",
    "                    alphas = []\n",
    "                    for alpha in ALPHA_GRID:\n",
    "                        alpha = float(alpha)\n",
    "                        h_mix = alpha * z_A_full + (1.0 - alpha) * z_B_full\n",
    "                        h_int = torch.abs(z_A_full - z_B_full)\n",
    "                        alpha_tensor = torch.tensor([alpha], dtype=torch.float32, device=device)\n",
    "                        x = torch.cat(\n",
    "                            [z_A_full, z_B_full, h_mix, h_int, alpha_tensor], dim=0\n",
    "                        )\n",
    "                        features.append(x)\n",
    "                        alphas.append(alpha)\n",
    "\n",
    "                    x_batch = torch.stack(features, dim=0)\n",
    "                    y_pred_scaled = model(x_batch)\n",
    "                    y_pred_scaled_np = y_pred_scaled.cpu().numpy()\n",
    "                    y_pred_original = y_pred_scaled_np * target_stds + target_means\n",
    "\n",
    "                    for k, alpha in enumerate(alphas):\n",
    "                        row = {\n",
    "                            \"smiles_A\": mol1,\n",
    "                            \"smiles_B\": mol2,\n",
    "                            \"alpha\": alpha,\n",
    "                        }\n",
    "                        for t_idx, name in enumerate(TARGET_COLUMNS):\n",
    "                            row[name] = float(y_pred_original[k, t_idx])\n",
    "                        rows_out.append(row)\n",
    "\n",
    "                    pbar.update(1)\n",
    "\n",
    "    print(\">> DataFrame으로 변환 및 CSV 저장 중...\")\n",
    "    out_df = pd.DataFrame(rows_out)\n",
    "    out_df.to_csv(GRID_CSV_PATH, index=False)\n",
    "    print(\">> CSV 저장 완료:\", GRID_CSV_PATH)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# main\n",
    "# -------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) 처음 한 번만 latent 캐시 생성\n",
    "    if not LATENT_CACHE_PATH.exists():\n",
    "        build_latent_cache()\n",
    "\n",
    "    # 2) MLP 학습\n",
    "    train_mixture_mlp()\n",
    "\n",
    "    # 3) 전체 조합 × alpha grid 서치\n",
    "    grid_search_all_pairs()\n"
   ],
   "id": "a494891fa2d25bee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> plastic.parquet 로드 중...\n",
      ">> latent dict 로드 중...\n",
      ">> 기존 체크포인트 발견: scaled MSE로 학습된 모델에서 이어서 학습합니다.\n",
      ">> target means: [2.7258392e+02 3.5290601e+02 5.7695581e+02 5.4055072e+02 2.0738430e+01\n",
      " 2.4608255e+01 2.1151849e+02 8.1102943e-01 4.3324103e+00 7.0267005e+00\n",
      " 1.9012366e-01 4.6613353e-01 7.1162233e+00]\n",
      ">> target stds: [2.7011644e+01 3.7132740e+01 2.6864326e+01 4.8827740e+02 1.3660261e+01\n",
      " 1.2297838e+01 1.2056164e+02 8.1845707e-01 3.9727042e+00 4.0180354e+00\n",
      " 2.5608444e-01 5.0978565e-01 4.6124973e+00]\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T06:26:59.155280Z",
     "start_time": "2025-12-01T06:18:07.125291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 이 코드는 다음이 이미 정의되어 있다고 가정:\n",
    "# - DATA_PATH (Path or str, e.g. Path(\"plastic.parquet\"))\n",
    "# - LATENT_CACHE_PATH (Path to \"latents.pt\")\n",
    "# - MODEL_CKPT_PATH (Path to \"mixture_mlp.pt\")\n",
    "# - TARGET_COLUMNS (list of 14 target names)\n",
    "# - MixtureDataset, MixtureMLP\n",
    "#\n",
    "# 만약 다른 파일에 있다면:\n",
    "# from your_module import DATA_PATH, LATENT_CACHE_PATH, MODEL_CKPT_PATH, TARGET_COLUMNS, MixtureDataset, MixtureMLP\n",
    "\n",
    "def compute_target_stats(df: pd.DataFrame):\n",
    "    y = df[TARGET_COLUMNS].values.astype(np.float32)\n",
    "    means = y.mean(axis=0)\n",
    "    stds = y.std(axis=0) + 1e-8\n",
    "    return means, stds\n",
    "\n",
    "\n",
    "def visualize_predictions_scatter_with_std():\n",
    "    print(\">> Loading data and latent dict...\")\n",
    "    df = pd.read_parquet(DATA_PATH)\n",
    "    latent_dict = torch.load(LATENT_CACHE_PATH)\n",
    "\n",
    "    # training 때와 동일한 target_means/target_stds 계산\n",
    "    target_means, target_stds = compute_target_stats(df)\n",
    "\n",
    "    # 전체 데이터셋 대상으로 Dataset 구성\n",
    "    full_dataset = MixtureDataset(df, latent_dict, target_means, target_stds)\n",
    "    full_loader = DataLoader(full_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    # 모델 로드\n",
    "    print(\">> Loading trained model...\")\n",
    "    ckpt = torch.load(MODEL_CKPT_PATH, map_location=\"cpu\", weights_only=False)\n",
    "    input_dim = ckpt[\"input_dim\"]\n",
    "    output_dim = ckpt[\"output_dim\"]\n",
    "    assert output_dim == len(TARGET_COLUMNS)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MixtureMLP(input_dim=input_dim, output_dim=output_dim, hidden_sizes=(512, 256))\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 예측 수집 (scaled space → original space 복원)\n",
    "    print(\">> Computing predictions for entire dataset...\")\n",
    "    all_y_true_scaled = []\n",
    "    all_y_pred_scaled = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch_scaled in full_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch_scaled = y_batch_scaled.to(device)\n",
    "\n",
    "            y_pred_scaled = model(x_batch)\n",
    "\n",
    "            all_y_true_scaled.append(y_batch_scaled.cpu().numpy())\n",
    "            all_y_pred_scaled.append(y_pred_scaled.cpu().numpy())\n",
    "\n",
    "    all_y_true_scaled = np.concatenate(all_y_true_scaled, axis=0)  # (N, T)\n",
    "    all_y_pred_scaled = np.concatenate(all_y_pred_scaled, axis=0)  # (N, T)\n",
    "\n",
    "    # 스케일 복원\n",
    "    target_means_np = np.array(target_means, dtype=np.float32)    # (T,)\n",
    "    target_stds_np = np.array(target_stds, dtype=np.float32)      # (T,)\n",
    "\n",
    "    all_y_true = all_y_true_scaled * target_stds_np + target_means_np\n",
    "    all_y_pred = all_y_pred_scaled * target_stds_np + target_means_np\n",
    "\n",
    "    print(\">> Plotting per-target scatter with ±std bands...\")\n",
    "\n",
    "    for t_idx, name in enumerate(TARGET_COLUMNS):\n",
    "        y_true = all_y_true[:, t_idx]\n",
    "        y_pred = all_y_pred[:, t_idx]\n",
    "        std_t = target_stds_np[t_idx]\n",
    "\n",
    "        # 산점도 범위 설정\n",
    "        min_val = min(y_true.min(), y_pred.min())\n",
    "        max_val = max(y_true.max(), y_pred.max())\n",
    "        margin = 0.05 * (max_val - min_val + 1e-8)\n",
    "        x_min = min_val - margin\n",
    "        x_max = max_val + margin\n",
    "\n",
    "        xs = np.linspace(x_min, x_max, 200)\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.scatter(y_true, y_pred, alpha=0.4, s=10)\n",
    "        # y = x (완벽한 예측)\n",
    "        plt.plot(xs, xs, linestyle=\"--\", linewidth=1.0, label=\"y = x\")\n",
    "\n",
    "        # y = x ± std_t (std 기준 양익)\n",
    "        plt.plot(xs, xs + std_t, linestyle=\":\", linewidth=1.0, label=\"y = x + std\")\n",
    "        plt.plot(xs, xs - std_t, linestyle=\":\", linewidth=1.0, label=\"y = x - std\")\n",
    "\n",
    "        plt.xlabel(f\"True {name}\")\n",
    "        plt.ylabel(f\"Predicted {name}\")\n",
    "        plt.title(f\"{name}: True vs Predicted (±1 std band)\")\n",
    "        plt.xlim(x_min, x_max)\n",
    "        plt.ylim(x_min, x_max)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        out_name = f\"scatter_{name}.png\"\n",
    "        plt.savefig(out_name, dpi=200)\n",
    "        plt.close()\n",
    "        print(f\"  saved {out_name}\")\n",
    "\n",
    "    print(\">> Done.\")\n",
    "\n",
    "\n",
    "# 함수 호출\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_predictions_scatter_with_std()\n"
   ],
   "id": "63105ef3f51fce69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading data and latent dict...\n",
      ">> Loading trained model...\n",
      ">> Computing predictions for entire dataset...\n",
      ">> Plotting per-target scatter with ±std bands...\n",
      "  saved scatter_Tg.png\n",
      "  saved scatter_Tm.png\n",
      "  saved scatter_Td.png\n",
      "  saved scatter_YM.png\n",
      "  saved scatter_TS_y.png\n",
      "  saved scatter_TS_b.png\n",
      "  saved scatter_eps_b.png\n",
      "  saved scatter_perm_O2.png\n",
      "  saved scatter_perm_CO2.png\n",
      "  saved scatter_perm_He.png\n",
      "  saved scatter_perm_N2.png\n",
      "  saved scatter_perm_CH4.png\n",
      "  saved scatter_perm_H2.png\n",
      ">> Done.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "13ce023d1b200007",
   "metadata": {},
   "source": [
    "[예측값 정의]\n",
    "\n",
    "logP_CO2  = log(P_CO2)\n",
    "logP_CH4  = log(P_CH4)\n",
    "logP_N2   = log(P_N2)\n",
    "\n",
    "Tg        = 유리전이온도\n",
    "Td        = 열분해온도\n",
    "YM        = Young's modulus\n",
    "TSb       = 인장강도 (stress at break)\n",
    "epsb      = 연신율 (strain at break)\n",
    "\n",
    "z1, z2    = 두 모노머의 latent vector\n",
    "\n",
    "\n",
    "[1. 분리 성능 보상항 J_sep]\n",
    "\n",
    "J_sep =\n",
    "    w_perm      * logP_CO2\n",
    "  + w_sel_CH4   * (logP_CO2 - logP_CH4)   # CO2/CH4 선택성 (log S_CO2/CH4)\n",
    "  + w_sel_N2    * (logP_CO2 - logP_N2)    # CO2/N2  선택성 (log S_CO2/N2)\n",
    "\n",
    "\n",
    "[2. 고온·고압용 물성 soft penalty]\n",
    "\n",
    "임계값(조건)을 다음처럼 둔다 (예시는 너가 quantile 보고 채우면 됨):\n",
    "\n",
    "Tg_req     = 원하는 최소 Tg\n",
    "Td_req     = 원하는 최소 Td\n",
    "YM_min     = 원하는 최소 YM\n",
    "TSb_min    = 원하는 최소 TSb\n",
    "epsb_min   = 원하는 최소 epsb\n",
    "\n",
    "각 penalty는 \"부족한 만큼만 제곱\"으로 정의:\n",
    "\n",
    "pen_Tg  = max(0, Tg_req  - Tg)^2\n",
    "pen_Td  = max(0, Td_req  - Td)^2\n",
    "pen_YM  = max(0, YM_min  - YM)^2\n",
    "pen_TS  = max(0, TSb_min - TSb)^2\n",
    "pen_eps = max(0, epsb_min - epsb)^2\n",
    "\n",
    "\n",
    "[3. latent prior penalty]\n",
    "\n",
    "pen_z = ||z1||^2 + ||z2||^2\n",
    "# (local search면 ||z1 - z1_0||^2 + ||z2 - z2_0||^2 로 바꿔도 됨)\n",
    "\n",
    "\n",
    "[4. 전체 목적함수 J]\n",
    "\n",
    "J =\n",
    "    J_sep\n",
    "  - λ_Tg  * pen_Tg\n",
    "  - λ_Td  * pen_Td\n",
    "  - λ_YM  * pen_YM\n",
    "  - λ_TS  * pen_TS\n",
    "  - λ_eps * pen_eps\n",
    "  - λ_z   * pen_z\n",
    "\n",
    "\n",
    "[5. 최종 최적화에 쓰는 loss]\n",
    "\n",
    "loss = -J\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
