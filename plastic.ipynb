{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be96a56646559a26",
   "metadata": {},
   "source": [
    "# Plastic Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feaf6ce639fffd7",
   "metadata": {},
   "source": [
    "We attempt to design new plastic molecule using LDM.\n",
    "Our main goal is on\n",
    "1. Straw\n",
    "2. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba3d462540fb66",
   "metadata": {},
   "source": [
    "## Setup enviroment\n",
    "!git clone https://github.com/Ahnd6474/KSEF\n",
    "\n",
    "%cd your_shit/GitHub/KSEF\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10c1e0914dd7ef6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:22:53.652807Z",
     "start_time": "2025-11-21T17:22:53.646399Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Iterator, Optional\n",
    "import pyarrow\n",
    "import torch\n",
    "from geoldm.configs import get_dataset_info\n",
    "from geoldm.qm9 import dataset, load_model, sampling, visualize_molecule_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b80a769160d02",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bb23c5d1539f3ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:22:57.691252Z",
     "start_time": "2025-11-21T17:22:55.674380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comp</th>\n",
       "      <th>Tg</th>\n",
       "      <th>Tm</th>\n",
       "      <th>Td</th>\n",
       "      <th>YM</th>\n",
       "      <th>TS_y</th>\n",
       "      <th>TS_b</th>\n",
       "      <th>eps_b</th>\n",
       "      <th>perm_O2</th>\n",
       "      <th>perm_CO2</th>\n",
       "      <th>perm_He</th>\n",
       "      <th>perm_N2</th>\n",
       "      <th>perm_CH4</th>\n",
       "      <th>perm_H2</th>\n",
       "      <th>smiles</th>\n",
       "      <th>num_side</th>\n",
       "      <th>num_back</th>\n",
       "      <th>end_group</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>246.341049</td>\n",
       "      <td>354.051544</td>\n",
       "      <td>524.699646</td>\n",
       "      <td>773.208252</td>\n",
       "      <td>25.838213</td>\n",
       "      <td>32.409279</td>\n",
       "      <td>103.836082</td>\n",
       "      <td>-0.074397</td>\n",
       "      <td>-0.109570</td>\n",
       "      <td>0.493981</td>\n",
       "      <td>-0.071000</td>\n",
       "      <td>-0.061771</td>\n",
       "      <td>0.296051</td>\n",
       "      <td>[[*]OCCC(=O)[*], [*]OCCC(=O)[*]]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[3.0, 3.0]</td>\n",
       "      <td>[, ]</td>\n",
       "      <td>[pha, pha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>313.798401</td>\n",
       "      <td>449.257751</td>\n",
       "      <td>502.616241</td>\n",
       "      <td>1866.776855</td>\n",
       "      <td>44.379429</td>\n",
       "      <td>38.010651</td>\n",
       "      <td>7.215131</td>\n",
       "      <td>-0.043089</td>\n",
       "      <td>-0.006026</td>\n",
       "      <td>0.462577</td>\n",
       "      <td>-0.049233</td>\n",
       "      <td>-0.031425</td>\n",
       "      <td>0.342873</td>\n",
       "      <td>[[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[3.0, 3.0]</td>\n",
       "      <td>[, O]</td>\n",
       "      <td>[pha, pha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>306.681305</td>\n",
       "      <td>446.618713</td>\n",
       "      <td>502.605896</td>\n",
       "      <td>1769.267456</td>\n",
       "      <td>44.743877</td>\n",
       "      <td>37.998493</td>\n",
       "      <td>8.727348</td>\n",
       "      <td>-0.050900</td>\n",
       "      <td>-0.028856</td>\n",
       "      <td>0.455602</td>\n",
       "      <td>-0.060321</td>\n",
       "      <td>-0.039934</td>\n",
       "      <td>0.322175</td>\n",
       "      <td>[[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[3.0, 3.0]</td>\n",
       "      <td>[, O]</td>\n",
       "      <td>[pha, pha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>298.431488</td>\n",
       "      <td>442.813019</td>\n",
       "      <td>503.664642</td>\n",
       "      <td>1653.973633</td>\n",
       "      <td>44.287182</td>\n",
       "      <td>37.676380</td>\n",
       "      <td>10.997544</td>\n",
       "      <td>-0.057812</td>\n",
       "      <td>-0.048113</td>\n",
       "      <td>0.458959</td>\n",
       "      <td>-0.069126</td>\n",
       "      <td>-0.047438</td>\n",
       "      <td>0.314363</td>\n",
       "      <td>[[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[3.0, 3.0]</td>\n",
       "      <td>[, O]</td>\n",
       "      <td>[pha, pha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>290.073944</td>\n",
       "      <td>436.398560</td>\n",
       "      <td>505.013275</td>\n",
       "      <td>1508.705688</td>\n",
       "      <td>42.840149</td>\n",
       "      <td>36.907192</td>\n",
       "      <td>14.239414</td>\n",
       "      <td>-0.065853</td>\n",
       "      <td>-0.065725</td>\n",
       "      <td>0.461121</td>\n",
       "      <td>-0.075789</td>\n",
       "      <td>-0.055182</td>\n",
       "      <td>0.302125</td>\n",
       "      <td>[[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[3.0, 3.0]</td>\n",
       "      <td>[, O]</td>\n",
       "      <td>[pha, pha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373498</th>\n",
       "      <td>0.0</td>\n",
       "      <td>325.252380</td>\n",
       "      <td>462.438263</td>\n",
       "      <td>502.465759</td>\n",
       "      <td>2680.520020</td>\n",
       "      <td>39.839355</td>\n",
       "      <td>68.497833</td>\n",
       "      <td>30.515642</td>\n",
       "      <td>0.011635</td>\n",
       "      <td>-0.060768</td>\n",
       "      <td>0.179605</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.056135</td>\n",
       "      <td>[[*]CC([*])O, [*]CC([*])O]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[PVA, PVA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>369.788818</td>\n",
       "      <td>509.966248</td>\n",
       "      <td>587.491455</td>\n",
       "      <td>2227.901367</td>\n",
       "      <td>33.554047</td>\n",
       "      <td>37.865055</td>\n",
       "      <td>1.950041</td>\n",
       "      <td>2.547638</td>\n",
       "      <td>11.337365</td>\n",
       "      <td>21.572718</td>\n",
       "      <td>0.559539</td>\n",
       "      <td>0.881040</td>\n",
       "      <td>24.865118</td>\n",
       "      <td>[[*]CC([*])c1ccccc1, [*]CC([*])c1ccccc1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[PS, PS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373500</th>\n",
       "      <td>0.0</td>\n",
       "      <td>344.375183</td>\n",
       "      <td>457.250885</td>\n",
       "      <td>574.055664</td>\n",
       "      <td>1724.992798</td>\n",
       "      <td>50.262169</td>\n",
       "      <td>46.697941</td>\n",
       "      <td>27.839941</td>\n",
       "      <td>0.099392</td>\n",
       "      <td>0.332474</td>\n",
       "      <td>1.581858</td>\n",
       "      <td>-0.005775</td>\n",
       "      <td>0.070204</td>\n",
       "      <td>1.482780</td>\n",
       "      <td>[[*]CC([*])Cl, [*]CC([*])Cl]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[PVC, PVC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373501</th>\n",
       "      <td>0.0</td>\n",
       "      <td>393.653473</td>\n",
       "      <td>536.671326</td>\n",
       "      <td>700.403137</td>\n",
       "      <td>1942.565186</td>\n",
       "      <td>44.805931</td>\n",
       "      <td>65.647507</td>\n",
       "      <td>27.729816</td>\n",
       "      <td>0.016741</td>\n",
       "      <td>0.138303</td>\n",
       "      <td>1.767082</td>\n",
       "      <td>-0.054595</td>\n",
       "      <td>-0.069961</td>\n",
       "      <td>1.336078</td>\n",
       "      <td>[[*]CCOC(=O)c1ccc2cc(C(=O)O[*])ccc2c1, [*]CCOC...</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[PEN, PEN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373502</th>\n",
       "      <td>0.0</td>\n",
       "      <td>317.276978</td>\n",
       "      <td>469.612549</td>\n",
       "      <td>686.914612</td>\n",
       "      <td>1508.900513</td>\n",
       "      <td>54.058247</td>\n",
       "      <td>40.808701</td>\n",
       "      <td>4.400261</td>\n",
       "      <td>-0.004739</td>\n",
       "      <td>-0.059302</td>\n",
       "      <td>0.252653</td>\n",
       "      <td>0.014024</td>\n",
       "      <td>0.044955</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>[[*]CCCCCC(=O)N[*], [*]CCCCCC(=O)N[*]]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[Nylon6, Nylon6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1373503 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         comp          Tg          Tm          Td           YM       TS_y  \\\n",
       "0         0.0  246.341049  354.051544  524.699646   773.208252  25.838213   \n",
       "1         0.1  313.798401  449.257751  502.616241  1866.776855  44.379429   \n",
       "2         0.2  306.681305  446.618713  502.605896  1769.267456  44.743877   \n",
       "3         0.3  298.431488  442.813019  503.664642  1653.973633  44.287182   \n",
       "4         0.4  290.073944  436.398560  505.013275  1508.705688  42.840149   \n",
       "...       ...         ...         ...         ...          ...        ...   \n",
       "1373498   0.0  325.252380  462.438263  502.465759  2680.520020  39.839355   \n",
       "1373499   0.0  369.788818  509.966248  587.491455  2227.901367  33.554047   \n",
       "1373500   0.0  344.375183  457.250885  574.055664  1724.992798  50.262169   \n",
       "1373501   0.0  393.653473  536.671326  700.403137  1942.565186  44.805931   \n",
       "1373502   0.0  317.276978  469.612549  686.914612  1508.900513  54.058247   \n",
       "\n",
       "              TS_b       eps_b   perm_O2   perm_CO2    perm_He   perm_N2  \\\n",
       "0        32.409279  103.836082 -0.074397  -0.109570   0.493981 -0.071000   \n",
       "1        38.010651    7.215131 -0.043089  -0.006026   0.462577 -0.049233   \n",
       "2        37.998493    8.727348 -0.050900  -0.028856   0.455602 -0.060321   \n",
       "3        37.676380   10.997544 -0.057812  -0.048113   0.458959 -0.069126   \n",
       "4        36.907192   14.239414 -0.065853  -0.065725   0.461121 -0.075789   \n",
       "...            ...         ...       ...        ...        ...       ...   \n",
       "1373498  68.497833   30.515642  0.011635  -0.060768   0.179605  0.002471   \n",
       "1373499  37.865055    1.950041  2.547638  11.337365  21.572718  0.559539   \n",
       "1373500  46.697941   27.839941  0.099392   0.332474   1.581858 -0.005775   \n",
       "1373501  65.647507   27.729816  0.016741   0.138303   1.767082 -0.054595   \n",
       "1373502  40.808701    4.400261 -0.004739  -0.059302   0.252653  0.014024   \n",
       "\n",
       "         perm_CH4    perm_H2  \\\n",
       "0       -0.061771   0.296051   \n",
       "1       -0.031425   0.342873   \n",
       "2       -0.039934   0.322175   \n",
       "3       -0.047438   0.314363   \n",
       "4       -0.055182   0.302125   \n",
       "...           ...        ...   \n",
       "1373498  0.001415   0.056135   \n",
       "1373499  0.881040  24.865118   \n",
       "1373500  0.070204   1.482780   \n",
       "1373501 -0.069961   1.336078   \n",
       "1373502  0.044955   0.000020   \n",
       "\n",
       "                                                    smiles    num_side  \\\n",
       "0                         [[*]OCCC(=O)[*], [*]OCCC(=O)[*]]  [0.0, 0.0]   \n",
       "1                      [[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]  [0.0, 0.0]   \n",
       "2                      [[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]  [0.0, 0.0]   \n",
       "3                      [[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]  [0.0, 0.0]   \n",
       "4                      [[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]  [0.0, 0.0]   \n",
       "...                                                    ...         ...   \n",
       "1373498                         [[*]CC([*])O, [*]CC([*])O]  [nan, nan]   \n",
       "1373499           [[*]CC([*])c1ccccc1, [*]CC([*])c1ccccc1]  [nan, nan]   \n",
       "1373500                       [[*]CC([*])Cl, [*]CC([*])Cl]  [nan, nan]   \n",
       "1373501  [[*]CCOC(=O)c1ccc2cc(C(=O)O[*])ccc2c1, [*]CCOC...  [nan, nan]   \n",
       "1373502             [[*]CCCCCC(=O)N[*], [*]CCCCCC(=O)N[*]]  [nan, nan]   \n",
       "\n",
       "           num_back     end_group             names  \n",
       "0        [3.0, 3.0]          [, ]        [pha, pha]  \n",
       "1        [3.0, 3.0]         [, O]        [pha, pha]  \n",
       "2        [3.0, 3.0]         [, O]        [pha, pha]  \n",
       "3        [3.0, 3.0]         [, O]        [pha, pha]  \n",
       "4        [3.0, 3.0]         [, O]        [pha, pha]  \n",
       "...             ...           ...               ...  \n",
       "1373498  [nan, nan]  [None, None]        [PVA, PVA]  \n",
       "1373499  [nan, nan]  [None, None]          [PS, PS]  \n",
       "1373500  [nan, nan]  [None, None]        [PVC, PVC]  \n",
       "1373501  [nan, nan]  [None, None]        [PEN, PEN]  \n",
       "1373502  [nan, nan]  [None, None]  [Nylon6, Nylon6]  \n",
       "\n",
       "[1373503 rows x 19 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_parquet('data/plastic.parquet')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb66463e2017a951",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:23:01.476586Z",
     "start_time": "2025-11-21T17:23:00.918738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comp</th>\n",
       "      <th>Tg</th>\n",
       "      <th>Tm</th>\n",
       "      <th>Td</th>\n",
       "      <th>YM</th>\n",
       "      <th>TS_y</th>\n",
       "      <th>TS_b</th>\n",
       "      <th>eps_b</th>\n",
       "      <th>perm_O2</th>\n",
       "      <th>perm_CO2</th>\n",
       "      <th>perm_He</th>\n",
       "      <th>perm_N2</th>\n",
       "      <th>perm_CH4</th>\n",
       "      <th>perm_H2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.997987e-01</td>\n",
       "      <td>2.725839e+02</td>\n",
       "      <td>3.529060e+02</td>\n",
       "      <td>5.769558e+02</td>\n",
       "      <td>5.405507e+02</td>\n",
       "      <td>2.073843e+01</td>\n",
       "      <td>2.460826e+01</td>\n",
       "      <td>2.115185e+02</td>\n",
       "      <td>8.110294e-01</td>\n",
       "      <td>4.332410e+00</td>\n",
       "      <td>7.026700e+00</td>\n",
       "      <td>1.901237e-01</td>\n",
       "      <td>4.661335e-01</td>\n",
       "      <td>7.116223e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.583418e-01</td>\n",
       "      <td>2.701165e+01</td>\n",
       "      <td>3.713276e+01</td>\n",
       "      <td>2.686434e+01</td>\n",
       "      <td>4.882776e+02</td>\n",
       "      <td>1.366027e+01</td>\n",
       "      <td>1.229784e+01</td>\n",
       "      <td>1.205617e+02</td>\n",
       "      <td>8.184573e-01</td>\n",
       "      <td>3.972706e+00</td>\n",
       "      <td>4.018037e+00</td>\n",
       "      <td>2.560846e-01</td>\n",
       "      <td>5.097858e-01</td>\n",
       "      <td>4.612499e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.020853e+02</td>\n",
       "      <td>2.794441e+02</td>\n",
       "      <td>4.246918e+02</td>\n",
       "      <td>9.102757e+01</td>\n",
       "      <td>3.014123e+00</td>\n",
       "      <td>4.558969e+00</td>\n",
       "      <td>4.952888e-01</td>\n",
       "      <td>-9.569907e-02</td>\n",
       "      <td>-1.267213e-01</td>\n",
       "      <td>1.728067e-01</td>\n",
       "      <td>-1.294968e-01</td>\n",
       "      <td>-1.326250e-01</td>\n",
       "      <td>2.026558e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>2.543430e+02</td>\n",
       "      <td>3.266345e+02</td>\n",
       "      <td>5.673138e+02</td>\n",
       "      <td>1.744876e+02</td>\n",
       "      <td>8.976903e+00</td>\n",
       "      <td>1.493889e+01</td>\n",
       "      <td>1.076303e+02</td>\n",
       "      <td>3.791428e-01</td>\n",
       "      <td>1.926220e+00</td>\n",
       "      <td>4.530061e+00</td>\n",
       "      <td>5.799794e-02</td>\n",
       "      <td>1.869081e-01</td>\n",
       "      <td>4.429616e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>2.699055e+02</td>\n",
       "      <td>3.456665e+02</td>\n",
       "      <td>5.827677e+02</td>\n",
       "      <td>3.266322e+02</td>\n",
       "      <td>1.742608e+01</td>\n",
       "      <td>2.180278e+01</td>\n",
       "      <td>2.111784e+02</td>\n",
       "      <td>6.681085e-01</td>\n",
       "      <td>3.541990e+00</td>\n",
       "      <td>6.793644e+00</td>\n",
       "      <td>1.420151e-01</td>\n",
       "      <td>3.605243e-01</td>\n",
       "      <td>6.750260e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000e-01</td>\n",
       "      <td>2.883478e+02</td>\n",
       "      <td>3.725729e+02</td>\n",
       "      <td>5.933018e+02</td>\n",
       "      <td>7.870456e+02</td>\n",
       "      <td>3.001149e+01</td>\n",
       "      <td>3.317859e+01</td>\n",
       "      <td>3.043738e+02</td>\n",
       "      <td>1.066986e+00</td>\n",
       "      <td>5.741556e+00</td>\n",
       "      <td>9.030967e+00</td>\n",
       "      <td>2.563907e-01</td>\n",
       "      <td>6.126437e-01</td>\n",
       "      <td>9.138975e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000e-01</td>\n",
       "      <td>4.033092e+02</td>\n",
       "      <td>5.529254e+02</td>\n",
       "      <td>7.004031e+02</td>\n",
       "      <td>3.081163e+03</td>\n",
       "      <td>7.795893e+01</td>\n",
       "      <td>1.298841e+02</td>\n",
       "      <td>5.666409e+02</td>\n",
       "      <td>3.425747e+01</td>\n",
       "      <td>1.253688e+02</td>\n",
       "      <td>1.188544e+02</td>\n",
       "      <td>8.895615e+00</td>\n",
       "      <td>1.651693e+01</td>\n",
       "      <td>1.671498e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               comp            Tg            Tm            Td            YM  \\\n",
       "count  1.373503e+06  1.373503e+06  1.373503e+06  1.373503e+06  1.373503e+06   \n",
       "mean   4.997987e-01  2.725839e+02  3.529060e+02  5.769558e+02  5.405507e+02   \n",
       "std    2.583418e-01  2.701165e+01  3.713276e+01  2.686434e+01  4.882776e+02   \n",
       "min    0.000000e+00  2.020853e+02  2.794441e+02  4.246918e+02  9.102757e+01   \n",
       "25%    3.000000e-01  2.543430e+02  3.266345e+02  5.673138e+02  1.744876e+02   \n",
       "50%    5.000000e-01  2.699055e+02  3.456665e+02  5.827677e+02  3.266322e+02   \n",
       "75%    7.000000e-01  2.883478e+02  3.725729e+02  5.933018e+02  7.870456e+02   \n",
       "max    9.000000e-01  4.033092e+02  5.529254e+02  7.004031e+02  3.081163e+03   \n",
       "\n",
       "               TS_y          TS_b         eps_b       perm_O2      perm_CO2  \\\n",
       "count  1.373503e+06  1.373503e+06  1.373503e+06  1.373503e+06  1.373503e+06   \n",
       "mean   2.073843e+01  2.460826e+01  2.115185e+02  8.110294e-01  4.332410e+00   \n",
       "std    1.366027e+01  1.229784e+01  1.205617e+02  8.184573e-01  3.972706e+00   \n",
       "min    3.014123e+00  4.558969e+00  4.952888e-01 -9.569907e-02 -1.267213e-01   \n",
       "25%    8.976903e+00  1.493889e+01  1.076303e+02  3.791428e-01  1.926220e+00   \n",
       "50%    1.742608e+01  2.180278e+01  2.111784e+02  6.681085e-01  3.541990e+00   \n",
       "75%    3.001149e+01  3.317859e+01  3.043738e+02  1.066986e+00  5.741556e+00   \n",
       "max    7.795893e+01  1.298841e+02  5.666409e+02  3.425747e+01  1.253688e+02   \n",
       "\n",
       "            perm_He       perm_N2      perm_CH4       perm_H2  \n",
       "count  1.373503e+06  1.373503e+06  1.373503e+06  1.373503e+06  \n",
       "mean   7.026700e+00  1.901237e-01  4.661335e-01  7.116223e+00  \n",
       "std    4.018037e+00  2.560846e-01  5.097858e-01  4.612499e+00  \n",
       "min    1.728067e-01 -1.294968e-01 -1.326250e-01  2.026558e-05  \n",
       "25%    4.530061e+00  5.799794e-02  1.869081e-01  4.429616e+00  \n",
       "50%    6.793644e+00  1.420151e-01  3.605243e-01  6.750260e+00  \n",
       "75%    9.030967e+00  2.563907e-01  6.126437e-01  9.138975e+00  \n",
       "max    1.188544e+02  8.895615e+00  1.651693e+01  1.671498e+02  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152fb9dcbe4c51d",
   "metadata": {},
   "source": [
    "## Property Prediction\n",
    "We use MLP multi regressor to predict property of plastic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980fc32927142b91",
   "metadata": {},
   "source": [
    "### Load Model and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f282a01a0bf16d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T07:36:52.366056Z",
     "start_time": "2025-11-22T07:36:52.331012Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import ast\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from geoldm import encode, load_model, smiles_to_3d\n",
    "from geoldm.configs import get_dataset_info\n",
    "from geoldm.qm9 import dataset\n",
    "\n",
    "TARGET_COLUMNS = [\n",
    "    \"Tg\",\n",
    "    \"Tm\",\n",
    "    \"Td\",\n",
    "    \"YM\",\n",
    "    \"TS_y\",\n",
    "    \"TS_b\",\n",
    "    \"eps_b\",\n",
    "    \"perm_O2\",\n",
    "    \"perm_CO2\",\n",
    "    \"perm_He\",\n",
    "    \"perm_N2\",\n",
    "    \"perm_CH4\",\n",
    "    \"perm_H2\",\n",
    "]\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def load_qm9_latent_diffusion(checkpoint_dir: Path):\n",
    "    \"\"\"Load a pretrained latent diffusion model and its dataset metadata.\"\"\"\n",
    "\n",
    "    args_path = checkpoint_dir / \"args.pickle\"\n",
    "    with args_path.open(\"rb\") as handle:\n",
    "        args = pickle.load(handle)\n",
    "\n",
    "    # Respect GPU availability explicitly.\n",
    "    setattr(args, \"cuda\", torch.cuda.is_available())\n",
    "\n",
    "    dataset_info = get_dataset_info(args.dataset, args.remove_h)\n",
    "    dataloaders, _ = dataset.retrieve_dataloaders(args)\n",
    "    train_loader = dataloaders[\"train\"]\n",
    "\n",
    "    model, nodes_dist, _ = load_model(\n",
    "        stage=\"latent_diffusion\",\n",
    "        args=args,\n",
    "        dataset_info=dataset_info,\n",
    "        dataloader_train=train_loader,\n",
    "        checkpoint_path=checkpoint_dir,\n",
    "    )\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    return model, dataset_info, nodes_dist, device\n",
    "\n",
    "\n",
    "def _clean_smiles_string(smiles: str) -> str:\n",
    "    \"\"\"Replace placeholder atoms with hydrogens and trim whitespace.\"\"\"\n",
    "\n",
    "    return smiles.replace(\"[*]\", \"\").strip()\n",
    "\n",
    "\n",
    "def _pick_base_smiles(smiles_entry: object) -> str:\n",
    "    \"\"\"Normalize the parquet smiles field to a single SMILES string.\"\"\"\n",
    "\n",
    "    def iterate_candidates(seq):\n",
    "        for candidate in seq:\n",
    "            if isinstance(candidate, str):\n",
    "                cleaned = _clean_smiles_string(candidate)\n",
    "                if cleaned:\n",
    "                    return cleaned\n",
    "        return None\n",
    "\n",
    "    if isinstance(smiles_entry, str):\n",
    "        try:\n",
    "            maybe_list = ast.literal_eval(smiles_entry)\n",
    "        except Exception:\n",
    "            maybe_list = None\n",
    "\n",
    "        if isinstance(maybe_list, (list, tuple, np.ndarray)):\n",
    "            candidate = iterate_candidates(maybe_list)\n",
    "            if candidate:\n",
    "                return candidate\n",
    "\n",
    "        bracketed = smiles_entry.strip()\n",
    "        if bracketed.startswith(\"[\") and bracketed.endswith(\"]\"):\n",
    "            inner = bracketed[1:-1]\n",
    "            parts = [part.strip() for part in inner.split(\",\") if part.strip()]\n",
    "            candidate = iterate_candidates(parts)\n",
    "            if candidate:\n",
    "                return candidate\n",
    "\n",
    "        return _clean_smiles_string(smiles_entry)\n",
    "\n",
    "    if isinstance(smiles_entry, (list, tuple, np.ndarray)):\n",
    "        candidate = iterate_candidates(smiles_entry)\n",
    "        if candidate:\n",
    "            return candidate\n",
    "\n",
    "    return str(smiles_entry)\n",
    "\n",
    "\n",
    "def conformer_to_tensors(\n",
    "    conformer: Dict,\n",
    "    dataset_info: Dict,\n",
    "    device: torch.device,\n",
    ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Convert a single RDKit conformer into GeoLDM-ready tensors.\"\"\"\n",
    "\n",
    "    atom_decoder: List[str] = dataset_info[\"atom_decoder\"]\n",
    "    atom_encoder = {symbol: idx for idx, symbol in enumerate(atom_decoder)}\n",
    "    atom_indices = torch.tensor(\n",
    "        [atom_encoder[symbol] for symbol in conformer[\"atom_symbols\"]],\n",
    "        dtype=torch.long,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    one_hot = F.one_hot(atom_indices, num_classes=len(atom_decoder)).float()\n",
    "    positions = torch.tensor(conformer[\"coordinates\"], dtype=torch.float32, device=device)\n",
    "\n",
    "    # Add batch dimensions and masks.\n",
    "    x = positions.unsqueeze(0)\n",
    "    h = {\n",
    "        \"categorical\": one_hot.unsqueeze(0),\n",
    "        # Use zero charges when absent.\n",
    "        \"integer\": torch.zeros(one_hot.shape[0], 1, device=device).unsqueeze(0),\n",
    "    }\n",
    "\n",
    "    node_mask = torch.ones(x.shape[0], x.shape[1], 1, device=device)\n",
    "    edge_mask = node_mask.squeeze(-1)[..., None] * node_mask.squeeze(-1)[:, None]\n",
    "    return x, h, node_mask, edge_mask\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def smiles_to_latent(\n",
    "    smiles: str,\n",
    "    model: torch.nn.Module,\n",
    "    dataset_info: Dict,\n",
    "    device: torch.device,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Encode a SMILES string into a pooled latent vector using the LDM encoder.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    conformers = smiles_to_3d(smiles)\n",
    "    if len(conformers) == 0:\n",
    "        raise ValueError(f\"Failed to build 3D structure for {smiles!r}\")\n",
    "\n",
    "    x, h, node_mask, edge_mask = conformer_to_tensors(conformers[0], dataset_info, device)\n",
    "    z_x, _, z_h, _ = encode(model, x, h, node_mask=node_mask, edge_mask=edge_mask)\n",
    "\n",
    "    pooled_x = z_x.mean(dim=1)\n",
    "    pooled_h = z_h.mean(dim=1)\n",
    "    latent = torch.cat([pooled_x.flatten(), pooled_h.flatten()], dim=-1)\n",
    "    return latent.cpu()\n",
    "\n",
    "\n",
    "def build_latent_matrix(\n",
    "    frame: pd.DataFrame,\n",
    "    model: torch.nn.Module,\n",
    "    dataset_info: Dict,\n",
    "    device: torch.device,\n",
    "    *,\n",
    "    sample_size: int = 512,\n",
    "    batch_size: int = 32,\n",
    "    cache_path: Optional[Path] = Path(\"data/plastic_latents.pt\"),\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, List[int]]:\n",
    "    \"\"\"Convert a dataframe of plastics into latent vectors and targets with batching.\n",
    "\n",
    "    The helper optionally caches results to avoid recomputation when the notebook\n",
    "    is rerun.\n",
    "    \"\"\"\n",
    "\n",
    "    if cache_path and cache_path.exists():\n",
    "        payload = torch.load(cache_path)\n",
    "        return payload[\"embeddings\"], payload[\"targets\"], payload[\"indices\"]\n",
    "\n",
    "    subset = frame.sample(min(sample_size, len(frame)), random_state=0).reset_index()\n",
    "    embeddings: List[torch.Tensor] = []\n",
    "    targets: List[torch.Tensor] = []\n",
    "    used_indices: List[int] = []\n",
    "\n",
    "    model.eval()\n",
    "    progress = tqdm(total=len(subset), desc=\"Encoding SMILES\", unit=\"sample\")\n",
    "    for start in range(0, len(subset), batch_size):\n",
    "        batch = subset.iloc[start : start + batch_size]\n",
    "        batch_payloads: List[Tuple[str, np.ndarray, int]] = []\n",
    "\n",
    "        for _, row in batch.iterrows():\n",
    "            base_smiles = _pick_base_smiles(row[\"smiles\"])\n",
    "            target_values = (\n",
    "                pd.to_numeric(row[TARGET_COLUMNS], errors=\"coerce\").astype(np.float32).to_numpy()\n",
    "            )\n",
    "            if not np.isfinite(target_values).all():\n",
    "                continue\n",
    "            batch_payloads.append((base_smiles, target_values, int(row[\"index\"])))\n",
    "\n",
    "        batch_latents: List[torch.Tensor] = []\n",
    "        batch_targets: List[torch.Tensor] = []\n",
    "        batch_indices: List[int] = []\n",
    "\n",
    "        for smiles_value, target_values, row_index in batch_payloads:\n",
    "            try:\n",
    "                latent = smiles_to_latent(smiles_value, model, dataset_info, device)\n",
    "            except Exception:\n",
    "                continue\n",
    "            batch_latents.append(latent)\n",
    "            batch_targets.append(torch.tensor(target_values, dtype=torch.float32))\n",
    "            batch_indices.append(row_index)\n",
    "\n",
    "        embeddings.extend(batch_latents)\n",
    "        targets.extend(batch_targets)\n",
    "        used_indices.extend(batch_indices)\n",
    "        progress.update(len(batch))\n",
    "\n",
    "    progress.close()\n",
    "\n",
    "    if not embeddings:\n",
    "        raise RuntimeError(\"No embeddings were created; check SMILES parsing\")\n",
    "\n",
    "    emb_tensor = torch.stack(embeddings)\n",
    "    target_tensor = torch.stack(targets)\n",
    "\n",
    "    if cache_path:\n",
    "        cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(\n",
    "            {\"embeddings\": emb_tensor, \"targets\": target_tensor, \"indices\": used_indices},\n",
    "            cache_path,\n",
    "        )\n",
    "\n",
    "    return emb_tensor, target_tensor, used_indices\n",
    "\n",
    "\n",
    "class MultiTaskMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        hidden_sizes=(512, 512, 256),\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_sizes = tuple(hidden_sizes)\n",
    "        self.dropout = dropout\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for width in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev, width))\n",
    "            layers.append(nn.SiLU())  # or GELU\n",
    "            if dropout > 0.0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev = width\n",
    "        layers.append(nn.Linear(prev, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "def save_mlp_checkpoint(\n",
    "    checkpoint_path: Path,\n",
    "    model: MultiTaskMLP,\n",
    "    x_scaler: StandardScaler,\n",
    "    y_scaler: StandardScaler,\n",
    "    *,\n",
    "    epoch: int,\n",
    "    val_loss: float,\n",
    ") -> None:\n",
    "    \"\"\"Persist the MLP weights and scalers for reuse.\"\"\"\n",
    "\n",
    "    checkpoint_path = Path(checkpoint_path)\n",
    "    checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"input_dim\": model.network[0].in_features,\n",
    "            \"output_dim\": model.network[-1].out_features,\n",
    "            \"hidden_sizes\": model.hidden_sizes,\n",
    "            \"dropout\": model.dropout,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"x_scaler\": x_scaler,\n",
    "            \"y_scaler\": y_scaler,\n",
    "        },\n",
    "        checkpoint_path,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_mlp_checkpoint(\n",
    "    checkpoint_path: Path, device: Optional[torch.device] = None\n",
    ") -> Tuple[MultiTaskMLP, StandardScaler, StandardScaler, Dict]:\n",
    "    \"\"\"Load a saved multitask MLP and its associated scalers.\n",
    "\n",
    "    PyTorch 2.6 defaults ``weights_only`` to ``True``, which blocks arbitrary\n",
    "    objects such as the ``StandardScaler`` instances stored in our checkpoint.\n",
    "    Explicitly disable that restriction when we trust the checkpoint source.\n",
    "    \"\"\"\n",
    "\n",
    "    # Allow the saved sklearn scalers to be restored when loading trusted files.\n",
    "    torch.serialization.add_safe_globals([StandardScaler])\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        checkpoint_path,\n",
    "        map_location=device or DEVICE,\n",
    "        weights_only=False,\n",
    "    )\n",
    "    model = MultiTaskMLP(\n",
    "        checkpoint[\"input_dim\"],\n",
    "        checkpoint[\"output_dim\"],\n",
    "        hidden_sizes=checkpoint.get(\"hidden_sizes\", (512, 512, 256)),\n",
    "        dropout=checkpoint.get(\"dropout\", 0.0),\n",
    "    )\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    model.to(device or DEVICE)\n",
    "    return model, checkpoint[\"x_scaler\"], checkpoint[\"y_scaler\"], checkpoint\n",
    "\n",
    "\n",
    "\n",
    "def train_multitask_mlp(\n",
    "    embeddings: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    *,\n",
    "    epochs: int = 15,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 1e-3,\n",
    "    validation_split: float = 0.1,\n",
    "    log_dir: Optional[str] = \"runs/plastic_mlp\",\n",
    "    checkpoint_path: Optional[Path] = Path(\"models/plastic_mlp_best.pt\"),\n",
    "    device: Optional[torch.device] = None,\n",
    ") -> Tuple[MultiTaskMLP, StandardScaler, StandardScaler, Dict[str, List[float]]]:\n",
    "    \"\"\"Train a simple multitask MLP on latent embeddings and log to TensorBoard.\"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    x_scaler = StandardScaler()\n",
    "    y_scaler = StandardScaler()\n",
    "\n",
    "    x_scaled = x_scaler.fit_transform(embeddings)\n",
    "    y_scaled = y_scaler.fit_transform(targets)\n",
    "\n",
    "    features = torch.tensor(x_scaled, dtype=torch.float32)\n",
    "    labels = torch.tensor(y_scaled, dtype=torch.float32)\n",
    "\n",
    "    dataset = TensorDataset(features, labels)\n",
    "    val_size = max(1, int(len(dataset) * validation_split))\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_set, val_set = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "\n",
    "    model = MultiTaskMLP(features.shape[1], labels.shape[1]).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_dir) if log_dir else None\n",
    "    history = {\"train\": [], \"val\": []}\n",
    "    best_val = float(\"inf\")\n",
    "    best_state: Optional[Dict[str, torch.Tensor]] = None\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training epochs\"):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(batch_x)\n",
    "            loss = criterion(preds, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * len(batch_x)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                preds = model(batch_x)\n",
    "                loss = criterion(preds, batch_y)\n",
    "                val_loss += loss.item() * len(batch_x)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        history[\"train\"].append(train_loss)\n",
    "        history[\"val\"].append(val_loss)\n",
    "        if writer:\n",
    "            writer.add_scalar(\"loss/train\", train_loss, epoch)\n",
    "            writer.add_scalar(\"loss/val\", val_loss, epoch)\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            if checkpoint_path:\n",
    "                save_mlp_checkpoint(\n",
    "                    checkpoint_path,\n",
    "                    model,\n",
    "                    x_scaler,\n",
    "                    y_scaler,\n",
    "                    epoch=epoch + 1,\n",
    "                    val_loss=val_loss,\n",
    "                )\n",
    "            print(f\"Saved new best model at epoch {epoch + 1}: val_loss={val_loss:.6f}\")\n",
    "            if writer:\n",
    "                writer.add_scalar(\"loss/best_val\", best_val, epoch)\n",
    "\n",
    "    if writer:\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, x_scaler, y_scaler, history\n",
    "\n",
    "\n",
    "def predict_properties(\n",
    "    smiles: str,\n",
    "    *,\n",
    "    ldm_model: torch.nn.Module,\n",
    "    dataset_info: Dict,\n",
    "    device: torch.device,\n",
    "    mlp_model: MultiTaskMLP,\n",
    "    x_scaler: StandardScaler,\n",
    "    y_scaler: StandardScaler,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Generate property predictions for a new SMILES string.\"\"\"\n",
    "\n",
    "    latent = smiles_to_latent(smiles, ldm_model, dataset_info, device)\n",
    "    scaled = x_scaler.transform(latent.unsqueeze(0))\n",
    "    mlp_model = mlp_model.to(device)\n",
    "    mlp_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_scaled = mlp_model(torch.tensor(scaled, dtype=torch.float32, device=device)).cpu().numpy()\n",
    "    pred = y_scaler.inverse_transform(pred_scaled)[0]\n",
    "    return {name: value for name, value in zip(TARGET_COLUMNS, pred)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9945bf64417b01f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T08:24:20.872560Z",
     "start_time": "2025-11-22T07:37:44.095823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of n_nodes: H[N] -2.475700616836548\n",
      "Autoencoder models are _not_ conditioned on time.\n",
      "alphas2 [9.99990000e-01 9.99988000e-01 9.99982000e-01 ... 2.59676966e-05\n",
      " 1.39959211e-05 1.00039959e-05]\n",
      "gamma [-11.51291546 -11.33059532 -10.92513058 ...  10.55863126  11.17673063\n",
      "  11.51251595]\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b82715e14247adb922054c9e88bdfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model at epoch 1: val_loss=0.702362\n",
      "Saved new best model at epoch 2: val_loss=0.663256\n",
      "Saved new best model at epoch 3: val_loss=0.652475\n",
      "Saved new best model at epoch 4: val_loss=0.647071\n",
      "Saved new best model at epoch 5: val_loss=0.646006\n",
      "Saved new best model at epoch 6: val_loss=0.642526\n",
      "Saved new best model at epoch 8: val_loss=0.640668\n",
      "Saved new best model at epoch 10: val_loss=0.640123\n",
      "Saved new best model at epoch 12: val_loss=0.639248\n",
      "Saved new best model at epoch 16: val_loss=0.638455\n",
      "Saved new best model at epoch 23: val_loss=0.638232\n",
      "Saved new best model at epoch 34: val_loss=0.637897\n",
      "Saved new best model at epoch 41: val_loss=0.637875\n",
      "Saved new best model at epoch 42: val_loss=0.637724\n",
      "Saved new best model at epoch 46: val_loss=0.637592\n",
      "Saved new best model at epoch 49: val_loss=0.637412\n",
      "Saved new best model at epoch 55: val_loss=0.637249\n",
      "Saved new best model at epoch 77: val_loss=0.637030\n",
      "Saved new best model at epoch 87: val_loss=0.636837\n",
      "Saved new best model at epoch 92: val_loss=0.636485\n",
      "Training loss trajectory: [0.7321848348628229, 0.6468321515908881, 0.6239348667637625, 0.614993879199028, 0.6111057680934223, 0.6092569493891588, 0.6079519212868676, 0.6069846145297164, 0.6065379273268714, 0.6061983586336249, 0.6056992490789783, 0.6055754018897441, 0.6050731944415106, 0.6050290933829635, 0.6048367304855318, 0.6047054431598577, 0.6045579885991652, 0.6043535407087696, 0.6041886506240759, 0.6041961151717314, 0.6040414421282597, 0.603936069456499, 0.6037455674427659, 0.6036813882749472, 0.6037109252276706, 0.6034925234362261, 0.6035847870983294, 0.6034620218090156, 0.6034420086910476, 0.6033568389736005, 0.6032420368603806, 0.6031922232615414, 0.6031923655520625, 0.603132501284578, 0.6030422353788988, 0.6030218668482197, 0.6028816243605827, 0.6028522591537504, 0.6028893375619134, 0.602943587610081, 0.6028605457117309, 0.6027453022394608, 0.6026596835255623, 0.6026788325096244, 0.6025956180469314, 0.6026876018741237, 0.6025359658874683, 0.6026761326861025, 0.60253560769469, 0.6025173414287283, 0.6024359604093565, 0.6024333341264013, 0.6023833558007853, 0.6024151401332954, 0.6023799603732665, 0.6022912912804689, 0.6022955372796129, 0.6023127049118725, 0.6022881741932968, 0.6022572614853062, 0.6022546553255906, 0.602258687877833, 0.6021062590248549, 0.6022205265956139, 0.6021647859687236, 0.6021703642873621, 0.6020703041553497, 0.6020387364101054, 0.6019981373957733, 0.6020442726585402, 0.6020561402502345, 0.6020160345533001, 0.6019888762929546, 0.6019888447113891, 0.6020174641084315, 0.6019390563733542, 0.6020007737359004, 0.6018965199011476, 0.6019937582781065, 0.6018487773101722, 0.6018745815442569, 0.601901850148813, 0.6018680170979073, 0.6018014803172937, 0.6017872358613939, 0.6017515342982848, 0.6018065169320178, 0.6018421882835787, 0.6017512637198861, 0.6017589595024265, 0.6017515127338581, 0.6016622086336364, 0.6016718332180336, 0.6017307975843771, 0.6016995066270899, 0.6017323554006975, 0.6016601236542659, 0.6016237711461623, 0.601710040426966, 0.6016058823110453, 0.6016467054833227, 0.6016009413929128, 0.6015932184591222, 0.6016043033395241, 0.6015550407233523, 0.6015823034741985, 0.6014922276005816, 0.6016219101735015, 0.6014637616766033, 0.601558772952699, 0.6014793756604194, 0.601525010355373, 0.6014456735751522, 0.6014533674005252, 0.6014658631362132, 0.6014640073544943, 0.6014919011361564, 0.6014417984681343, 0.601498991339954, 0.6014821356001185, 0.6014355088970554, 0.6014230339295829, 0.6015080019208923, 0.6014423673366432, 0.6013545723059284, 0.6014205892851103, 0.6014091024825822, 0.6014857743524793, 0.6013445379618388, 0.6014006883884544, 0.6014114908540427, 0.601342624835114, 0.6013981117597267, 0.6014285952964825, 0.6013694528768312, 0.6012719927439049, 0.6013540569570527, 0.601389648620762, 0.6013783258066249, 0.601295640655418, 0.6012520288842828, 0.6013422639200936, 0.6013375163256233, 0.6012501245025379, 0.6012136975019725, 0.60134257359736, 0.6012486367883967, 0.601263461153009, 0.6013288744038611, 0.6013067434172132, 0.601294519340814, 0.6012760278538092, 0.6012490863986869, 0.6012723263164065, 0.6012497254583373, 0.601217298080672, 0.6012021859501725, 0.6012996937712627, 0.6012244036615785, 0.6012171574048142, 0.6011539725759136, 0.6011966205755277, 0.6010915106801844, 0.6012190848544462, 0.6010505778487049, 0.6012095982103205, 0.6012518843963964, 0.6012645813244493, 0.6010801083739125, 0.601087316523737, 0.6011602576632998, 0.6010873683264006, 0.6011898490608628, 0.6011044595579603, 0.6011999940293938, 0.6010669499813621, 0.6011485401240747, 0.6011030677183351, 0.6011178670267561, 0.6011359886091147, 0.6010953398026637, 0.6011318813153167, 0.6010298971542671, 0.6010954811457377, 0.6010839446978783, 0.6011281643518761, 0.6010386201161058, 0.601131585195883, 0.6010542924101673, 0.6010636078599674, 0.601041557379623, 0.6009997398283944, 0.6010310917247588, 0.6010552607054142, 0.6010669302673483, 0.6010776375701178, 0.6009739762855999, 0.6010967185604038, 0.6010352155254848, 0.6010576380010861, 0.6009620619754293, 0.6010424159222575, 0.6009753282479385, 0.6010261658768156, 0.6008870600453063, 0.6009908978440869, 0.6010619772459144, 0.6010136567211862, 0.6009716786911239, 0.600971213708173, 0.6009764390427674, 0.6008967737742325, 0.6009139718315495, 0.6009874234626542, 0.6009368235628997, 0.6009675125533075, 0.6009508582015536, 0.6009521677734246, 0.6009369843175162, 0.6010208476924185, 0.6009975604423836, 0.6009098676647713, 0.6009462598989259, 0.6009078321439116, 0.6009013280005597, 0.6009666817846583, 0.6009769351535769, 0.6008875644384924, 0.6009476213223899, 0.6009148550122532, 0.6008658242492534, 0.6008356409820158, 0.6008918104687734, 0.6009059313873747, 0.6009044112821124, 0.6008846064201042, 0.6008454932027788, 0.6008917137669093, 0.6009092627310041, 0.6009122986135198, 0.6008307363915799, 0.6008046849716955, 0.6007990080207142, 0.6008495397621126, 0.60087658118401, 0.6008898604183055, 0.6008890900949934, 0.6008233639197563, 0.6007886535685454, 0.600882484076628, 0.6008573001089381, 0.6008155815725896, 0.6008452559273635, 0.600815766453743, 0.600792506799769, 0.6008113623732951, 0.6008427710319633, 0.600746247803987, 0.6008086932461654, 0.6008350818370706, 0.6007922542006222, 0.600804537423511, 0.6006548345667213, 0.6007915534813013, 0.6007697131696033, 0.600753807923687, 0.6007993599816934, 0.6008378439074132, 0.6007517290827054, 0.6007856050356111, 0.6007851276691281, 0.6007643508110473, 0.6007935342237131, 0.6008247655185301, 0.6007969914665863, 0.6007477036696761, 0.60071066894638, 0.6007693339105862, 0.6007130188550522, 0.6007056783028503, 0.6007811951548306, 0.6007935155772451, 0.6007638749257842, 0.600759071022717, 0.6007264779041063, 0.6007570137506101, 0.6007005110427515, 0.6007342745296991, 0.6006363146207225, 0.6007372143731189, 0.6007048551671541, 0.6006547895296296, 0.6006990930215637, 0.6007015715784101, 0.6006726685744613, 0.6006769093114939, 0.60068053697917, 0.6007176528270565, 0.600705214213969, 0.600687392456318]\n",
      "Validation loss trajectory: [0.702361950470192, 0.6632563686297789, 0.6524754350005845, 0.6470706140741814, 0.6460055438835268, 0.6425256776640551, 0.6431155183104902, 0.6406681692707339, 0.6419855777066957, 0.6401234950834933, 0.6411669949532035, 0.6392481555597236, 0.640715890277614, 0.6399178755202288, 0.6394423449853137, 0.6384549210788361, 0.6403744125654451, 0.6436088567576354, 0.6400965061093465, 0.6388891140436428, 0.6391403932343621, 0.6389518140090141, 0.6382316421295492, 0.6390395756332693, 0.6390478202201949, 0.6387474093826719, 0.6391781941362756, 0.6396769977203395, 0.6389899983202697, 0.6390731815636863, 0.6384261260028851, 0.639863380366154, 0.6390897339777105, 0.637896763005852, 0.640426717502786, 0.6386199229740994, 0.6385520776881848, 0.638221585205626, 0.6383210390108117, 0.6384281159378041, 0.6378753623937582, 0.6377240078378791, 0.6384981348891671, 0.638430515939268, 0.6380446111237149, 0.6375916937333131, 0.6383916910519759, 0.6385092733313881, 0.6374116014142189, 0.6374457313110482, 0.6380019203565271, 0.6396385027078004, 0.6376169201844526, 0.6381500608554146, 0.6372485859934924, 0.6375689406297277, 0.6388762578031657, 0.6377734180892327, 0.6377943484821805, 0.6380781568089537, 0.6376512652905298, 0.637531459258952, 0.6378061671684775, 0.6372999971764243, 0.6377119038264344, 0.6377074998108249, 0.6374341093961422, 0.638273020258439, 0.6382234177394455, 0.6378260786677876, 0.6372857948798155, 0.6373809365158908, 0.6381106539933951, 0.638079222046191, 0.6373833832312628, 0.6372617512513504, 0.6370298418187242, 0.6381410986883155, 0.6372348155941113, 0.6376136540440807, 0.63774770773505, 0.6371070667894889, 0.6371631327143605, 0.6375749416579909, 0.6379169051647787, 0.6372841396236508, 0.6368371806310053, 0.6373935525963382, 0.6377365716723108, 0.6371776987939645, 0.6370366304554874, 0.636484704515754, 0.6373602360444449, 0.6376974082080176, 0.6370778133417595, 0.6369172680484049, 0.6374616692908094, 0.6380593432766416, 0.6375493307216132, 0.6372513155819697, 0.637466170845159, 0.6369500722385516, 0.6375907318896925, 0.6378827874581026, 0.6373111849422269, 0.6373629944569297, 0.6370228505356768, 0.6370988199181605, 0.6371937518720913, 0.6372431119941923, 0.6374859401660485, 0.6368357900177619, 0.6372151430363159, 0.636851822788975, 0.636549323709653, 0.6374833640278802, 0.6373902748362883, 0.6375743289326512, 0.6372686986978864, 0.6377573643655683, 0.6369192811025799, 0.6377752280717692, 0.6366904484501074, 0.6369445096704824, 0.6373870980278209, 0.63708354331426, 0.6371639367590503, 0.6370673294797391, 0.6373044246941086, 0.6374539283058344, 0.637084872756554, 0.637598896510014, 0.6368887434552393, 0.6369237323750497, 0.6375598077829694, 0.6371769028510228, 0.6369475658910119, 0.637390193350919, 0.6367146306389796, 0.6376712234386939, 0.6364878173559421, 0.637819956763847, 0.6371928633195149, 0.636994346642252, 0.6373606211276811, 0.637129234227773, 0.6378675097632934, 0.6369543895169175, 0.6371022462544599, 0.6374108557181278, 0.637294606859323, 0.636545489578701, 0.6369427451608203, 0.6369913446418826, 0.637807448840984, 0.6371177427089741, 0.6371072210279543, 0.6373923023800317, 0.6373317543770322, 0.6369790507948235, 0.6365822408063397, 0.6371557380228137, 0.6370112615757191, 0.6368162589027744, 0.6379112381021759, 0.6369825257616273, 0.6370099123269282, 0.637226992795994, 0.6378593987426152, 0.6379049382900553, 0.6370096410684968, 0.6377380143814448, 0.6371254051435244, 0.6368390462774042, 0.6376008964757877, 0.6376932591222394, 0.6369296709838237, 0.6373301417438683, 0.637602321555238, 0.6368431512989813, 0.6365000170445765, 0.6365669194601733, 0.637088153198838, 0.6371670563448567, 0.6373311876775275, 0.6370672939426402, 0.6365352417032341, 0.637016954830137, 0.6367885872350113, 0.6371425677833064, 0.6369409746461566, 0.6371430833952602, 0.6368068064723345, 0.6371188410913181, 0.6372641062978713, 0.6372810822271417, 0.6370715281518178, 0.6370285550394643, 0.6368263202278632, 0.6373672739538275, 0.6376230250141721, 0.6370939322447118, 0.6369795687962675, 0.637275359783412, 0.6376252137221494, 0.6369312515027397, 0.6375595920808108, 0.6372271233822595, 0.6369909874668911, 0.6377272146988772, 0.6367382399181747, 0.6369856413541077, 0.6373639684231472, 0.636867343306031, 0.6370180720731901, 0.6368753695385891, 0.636895387891898, 0.6371797258297849, 0.6381499824175836, 0.6368578248981385, 0.6366451761002164, 0.6374032373052432, 0.6371543137865656, 0.6371389984378946, 0.6372394008631871, 0.637258396977572, 0.6370573850395037, 0.6373595323458335, 0.6369568516201061, 0.6372351051452788, 0.6370064978730258, 0.6370651939910882, 0.6367685075681342, 0.6367954825431211, 0.6370396936818521, 0.6366393213262687, 0.6368625587334552, 0.6368461572776642, 0.6370941556332724, 0.6372099553780095, 0.6371840651965484, 0.6364864915391678, 0.6373738495481474, 0.6371092472881287, 0.6375623994590793, 0.6367893482513626, 0.6367192341332352, 0.6370383795322955, 0.6373980769847064, 0.6368641622114938, 0.6367064669594418, 0.6367551561001816, 0.6372543953771346, 0.6372672159128732, 0.6369458432912566, 0.6376525316778053, 0.636579219792634, 0.6373876552419252, 0.6371785020404894, 0.6369255754101157, 0.6369175033093539, 0.6367464887611453, 0.637562929275364, 0.6369327708882375, 0.6369803508076056, 0.6369004722166378, 0.6371245664049382, 0.6372022786615638, 0.6370987982826403, 0.6369513092913943, 0.6372336338398269, 0.6367769220515669, 0.6374231215793773, 0.6369726174564888, 0.6371467461134422, 0.6367541103892078, 0.636628994840327, 0.636927627767026, 0.6376448396135763, 0.6375735715418196, 0.6373451049592471, 0.6365053811898791, 0.636664870083357, 0.6369537088624997, 0.6368458048040886, 0.6366973233086848, 0.6370675394821503, 0.6372198958124085, 0.6372277695704703, 0.6374541554049855, 0.6370725352281127, 0.6374692029906364, 0.6369692838524262, 0.6375228119816251, 0.6365299328139543, 0.6373410268225624, 0.6368420075013109, 0.636811508576222, 0.6368207031794173, 0.6368923705412433]\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL sklearn.preprocessing._data.StandardScaler was not an allowed global by default. Please use `torch.serialization.add_safe_globals([StandardScaler])` or the `torch.serialization.safe_globals([StandardScaler])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mValidation loss trajectory:\u001b[39m\u001b[33m\"\u001b[39m, history[\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Reload the best-performing weights and scalers from disk for downstream use.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m mlp_model, x_scaler, y_scaler, checkpoint_meta = \u001b[43mload_mlp_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     37\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded checkpoint from epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_meta[\u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith val_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_meta[\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     39\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 306\u001b[39m, in \u001b[36mload_mlp_checkpoint\u001b[39m\u001b[34m(checkpoint_path, device)\u001b[39m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_mlp_checkpoint\u001b[39m(\n\u001b[32m    302\u001b[39m     checkpoint_path: Path, device: Optional[torch.device] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    303\u001b[39m ) -> Tuple[MultiTaskMLP, StandardScaler, StandardScaler, Dict]:\n\u001b[32m    304\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a saved multitask MLP and its associated scalers.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m     model = MultiTaskMLP(\n\u001b[32m    308\u001b[39m         checkpoint[\u001b[33m\"\u001b[39m\u001b[33minput_dim\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    309\u001b[39m         checkpoint[\u001b[33m\"\u001b[39m\u001b[33moutput_dim\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    310\u001b[39m         hidden_sizes=checkpoint.get(\u001b[33m\"\u001b[39m\u001b[33mhidden_sizes\u001b[39m\u001b[33m\"\u001b[39m, (\u001b[32m512\u001b[39m, \u001b[32m512\u001b[39m, \u001b[32m256\u001b[39m)),\n\u001b[32m    311\u001b[39m         dropout=checkpoint.get(\u001b[33m\"\u001b[39m\u001b[33mdropout\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0.0\u001b[39m),\n\u001b[32m    312\u001b[39m     )\n\u001b[32m    313\u001b[39m     model.load_state_dict(checkpoint[\u001b[33m\"\u001b[39m\u001b[33mstate_dict\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\KSEF\\.venv\\Lib\\site-packages\\torch\\serialization.py:1470\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1462\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1463\u001b[39m                     opened_zipfile,\n\u001b[32m   1464\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1467\u001b[39m                     **pickle_load_args,\n\u001b[32m   1468\u001b[39m                 )\n\u001b[32m   1469\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1471\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1472\u001b[39m             opened_zipfile,\n\u001b[32m   1473\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1476\u001b[39m             **pickle_load_args,\n\u001b[32m   1477\u001b[39m         )\n\u001b[32m   1478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL sklearn.preprocessing._data.StandardScaler was not an allowed global by default. Please use `torch.serialization.add_safe_globals([StandardScaler])` or the `torch.serialization.safe_globals([StandardScaler])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "# Load pretrained latent diffusion encoder\n",
    "checkpoint_dir = Path(\"./qm9_latent2\")\n",
    "ldm_model, dataset_info, nodes_dist, device = load_qm9_latent_diffusion(checkpoint_dir)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Prepare the plastics dataframe and build (or load) latent embeddings.\n",
    "plastic_df = pd.read_parquet(\"data/plastic.parquet\")\n",
    "embeddings, targets, used_indices = build_latent_matrix(\n",
    "    plastic_df,\n",
    "    ldm_model,\n",
    "    dataset_info,\n",
    "    device,\n",
    "    sample_size=1373503,  # 1373503\n",
    "    cache_path=Path(\"data/plastic_latents.pt\"),\n",
    ")\n",
    "\n",
    "# Train the multi-task MLP regressor and persist the best checkpoint.\n",
    "checkpoint_path = Path(\"models/plastic_mlp_best.pt\")\n",
    "mlp_model, x_scaler, y_scaler, history = train_multitask_mlp(\n",
    "    embeddings.numpy(),\n",
    "    targets.numpy(),\n",
    "    epochs=300,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"Training loss trajectory:\", history[\"train\"])\n",
    "print(\"Validation loss trajectory:\", history[\"val\"])\n",
    "\n",
    "# Reload the best-performing weights and scalers from disk for downstream use.\n",
    "mlp_model, x_scaler, y_scaler, checkpoint_meta = load_mlp_checkpoint(\n",
    "    checkpoint_path, device=device\n",
    ")\n",
    "print(\n",
    "    f\"Loaded checkpoint from epoch {checkpoint_meta['epoch']} \"\n",
    "    f\"with val_loss={checkpoint_meta['val_loss']:.6f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ce023d1b200007",
   "metadata": {},
   "source": [
    "## Optimization 1: Straw for Cold Brew\n",
    "Optimization Boundary:\n",
    "\n",
    "Tg: -30\\~10 Â°C\n",
    "\n",
    "Tm: 120\\~200 Â°C\n",
    "\n",
    "Td: 250Â°C ì´ìƒ\n",
    "\n",
    "YM: 0.8\\~1.8GPa->ë²”ìœ„ ì•ˆì— ê°€ë‘ì–´ì•¼ í•¨(1.2 ê·¼ì²˜)\n",
    "\n",
    "TS_b: 20MPa ì´ìƒ-> ìµœëŒ€í™” ëŒ€ìƒ(2)\n",
    "\n",
    "eps_b : 200\\~1000%->ìµœëŒ€í™” ëŒ€ìƒ(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b0106e06603836",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T01:27:01.810916700Z",
     "start_time": "2025-11-19T23:54:10.534685Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example inference call\n",
    "example_smiles = _pick_base_smiles(plastic_df.loc[0, \"smiles\"])\n",
    "predicted = predict_properties(\n",
    "    example_smiles,\n",
    "    ldm_model=ldm_model,\n",
    "    dataset_info=dataset_info,\n",
    "    device=device,\n",
    "    mlp_model=mlp_model,\n",
    "    x_scaler=x_scaler,\n",
    "    y_scaler=y_scaler,\n",
    ")\n",
    "\n",
    "for name, value in predicted.items():\n",
    "    print(f\"{name}: {value:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
