{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be96a56646559a26",
   "metadata": {},
   "source": [
    "# Plastic Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feaf6ce639fffd7",
   "metadata": {},
   "source": [
    "We attempt to design new plastic molecule using LDM.\n",
    "Our main goal is on\n",
    "1. Straw\n",
    "2. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba3d462540fb66",
   "metadata": {},
   "source": [
    "## Setup enviroment\n",
    "!git clone https://github.com/Ahnd6474/KSEF\n",
    "\n",
    "%cd your_shit/GitHub/KSEF\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "id": "10c1e0914dd7ef6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:22:53.652807Z",
     "start_time": "2025-11-21T17:22:53.646399Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Iterator, Optional\n",
    "import pyarrow\n",
    "import torch\n",
    "from geoldm.configs import get_dataset_info\n",
    "from geoldm.qm9 import dataset, load_model, sampling, visualize_molecule_3d"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "9c5b80a769160d02",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "1bb23c5d1539f3ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:22:57.691252Z",
     "start_time": "2025-11-21T17:22:55.674380Z"
    }
   },
   "source": [
    "df=pd.read_parquet('data/plastic.parquet')\n",
    "df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         comp          Tg          Tm          Td           YM       TS_y  \\\n",
       "0         0.0  246.341049  354.051544  524.699646   773.208252  25.838213   \n",
       "1         0.1  313.798401  449.257751  502.616241  1866.776855  44.379429   \n",
       "2         0.2  306.681305  446.618713  502.605896  1769.267456  44.743877   \n",
       "3         0.3  298.431488  442.813019  503.664642  1653.973633  44.287182   \n",
       "4         0.4  290.073944  436.398560  505.013275  1508.705688  42.840149   \n",
       "...       ...         ...         ...         ...          ...        ...   \n",
       "1373498   0.0  325.252380  462.438263  502.465759  2680.520020  39.839355   \n",
       "1373499   0.0  369.788818  509.966248  587.491455  2227.901367  33.554047   \n",
       "1373500   0.0  344.375183  457.250885  574.055664  1724.992798  50.262169   \n",
       "1373501   0.0  393.653473  536.671326  700.403137  1942.565186  44.805931   \n",
       "1373502   0.0  317.276978  469.612549  686.914612  1508.900513  54.058247   \n",
       "\n",
       "              TS_b       eps_b   perm_O2   perm_CO2    perm_He   perm_N2  \\\n",
       "0        32.409279  103.836082 -0.074397  -0.109570   0.493981 -0.071000   \n",
       "1        38.010651    7.215131 -0.043089  -0.006026   0.462577 -0.049233   \n",
       "2        37.998493    8.727348 -0.050900  -0.028856   0.455602 -0.060321   \n",
       "3        37.676380   10.997544 -0.057812  -0.048113   0.458959 -0.069126   \n",
       "4        36.907192   14.239414 -0.065853  -0.065725   0.461121 -0.075789   \n",
       "...            ...         ...       ...        ...        ...       ...   \n",
       "1373498  68.497833   30.515642  0.011635  -0.060768   0.179605  0.002471   \n",
       "1373499  37.865055    1.950041  2.547638  11.337365  21.572718  0.559539   \n",
       "1373500  46.697941   27.839941  0.099392   0.332474   1.581858 -0.005775   \n",
       "1373501  65.647507   27.729816  0.016741   0.138303   1.767082 -0.054595   \n",
       "1373502  40.808701    4.400261 -0.004739  -0.059302   0.252653  0.014024   \n",
       "\n",
       "         perm_CH4    perm_H2  \\\n",
       "0       -0.061771   0.296051   \n",
       "1       -0.031425   0.342873   \n",
       "2       -0.039934   0.322175   \n",
       "3       -0.047438   0.314363   \n",
       "4       -0.055182   0.302125   \n",
       "...           ...        ...   \n",
       "1373498  0.001415   0.056135   \n",
       "1373499  0.881040  24.865118   \n",
       "1373500  0.070204   1.482780   \n",
       "1373501 -0.069961   1.336078   \n",
       "1373502  0.044955   0.000020   \n",
       "\n",
       "                                                    smiles    num_side  \\\n",
       "0                         [[*]OCCC(=O)[*], [*]OCCC(=O)[*]]  [0.0, 0.0]   \n",
       "1                      [[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]  [0.0, 0.0]   \n",
       "2                      [[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]  [0.0, 0.0]   \n",
       "3                      [[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]  [0.0, 0.0]   \n",
       "4                      [[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]  [0.0, 0.0]   \n",
       "...                                                    ...         ...   \n",
       "1373498                         [[*]CC([*])O, [*]CC([*])O]  [nan, nan]   \n",
       "1373499           [[*]CC([*])c1ccccc1, [*]CC([*])c1ccccc1]  [nan, nan]   \n",
       "1373500                       [[*]CC([*])Cl, [*]CC([*])Cl]  [nan, nan]   \n",
       "1373501  [[*]CCOC(=O)c1ccc2cc(C(=O)O[*])ccc2c1, [*]CCOC...  [nan, nan]   \n",
       "1373502             [[*]CCCCCC(=O)N[*], [*]CCCCCC(=O)N[*]]  [nan, nan]   \n",
       "\n",
       "           num_back     end_group             names  \n",
       "0        [3.0, 3.0]          [, ]        [pha, pha]  \n",
       "1        [3.0, 3.0]         [, O]        [pha, pha]  \n",
       "2        [3.0, 3.0]         [, O]        [pha, pha]  \n",
       "3        [3.0, 3.0]         [, O]        [pha, pha]  \n",
       "4        [3.0, 3.0]         [, O]        [pha, pha]  \n",
       "...             ...           ...               ...  \n",
       "1373498  [nan, nan]  [None, None]        [PVA, PVA]  \n",
       "1373499  [nan, nan]  [None, None]          [PS, PS]  \n",
       "1373500  [nan, nan]  [None, None]        [PVC, PVC]  \n",
       "1373501  [nan, nan]  [None, None]        [PEN, PEN]  \n",
       "1373502  [nan, nan]  [None, None]  [Nylon6, Nylon6]  \n",
       "\n",
       "[1373503 rows x 19 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comp</th>\n",
       "      <th>Tg</th>\n",
       "      <th>Tm</th>\n",
       "      <th>Td</th>\n",
       "      <th>YM</th>\n",
       "      <th>TS_y</th>\n",
       "      <th>TS_b</th>\n",
       "      <th>eps_b</th>\n",
       "      <th>perm_O2</th>\n",
       "      <th>perm_CO2</th>\n",
       "      <th>perm_He</th>\n",
       "      <th>perm_N2</th>\n",
       "      <th>perm_CH4</th>\n",
       "      <th>perm_H2</th>\n",
       "      <th>smiles</th>\n",
       "      <th>num_side</th>\n",
       "      <th>num_back</th>\n",
       "      <th>end_group</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>246.341049</td>\n",
       "      <td>354.051544</td>\n",
       "      <td>524.699646</td>\n",
       "      <td>773.208252</td>\n",
       "      <td>25.838213</td>\n",
       "      <td>32.409279</td>\n",
       "      <td>103.836082</td>\n",
       "      <td>-0.074397</td>\n",
       "      <td>-0.109570</td>\n",
       "      <td>0.493981</td>\n",
       "      <td>-0.071000</td>\n",
       "      <td>-0.061771</td>\n",
       "      <td>0.296051</td>\n",
       "      <td>[[*]OCCC(=O)[*], [*]OCCC(=O)[*]]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[3.0, 3.0]</td>\n",
       "      <td>[, ]</td>\n",
       "      <td>[pha, pha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>313.798401</td>\n",
       "      <td>449.257751</td>\n",
       "      <td>502.616241</td>\n",
       "      <td>1866.776855</td>\n",
       "      <td>44.379429</td>\n",
       "      <td>38.010651</td>\n",
       "      <td>7.215131</td>\n",
       "      <td>-0.043089</td>\n",
       "      <td>-0.006026</td>\n",
       "      <td>0.462577</td>\n",
       "      <td>-0.049233</td>\n",
       "      <td>-0.031425</td>\n",
       "      <td>0.342873</td>\n",
       "      <td>[[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[3.0, 3.0]</td>\n",
       "      <td>[, O]</td>\n",
       "      <td>[pha, pha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>306.681305</td>\n",
       "      <td>446.618713</td>\n",
       "      <td>502.605896</td>\n",
       "      <td>1769.267456</td>\n",
       "      <td>44.743877</td>\n",
       "      <td>37.998493</td>\n",
       "      <td>8.727348</td>\n",
       "      <td>-0.050900</td>\n",
       "      <td>-0.028856</td>\n",
       "      <td>0.455602</td>\n",
       "      <td>-0.060321</td>\n",
       "      <td>-0.039934</td>\n",
       "      <td>0.322175</td>\n",
       "      <td>[[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[3.0, 3.0]</td>\n",
       "      <td>[, O]</td>\n",
       "      <td>[pha, pha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>298.431488</td>\n",
       "      <td>442.813019</td>\n",
       "      <td>503.664642</td>\n",
       "      <td>1653.973633</td>\n",
       "      <td>44.287182</td>\n",
       "      <td>37.676380</td>\n",
       "      <td>10.997544</td>\n",
       "      <td>-0.057812</td>\n",
       "      <td>-0.048113</td>\n",
       "      <td>0.458959</td>\n",
       "      <td>-0.069126</td>\n",
       "      <td>-0.047438</td>\n",
       "      <td>0.314363</td>\n",
       "      <td>[[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[3.0, 3.0]</td>\n",
       "      <td>[, O]</td>\n",
       "      <td>[pha, pha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>290.073944</td>\n",
       "      <td>436.398560</td>\n",
       "      <td>505.013275</td>\n",
       "      <td>1508.705688</td>\n",
       "      <td>42.840149</td>\n",
       "      <td>36.907192</td>\n",
       "      <td>14.239414</td>\n",
       "      <td>-0.065853</td>\n",
       "      <td>-0.065725</td>\n",
       "      <td>0.461121</td>\n",
       "      <td>-0.075789</td>\n",
       "      <td>-0.055182</td>\n",
       "      <td>0.302125</td>\n",
       "      <td>[[*]OCCC(=O)[*], [*]OC(O)CC(=O)[*]]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[3.0, 3.0]</td>\n",
       "      <td>[, O]</td>\n",
       "      <td>[pha, pha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373498</th>\n",
       "      <td>0.0</td>\n",
       "      <td>325.252380</td>\n",
       "      <td>462.438263</td>\n",
       "      <td>502.465759</td>\n",
       "      <td>2680.520020</td>\n",
       "      <td>39.839355</td>\n",
       "      <td>68.497833</td>\n",
       "      <td>30.515642</td>\n",
       "      <td>0.011635</td>\n",
       "      <td>-0.060768</td>\n",
       "      <td>0.179605</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.056135</td>\n",
       "      <td>[[*]CC([*])O, [*]CC([*])O]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[PVA, PVA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>369.788818</td>\n",
       "      <td>509.966248</td>\n",
       "      <td>587.491455</td>\n",
       "      <td>2227.901367</td>\n",
       "      <td>33.554047</td>\n",
       "      <td>37.865055</td>\n",
       "      <td>1.950041</td>\n",
       "      <td>2.547638</td>\n",
       "      <td>11.337365</td>\n",
       "      <td>21.572718</td>\n",
       "      <td>0.559539</td>\n",
       "      <td>0.881040</td>\n",
       "      <td>24.865118</td>\n",
       "      <td>[[*]CC([*])c1ccccc1, [*]CC([*])c1ccccc1]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[PS, PS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373500</th>\n",
       "      <td>0.0</td>\n",
       "      <td>344.375183</td>\n",
       "      <td>457.250885</td>\n",
       "      <td>574.055664</td>\n",
       "      <td>1724.992798</td>\n",
       "      <td>50.262169</td>\n",
       "      <td>46.697941</td>\n",
       "      <td>27.839941</td>\n",
       "      <td>0.099392</td>\n",
       "      <td>0.332474</td>\n",
       "      <td>1.581858</td>\n",
       "      <td>-0.005775</td>\n",
       "      <td>0.070204</td>\n",
       "      <td>1.482780</td>\n",
       "      <td>[[*]CC([*])Cl, [*]CC([*])Cl]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[PVC, PVC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373501</th>\n",
       "      <td>0.0</td>\n",
       "      <td>393.653473</td>\n",
       "      <td>536.671326</td>\n",
       "      <td>700.403137</td>\n",
       "      <td>1942.565186</td>\n",
       "      <td>44.805931</td>\n",
       "      <td>65.647507</td>\n",
       "      <td>27.729816</td>\n",
       "      <td>0.016741</td>\n",
       "      <td>0.138303</td>\n",
       "      <td>1.767082</td>\n",
       "      <td>-0.054595</td>\n",
       "      <td>-0.069961</td>\n",
       "      <td>1.336078</td>\n",
       "      <td>[[*]CCOC(=O)c1ccc2cc(C(=O)O[*])ccc2c1, [*]CCOC...</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[PEN, PEN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373502</th>\n",
       "      <td>0.0</td>\n",
       "      <td>317.276978</td>\n",
       "      <td>469.612549</td>\n",
       "      <td>686.914612</td>\n",
       "      <td>1508.900513</td>\n",
       "      <td>54.058247</td>\n",
       "      <td>40.808701</td>\n",
       "      <td>4.400261</td>\n",
       "      <td>-0.004739</td>\n",
       "      <td>-0.059302</td>\n",
       "      <td>0.252653</td>\n",
       "      <td>0.014024</td>\n",
       "      <td>0.044955</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>[[*]CCCCCC(=O)N[*], [*]CCCCCC(=O)N[*]]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>[Nylon6, Nylon6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1373503 rows \u00d7 19 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "bb66463e2017a951",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:23:01.476586Z",
     "start_time": "2025-11-21T17:23:00.918738Z"
    }
   },
   "source": [
    "df.describe()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               comp            Tg            Tm            Td            YM  \\\n",
       "count  1.373503e+06  1.373503e+06  1.373503e+06  1.373503e+06  1.373503e+06   \n",
       "mean   4.997987e-01  2.725839e+02  3.529060e+02  5.769558e+02  5.405507e+02   \n",
       "std    2.583418e-01  2.701165e+01  3.713276e+01  2.686434e+01  4.882776e+02   \n",
       "min    0.000000e+00  2.020853e+02  2.794441e+02  4.246918e+02  9.102757e+01   \n",
       "25%    3.000000e-01  2.543430e+02  3.266345e+02  5.673138e+02  1.744876e+02   \n",
       "50%    5.000000e-01  2.699055e+02  3.456665e+02  5.827677e+02  3.266322e+02   \n",
       "75%    7.000000e-01  2.883478e+02  3.725729e+02  5.933018e+02  7.870456e+02   \n",
       "max    9.000000e-01  4.033092e+02  5.529254e+02  7.004031e+02  3.081163e+03   \n",
       "\n",
       "               TS_y          TS_b         eps_b       perm_O2      perm_CO2  \\\n",
       "count  1.373503e+06  1.373503e+06  1.373503e+06  1.373503e+06  1.373503e+06   \n",
       "mean   2.073843e+01  2.460826e+01  2.115185e+02  8.110294e-01  4.332410e+00   \n",
       "std    1.366027e+01  1.229784e+01  1.205617e+02  8.184573e-01  3.972706e+00   \n",
       "min    3.014123e+00  4.558969e+00  4.952888e-01 -9.569907e-02 -1.267213e-01   \n",
       "25%    8.976903e+00  1.493889e+01  1.076303e+02  3.791428e-01  1.926220e+00   \n",
       "50%    1.742608e+01  2.180278e+01  2.111784e+02  6.681085e-01  3.541990e+00   \n",
       "75%    3.001149e+01  3.317859e+01  3.043738e+02  1.066986e+00  5.741556e+00   \n",
       "max    7.795893e+01  1.298841e+02  5.666409e+02  3.425747e+01  1.253688e+02   \n",
       "\n",
       "            perm_He       perm_N2      perm_CH4       perm_H2  \n",
       "count  1.373503e+06  1.373503e+06  1.373503e+06  1.373503e+06  \n",
       "mean   7.026700e+00  1.901237e-01  4.661335e-01  7.116223e+00  \n",
       "std    4.018037e+00  2.560846e-01  5.097858e-01  4.612499e+00  \n",
       "min    1.728067e-01 -1.294968e-01 -1.326250e-01  2.026558e-05  \n",
       "25%    4.530061e+00  5.799794e-02  1.869081e-01  4.429616e+00  \n",
       "50%    6.793644e+00  1.420151e-01  3.605243e-01  6.750260e+00  \n",
       "75%    9.030967e+00  2.563907e-01  6.126437e-01  9.138975e+00  \n",
       "max    1.188544e+02  8.895615e+00  1.651693e+01  1.671498e+02  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comp</th>\n",
       "      <th>Tg</th>\n",
       "      <th>Tm</th>\n",
       "      <th>Td</th>\n",
       "      <th>YM</th>\n",
       "      <th>TS_y</th>\n",
       "      <th>TS_b</th>\n",
       "      <th>eps_b</th>\n",
       "      <th>perm_O2</th>\n",
       "      <th>perm_CO2</th>\n",
       "      <th>perm_He</th>\n",
       "      <th>perm_N2</th>\n",
       "      <th>perm_CH4</th>\n",
       "      <th>perm_H2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "      <td>1.373503e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.997987e-01</td>\n",
       "      <td>2.725839e+02</td>\n",
       "      <td>3.529060e+02</td>\n",
       "      <td>5.769558e+02</td>\n",
       "      <td>5.405507e+02</td>\n",
       "      <td>2.073843e+01</td>\n",
       "      <td>2.460826e+01</td>\n",
       "      <td>2.115185e+02</td>\n",
       "      <td>8.110294e-01</td>\n",
       "      <td>4.332410e+00</td>\n",
       "      <td>7.026700e+00</td>\n",
       "      <td>1.901237e-01</td>\n",
       "      <td>4.661335e-01</td>\n",
       "      <td>7.116223e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.583418e-01</td>\n",
       "      <td>2.701165e+01</td>\n",
       "      <td>3.713276e+01</td>\n",
       "      <td>2.686434e+01</td>\n",
       "      <td>4.882776e+02</td>\n",
       "      <td>1.366027e+01</td>\n",
       "      <td>1.229784e+01</td>\n",
       "      <td>1.205617e+02</td>\n",
       "      <td>8.184573e-01</td>\n",
       "      <td>3.972706e+00</td>\n",
       "      <td>4.018037e+00</td>\n",
       "      <td>2.560846e-01</td>\n",
       "      <td>5.097858e-01</td>\n",
       "      <td>4.612499e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.020853e+02</td>\n",
       "      <td>2.794441e+02</td>\n",
       "      <td>4.246918e+02</td>\n",
       "      <td>9.102757e+01</td>\n",
       "      <td>3.014123e+00</td>\n",
       "      <td>4.558969e+00</td>\n",
       "      <td>4.952888e-01</td>\n",
       "      <td>-9.569907e-02</td>\n",
       "      <td>-1.267213e-01</td>\n",
       "      <td>1.728067e-01</td>\n",
       "      <td>-1.294968e-01</td>\n",
       "      <td>-1.326250e-01</td>\n",
       "      <td>2.026558e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>2.543430e+02</td>\n",
       "      <td>3.266345e+02</td>\n",
       "      <td>5.673138e+02</td>\n",
       "      <td>1.744876e+02</td>\n",
       "      <td>8.976903e+00</td>\n",
       "      <td>1.493889e+01</td>\n",
       "      <td>1.076303e+02</td>\n",
       "      <td>3.791428e-01</td>\n",
       "      <td>1.926220e+00</td>\n",
       "      <td>4.530061e+00</td>\n",
       "      <td>5.799794e-02</td>\n",
       "      <td>1.869081e-01</td>\n",
       "      <td>4.429616e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>2.699055e+02</td>\n",
       "      <td>3.456665e+02</td>\n",
       "      <td>5.827677e+02</td>\n",
       "      <td>3.266322e+02</td>\n",
       "      <td>1.742608e+01</td>\n",
       "      <td>2.180278e+01</td>\n",
       "      <td>2.111784e+02</td>\n",
       "      <td>6.681085e-01</td>\n",
       "      <td>3.541990e+00</td>\n",
       "      <td>6.793644e+00</td>\n",
       "      <td>1.420151e-01</td>\n",
       "      <td>3.605243e-01</td>\n",
       "      <td>6.750260e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000e-01</td>\n",
       "      <td>2.883478e+02</td>\n",
       "      <td>3.725729e+02</td>\n",
       "      <td>5.933018e+02</td>\n",
       "      <td>7.870456e+02</td>\n",
       "      <td>3.001149e+01</td>\n",
       "      <td>3.317859e+01</td>\n",
       "      <td>3.043738e+02</td>\n",
       "      <td>1.066986e+00</td>\n",
       "      <td>5.741556e+00</td>\n",
       "      <td>9.030967e+00</td>\n",
       "      <td>2.563907e-01</td>\n",
       "      <td>6.126437e-01</td>\n",
       "      <td>9.138975e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000e-01</td>\n",
       "      <td>4.033092e+02</td>\n",
       "      <td>5.529254e+02</td>\n",
       "      <td>7.004031e+02</td>\n",
       "      <td>3.081163e+03</td>\n",
       "      <td>7.795893e+01</td>\n",
       "      <td>1.298841e+02</td>\n",
       "      <td>5.666409e+02</td>\n",
       "      <td>3.425747e+01</td>\n",
       "      <td>1.253688e+02</td>\n",
       "      <td>1.188544e+02</td>\n",
       "      <td>8.895615e+00</td>\n",
       "      <td>1.651693e+01</td>\n",
       "      <td>1.671498e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "5152fb9dcbe4c51d",
   "metadata": {},
   "source": [
    "## Property Prediction\n",
    "We use MLP multi regressor to predict property of plastic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980fc32927142b91",
   "metadata": {},
   "source": [
    "### Load Model and helpers"
   ]
  },
  {
   "cell_type": "code",
   "id": "6f282a01a0bf16d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:27:28.687672Z",
     "start_time": "2025-11-21T17:27:28.654370Z"
    }
   },
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import ast\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from geoldm import encode, load_model, smiles_to_3d\n",
    "from geoldm.configs import get_dataset_info\n",
    "from geoldm.qm9 import dataset\n",
    "\n",
    "TARGET_COLUMNS = [\n",
    "    \"Tg\",\n",
    "    \"Tm\",\n",
    "    \"Td\",\n",
    "    \"YM\",\n",
    "    \"TS_y\",\n",
    "    \"TS_b\",\n",
    "    \"eps_b\",\n",
    "    \"perm_O2\",\n",
    "    \"perm_CO2\",\n",
    "    \"perm_He\",\n",
    "    \"perm_N2\",\n",
    "    \"perm_CH4\",\n",
    "    \"perm_H2\",\n",
    "]\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def load_qm9_latent_diffusion(checkpoint_dir: Path):\n",
    "    \"\"\"Load a pretrained latent diffusion model and its dataset metadata.\"\"\"\n",
    "\n",
    "    args_path = checkpoint_dir / \"args.pickle\"\n",
    "    with args_path.open(\"rb\") as handle:\n",
    "        args = pickle.load(handle)\n",
    "\n",
    "    # Respect GPU availability explicitly.\n",
    "    setattr(args, \"cuda\", torch.cuda.is_available())\n",
    "\n",
    "    dataset_info = get_dataset_info(args.dataset, args.remove_h)\n",
    "    dataloaders, _ = dataset.retrieve_dataloaders(args)\n",
    "    train_loader = dataloaders[\"train\"]\n",
    "\n",
    "    model, nodes_dist, _ = load_model(\n",
    "        stage=\"latent_diffusion\",\n",
    "        args=args,\n",
    "        dataset_info=dataset_info,\n",
    "        dataloader_train=train_loader,\n",
    "        checkpoint_path=checkpoint_dir,\n",
    "    )\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    return model, dataset_info, nodes_dist, device\n",
    "\n",
    "\n",
    "def _clean_smiles_string(smiles: str) -> str:\n",
    "    \"\"\"Replace placeholder atoms with hydrogens and trim whitespace.\"\"\"\n",
    "\n",
    "    return smiles.replace(\"[*]\", \"\").strip()\n",
    "\n",
    "\n",
    "def _pick_base_smiles(smiles_entry: object) -> str:\n",
    "    \"\"\"Normalize the parquet smiles field to a single SMILES string.\"\"\"\n",
    "\n",
    "    def iterate_candidates(seq):\n",
    "        for candidate in seq:\n",
    "            if isinstance(candidate, str):\n",
    "                cleaned = _clean_smiles_string(candidate)\n",
    "                if cleaned:\n",
    "                    return cleaned\n",
    "        return None\n",
    "\n",
    "    if isinstance(smiles_entry, str):\n",
    "        try:\n",
    "            maybe_list = ast.literal_eval(smiles_entry)\n",
    "        except Exception:\n",
    "            maybe_list = None\n",
    "\n",
    "        if isinstance(maybe_list, (list, tuple, np.ndarray)):\n",
    "            candidate = iterate_candidates(maybe_list)\n",
    "            if candidate:\n",
    "                return candidate\n",
    "\n",
    "        bracketed = smiles_entry.strip()\n",
    "        if bracketed.startswith(\"[\") and bracketed.endswith(\"]\"):\n",
    "            inner = bracketed[1:-1]\n",
    "            parts = [part.strip() for part in inner.split(\",\") if part.strip()]\n",
    "            candidate = iterate_candidates(parts)\n",
    "            if candidate:\n",
    "                return candidate\n",
    "\n",
    "        return _clean_smiles_string(smiles_entry)\n",
    "\n",
    "    if isinstance(smiles_entry, (list, tuple, np.ndarray)):\n",
    "        candidate = iterate_candidates(smiles_entry)\n",
    "        if candidate:\n",
    "            return candidate\n",
    "\n",
    "    return str(smiles_entry)\n",
    "\n",
    "\n",
    "def conformer_to_tensors(\n",
    "    conformer: Dict,\n",
    "    dataset_info: Dict,\n",
    "    device: torch.device,\n",
    ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Convert a single RDKit conformer into GeoLDM-ready tensors.\"\"\"\n",
    "\n",
    "    atom_decoder: List[str] = dataset_info[\"atom_decoder\"]\n",
    "    atom_encoder = {symbol: idx for idx, symbol in enumerate(atom_decoder)}\n",
    "    atom_indices = torch.tensor(\n",
    "        [atom_encoder[symbol] for symbol in conformer[\"atom_symbols\"]],\n",
    "        dtype=torch.long,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    one_hot = F.one_hot(atom_indices, num_classes=len(atom_decoder)).float()\n",
    "    positions = torch.tensor(conformer[\"coordinates\"], dtype=torch.float32, device=device)\n",
    "\n",
    "    # Add batch dimensions and masks.\n",
    "    x = positions.unsqueeze(0)\n",
    "    h = {\n",
    "        \"categorical\": one_hot.unsqueeze(0),\n",
    "        # Use zero charges when absent.\n",
    "        \"integer\": torch.zeros(one_hot.shape[0], 1, device=device).unsqueeze(0),\n",
    "    }\n",
    "\n",
    "    node_mask = torch.ones(x.shape[0], x.shape[1], 1, device=device)\n",
    "    edge_mask = node_mask.squeeze(-1)[..., None] * node_mask.squeeze(-1)[:, None]\n",
    "    return x, h, node_mask, edge_mask\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def smiles_to_latent(\n",
    "    smiles: str,\n",
    "    model: torch.nn.Module,\n",
    "    dataset_info: Dict,\n",
    "    device: torch.device,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Encode a SMILES string into a pooled latent vector using the LDM encoder.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    conformers = smiles_to_3d(smiles)\n",
    "    if len(conformers) == 0:\n",
    "        raise ValueError(f\"Failed to build 3D structure for {smiles!r}\")\n",
    "\n",
    "    x, h, node_mask, edge_mask = conformer_to_tensors(conformers[0], dataset_info, device)\n",
    "    z_x, _, z_h, _ = encode(model, x, h, node_mask=node_mask, edge_mask=edge_mask)\n",
    "\n",
    "    pooled_x = z_x.mean(dim=1)\n",
    "    pooled_h = z_h.mean(dim=1)\n",
    "    latent = torch.cat([pooled_x.flatten(), pooled_h.flatten()], dim=-1)\n",
    "    return latent.cpu()\n",
    "\n",
    "\n",
    "def build_latent_matrix(\n",
    "    frame: pd.DataFrame,\n",
    "    model: torch.nn.Module,\n",
    "    dataset_info: Dict,\n",
    "    device: torch.device,\n",
    "    *,\n",
    "    sample_size: int = 512,\n",
    "    batch_size: int = 32,\n",
    "    cache_path: Optional[Path] = Path(\"data/plastic_latents.pt\"),\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, List[int]]:\n",
    "    \"\"\"Convert a dataframe of plastics into latent vectors and targets with batching.\n",
    "\n",
    "    The helper optionally caches results to avoid recomputation when the notebook\n",
    "    is rerun.\n",
    "    \"\"\"\n",
    "\n",
    "    if cache_path and cache_path.exists():\n",
    "        payload = torch.load(cache_path)\n",
    "        return payload[\"embeddings\"], payload[\"targets\"], payload[\"indices\"]\n",
    "\n",
    "    subset = frame.sample(min(sample_size, len(frame)), random_state=0).reset_index()\n",
    "    embeddings: List[torch.Tensor] = []\n",
    "    targets: List[torch.Tensor] = []\n",
    "    used_indices: List[int] = []\n",
    "\n",
    "    model.eval()\n",
    "    progress = tqdm(total=len(subset), desc=\"Encoding SMILES\", unit=\"sample\")\n",
    "    for start in range(0, len(subset), batch_size):\n",
    "        batch = subset.iloc[start : start + batch_size]\n",
    "        batch_payloads: List[Tuple[str, np.ndarray, int]] = []\n",
    "\n",
    "        for _, row in batch.iterrows():\n",
    "            base_smiles = _pick_base_smiles(row[\"smiles\"])\n",
    "            target_values = (\n",
    "                pd.to_numeric(row[TARGET_COLUMNS], errors=\"coerce\").astype(np.float32).to_numpy()\n",
    "            )\n",
    "            if not np.isfinite(target_values).all():\n",
    "                continue\n",
    "            batch_payloads.append((base_smiles, target_values, int(row[\"index\"])))\n",
    "\n",
    "        batch_latents: List[torch.Tensor] = []\n",
    "        batch_targets: List[torch.Tensor] = []\n",
    "        batch_indices: List[int] = []\n",
    "\n",
    "        for smiles_value, target_values, row_index in batch_payloads:\n",
    "            try:\n",
    "                latent = smiles_to_latent(smiles_value, model, dataset_info, device)\n",
    "            except Exception:\n",
    "                continue\n",
    "            batch_latents.append(latent)\n",
    "            batch_targets.append(torch.tensor(target_values, dtype=torch.float32))\n",
    "            batch_indices.append(row_index)\n",
    "\n",
    "        embeddings.extend(batch_latents)\n",
    "        targets.extend(batch_targets)\n",
    "        used_indices.extend(batch_indices)\n",
    "        progress.update(len(batch))\n",
    "\n",
    "    progress.close()\n",
    "\n",
    "    if not embeddings:\n",
    "        raise RuntimeError(\"No embeddings were created; check SMILES parsing\")\n",
    "\n",
    "    emb_tensor = torch.stack(embeddings)\n",
    "    target_tensor = torch.stack(targets)\n",
    "\n",
    "    if cache_path:\n",
    "        cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(\n",
    "            {\"embeddings\": emb_tensor, \"targets\": target_tensor, \"indices\": used_indices},\n",
    "            cache_path,\n",
    "        )\n",
    "\n",
    "    return emb_tensor, target_tensor, used_indices\n",
    "\n",
    "\n",
    "class MultiTaskMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        hidden_sizes: Iterable[int] = (256, 128),\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = []\n",
    "        prev = input_dim\n",
    "        for width in hidden_sizes:\n",
    "            layers.extend([nn.Linear(prev, width), nn.ReLU(), nn.Dropout(dropout)])\n",
    "            prev = width\n",
    "        layers.append(nn.Linear(prev, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # noqa: D401\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "def train_multitask_mlp(\n",
    "    embeddings: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    *,\n",
    "    epochs: int = 15,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 1e-3,\n",
    "    validation_split: float = 0.1,\n",
    "    log_dir: Optional[str] = \"runs/plastic_mlp\",\n",
    "    device: Optional[torch.device] = None,\n",
    ") -> Tuple[MultiTaskMLP, StandardScaler, StandardScaler, Dict[str, List[float]]]:\n",
    "    \"\"\"Train a simple multitask MLP on latent embeddings and log to TensorBoard.\"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    x_scaler = StandardScaler()\n",
    "    y_scaler = StandardScaler()\n",
    "\n",
    "    x_scaled = x_scaler.fit_transform(embeddings)\n",
    "    y_scaled = y_scaler.fit_transform(targets)\n",
    "\n",
    "    features = torch.tensor(x_scaled, dtype=torch.float32)\n",
    "    labels = torch.tensor(y_scaled, dtype=torch.float32)\n",
    "\n",
    "    dataset = TensorDataset(features, labels)\n",
    "    val_size = max(1, int(len(dataset) * validation_split))\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_set, val_set = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "\n",
    "    model = MultiTaskMLP(features.shape[1], labels.shape[1]).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_dir) if log_dir else None\n",
    "    history = {\"train\": [], \"val\": []}\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training epochs\"):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(batch_x)\n",
    "            loss = criterion(preds, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * len(batch_x)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                preds = model(batch_x)\n",
    "                loss = criterion(preds, batch_y)\n",
    "                val_loss += loss.item() * len(batch_x)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        history[\"train\"].append(train_loss)\n",
    "        history[\"val\"].append(val_loss)\n",
    "        if writer:\n",
    "            writer.add_scalar(\"loss/train\", train_loss, epoch)\n",
    "            writer.add_scalar(\"loss/val\", val_loss, epoch)\n",
    "\n",
    "    if writer:\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "\n",
    "    return model, x_scaler, y_scaler, history\n",
    "\n",
    "\n",
    "def predict_properties(\n",
    "    smiles: str,\n",
    "    *,\n",
    "    ldm_model: torch.nn.Module,\n",
    "    dataset_info: Dict,\n",
    "    device: torch.device,\n",
    "    mlp_model: MultiTaskMLP,\n",
    "    x_scaler: StandardScaler,\n",
    "    y_scaler: StandardScaler,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Generate property predictions for a new SMILES string.\"\"\"\n",
    "\n",
    "    latent = smiles_to_latent(smiles, ldm_model, dataset_info, device)\n",
    "    scaled = x_scaler.transform(latent.unsqueeze(0))\n",
    "    mlp_model = mlp_model.to(device)\n",
    "    mlp_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_scaled = mlp_model(torch.tensor(scaled, dtype=torch.float32, device=device)).cpu().numpy()\n",
    "    pred = y_scaler.inverse_transform(pred_scaled)[0]\n",
    "    return {name: value for name, value in zip(TARGET_COLUMNS, pred)}\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "a9945bf64417b01f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:27:33.505632Z",
     "start_time": "2025-11-21T17:27:30.153552Z"
    }
   },
   "source": [
    "# Load pretrained latent diffusion encoder\n",
    "checkpoint_dir = Path(\"./qm9_latent2\")\n",
    "ldm_model, dataset_info, nodes_dist, device = load_qm9_latent_diffusion(checkpoint_dir)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Prepare the plastics dataframe and build (or load) latent embeddings.\n",
    "plastic_df = pd.read_parquet(\"data/plastic.parquet\")\n",
    "embeddings, targets, used_indices = build_latent_matrix(\n",
    "    plastic_df,\n",
    "    ldm_model,\n",
    "    dataset_info,\n",
    "    device,\n",
    "    sample_size=1373503,  # keep runtime manageable inside the notebook\n",
    "    cache_path=Path(\"data/plastic_latents.pt\"),\n",
    ")\n",
    "\n",
    "# Train the multi-task MLP regressor.\n",
    "mlp_model, x_scaler, y_scaler, history = train_multitask_mlp(\n",
    "    embeddings.numpy(),\n",
    "    targets.numpy(),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"Training loss trajectory:\", history[\"train\"])\n",
    "print(\"Validation loss trajectory:\", history[\"val\"])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of n_nodes: H[N] -2.475700616836548\n",
      "Autoencoder models are _not_ conditioned on time.\n",
      "alphas2 [9.99990000e-01 9.99988000e-01 9.99982000e-01 ... 2.59676966e-05\n",
      " 1.39959211e-05 1.00039959e-05]\n",
      "gamma [-11.51291546 -11.33059532 -10.92513058 ...  10.55863126  11.17673063\n",
      "  11.51251595]\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a768d7a246744687b0648c74e7a696f5"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss trajectory: [0.7213559150695801, 0.6960547566413879, 0.6741628050804138, 0.659094512462616, 0.641487717628479, 0.6278648376464844, 0.6202551126480103, 0.602785587310791, 0.588709831237793, 0.5856204628944397, 0.5680708885192871, 0.5640943646430969, 0.5469827651977539, 0.5426632761955261, 0.5333215594291687, 0.5264945030212402, 0.5124285221099854, 0.5057541131973267, 0.5013586282730103, 0.5008395314216614, 0.4830620288848877, 0.48020049929618835, 0.47700464725494385, 0.46772468090057373, 0.47418147325515747, 0.4639487862586975, 0.4521016776561737, 0.4430146813392639, 0.44495415687561035, 0.4314056932926178, 0.4230439364910126, 0.44139930605888367, 0.40838757157325745, 0.42901816964149475, 0.41102805733680725, 0.41057753562927246, 0.40953168272972107, 0.40717193484306335, 0.4165004789829254, 0.39024287462234497, 0.40909937024116516, 0.39196088910102844, 0.3848228454589844, 0.3979015052318573, 0.3907051086425781, 0.39161014556884766, 0.3875423073768616, 0.3890531361103058, 0.375301331281662, 0.3585972487926483, 0.3623924255371094, 0.37173378467559814, 0.35779625177383423, 0.36800479888916016, 0.37009960412979126, 0.3401159346103668, 0.3561865985393524, 0.3603528141975403, 0.3414281904697418, 0.33979886770248413, 0.36966395378112793, 0.34384846687316895, 0.3461780548095703, 0.3387780785560608, 0.33826252818107605, 0.3536588251590729, 0.35801661014556885, 0.32402274012565613, 0.3211286664009094, 0.3320339322090149, 0.3262143135070801, 0.344041109085083, 0.32947292923927307, 0.3362301290035248, 0.33536168932914734, 0.32995331287384033, 0.32792550325393677, 0.3303687274456024, 0.3126695454120636, 0.32241684198379517, 0.3089572489261627, 0.30063584446907043, 0.3204105794429779, 0.3128512501716614, 0.31047874689102173, 0.3242619037628174, 0.3142629861831665, 0.30071255564689636, 0.29068732261657715, 0.3167215585708618, 0.3000844419002533, 0.3013647198677063, 0.29694950580596924, 0.30482956767082214, 0.30458688735961914, 0.3040941059589386, 0.31337636709213257, 0.2950499951839447, 0.30370861291885376, 0.3008982837200165]\n",
      "Validation loss trajectory: [3.700892925262451, 3.6967318058013916, 3.6934399604797363, 3.6893858909606934, 3.684250831604004, 3.674896001815796, 3.6611580848693848, 3.642821788787842, 3.6241328716278076, 3.6060845851898193, 3.5883028507232666, 3.5703775882720947, 3.553079605102539, 3.5359158515930176, 3.5177907943725586, 3.501359224319458, 3.486967086791992, 3.4741969108581543, 3.4623749256134033, 3.456195592880249, 3.4510204792022705, 3.4471631050109863, 3.4413318634033203, 3.4351694583892822, 3.429353952407837, 3.424379825592041, 3.4195613861083984, 3.414299249649048, 3.4102461338043213, 3.4071919918060303, 3.4062867164611816, 3.4077072143554688, 3.4124958515167236, 3.412510633468628, 3.409433364868164, 3.4037399291992188, 3.398444175720215, 3.400675058364868, 3.405819892883301, 3.4168593883514404, 3.428953170776367, 3.4385061264038086, 3.4375245571136475, 3.4354279041290283, 3.4342055320739746, 3.4291157722473145, 3.4222500324249268, 3.4129538536071777, 3.4052412509918213, 3.397655487060547, 3.3908398151397705, 3.3899242877960205, 3.3950698375701904, 3.400541067123413, 3.405362129211426, 3.4084665775299072, 3.412395477294922, 3.422065258026123, 3.437042713165283, 3.4495065212249756, 3.4585442543029785, 3.4639182090759277, 3.4670145511627197, 3.47273588180542, 3.480700969696045, 3.484962224960327, 3.483069896697998, 3.4854352474212646, 3.4952635765075684, 3.513878583908081, 3.5299367904663086, 3.543139934539795, 3.5511257648468018, 3.5488758087158203, 3.539330244064331, 3.5272839069366455, 3.514859199523926, 3.510416030883789, 3.510326385498047, 3.5160999298095703, 3.5311145782470703, 3.542682409286499, 3.5573458671569824, 3.5728561878204346, 3.5858044624328613, 3.594665050506592, 3.601593017578125, 3.606205701828003, 3.605794906616211, 3.6030807495117188, 3.608701467514038, 3.6133310794830322, 3.6197946071624756, 3.6246225833892822, 3.635068893432617, 3.638284921646118, 3.6461849212646484, 3.6476001739501953, 3.64788818359375, 3.647390842437744]\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "13ce023d1b200007",
   "metadata": {},
   "source": [
    "## Optimization 1: Straw for Cold Brew\n",
    "Optimization Boundary:\n",
    "\n",
    "Tg: -30\\~10 \u00b0C\n",
    "\n",
    "Tm: 120\\~200 \u00b0C\n",
    "\n",
    "Td: 250\u00b0C \uc774\uc0c1\n",
    "\n",
    "YM: 0.8\\~1.8GPa->\ubc94\uc704 \uc548\uc5d0 \uac00\ub450\uc5b4\uc57c \ud568(1.2 \uadfc\ucc98)\n",
    "\n",
    "TS_b: 20MPa \uc774\uc0c1-> \ucd5c\ub300\ud654 \ub300\uc0c1(2)\n",
    "\n",
    "eps_b : 200\\~1000%->\ucd5c\ub300\ud654 \ub300\uc0c1(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b0106e06603836",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T01:27:01.810916700Z",
     "start_time": "2025-11-19T23:54:10.534685Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example inference call\n",
    "example_smiles = _pick_base_smiles(plastic_df.loc[0, \"smiles\"])\n",
    "predicted = predict_properties(\n",
    "    example_smiles,\n",
    "    ldm_model=ldm_model,\n",
    "    dataset_info=dataset_info,\n",
    "    device=device,\n",
    "    mlp_model=mlp_model,\n",
    "    x_scaler=x_scaler,\n",
    "    y_scaler=y_scaler,\n",
    ")\n",
    "\n",
    "for name, value in predicted.items():\n",
    "    print(f\"{name}: {value:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}