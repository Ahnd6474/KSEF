    "import pandas as pd\n",
   "execution_count": 76,
   "outputs": [],
   ]
   "execution_count": 77,
       "model_id": "eb1820a533034147a18f380d88202d15",
       "version_minor": 0
      },
      "text/plain": [
       "Encoding SMILES:   0%|          | 0/1373503 [00:00<?, ?sample/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Prepare the plastics dataframe and build (or load) latent embeddings.\u001b[39;00m\n\u001b[32m      7\u001b[39m plastic_df = pd.read_parquet(\u001b[33m\"\u001b[39m\u001b[33mdata/plastic.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m embeddings, targets, used_indices, structures, base_smiles_pairs, alpha_values = \u001b[43mbuild_latent_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplastic_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mldm_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1373503\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 1373503\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/plastic_latents.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Train the multi-task MLP regressor and persist the best checkpoint.\u001b[39;00m\n\u001b[32m     18\u001b[39m checkpoint_path = Path(\u001b[33m\"\u001b[39m\u001b[33mmodels/plastic_mlp_best.pt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 364\u001b[39m, in \u001b[36mbuild_latent_matrix\u001b[39m\u001b[34m(frame, model, dataset_info, device, sample_size, batch_size, cache_path, alpha_column)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m embeddings:\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo embeddings were created; check SMILES parsing\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m emb_tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m target_tensor = torch.stack(targets)\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cache_path:\n",
      "\u001b[31mRuntimeError\u001b[39m: stack expects each tensor to be equal size, but got [311] at entry 0 and [371] at entry 1"
   "source": [
    "# Load pretrained latent diffusion encoder\n",
    "checkpoint_dir = Path(\"./qm9_latent2\")\n",
    "ldm_model, dataset_info, nodes_dist, device = load_qm9_latent_diffusion(checkpoint_dir)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Prepare the plastics dataframe and build (or load) latent embeddings.\n",
    "plastic_df = pd.read_parquet(\"data/plastic.parquet\")\n",
    "embeddings, targets, used_indices, structures, base_smiles_pairs, alpha_values = build_latent_matrix(\n",
    "    plastic_df,\n",
    "    ldm_model,\n",
    "    dataset_info,\n",
    "    device,\n",
    "    sample_size=1373503,  # 1373503\n",
    "    cache_path=Path(\"data/plastic_latents.pt\"),\n",
    ")\n",
    "\n",
    "# Train the multi-task MLP regressor and persist the best checkpoint.\n",
    "checkpoint_path = Path(\"models/plastic_mlp_best.pt\")\n",
    "mlp_model, x_scaler, y_scaler, history = train_multitask_mlp(\n",
    "    embeddings.numpy(),\n",
    "    targets.numpy(),\n",
    "    epochs=300,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"Training loss trajectory:\", history[\"train\"])\n",
    "print(\"Validation loss trajectory:\", history[\"val\"])\n",
    "\n",
    "# Reload the best-performing weights and scalers from disk for downstream use.\n",
    "mlp_model, x_scaler, y_scaler, checkpoint_meta = load_mlp_checkpoint(\n",
    "    checkpoint_path, device=device\n",
    ")\n",
    "print(\n",
    "    f\"Loaded checkpoint from epoch {checkpoint_meta['epoch']} \"\n",
    "    f\"with val_loss={checkpoint_meta['val_loss']:.6f}\"\n",
    ")\n"
   ]
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m atom_types = decoded_features[\u001b[33m\"\u001b[39m\u001b[33mcategorical\u001b[39m\u001b[33m\"\u001b[39m].argmax(dim=-\u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m].cpu().numpy()\n\u001b[32m     39\u001b[39m atom_symbols = [dataset_info[\u001b[33m\"\u001b[39m\u001b[33matom_decoder\u001b[39m\u001b[33m\"\u001b[39m][i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m atom_types]\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m new_smiles, _ = \u001b[43mstructure_to_smiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43matom_symbols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampled_positions\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated SMILES: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_smiles\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\KSEF\\geoldm\\qm9\\model_module.py:530\u001b[39m, in \u001b[36mstructure_to_smiles\u001b[39m\u001b[34m(atom_symbols, coordinates, charge, sanitize)\u001b[39m\n\u001b[32m    527\u001b[39m             AllChem.MMFFOptimizeMolecule(mol, confId=conf_id)\n\u001b[32m    528\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    529\u001b[39m             \u001b[38;5;66;03m# Optimisation occasionally fails for unusual molecules.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    532\u001b[39m atom_symbols = [atom.GetSymbol() \u001b[38;5;28;01mfor\u001b[39;00m atom \u001b[38;5;129;01min\u001b[39;00m mol.GetAtoms()]\n\u001b[32m    533\u001b[39m conformers: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] = []\n",
      "\u001b[31mValueError\u001b[39m: Final molecular charge (0) does not match input (9); could not find valid bond ordering"
    "    \"\"\"\n",
    "    \"\"\"\n",
